public interface GoogleMailEndpointBuilder              extends                  GoogleMailEndpointConsumerBuilder, GoogleMailEndpointProducerBuilder {          default AdvancedGoogleMailEndpointBuilder advanced() {
* Process smart name processing - removes unused classes and does referencing     * starting with minimum set of names.     */   final PassFactory smartNamePass =        new PassFactory("smartNamePass", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {
}   @Test  void userConfigurationBeans() {    this.contextRunner     .withUserConfiguration(LiquibaseUserConfiguration.class, EmbeddedDataSourceConfiguration.class)     .run((context) -> {
};       final PassFactory objectPropertyStringPostprocess =        new PassFactory("ObjectPropertyStringPostprocess", true) {      @Override      protected CompilerPass createInternal(AbstractCompiler compiler) {
};       final PassFactory closureCodeRemoval =        new PassFactory("closureCodeRemoval", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {
if (body.endsWith("3") || body.endsWith("7")) {                                              log.info("5th message received.  Rolling back.");                                             exchange.getOut().setFault(true);                                             exchange.getOut().setBody("5th message received.  Rolling back.");
* A lot of the options are not configurable, because ReplaceMessages     * has a lot of legacy logic.     */   final PassFactory replaceMessages =        new PassFactory("replaceMessages", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {
}       final PassFactory renameLabels =        new PassFactory("renameLabels", true) {      @Override      protected CompilerPass createInternal(AbstractCompiler compiler) {
final long committedOffset = KafkaJournal.this.committedOffset.get();              final HashSet<LogSegment> logSegments = Sets.newHashSet(                     JavaConversions.asIterable(kafkaLog.logSegments(committedOffset, Long.MAX_VALUE))              );              loggerForCleaner.debug("[cleanup-committed] Keeping segments {}", logSegments);              return kafkaLog.deleteOldSegments(new AbstractFunction1<LogSegment, Object>() {
int i = 0;          if (boi.getOutput()!= null) {              for (MessagePartInfo partInfo : boi.getOutput().getMessageParts()) {                 if (elements.size() > i) {                      answer.put(partInfo, elements.get(i++));                  }              }
public interface FtpEndpointProducerBuilder              extends                 EndpointProducerBuilder {          AdvancedFtpEndpointProducerBuilder advanced() {
} else {          for (int times = mLeft; times < mRight; times ++) {            long startTimeMs = System.currentTimeMillis();           String filePath = FILE_NAME + (mWorkerId + BASE_FILE_NUMBER);            InputStream is = mHdfsFs.open(new Path(filePath));            long len = BLOCKS_PER_FILE * BLOCK_SIZE_BYTES;
throws IOException          {              generator.writeStartObject();              generator.writeStringField("type", value.getType().getCanonicalName());               generator.writeFieldName("value");              if (value.getValue() == null) {                  generator.writeNull();
*     * @return The closed DataStream    */  public DataStreamSink<OUT> writeAsCsv(String path, long millis) {    return writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);   }
@Resource      private ConnectionFactory defaultConnectionFactory;     @Resource(lookup = "java:/ConnectionFactory")      private ConnectionFactory regularConnectionFactory;      public void sendWithDefaultJMSConnectionFactory(Destination destination, String text) throws Exception {
failureCause.compareAndSet(null, toFailure(throwable));          boolean failed = queryState.setIf(FAILED, currentState ->!currentState.isDone());          if (failed) {             log.error(throwable, "Query %s failed", queryId);          }          else  {              log.debug(throwable, "Failure after query %s finished", queryId);
* @param hostName the name of the host.       */      @LogMessage(level = Level.WARN)     @Message(id = 10804, value = "Interrupted awaiting final response from host %s")      void interruptedAwaitingFinalResponse(String hostName);      /**
record.setExpirationTime(expiryTime);                  if (isEventsEnabled()) {                      CacheEventContext cacheEventContext =                             createBaseEventContext(CacheEventType.EXPIRATION_TIME_UPDATED, toHeapData(key),                                                      toEventData(record.getValue()), expiryTime, null, IGNORE_COMPLETION);                      cacheEventContext.setAccessHit(record.getAccessHit());                      publishEvent(cacheEventContext);
ObjectMapper mapper = new ObjectMapper();     JsonNode rootNode = mapper.readTree(dataString);     int returnCode = rootNode.get("ReturnCode").asInt();    logger.debug("myq ReturnCode: {}", returnCode);     MyQResponseCode rc = MyQResponseCode.fromCode(returnCode);
}             }         }        return r;      }      private void notifyIdleListener() {
*       * @return the message.       */     @Message(id = 10862, value = "Invalid '%s' value: %d, the maximum index is %d")      String invalidValue(String name, int value, int maxIndex);      /**
* @see <a href="http:       */      public ResponseList<Status> getRetweets(long statusId) throws TwitterException {         return Status.createStatuseList(get(getBaseURL()                  + "statuses/retweets/" + statusId + ".json", true));      }
* Some simple, local collapses (e.g., {@code var x; var y;} becomes     * {@code var x,y;}.     */   final PassFactory exploitAssign =        new PassFactory("expointAssign", true) {      @Override      protected CompilerPass createInternal(AbstractCompiler compiler) {
managedSpan.deactivate();               }           } catch (Exception t) {                LOG.warn("OpenTracing: Failed to capture tracing data", t);              }         }
};       final PassFactory latePeepholeOptimizations =        new PassFactory("latePeepholeOptimizations", true) {      @Override      protected CompilerPass createInternal(AbstractCompiler compiler) {
}    public void normalize() {     logger.info("normalizing");      startPass("normalize");      process(new Normalize(this, false));      endPass();
public interface HazelcastMultimapEndpointProducerBuilder              extends                  EndpointProducerBuilder {          AdvancedHazelcastMultimapEndpointProducerBuilder advanced() {
