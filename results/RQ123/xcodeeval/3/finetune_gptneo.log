Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=1024, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=1024, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=1024, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=1024, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=1024, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00201
  global_step = 136
  train_loss = 3.3952
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00201
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.89 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.89
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00193
  global_step = 271
  train_loss = 2.4725
  ********************
Previous best ppl:1.00201
Achieve Best ppl:1.00193
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.84 	 Previous best codebleu 75.89
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00192
  global_step = 406
  train_loss = 2.304
  ********************
Previous best ppl:1.00193
Achieve Best ppl:1.00192
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.77 	 Previous best codebleu 75.89
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00196
  global_step = 541
  train_loss = 2.1706
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.72 	 Previous best codebleu 75.89
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00202
  global_step = 676
  train_loss = 1.963
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.69 	 Previous best codebleu 75.89
  ********************
reload model from xcodeeval/3/finetune_gptneo/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 74.84 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 74.84 
[0.79353207961192, 0.5552567215868718, 0.7433435050756219, 0.9448063280540735, 0.9392025498860357, 0.08595934932482159, 0.25656679901645385, 0.7669842146074164, 0.6498389062539932, 0.5030724841624163, 0.5564880010186164, 0.8164468849403668, 0.8805670984187464, 0.9432628994248902, 0.9248624258832823, 0.9175636111085979, 0.38735431410528753, 0.947175667983928, 0.7822722252676139, 0.8614910167908467, 0.9705776794701617, 0.9735791100966684, 0.8333813540442871, 0.5524112337326081, 0.7657560530824576, 0.9450171055839813, 0.8324183912964096, 0.9458873556263774, 0.5790261988125887, 0.8896800467143886, 0.91412960455335, 0.4328828981880556, 0.63014750305347, 0.9620842458183307, 0.5925258544448675, 0.7259849312989834, 0.7350229143730664, 0.9025560323310975, 0.5717465644724119, 0.9670641264245079, 0.867980815381816, 0.8583791034780668, 0.9306240015142291, 0.955626584171517, 0.7807079788494218, 0.8470562047308248, 0.9847226293373537, 0.6788771844775876, 0.9333810055461855, 0.8665862994888722, 0.9135218553545446, 0.9758267649688364, 0.9549147975108003, 0.8633963363946109, 0.7801949080730365, 0.9842293971176908, 0.8364536749667827, 0.9477813702498612, 0.951315622822358, 0.33295797590328613, 0.35624122019684656, 0.7982100679121421, 0.9217008915274616, 0.878087949536615, 0.837234045225426, 0.9747386881408853, 0.9859658995808198, 0.15351946048762036, 0.9289471675486692, 0.8339849455214231, 0.9279004680412364, 0.9669138743101213, 0.9503712900963834, 0.9444842541441154, 0.9366238579444699, 0.5227054038899412, 0.8073057430094908, 0.8657268209604377, 0.6927725662122917, 0.8269793321315543, 0.8947151965627762, 0.9572529392338871, 0.7761391783604946, 0.9512693309096103, 0.5677893088427866, 0.7763879223350485, 0.26765125761038455, 0.9748374025939537, 0.9743329739145192, 0.8980952928375916, 0.5797226856861304, 0.48768804749002387, 0.9498733743967636, 0.6783819388991533, 0.30278837455257246, 0.821773975796796, 0.4196734157127252, 0.5331629480186568, 0.823799855709568, 0.3902983998016769, 0.8978639063053666, 0.9174195483383916, 0.47266766836385843, 0.9534278780598358, 0.9393217253155881, 0.8897945586059963, 0.3354672506106251, 0.35148053648224986, 0.8956002753268077, 0.3868507250835175, 0.919755073180808, 0.3455555205176508, 0.4954735182274035, 0.9427008679312978, 0.6078859152623858, 0.3811799198718092, 0.8759700055210595, 0.7052622199542771, 0.9732456940628658, 0.817850581479403, 0.6131424394366902, 0.6647343106206701, 0.7934303369700066, 0.9764351042658364, 0.48469916091476684, 0.6902739264054606, 0.9294284870142819, 0.275865974784248, 0.37224533982578545, 0.4191471722230175, 0.9393874070011292, 0.9343166704551076, 0.7311702843164789, 0.4094542044720756, 0.3357637568330429]
Finish training and take 1h4m
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00201
  global_step = 136
  train_loss = 3.3952
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00201
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00201
  global_step = 136
  train_loss = 3.3952
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00201
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.9 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00193
  global_step = 271
  train_loss = 2.4725
  ********************
Previous best ppl:1.00201
Achieve Best ppl:1.00193
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.85 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00192
  global_step = 406
  train_loss = 2.304
  ********************
Previous best ppl:1.00193
Achieve Best ppl:1.00192
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.78 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00196
  global_step = 541
  train_loss = 2.1706
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.74 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00202
  global_step = 676
  train_loss = 1.963
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.71 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00218
  global_step = 811
  train_loss = 1.7237
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.07 	 Previous best codebleu 75.9
  ********************
reload model from xcodeeval/3/finetune_gptneo/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 74.86 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 74.86 
[0.79353207961192, 0.5552567215868718, 0.7433435050756219, 0.9448063280540735, 0.9392025498860357, 0.08595934932482159, 0.25656679901645385, 0.7669842146074164, 0.6498389062539932, 0.5124474841624163, 0.5564880010186164, 0.8164468849403668, 0.8805670984187464, 0.9432628994248902, 0.9248624258832823, 0.9175636111085979, 0.38735431410528753, 0.947175667983928, 0.7822722252676139, 0.8614910167908467, 0.9705776794701617, 0.9735791100966684, 0.8333813540442871, 0.5524112337326081, 0.7657560530824576, 0.9450171055839813, 0.8324183912964096, 0.9458873556263774, 0.5790261988125887, 0.8896800467143886, 0.91412960455335, 0.4328828981880556, 0.63014750305347, 0.9620842458183307, 0.5925258544448675, 0.7259849312989834, 0.7350229143730664, 0.9025560323310975, 0.5717465644724119, 0.9670641264245079, 0.867980815381816, 0.8583791034780668, 0.9306240015142291, 0.955626584171517, 0.7807079788494218, 0.8470562047308248, 0.9847226293373537, 0.6788771844775876, 0.9333810055461855, 0.8665862994888722, 0.9135218553545446, 0.9758267649688364, 0.9549147975108003, 0.8633963363946109, 0.7801949080730365, 0.9842293971176908, 0.8442458827589905, 0.9477813702498612, 0.951315622822358, 0.33295797590328613, 0.35624122019684656, 0.7982100679121421, 0.9217008915274616, 0.878087949536615, 0.837234045225426, 0.9747386881408853, 0.9859658995808198, 0.15351946048762036, 0.9289471675486692, 0.8339849455214231, 0.9279004680412364, 0.9669138743101213, 0.9503712900963834, 0.9444842541441154, 0.9366238579444699, 0.5227054038899412, 0.8073057430094908, 0.8657268209604377, 0.6927725662122917, 0.8269793321315543, 0.8947151965627762, 0.9572529392338871, 0.7761391783604946, 0.9512693309096103, 0.5677893088427866, 0.7763879223350485, 0.26765125761038455, 0.9748374025939537, 0.9743329739145192, 0.8980952928375916, 0.5797226856861304, 0.48768804749002387, 0.9498733743967636, 0.6783819388991533, 0.30278837455257246, 0.8333124373352575, 0.4196734157127252, 0.5331629480186568, 0.823799855709568, 0.3902983998016769, 0.8978639063053666, 0.9174195483383916, 0.47266766836385843, 0.9534278780598358, 0.9393217253155881, 0.8897945586059963, 0.3354672506106251, 0.35148053648224986, 0.8956002753268077, 0.3868507250835175, 0.919755073180808, 0.3455555205176508, 0.4954735182274035, 0.9427008679312978, 0.6078859152623858, 0.3811799198718092, 0.8759700055210595, 0.7052622199542771, 0.9732456940628658, 0.817850581479403, 0.6131424394366902, 0.6647343106206701, 0.7934303369700066, 0.9764351042658364, 0.48469916091476684, 0.6902739264054606, 0.9294284870142819, 0.275865974784248, 0.37224533982578545, 0.4191471722230175, 0.9393874070011292, 0.9343166704551076, 0.7311702843164789, 0.4094542044720756, 0.3357637568330429]
Finish training and take 1h15m
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00201
  global_step = 136
  train_loss = 3.3952
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00201
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.91 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.91
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00193
  global_step = 271
  train_loss = 2.4725
  ********************
Previous best ppl:1.00201
Achieve Best ppl:1.00193
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.87 	 Previous best codebleu 75.91
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00192
  global_step = 406
  train_loss = 2.304
  ********************
Previous best ppl:1.00193
Achieve Best ppl:1.00192
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.8 	 Previous best codebleu 75.91
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00196
  global_step = 541
  train_loss = 2.1706
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.75 	 Previous best codebleu 75.91
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00202
  global_step = 676
  train_loss = 1.963
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.73 	 Previous best codebleu 75.91
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00218
  global_step = 811
  train_loss = 1.7237
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.1 	 Previous best codebleu 75.91
  ********************
reload model from xcodeeval/3/finetune_gptneo/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 74.85 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 74.85 
[0.79353207961192, 0.5552567215868718, 0.7433435050756219, 0.9448063280540735, 0.9392025498860357, 0.08595934932482159, 0.25656679901645385, 0.7669842146074164, 0.6498389062539932, 0.5030724841624163, 0.5564880010186164, 0.8164468849403668, 0.8805670984187464, 0.9432628994248902, 0.9248624258832823, 0.9175636111085979, 0.38735431410528753, 0.947175667983928, 0.7822722252676139, 0.8695991248989547, 0.9705776794701617, 0.9735791100966684, 0.8333813540442871, 0.5524112337326081, 0.7657560530824576, 0.9450171055839813, 0.8324183912964096, 0.9458873556263774, 0.5790261988125887, 0.8896800467143886, 0.91412960455335, 0.4328828981880556, 0.63014750305347, 0.9620842458183307, 0.5925258544448675, 0.7259849312989834, 0.7350229143730664, 0.9025560323310975, 0.5717465644724119, 0.9670641264245079, 0.867980815381816, 0.8583791034780668, 0.9306240015142291, 0.955626584171517, 0.7807079788494218, 0.8470562047308248, 0.9847226293373537, 0.6758211573129016, 0.9333810055461855, 0.8665862994888722, 0.9135218553545446, 0.9758267649688364, 0.9549147975108003, 0.8633963363946109, 0.7801949080730365, 0.9842293971176908, 0.8403497788628866, 0.9477813702498612, 0.951315622822358, 0.33295797590328613, 0.35624122019684656, 0.7982100679121421, 0.9217008915274616, 0.878087949536615, 0.837234045225426, 0.9747386881408853, 0.9859658995808198, 0.15351946048762036, 0.9289471675486692, 0.8339849455214231, 0.9279004680412364, 0.9669138743101213, 0.9503712900963834, 0.9444842541441154, 0.9366238579444699, 0.5227054038899412, 0.8073057430094908, 0.8657268209604377, 0.6927725662122917, 0.8269793321315543, 0.8947151965627762, 0.9572529392338871, 0.7761391783604946, 0.9512693309096103, 0.5677893088427866, 0.7763879223350485, 0.26765125761038455, 0.9748374025939537, 0.9743329739145192, 0.8980952928375916, 0.5797226856861304, 0.48768804749002387, 0.9498733743967636, 0.6783819388991533, 0.30278837455257246, 0.8333124373352575, 0.4196734157127252, 0.5331629480186568, 0.823799855709568, 0.3902983998016769, 0.8978639063053666, 0.9174195483383916, 0.47266766836385843, 0.9534278780598358, 0.9393217253155881, 0.8897945586059963, 0.3354672506106251, 0.35148053648224986, 0.8956002753268077, 0.3868507250835175, 0.919755073180808, 0.3455555205176508, 0.4954735182274035, 0.9427008679312978, 0.6078859152623858, 0.3811799198718092, 0.8759700055210595, 0.7052622199542771, 0.9732456940628658, 0.817850581479403, 0.6136977012029514, 0.6647343106206701, 0.7934303369700066, 0.9764351042658364, 0.48469916091476684, 0.6902739264054606, 0.9294284870142819, 0.275865974784248, 0.37224533982578545, 0.4191471722230175, 0.9393874070011292, 0.9343166704551076, 0.7311702843164789, 0.4094542044720756, 0.3357637568330429]
Finish training and take 1h15m
Namespace(log_name='./xcodeeval/3/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='c', output_dir='xcodeeval/3/finetune_gptneo', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00201
  global_step = 136
  train_loss = 3.3952
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00201
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.9 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00193
  global_step = 271
  train_loss = 2.4725
  ********************
Previous best ppl:1.00201
Achieve Best ppl:1.00193
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.86 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00192
  global_step = 406
  train_loss = 2.304
  ********************
Previous best ppl:1.00193
Achieve Best ppl:1.00192
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.79 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00196
  global_step = 541
  train_loss = 2.1706
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.74 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00202
  global_step = 676
  train_loss = 1.963
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.72 	 Previous best codebleu 75.9
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00218
  global_step = 811
  train_loss = 1.7237
  ********************
Previous best ppl:1.00192
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 75.09 	 Previous best codebleu 75.9
  ********************
reload model from xcodeeval/3/finetune_gptneo/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 74.85 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 74.85 
[0.79353207961192, 0.5552567215868718, 0.7433435050756219, 0.9448063280540735, 0.9392025498860357, 0.08595934932482159, 0.25656679901645385, 0.7669842146074164, 0.6498389062539932, 0.5030724841624163, 0.5564880010186164, 0.8164468849403668, 0.8805670984187464, 0.9432628994248902, 0.9248624258832823, 0.9175636111085979, 0.38735431410528753, 0.947175667983928, 0.7822722252676139, 0.8614910167908467, 0.9705776794701617, 0.9735791100966684, 0.8333813540442871, 0.5524112337326081, 0.7657560530824576, 0.9450171055839813, 0.8324183912964096, 0.9458873556263774, 0.5790261988125887, 0.8896800467143886, 0.91412960455335, 0.4328828981880556, 0.63014750305347, 0.9620842458183307, 0.5925258544448675, 0.7259849312989834, 0.7350229143730664, 0.9025560323310975, 0.5717465644724119, 0.9670641264245079, 0.867980815381816, 0.8583791034780668, 0.9306240015142291, 0.955626584171517, 0.7807079788494218, 0.8470562047308248, 0.9847226293373537, 0.6758211573129016, 0.9333810055461855, 0.8665862994888722, 0.9135218553545446, 0.9758267649688364, 0.9549147975108003, 0.8633963363946109, 0.7801949080730365, 0.9842293971176908, 0.8364536749667827, 0.9477813702498612, 0.951315622822358, 0.33295797590328613, 0.35624122019684656, 0.7982100679121421, 0.9217008915274616, 0.878087949536615, 0.837234045225426, 0.9747386881408853, 0.9859658995808198, 0.15351946048762036, 0.9289471675486692, 0.8339849455214231, 0.9279004680412364, 0.9669138743101213, 0.9503712900963834, 0.9444842541441154, 0.9366238579444699, 0.5227054038899412, 0.8073057430094908, 0.8657268209604377, 0.6927725662122917, 0.8269793321315543, 0.8947151965627762, 0.9572529392338871, 0.7761391783604946, 0.9512693309096103, 0.5677893088427866, 0.7763879223350485, 0.26765125761038455, 0.9748374025939537, 0.9743329739145192, 0.8980952928375916, 0.5797226856861304, 0.48768804749002387, 0.9498733743967636, 0.6783819388991533, 0.30278837455257246, 0.8333124373352575, 0.4196734157127252, 0.5331629480186568, 0.823799855709568, 0.3902983998016769, 0.8978639063053666, 0.9174195483383916, 0.47266766836385843, 0.9534278780598358, 0.9393217253155881, 0.8897945586059963, 0.3354672506106251, 0.35148053648224986, 0.8956002753268077, 0.3868507250835175, 0.919755073180808, 0.3455555205176508, 0.4954735182274035, 0.9427008679312978, 0.6078859152623858, 0.3811799198718092, 0.8759700055210595, 0.7052622199542771, 0.9732456940628658, 0.817850581479403, 0.6136977012029514, 0.6647343106206701, 0.7934303369700066, 0.9764351042658364, 0.48469916091476684, 0.6902739264054606, 0.9294284870142819, 0.275865974784248, 0.37224533982578545, 0.4191471722230175, 0.9393874070011292, 0.9343166704551076, 0.7311702843164789, 0.4094542044720756, 0.3357637568330429]
Finish training and take 1h15m
