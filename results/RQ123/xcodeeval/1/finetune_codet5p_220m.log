Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/1/finetune_codet5p_220m', data_dir='./data/xcodeeval/1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00067
  global_step = 136
  train_loss = 0.4169
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00067
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.44 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:72.44
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00066
  global_step = 271
  train_loss = 0.3028
  ********************
Previous best ppl:1.00067
Achieve Best ppl:1.00066
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.51 	 Previous best codebleu 72.44
  ********************
 Achieve Best bleu:72.51
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00066
  global_step = 406
  train_loss = 0.2445
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.45 	 Previous best codebleu 72.51
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00067
  global_step = 541
  train_loss = 0.208
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.52 	 Previous best codebleu 72.51
  ********************
 Achieve Best bleu:72.52
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0007
  global_step = 676
  train_loss = 0.1804
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.57 	 Previous best codebleu 72.52
  ********************
 Achieve Best bleu:72.57
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00073
  global_step = 811
  train_loss = 0.1509
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.47 	 Previous best codebleu 72.57
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00078
  global_step = 946
  train_loss = 0.127
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.43 	 Previous best codebleu 72.57
  ********************
reload model from xcodeeval/1/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/1/test.jsonl
  codebleu = 73.43 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 73.43 
[0.6741254939692773, 0.6055239846395822, 0.7035272879003251, 0.8078005301884837, 0.8482415780791264, 0.8470661236484578, 0.8506318170022356, 0.6433992707996272, 0.475, 0.869097355083081, 0.8218348299362244, 0.8881108167610436, 0.7612037516644444, 0.8095193448343525, 0.8992587233632239, 0.7938229814514498, 0.727268027720646, 0.8365125728430938, 0.7023997010144827, 0.8890119901196314, 0.8819989837824207, 0.6035971947269193, 0.8504318413102254, 0.8517064703343284, 0.8272288242905221, 0.8759794301856842, 0.6538129485106954, 0.6958702394644983, 0.8437641571592116, 0.8629852137929874, 0.8973934740426551, 0.8071708182046731, 0.4256938428427359, 0.8772273751686793, 0.726906245805339, 0.8256815726664408, 0.5681163637298114, 0.8507183146596586, 0.7647346904829493, 0.6982499480365546, 0.6694040108132734, 0.7385923732495516, 0.5487940158683715, 0.647487961996232, 0.5325804735306264, 0.9253363255944331, 0.8279104785599881, 0.7330177268778751, 0.6987438117048423, 0.811100358881859, 0.8805315670914018, 0.3414480260596623, 0.7427116405536602, 0.9109778546090423, 0.46344533813736777, 0.8034118760208823, 0.9259264314733622, 0.7041410201759473, 0.8260661662085544, 0.5160682766833452, 0.8799057191503341, 0.9265235989881684, 0.5076074703740681, 0.8535048490263217, 0.8545893714632256, 0.336963498695384, 0.7489533595723943, 0.22083634630817556, 0.7622600531135202, 0.7707471127673067, 0.4050873617790997, 0.8418611860573935, 0.4895865072343232, 0.8124797095951521, 0.8145273052967352, 0.8717455174648483, 0.7701328007329863, 0.7694578850257566, 0.849803040412304, 0.8255681126593256, 0.8719598303409888, 0.7927812326921357, 0.7674641460255568, 0.8476031916439257, 0.8696923233625582, 0.9298125945729137, 0.7575006467470722, 0.6357279870462479, 0.6449446745956003, 0.659532270654534, 0.982840810414882, 0.8710445728632014, 0.4650551733188451, 0.6476753884536018, 0.5355322041447821, 0.9498733743967636, 0.6281802407830452, 0.8350035289796414, 0.6184859920453802, 0.906923058378295, 0.8985810813576707, 0.5644982344246664, 0.8157728177025623, 0.19782713521617065, 0.7134134951513778, 0.8396663263205362, 0.4921529174785757, 0.9365739931432338, 0.42779770128383576, 0.8769250379411355, 0.4867368064069292, 0.7862446843835789, 0.7639496783614648, 0.6451144593926297, 0.7011151330217786, 0.88353709334584, 0.9503702205380173, 0.762314097845589, 0.7780707146601717, 0.8996726638725223, 0.7782021119672806, 0.3794955048760881, 0.788330513115518, 0.4140107071948905, 0.8119506730626463, 0.7211817541923293, 0.7286144481170467, 0.8318325665241797, 0.45601045567543047, 0.724315616661136, 0.6017150080688712, 0.876722724918082, 0.9013866678857929, 0.48246469658822144, 0.5905688151805155]
Finish training and take 12m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/2/finetune_codet5p_220m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00055
  global_step = 136
  train_loss = 0.4138
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00055
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.79 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:73.79
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00055
  global_step = 271
  train_loss = 0.3094
  ********************
Previous best ppl:1.00055
Achieve Best ppl:1.00055
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.06 	 Previous best codebleu 73.79
  ********************
 Achieve Best bleu:74.06
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00055
  global_step = 406
  train_loss = 0.2488
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.07 	 Previous best codebleu 74.06
  ********************
 Achieve Best bleu:74.07
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00057
  global_step = 541
  train_loss = 0.2066
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.09 	 Previous best codebleu 74.07
  ********************
 Achieve Best bleu:74.09
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00056
  global_step = 676
  train_loss = 0.1732
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.11 	 Previous best codebleu 74.09
  ********************
 Achieve Best bleu:74.11
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00061
  global_step = 811
  train_loss = 0.1434
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.96 	 Previous best codebleu 74.11
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00062
  global_step = 946
  train_loss = 0.1219
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.95 	 Previous best codebleu 74.11
  ********************
reload model from xcodeeval/2/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 70.59 
  Total = 135 
  Exact Fixed = 1 
[107]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 1 
[107]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 70.59 
[0.4079267132112413, 0.5929525010165554, 0.8723015765088757, 0.8981156303935836, 0.628105232559099, 0.8341852120711315, 0.8992357102776094, 0.709526995055453, 0.873793759962344, 0.6467062393002753, 0.3854267824187835, 0.5844229263024725, 0.904590355528839, 0.3325195752446274, 0.04505175233917717, 0.8609914309897955, 0.5726261131685282, 0.7482112321606759, 0.8724723036671462, 0.592207910649987, 0.446627993682677, 0.8248170962252077, 0.75750508267751, 0.871717936262084, 0.8154562777909085, 0.6201159722273546, 0.891419774625108, 0.6917596764211111, 0.7710815068968027, 0.34185814850653357, 0.45631161811982324, 0.5818754564013621, 0.8614283266451082, 0.8382191698805657, 0.6004280366320209, 0.5191643807480444, 0.4524934938238685, 0.8460671937742572, 0.5436225904261595, 0.39054140447869534, 0.7503272906104577, 0.7128798335563746, 0.9120861554864073, 0.7784591158731673, 0.8375542987872413, 0.9041581967027494, 0.7801744066714751, 0.940387748772268, 0.79251028814738, 0.8036910613939219, 0.3120288913954662, 0.7716491424868305, 0.5310909326338049, 0.6384499261756515, 0.8852384894814247, 0.7109606270857536, 0.7764975463278132, 0.6777766309740647, 0.5346863070936048, 0.9470228248649237, 0.670183569679071, 0.8396663263205362, 0.9360674082443785, 0.8633447636989494, 0.43387818601959904, 0.7497872722734309, 0.5918835459549441, 0.869474849888572, 0.8946275734848934, 0.8537257078219596, 0.8064069121671387, 0.8196951483104229, 0.7368070849073924, 0.4612620280518578, 0.3025273885289328, 0.8648579538144856, 0.7496420585309386, 0.7602186940402952, 0.7631177288471214, 0.7875865283547716, 0.39578612527759205, 0.30666666666666664, 0.7269618142997433, 0.8753727486979741, 0.5393773280455789, 0.7947820674037744, 0.3852685514477996, 0.76987879200941, 0.8806974736497433, 0.8594524113191127, 0.6979780721244926, 0.8388209385489929, 0.6626435137538318, 0.8416117635801648, 0.7784360171669422, 0.521559233346504, 0.7833819946139522, 0.7488997676737696, 0.6333744987141046, 0.7410691472083426, 0.4585240670925822, 0.5715313815373253, 0.6739839265768114, 0.39198242100783887, 0.7388705197090006, 0.6951644390669244, 1.0, 0.46673071932457155, 0.9088830127891061, 0.5614360882955788, 0.6490688753903175, 0.8339055840616187, 0.8729244091162942, 0.912198238083447, 0.6618943847067642, 0.8543126179887541, 0.9341190775741011, 0.4335448943198981, 0.8573326876937928, 0.7704569402736815, 0.4220974061228139, 0.8196069166590214, 0.7032096029468522, 0.6413253944664965, 0.7101520621290818, 0.6715603052093865, 0.9131807795832447, 0.6876287414964073, 0.3128264897154612, 0.853191132295444, 0.8164232694020533, 0.9311001454085734, 0.9612737738433852, 0.8859453911289024, 0.7966006161104328]
Finish training and take 13m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/3/finetune_codet5p_220m', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0006
  global_step = 136
  train_loss = 0.3933
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0006
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.17 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.17
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00059
  global_step = 271
  train_loss = 0.2923
  ********************
Previous best ppl:1.0006
Achieve Best ppl:1.00059
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.55 	 Previous best codebleu 74.17
  ********************
 Achieve Best bleu:74.55
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00061
  global_step = 406
  train_loss = 0.2329
  ********************
Previous best ppl:1.00059
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.09 	 Previous best codebleu 74.55
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00062
  global_step = 541
  train_loss = 0.1961
  ********************
Previous best ppl:1.00059
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.18 	 Previous best codebleu 74.55
  ********************
reload model from xcodeeval/3/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 71.27 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 71.27 
[0.7849538767776651, 0.7663371990492582, 0.6976910864186706, 0.8512897119997327, 0.8536358763066614, 0.08406889937820157, 0.34898094225543036, 0.7669842146074164, 0.6756324089520065, 0.6054199120209949, 0.5478590325489547, 0.8164468849403668, 0.9175351478585267, 0.8726774356123554, 0.8059634988113191, 0.872147466111713, 0.28456980887865324, 0.7534037242254013, 0.5654675203716997, 0.7830064735910691, 0.9185412013134608, 0.8245561139312393, 0.8138327672660688, 0.5524112337326081, 0.7816849127345435, 0.802138971106996, 0.8324183912964096, 0.8448616344160145, 0.6982021521970254, 0.7921624393976985, 0.91412960455335, 0.4739726324720028, 0.63014750305347, 0.9040862722278158, 0.6345448464057466, 0.7259849312989834, 0.7540469087814428, 0.8313915586456186, 0.6121972533149926, 0.8569685752263538, 0.7932902743460295, 0.630150618578014, 0.6759728227587174, 0.790982444268264, 0.7807079788494218, 0.8296660707943342, 0.672785744652446, 0.7867507949022507, 0.8536334434067134, 0.5685250847298065, 0.8219514476194971, 0.8419978367899081, 0.9092430471124722, 0.9104392373575145, 0.7182482958421927, 0.941768888951575, 0.6455873914646095, 0.8138225102863303, 0.8654937656858617, 0.4142301377471687, 0.5112111419002278, 0.8297929734818674, 0.8963193939836629, 0.878087949536615, 0.8112486866006651, 0.7944974662629933, 0.8546770093840449, 0.23178378342054765, 0.7563107817326995, 0.8339849455214231, 0.8155779083654062, 0.901355655227414, 0.754778959829329, 0.7323223294379341, 0.802980404547649, 0.5923377715628043, 0.8125459461294413, 0.720188397702945, 0.6987172522283688, 0.7925405308747024, 0.900395087233911, 0.837457300065938, 0.7087785501314359, 0.7700652915417614, 0.5677893088427866, 0.7763879223350485, 0.3417564054664891, 0.7704569402736815, 0.7509157324200985, 0.7827835412006432, 0.5672747648078345, 0.647487961996232, 0.8819989837824207, 0.7470362018806367, 0.4154808998544015, 0.811364833324377, 0.5211121075488702, 0.5309005587685636, 0.823799855709568, 0.5271882025547743, 0.8077815485625466, 0.8270372589884398, 0.515209831912117, 0.7848352750832455, 0.8445923838454312, 0.8897945586059963, 0.33520188382167504, 0.4168626518355933, 0.852945187183654, 0.3975734208323345, 0.8811002176557108, 0.5457447578545462, 0.5285433525156937, 0.831862639858802, 0.7142029351288322, 0.4323025955698975, 0.8759700055210595, 0.7192634200448921, 0.9150668370807107, 0.7135938825045871, 0.6257365724699333, 0.8545307032267597, 0.7904010053030581, 0.8806594193232651, 0.5346863070936048, 0.6944280490257584, 0.8877072716538504, 0.24170769468996064, 0.2142677455447746, 0.5976804074013286, 0.9003747650468552, 0.8806974736497433, 0.6294449559412665, 0.5963350635154259, 0.24173832962493158]
Finish training and take 8m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/1/finetune_codet5p_220m', data_dir='./data/xcodeeval/1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00067
  global_step = 136
  train_loss = 0.4169
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00067
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.46 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:72.46
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00066
  global_step = 271
  train_loss = 0.3028
  ********************
Previous best ppl:1.00067
Achieve Best ppl:1.00066
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.53 	 Previous best codebleu 72.46
  ********************
 Achieve Best bleu:72.53
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00066
  global_step = 406
  train_loss = 0.2445
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/1/finetune_codet5p_220m', data_dir='./data/xcodeeval/1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00067
  global_step = 136
  train_loss = 0.4169
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00067
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.44 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:72.44
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00066
  global_step = 271
  train_loss = 0.3028
  ********************
Previous best ppl:1.00067
Achieve Best ppl:1.00066
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.51 	 Previous best codebleu 72.44
  ********************
 Achieve Best bleu:72.51
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00066
  global_step = 406
  train_loss = 0.2445
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.45 	 Previous best codebleu 72.51
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00067
  global_step = 541
  train_loss = 0.208
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.52 	 Previous best codebleu 72.51
  ********************
 Achieve Best bleu:72.52
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0007
  global_step = 676
  train_loss = 0.1804
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.57 	 Previous best codebleu 72.52
  ********************
 Achieve Best bleu:72.57
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00073
  global_step = 811
  train_loss = 0.1509
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.47 	 Previous best codebleu 72.57
  ********************
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/1/finetune_codet5p_220m', data_dir='./data/xcodeeval/1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00067
  global_step = 136
  train_loss = 0.4169
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00067
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.45 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:72.45
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00066
  global_step = 271
  train_loss = 0.3028
  ********************
Previous best ppl:1.00067
Achieve Best ppl:1.00066
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.52 	 Previous best codebleu 72.45
  ********************
 Achieve Best bleu:72.52
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00066
  global_step = 406
  train_loss = 0.2445
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.45 	 Previous best codebleu 72.52
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00067
  global_step = 541
  train_loss = 0.208
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.53 	 Previous best codebleu 72.52
  ********************
 Achieve Best bleu:72.53
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0007
  global_step = 676
  train_loss = 0.1804
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.58 	 Previous best codebleu 72.53
  ********************
 Achieve Best bleu:72.58
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00073
  global_step = 811
  train_loss = 0.1509
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.48 	 Previous best codebleu 72.58
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00078
  global_step = 946
  train_loss = 0.127
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.44 	 Previous best codebleu 72.58
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00081
  global_step = 1081
  train_loss = 0.1076
  ********************
Previous best ppl:1.00066
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.56 	 Previous best codebleu 72.58
  ********************
reload model from xcodeeval/1/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/1/test.jsonl
  codebleu = 73.43 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 73.43 
[0.6741254939692773, 0.6055239846395822, 0.7035272879003251, 0.8078005301884837, 0.8482415780791264, 0.8470661236484578, 0.8506318170022356, 0.6433992707996272, 0.475, 0.869097355083081, 0.8218348299362244, 0.8881108167610436, 0.7612037516644444, 0.8095193448343525, 0.8992587233632239, 0.7938229814514498, 0.727268027720646, 0.8365125728430938, 0.7023997010144827, 0.8890119901196314, 0.8819989837824207, 0.6035971947269193, 0.8504318413102254, 0.8517064703343284, 0.8272288242905221, 0.8759794301856842, 0.6538129485106954, 0.6958702394644983, 0.8437641571592116, 0.8629852137929874, 0.8973934740426551, 0.8071708182046731, 0.4256938428427359, 0.8772273751686793, 0.726906245805339, 0.8256815726664408, 0.5681163637298114, 0.8507183146596586, 0.7647346904829493, 0.6982499480365546, 0.6694040108132734, 0.7385923732495516, 0.5487940158683715, 0.6359495004577705, 0.5325804735306264, 0.9253363255944331, 0.8279104785599881, 0.7330177268778751, 0.6987438117048423, 0.811100358881859, 0.8805315670914018, 0.3414480260596623, 0.7427116405536602, 0.9109778546090423, 0.46344533813736777, 0.8034118760208823, 0.9259264314733622, 0.7041410201759473, 0.8260661662085544, 0.5160682766833452, 0.8799057191503341, 0.9265235989881684, 0.5076074703740681, 0.8535048490263217, 0.8545893714632256, 0.336963498695384, 0.7489533595723943, 0.22083634630817556, 0.7622600531135202, 0.7707471127673067, 0.4050873617790997, 0.8418611860573935, 0.4895865072343232, 0.8124797095951521, 0.8145273052967352, 0.8717455174648483, 0.7701328007329863, 0.7694578850257566, 0.849803040412304, 0.8255681126593256, 0.8719598303409888, 0.7927812326921357, 0.7674641460255568, 0.8476031916439257, 0.8696923233625582, 0.9298125945729137, 0.7575006467470722, 0.6357279870462479, 0.6449446745956003, 0.659532270654534, 0.982840810414882, 0.8710445728632014, 0.4650551733188451, 0.6476753884536018, 0.5355322041447821, 0.9498733743967636, 0.6281802407830452, 0.8350035289796414, 0.6184859920453802, 0.906923058378295, 0.8985810813576707, 0.5644982344246664, 0.8157728177025623, 0.19782713521617065, 0.7134134951513778, 0.8396663263205362, 0.4921529174785757, 0.9365739931432338, 0.42779770128383576, 0.8769250379411355, 0.4867368064069292, 0.7862446843835789, 0.7639496783614648, 0.6451144593926297, 0.7011151330217786, 0.88353709334584, 0.9503702205380173, 0.762314097845589, 0.7780707146601717, 0.8996726638725223, 0.7878795313221194, 0.3794955048760881, 0.788330513115518, 0.4140107071948905, 0.8119506730626463, 0.7211817541923293, 0.7286144481170467, 0.8318325665241797, 0.45601045567543047, 0.724315616661136, 0.6017150080688712, 0.876722724918082, 0.9013866678857929, 0.48246469658822144, 0.5905688151805155]
Finish training and take 14m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/2/finetune_codet5p_220m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00055
  global_step = 136
  train_loss = 0.4138
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00055
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.79 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:73.79
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00055
  global_step = 271
  train_loss = 0.3094
  ********************
Previous best ppl:1.00055
Achieve Best ppl:1.00055
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.06 	 Previous best codebleu 73.79
  ********************
 Achieve Best bleu:74.06
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00055
  global_step = 406
  train_loss = 0.2488
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.07 	 Previous best codebleu 74.06
  ********************
 Achieve Best bleu:74.07
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00057
  global_step = 541
  train_loss = 0.2066
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.09 	 Previous best codebleu 74.07
  ********************
 Achieve Best bleu:74.09
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00056
  global_step = 676
  train_loss = 0.1732
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.11 	 Previous best codebleu 74.09
  ********************
 Achieve Best bleu:74.11
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00061
  global_step = 811
  train_loss = 0.1434
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.96 	 Previous best codebleu 74.11
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00062
  global_step = 946
  train_loss = 0.1219
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.95 	 Previous best codebleu 74.11
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00067
  global_step = 1081
  train_loss = 0.1038
  ********************
Previous best ppl:1.00055
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.85 	 Previous best codebleu 74.11
  ********************
reload model from xcodeeval/2/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 70.66 
  Total = 135 
  Exact Fixed = 1 
[107]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 1 
[107]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 70.66 
[0.4079267132112413, 0.6065888646529191, 0.8723015765088757, 0.8981156303935836, 0.628105232559099, 0.8341852120711315, 0.8992357102776094, 0.709526995055453, 0.873793759962344, 0.6467062393002753, 0.3854267824187835, 0.5844229263024725, 0.904590355528839, 0.3325195752446274, 0.04505175233917717, 0.8609914309897955, 0.5726261131685282, 0.7482112321606759, 0.8724723036671462, 0.592207910649987, 0.446627993682677, 0.8248170962252077, 0.75750508267751, 0.871717936262084, 0.8154562777909085, 0.6201159722273546, 0.891419774625108, 0.6917596764211111, 0.7710815068968027, 0.34185814850653357, 0.45631161811982324, 0.5818754564013621, 0.8614283266451082, 0.8382191698805657, 0.6004280366320209, 0.560543691092872, 0.4524934938238685, 0.8460671937742572, 0.5436225904261595, 0.39054140447869534, 0.7503272906104577, 0.7128798335563746, 0.9120861554864073, 0.7784591158731673, 0.8375542987872413, 0.9041581967027494, 0.7801744066714751, 0.940387748772268, 0.79251028814738, 0.8036910613939219, 0.3120288913954662, 0.7716491424868305, 0.5310909326338049, 0.6384499261756515, 0.8852384894814247, 0.7109606270857536, 0.7764975463278132, 0.6992052024026362, 0.5346863070936048, 0.9470228248649237, 0.670183569679071, 0.8396663263205362, 0.9360674082443785, 0.8633447636989494, 0.43387818601959904, 0.7497872722734309, 0.5918835459549441, 0.869474849888572, 0.8946275734848934, 0.8537257078219596, 0.8064069121671387, 0.8196951483104229, 0.7368070849073924, 0.4612620280518578, 0.3025273885289328, 0.8648579538144856, 0.7496420585309386, 0.7602186940402952, 0.7631177288471214, 0.7875865283547716, 0.39578612527759205, 0.30666666666666664, 0.7269618142997433, 0.8753727486979741, 0.5393773280455789, 0.7947820674037744, 0.3852685514477996, 0.76987879200941, 0.8806974736497433, 0.8594524113191127, 0.6979780721244926, 0.8388209385489929, 0.6626435137538318, 0.8416117635801648, 0.7784360171669422, 0.521559233346504, 0.7833819946139522, 0.7488997676737696, 0.6333744987141046, 0.7410691472083426, 0.4585240670925822, 0.5715313815373253, 0.6739839265768114, 0.39198242100783887, 0.7388705197090006, 0.6951644390669244, 1.0, 0.46673071932457155, 0.9088830127891061, 0.5614360882955788, 0.6490688753903175, 0.8339055840616187, 0.8729244091162942, 0.912198238083447, 0.682584039879178, 0.8543126179887541, 0.9341190775741011, 0.4335448943198981, 0.8573326876937928, 0.7704569402736815, 0.4220974061228139, 0.8196069166590214, 0.7032096029468522, 0.6413253944664965, 0.7101520621290818, 0.6715603052093865, 0.9131807795832447, 0.6876287414964073, 0.3128264897154612, 0.853191132295444, 0.8164232694020533, 0.9311001454085734, 0.9612737738433852, 0.8859453911289024, 0.7966006161104328]
Finish training and take 14m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/3/finetune_codet5p_220m', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0006
  global_step = 136
  train_loss = 0.3933
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0006
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.16 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.16
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00059
  global_step = 271
  train_loss = 0.2923
  ********************
Previous best ppl:1.0006
Achieve Best ppl:1.00059
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.54 	 Previous best codebleu 74.16
  ********************
 Achieve Best bleu:74.54
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00061
  global_step = 406
  train_loss = 0.2329
  ********************
Previous best ppl:1.00059
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.08 	 Previous best codebleu 74.54
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00062
  global_step = 541
  train_loss = 0.1961
  ********************
Previous best ppl:1.00059
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.18 	 Previous best codebleu 74.54
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00066
  global_step = 676
  train_loss = 0.1638
  ********************
Previous best ppl:1.00059
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.03 	 Previous best codebleu 74.54
  ********************
reload model from xcodeeval/3/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 71.26 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 71.26 
[0.7849538767776651, 0.7663371990492582, 0.6976910864186706, 0.8512897119997327, 0.8536358763066614, 0.08406889937820157, 0.34898094225543036, 0.7669842146074164, 0.6756324089520065, 0.6054199120209949, 0.5478590325489547, 0.8164468849403668, 0.9175351478585267, 0.8726774356123554, 0.8059634988113191, 0.872147466111713, 0.28456980887865324, 0.7534037242254013, 0.5654675203716997, 0.7830064735910691, 0.9185412013134608, 0.8245561139312393, 0.8138327672660688, 0.5524112337326081, 0.7816849127345435, 0.802138971106996, 0.8324183912964096, 0.8448616344160145, 0.6982021521970254, 0.7921624393976985, 0.91412960455335, 0.4739726324720028, 0.63014750305347, 0.9040862722278158, 0.6345448464057466, 0.7259849312989834, 0.7540469087814428, 0.8313915586456186, 0.6121972533149926, 0.8569685752263538, 0.7932902743460295, 0.630150618578014, 0.6759728227587174, 0.790982444268264, 0.7807079788494218, 0.8296660707943342, 0.672785744652446, 0.7867507949022507, 0.8536334434067134, 0.5685250847298065, 0.8219514476194971, 0.8419978367899081, 0.9092430471124722, 0.9104392373575145, 0.6951713727652697, 0.941768888951575, 0.6455873914646095, 0.8138225102863303, 0.8654937656858617, 0.4142301377471687, 0.5112111419002278, 0.8297929734818674, 0.8963193939836629, 0.878087949536615, 0.8112486866006651, 0.7944974662629933, 0.8546770093840449, 0.23178378342054765, 0.7563107817326995, 0.8339849455214231, 0.8155779083654062, 0.901355655227414, 0.754778959829329, 0.7323223294379341, 0.802980404547649, 0.5923377715628043, 0.8125459461294413, 0.720188397702945, 0.6987172522283688, 0.7925405308747024, 0.900395087233911, 0.837457300065938, 0.7087785501314359, 0.7700652915417614, 0.5677893088427866, 0.7763879223350485, 0.3417564054664891, 0.7704569402736815, 0.7509157324200985, 0.7827835412006432, 0.5672747648078345, 0.647487961996232, 0.8819989837824207, 0.7470362018806367, 0.4154808998544015, 0.811364833324377, 0.5211121075488702, 0.5309005587685636, 0.823799855709568, 0.5271882025547743, 0.8077815485625466, 0.8270372589884398, 0.515209831912117, 0.7848352750832455, 0.8445923838454312, 0.8897945586059963, 0.33520188382167504, 0.4168626518355933, 0.852945187183654, 0.3975734208323345, 0.8811002176557108, 0.5457447578545462, 0.5285433525156937, 0.831862639858802, 0.7142029351288322, 0.4323025955698975, 0.8759700055210595, 0.7192634200448921, 0.9150668370807107, 0.7135938825045871, 0.6257365724699333, 0.8545307032267597, 0.7904010053030581, 0.8806594193232651, 0.5346863070936048, 0.6944280490257584, 0.8877072716538504, 0.24170769468996064, 0.2142677455447746, 0.5976804074013286, 0.9003747650468552, 0.8806974736497433, 0.6294449559412665, 0.5963350635154259, 0.24173832962493158]
Finish training and take 9m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/1/finetune_codet5p_220m', data_dir='./data/xcodeeval/1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00069
  global_step = 136
  train_loss = 0.4137
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00069
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.37 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:72.37
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00064
  global_step = 271
  train_loss = 0.3035
  ********************
Previous best ppl:1.00069
Achieve Best ppl:1.00064
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.59 	 Previous best codebleu 72.37
  ********************
 Achieve Best bleu:72.59
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00067
  global_step = 406
  train_loss = 0.2457
  ********************
Previous best ppl:1.00064
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.45 	 Previous best codebleu 72.59
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0007
  global_step = 541
  train_loss = 0.2072
  ********************
Previous best ppl:1.00064
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.51 	 Previous best codebleu 72.59
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0007
  global_step = 676
  train_loss = 0.1718
  ********************
Previous best ppl:1.00064
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.47 	 Previous best codebleu 72.59
  ********************
reload model from xcodeeval/1/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/1/test.jsonl
  codebleu = 73.33 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 73.33 
[0.6741254939692773, 0.6055239846395822, 0.7035272879003251, 0.8078005301884837, 0.8482415780791264, 0.8470661236484578, 0.8506318170022356, 0.6433992707996272, 0.47822580645161283, 0.869097355083081, 0.8218348299362244, 0.8881108167610436, 0.7612037516644444, 0.8095193448343525, 0.8992587233632239, 0.7671955666865656, 0.727268027720646, 0.8365125728430938, 0.7023997010144827, 0.8890119901196314, 0.8819989837824207, 0.6035971947269193, 0.8504318413102254, 0.8517064703343284, 0.8272288242905221, 0.8759794301856842, 0.6538129485106954, 0.6958702394644983, 0.8475703733605322, 0.812306762981212, 0.8973934740426551, 0.8071708182046731, 0.4256938428427359, 0.8772273751686793, 0.7275706976325815, 0.8256815726664408, 0.5681163637298114, 0.8514896480093217, 0.7647346904829493, 0.7378248767682071, 0.6540110120458642, 0.7385923732495516, 0.5249478620222177, 0.647487961996232, 0.5325804735306264, 0.9253363255944331, 0.8175656509737812, 0.7330177268778751, 0.681096752881313, 0.811100358881859, 0.8805315670914018, 0.3414480260596623, 0.7427116405536602, 0.9109778546090423, 0.46344533813736777, 0.8035773816874769, 0.9259264314733622, 0.7041410201759473, 0.8260661662085544, 0.5282624183587723, 0.8799057191503341, 0.9719123091671682, 0.5076074703740681, 0.8535048490263217, 0.8545893714632256, 0.24473684210526314, 0.7489533595723943, 0.22083634630817556, 0.7622600531135202, 0.7707471127673067, 0.4050873617790997, 0.8418611860573935, 0.4895865072343232, 0.8124797095951521, 0.8145273052967352, 0.8717455174648483, 0.7701328007329863, 0.7694578850257566, 0.849803040412304, 0.8255681126593256, 0.8719598303409888, 0.7927812326921357, 0.7674641460255568, 0.8476031916439257, 0.8696923233625582, 0.9298125945729137, 0.7575006467470722, 0.6016538902088896, 0.8275183621461346, 0.659532270654534, 0.982840810414882, 0.8710445728632014, 0.4650551733188451, 0.6476753884536018, 0.5355322041447821, 0.9498733743967636, 0.6281802407830452, 0.8350035289796414, 0.6184859920453802, 0.906923058378295, 0.8985810813576707, 0.5644982344246664, 0.7993588411755785, 0.19782713521617065, 0.7134134951513778, 0.8396663263205362, 0.4921529174785757, 0.9365739931432338, 0.42779770128383576, 0.8769250379411355, 0.4867368064069292, 0.7862446843835789, 0.7639496783614648, 0.6451144593926297, 0.7011151330217786, 0.88353709334584, 0.9503702205380173, 0.762314097845589, 0.7780707146601717, 0.8996726638725223, 0.7782021119672806, 0.3794955048760881, 0.8336721496048931, 0.4140107071948905, 0.7649756224565446, 0.7211817541923293, 0.7286144481170467, 0.8036723397559997, 0.45601045567543047, 0.724315616661136, 0.6017150080688712, 0.7683245403282871, 0.9013866678857929, 0.48246469658822144, 0.5905688151805155]
Finish training and take 11m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/2/finetune_codet5p_220m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00059
  global_step = 136
  train_loss = 0.4206
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00059
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.06 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:73.06
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00054
  global_step = 271
  train_loss = 0.3176
  ********************
Previous best ppl:1.00059
Achieve Best ppl:1.00054
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.93 	 Previous best codebleu 73.06
  ********************
 Achieve Best bleu:73.93
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00054
  global_step = 406
  train_loss = 0.25
  ********************
Previous best ppl:1.00054
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.12 	 Previous best codebleu 73.93
  ********************
 Achieve Best bleu:74.12
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00058
  global_step = 541
  train_loss = 0.2083
  ********************
Previous best ppl:1.00054
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.12 	 Previous best codebleu 74.12
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00057
  global_step = 676
  train_loss = 0.1783
  ********************
Previous best ppl:1.00054
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 74.08 	 Previous best codebleu 74.12
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 5
  eval_ppl = 1.0006
  global_step = 811
  train_loss = 0.1482
  ********************
Previous best ppl:1.00054
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 73.89 	 Previous best codebleu 74.12
  ********************
reload model from xcodeeval/2/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 70.82 
  Total = 135 
  Exact Fixed = 1 
[107]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 1 
[107]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 70.82 
[0.4079267132112413, 0.6065888646529191, 0.8723015765088757, 0.8981156303935836, 0.628105232559099, 0.8341852120711315, 0.9125880074596351, 0.709526995055453, 0.873793759962344, 0.646374013386654, 0.3854267824187835, 0.5844229263024725, 0.904590355528839, 0.3325195752446274, 0.04505175233917717, 0.8609914309897955, 0.5726261131685282, 0.7482112321606759, 0.8724723036671462, 0.592207910649987, 0.446627993682677, 0.8248170962252077, 0.75750508267751, 0.871717936262084, 0.8154562777909085, 0.6201159722273546, 0.891419774625108, 0.6917596764211111, 0.7710815068968027, 0.33367876486980974, 0.45631161811982324, 0.5818754564013621, 0.8614283266451082, 0.8382191698805657, 0.6004280366320209, 0.560543691092872, 0.4524934938238685, 0.8460671937742572, 0.5436225904261595, 0.39054140447869534, 0.7503272906104577, 0.7128798335563746, 0.9120861554864073, 0.7784591158731673, 0.8375542987872413, 0.9041581967027494, 0.7801744066714751, 0.940387748772268, 0.79251028814738, 0.8036910613939219, 0.3120288913954662, 0.7716491424868305, 0.5310909326338049, 0.6384499261756515, 0.8852384894814247, 0.7109606270857536, 0.7764975463278132, 0.6992052024026362, 0.5346863070936048, 0.9470228248649237, 0.7507584829026313, 0.8396663263205362, 0.9360674082443785, 0.8633447636989494, 0.43387818601959904, 0.7497872722734309, 0.5918835459549441, 0.869474849888572, 0.8946275734848934, 0.8537257078219596, 0.8064069121671387, 0.8196951483104229, 0.7368070849073924, 0.4612620280518578, 0.4351775916942335, 0.8648579538144856, 0.7496420585309386, 0.7602186940402952, 0.7631177288471214, 0.7875865283547716, 0.39578612527759205, 0.30666666666666664, 0.7269618142997433, 0.8753727486979741, 0.5393773280455789, 0.7947820674037744, 0.3852685514477996, 0.76987879200941, 0.8806974736497433, 0.8594524113191127, 0.6979780721244926, 0.8388209385489929, 0.6626435137538318, 0.8416117635801648, 0.7784360171669422, 0.521559233346504, 0.7833819946139522, 0.7488997676737696, 0.6333744987141046, 0.7410691472083426, 0.45435740042591555, 0.5715313815373253, 0.6739839265768114, 0.39198242100783887, 0.7388705197090006, 0.6951644390669244, 1.0, 0.46673071932457155, 0.9088830127891061, 0.5614360882955788, 0.6751558319120567, 0.8339055840616187, 0.8729244091162942, 0.912198238083447, 0.6618943847067642, 0.8543126179887541, 0.9341190775741011, 0.4335448943198981, 0.8573326876937928, 0.7704569402736815, 0.4220974061228139, 0.8196069166590214, 0.7032096029468522, 0.6413253944664965, 0.7101520621290818, 0.6715603052093865, 0.9131807795832447, 0.6876287414964073, 0.3128264897154612, 0.853191132295444, 0.8164232694020533, 0.9311001454085734, 0.9612737738433852, 0.8859453911289024, 0.7966006161104328]
Finish training and take 14m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/3/finetune_codet5p_220m', data_dir='./data/xcodeeval/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00062
  global_step = 136
  train_loss = 0.3997
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00062
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 73.7 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:73.7
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00058
  global_step = 271
  train_loss = 0.2901
  ********************
Previous best ppl:1.00062
Achieve Best ppl:1.00058
  ********************
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.41 	 Previous best codebleu 73.7
  ********************
 Achieve Best bleu:74.41
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0006
  global_step = 406
  train_loss = 0.2324
  ********************
Previous best ppl:1.00058
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.24 	 Previous best codebleu 74.41
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00062
  global_step = 541
  train_loss = 0.193
  ********************
Previous best ppl:1.00058
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.05 	 Previous best codebleu 74.41
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00066
  global_step = 676
  train_loss = 0.161
  ********************
Previous best ppl:1.00058
BLEU file: ./data/xcodeeval/3/validation.jsonl
  codebleu-4 = 74.09 	 Previous best codebleu 74.41
  ********************
reload model from xcodeeval/3/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/3/test.jsonl
  codebleu = 71.32 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 71.32 
[0.7849538767776651, 0.7663371990492582, 0.6976910864186706, 0.8512897119997327, 0.8536358763066614, 0.08406889937820157, 0.34898094225543036, 0.7669842146074164, 0.6756324089520065, 0.6054199120209949, 0.5601962677755853, 0.8164468849403668, 0.9175351478585267, 0.8726774356123554, 0.7609111405654982, 0.872147466111713, 0.28456980887865324, 0.7534037242254013, 0.5654675203716997, 0.7830064735910691, 0.9185412013134608, 0.8245561139312393, 0.8333813540442871, 0.5524112337326081, 0.7816849127345435, 0.802138971106996, 0.8324183912964096, 0.8448616344160145, 0.6982021521970254, 0.7921624393976985, 0.91412960455335, 0.4739726324720028, 0.63014750305347, 0.9040862722278158, 0.6345448464057466, 0.7259849312989834, 0.7540469087814428, 0.8313915586456186, 0.6121972533149926, 0.8569685752263538, 0.7932902743460295, 0.630150618578014, 0.6759728227587174, 0.790982444268264, 0.7807079788494218, 0.8296660707943342, 0.672785744652446, 0.7867507949022507, 0.8536334434067134, 0.5685250847298065, 0.8219514476194971, 0.8419978367899081, 0.9092430471124722, 0.9104392373575145, 0.7182482958421927, 0.941768888951575, 0.6455873914646095, 0.8138225102863303, 0.8654937656858617, 0.4142301377471687, 0.5112111419002278, 0.8297929734818674, 0.9217008915274616, 0.878087949536615, 0.7817544938474634, 0.7944974662629933, 0.8546770093840449, 0.23178378342054765, 0.7563107817326995, 0.8339849455214231, 0.8155779083654062, 0.9398076620328746, 0.754778959829329, 0.7773823007220496, 0.802980404547649, 0.5923377715628043, 0.8125459461294413, 0.720188397702945, 0.6987172522283688, 0.7925405308747024, 0.900395087233911, 0.837457300065938, 0.7087785501314359, 0.7700652915417614, 0.5677893088427866, 0.7763879223350485, 0.3417564054664891, 0.7704569402736815, 0.7509157324200985, 0.7827835412006432, 0.5672747648078345, 0.647487961996232, 0.8819989837824207, 0.7470362018806367, 0.4154808998544015, 0.811364833324377, 0.5211121075488702, 0.5309005587685636, 0.823799855709568, 0.5271882025547743, 0.8077815485625466, 0.8207432951321398, 0.5105547910952926, 0.7848352750832455, 0.8445923838454312, 0.8897945586059963, 0.33520188382167504, 0.4168626518355933, 0.852945187183654, 0.3975734208323345, 0.8811002176557108, 0.5534803180860053, 0.5285433525156937, 0.831862639858802, 0.7142029351288322, 0.42918313960976945, 0.8759700055210595, 0.7192634200448921, 0.9150668370807107, 0.7135938825045871, 0.6257365724699333, 0.8545307032267597, 0.7904010053030581, 0.8806594193232651, 0.5346863070936048, 0.6944280490257584, 0.8877072716538504, 0.24170769468996064, 0.2142677455447746, 0.5976804074013286, 0.9003747650468552, 0.8806974736497433, 0.6294449559412665, 0.5963350635154259, 0.24173832962493158]
Finish training and take 12m
Namespace(log_name='./xcodeeval/1/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/1/finetune_codet5p_220m', data_dir='./data/xcodeeval/1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1082 training instances 
***** Running training *****
  Num examples = 1082
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00069
  global_step = 136
  train_loss = 0.4137
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00069
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.37 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:72.37
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00064
  global_step = 271
  train_loss = 0.3035
  ********************
Previous best ppl:1.00069
Achieve Best ppl:1.00064
  ********************
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.59 	 Previous best codebleu 72.37
  ********************
 Achieve Best bleu:72.59
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00067
  global_step = 406
  train_loss = 0.2457
  ********************
Previous best ppl:1.00064
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.45 	 Previous best codebleu 72.59
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0007
  global_step = 541
  train_loss = 0.2072
  ********************
Previous best ppl:1.00064
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.5 	 Previous best codebleu 72.59
  ********************

***** Running evaluation *****
  Num examples = 135
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0007
  global_step = 676
  train_loss = 0.1718
  ********************
Previous best ppl:1.00064
BLEU file: ./data/xcodeeval/1/validation.jsonl
  codebleu-4 = 72.47 	 Previous best codebleu 72.59
  ********************
reload model from xcodeeval/1/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/1/test.jsonl
  codebleu = 73.37 
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 73.37 
[0.6741254939692773, 0.6055239846395822, 0.7035272879003251, 0.8078005301884837, 0.8482415780791264, 0.8470661236484578, 0.8506318170022356, 0.6433992707996272, 0.47822580645161283, 0.869097355083081, 0.8218348299362244, 0.8881108167610436, 0.7612037516644444, 0.8095193448343525, 0.8992587233632239, 0.7671955666865656, 0.727268027720646, 0.8365125728430938, 0.7023997010144827, 0.8890119901196314, 0.8819989837824207, 0.6035971947269193, 0.8504318413102254, 0.8517064703343284, 0.8272288242905221, 0.8759794301856842, 0.6538129485106954, 0.6958702394644983, 0.8475703733605322, 0.812306762981212, 0.8973934740426551, 0.8071708182046731, 0.4256938428427359, 0.8772273751686793, 0.7275706976325815, 0.8256815726664408, 0.5781163637298115, 0.8514896480093217, 0.7647346904829493, 0.7378248767682071, 0.6540110120458642, 0.7385923732495516, 0.5249478620222177, 0.647487961996232, 0.5325804735306264, 0.9253363255944331, 0.8279104785599881, 0.7330177268778751, 0.6987438117048423, 0.811100358881859, 0.8805315670914018, 0.3414480260596623, 0.7427116405536602, 0.9109778546090423, 0.46344533813736777, 0.8035773816874769, 0.9259264314733622, 0.7041410201759473, 0.8260661662085544, 0.5282624183587723, 0.8799057191503341, 0.9719123091671682, 0.5076074703740681, 0.8535048490263217, 0.8545893714632256, 0.24473684210526314, 0.7489533595723943, 0.22083634630817556, 0.7622600531135202, 0.7707471127673067, 0.4050873617790997, 0.8418611860573935, 0.4895865072343232, 0.8124797095951521, 0.8145273052967352, 0.8717455174648483, 0.7701328007329863, 0.7694578850257566, 0.849803040412304, 0.8255681126593256, 0.8719598303409888, 0.7927812326921357, 0.7674641460255568, 0.8476031916439257, 0.8696923233625582, 0.9298125945729137, 0.7575006467470722, 0.6016538902088896, 0.8275183621461346, 0.659532270654534, 0.982840810414882, 0.8710445728632014, 0.4650551733188451, 0.6476753884536018, 0.5355322041447821, 0.9498733743967636, 0.6281802407830452, 0.8350035289796414, 0.6184859920453802, 0.906923058378295, 0.8985810813576707, 0.5644982344246664, 0.7993588411755785, 0.19782713521617065, 0.7134134951513778, 0.8396663263205362, 0.4921529174785757, 0.9365739931432338, 0.42779770128383576, 0.8769250379411355, 0.4867368064069292, 0.7862446843835789, 0.7639496783614648, 0.6451144593926297, 0.7011151330217786, 0.88353709334584, 0.9503702205380173, 0.762314097845589, 0.7780707146601717, 0.8996726638725223, 0.7878795313221194, 0.3794955048760881, 0.8336721496048931, 0.4140107071948905, 0.7649756224565446, 0.7211817541923293, 0.7286144481170467, 0.8036723397559997, 0.45601045567543047, 0.724315616661136, 0.6017150080688712, 0.7776995403282871, 0.9013866678857929, 0.48246469658822144, 0.5905688151805155]
Finish training and take 11m
