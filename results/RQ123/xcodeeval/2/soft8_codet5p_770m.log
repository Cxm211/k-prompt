Namespace(log_name='./xcodeeval/2/soft8_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='xcodeeval/2/soft8_codet5p_770m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'Please fix an buggy program', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' #include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': 'with tags', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' binary search, implementation', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 272
  train_loss = 67.3771
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 75.68 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.68
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 543
  train_loss = 54.6264
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.15 	 Previous best codebleu 75.68
  ********************
 Achieve Best bleu:76.15
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 814
  train_loss = 40.619
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 75.77 	 Previous best codebleu 76.15
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 1085
  train_loss = 28.5917
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.03 	 Previous best codebleu 76.15
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 1356
  train_loss = 19.5096
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.4 	 Previous best codebleu 76.15
  ********************
 Achieve Best bleu:76.4
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1627
  train_loss = 13.5165
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.17 	 Previous best codebleu 76.4
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 1898
  train_loss = 8.7987
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.18 	 Previous best codebleu 76.4
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 2169
  train_loss = 5.6613
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.17 	 Previous best codebleu 76.4
  ********************
early stopping!!!
reload model from xcodeeval/2/soft8_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 72.52 
  Total = 135 
  Exact Fixed = 7 
[27, 73, 88, 107, 114, 117, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  ********************
  Total = 135 
  Exact Fixed = 7 
[27, 73, 88, 107, 114, 117, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  codebleu = 72.52 
[0.3972168130134871, 0.5119460199048119, 0.9504928408158546, 0.8559665188829002, 0.6343711128854987, 0.9852691294317872, 0.9895907412940455, 0.4618799345363792, 0.9413237717631426, 0.6802077739515769, 0.24210607807744522, 0.584899116778663, 0.9117781363015256, 0.11127105463702613, 0.03281115469695418, 0.9690470390182255, 0.5381574333299562, 0.7356931723051547, 0.7106363484445642, 0.40990352859103496, 0.51279090278026, 0.8919774150628842, 0.9780384047260811, 0.9617720754477719, 0.6882647766792233, 0.6333671278513916, 1.0, 0.8995255436220362, 0.9120424742934687, 0.34542506968400094, 0.4222704774639776, 0.8550346580697584, 0.9617570994669966, 0.946665089615236, 0.5622121896820453, 0.5998058426022883, 0.4796238829408068, 0.9557465581089486, 0.31973424908951587, 0.5464689953930467, 0.6863757819221206, 0.8359122348190775, 0.9353176101067031, 0.6517624271254832, 0.9602165068498714, 0.9298931791730787, 0.9687829854371817, 0.940387748772268, 0.9756268047137295, 0.9758234675444843, 0.32151517522245066, 0.37582974298561705, 0.5047577306661156, 0.7922200386796246, 0.9081381892595939, 0.9403863481322654, 0.8918142482329203, 0.8055092965687558, 0.49077560378130586, 0.607835221839913, 0.8920068493904778, 0.938605671540292, 0.8109656805163296, 0.5722769208847139, 0.3326127917717159, 0.7497872722734309, 0.7127500077175998, 0.571438355469486, 0.9768438379914091, 0.9671007961854172, 0.9430295145286021, 0.8369635941502973, 0.9715441710080255, 0.6438504303659041, 0.17475974450900342, 0.7878465415346224, 0.7595166666814087, 0.979304849374673, 0.9278876679975507, 0.9511842736268326, 0.26676325783756394, 0.28714285714285714, 0.7269618142997433, 0.727419331945831, 0.40897025914278523, 0.6097416277258915, 0.3634121790752288, 0.9872114221460186, 0.9343166704551076, 0.6538001263697061, 0.7110836621866043, 0.9603861885822986, 0.8741900115980714, 0.9788259925881726, 0.7961104119141532, 0.5959377733666189, 0.9586895873203385, 0.8558529762577309, 0.6181710046016234, 0.9157905304447675, 0.41250375321647825, 0.5767877917937355, 0.4668795825192024, 0.25916735484337194, 0.98905613398207, 0.6765153707439431, 1.0, 0.43480243860838597, 0.9371536949899508, 0.43063362247391185, 0.6997993560942393, 0.7738077478644633, 0.8809820875819727, 1.0, 0.9286542954941046, 0.9404076659244109, 1.0, 0.5002115609865647, 0.9217262750138406, 1.0, 0.45172428704995954, 0.9498733743967636, 0.6949039715476987, 0.6628618028697012, 0.6212987985364005, 0.4247334619586536, 0.9645896373785567, 0.5210011744246135, 0.2873199576689667, 0.7594307197061102, 0.7812366325166946, 0.8778488047731399, 0.9642957518653632, 0.9750949338196533, 0.7829994808464917]
Finish training and take 1h20m
