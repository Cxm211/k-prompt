Namespace(log_name='./xcodeeval/2/soft3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/2/soft3_codet5p_220m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=6, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 6
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 182
  train_loss = 77.8308
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.49 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.49
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 363
  train_loss = 63.9643
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.46 	 Previous best codebleu 77.49
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 544
  train_loss = 51.5582
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.48 	 Previous best codebleu 77.49
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 725
  train_loss = 42.8009
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.73 	 Previous best codebleu 77.49
  ********************
 Achieve Best bleu:77.73
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 906
  train_loss = 35.6599
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.43 	 Previous best codebleu 77.73
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1087
  train_loss = 29.9504
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.63 	 Previous best codebleu 77.73
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 1268
  train_loss = 25.4799
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.56 	 Previous best codebleu 77.73
  ********************
early stopping!!!
reload model from xcodeeval/2/soft3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 74.27 
  Total = 135 
  Exact Fixed = 5 
[27, 73, 88, 107, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  ********************
  Total = 135 
  Exact Fixed = 5 
[27, 73, 88, 107, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  codebleu = 74.27 
[0.39866608837580597, 0.5023065794143076, 0.9435265486810231, 0.8123415199409241, 0.6140107726309088, 0.9852691294317872, 0.9895907412940455, 0.7530784364090384, 0.9701508885525887, 0.7141494776867402, 0.2559598986754519, 0.5844229263024725, 0.904590355528839, 0.3325195752446274, 0.03638680962604285, 0.9690929327610214, 0.5566079466715802, 0.7482112321606759, 0.8840280108223306, 0.3503170107454441, 0.5123718159484094, 0.8861971446522757, 0.9780384047260811, 0.9617720754477719, 0.8424031755937416, 0.6201159722273546, 1.0, 0.947466474526101, 0.9054724259842899, 0.29476857896785025, 0.3878737250839708, 0.8605247897664112, 0.978658507917701, 0.9510814452964713, 0.6004280366320209, 0.6264576449062644, 0.4782027818671871, 0.9483958118402918, 0.5897800437101466, 0.596030258895899, 0.7219090712625591, 0.8207174296242723, 0.9251568164016057, 0.6406272585514035, 0.9659416709906494, 0.9144169886968883, 0.9687829854371817, 0.940387748772268, 0.9840793393103271, 0.9747938432670278, 0.3047297501212361, 0.7162143583702325, 0.4657133305757337, 0.6110771271874474, 0.897868274842214, 0.9402139343391618, 0.8862841277509927, 0.9512940375189334, 0.4953415641367399, 0.7800574440621352, 0.7507584829026313, 0.9582778026878329, 0.9360674082443785, 0.556376903304558, 0.33070238333956836, 0.8886223888442533, 0.6691335802853828, 0.7510867123236671, 0.9768438379914091, 0.9622793098301157, 0.9409244155109817, 0.8196951483104229, 0.9715441710080255, 0.6423888919043657, 0.3791972352076457, 0.7852033214625844, 0.7496420585309386, 0.978606091610698, 0.9262642913741741, 0.9609336279140019, 0.31976874551780043, 0.30666666666666664, 0.7269618142997433, 0.7157249744648765, 0.45018476558326015, 0.6178500780511474, 0.35565802228866766, 0.993565204007768, 0.9256050837711551, 0.709805001952248, 0.6979780721244926, 0.9637360645128197, 0.8688277339527386, 0.9788259925881726, 0.8249919755332582, 0.5461719247723633, 0.9652541117011371, 0.8069755829126246, 0.7067236361805709, 0.9178250269397255, 0.4466796632469585, 0.5715313815373253, 0.4920880344612968, 0.28574466030417256, 0.98905613398207, 0.6951644390669244, 1.0, 0.4233020686260425, 0.9345121855559886, 0.4559058992495705, 0.7369736876450416, 0.7472051837618991, 0.8856566772820293, 0.912198238083447, 0.9143685812083902, 0.9362813438627395, 0.9533257646563149, 0.4335448943198981, 0.9615049129245392, 1.0, 0.3551376820682588, 0.9498733743967636, 0.8552585407231501, 0.8171220069513339, 0.6050860325789538, 0.50980793004376, 0.9645896373785567, 0.5818842126352378, 0.2621892956480607, 0.7390132416145816, 0.7680752406649723, 0.9056265825509175, 0.9612737738433852, 0.9750949338196533, 0.7348529689836785]
Finish training and take 39m
Namespace(log_name='./xcodeeval/2/soft3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='xcodeeval/2/soft3_codet5p_220m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=6, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'Fix', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' #include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': 'fixed program is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 6
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 182
  train_loss = 76.8641
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 78.24 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:78.24
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 363
  train_loss = 64.6543
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 78.09 	 Previous best codebleu 78.24
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 544
  train_loss = 51.8264
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 78.21 	 Previous best codebleu 78.24
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 725
  train_loss = 42.6493
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 78.27 	 Previous best codebleu 78.24
  ********************
 Achieve Best bleu:78.27
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 906
  train_loss = 35.7399
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 78.18 	 Previous best codebleu 78.27
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1087
  train_loss = 29.9174
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 78.23 	 Previous best codebleu 78.27
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 1268
  train_loss = 25.3751
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.84 	 Previous best codebleu 78.27
  ********************
early stopping!!!
reload model from xcodeeval/2/soft3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 75.74 
  Total = 135 
  Exact Fixed = 6 
[27, 73, 88, 107, 117, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  ********************
  Total = 135 
  Exact Fixed = 6 
[27, 73, 88, 107, 117, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  codebleu = 75.74 
[0.4064005497550015, 0.5710830419145426, 0.904431134826744, 0.8591618681036157, 0.6699587833586085, 0.9851146973603104, 0.9895563434849906, 0.5361516691454101, 0.9441068380038105, 0.8273038903329868, 0.29401413684550026, 0.6538455721121705, 0.9185220506212959, 0.36957414416134826, 0.031753487703293275, 0.9687829854371817, 0.5941545284279156, 0.7980055844927083, 0.6954839700002162, 0.4114600399348994, 0.5596684190586658, 0.9115629644341452, 0.9777731954578366, 0.9613758560746299, 0.802942058233229, 0.6886552571369082, 1.0, 0.9539744365536089, 0.9148901415551143, 0.3031106941385726, 0.4817170738851859, 0.9519751693272467, 0.9783872919608811, 0.9492391717548732, 0.6039284723863361, 0.728657178185771, 0.5479326977753471, 0.9554738454914684, 0.5995760376005899, 0.5307132484654452, 0.6533171909165527, 0.7405069937016574, 0.9342461108782714, 0.7867166637523201, 0.9657843638127719, 0.9263893154093195, 0.9685667308405901, 0.9396930654304276, 0.9839634781567539, 0.9786767284140496, 0.34782507166037085, 0.5438156629758828, 0.5134447979771696, 0.6552088209047222, 0.9134831878811485, 0.9426743974892482, 0.7534989695020072, 0.9471523758712838, 0.516450178566324, 0.7541563100264306, 0.6139672742465931, 0.9356168989023086, 0.9493719608346454, 0.5685957173316762, 0.387899041928529, 0.8876313229260933, 0.7158604801795241, 0.7347399665451808, 0.9768438379914091, 0.9833785327366187, 0.9547306554202579, 0.8350790685915768, 0.9712274443618227, 0.6612758121025245, 0.4676174832475913, 0.7937711874475846, 0.9075841164729965, 0.9900191350889589, 0.9345858816164352, 0.9688759947448535, 0.3088218278186061, 0.39107142857142857, 0.7291122323054859, 0.7356734318309193, 0.4359183208035675, 0.666737376550815, 0.4169343940475574, 0.9935485343815385, 0.9338245631919015, 0.6851778816136055, 0.7626488369656692, 0.97253067752308, 0.8758833086806956, 0.9786581393419791, 0.8425488151324652, 0.600390482851084, 0.9128344485571482, 0.8668930417407394, 0.9404488256179535, 0.8906560653347539, 0.5187733487835448, 0.5880009068221708, 0.5804381394700552, 0.33778848004697615, 0.9888765931470758, 0.7440547730426975, 1.0, 0.5280805187548091, 0.9396102114814104, 0.5088414645774583, 0.7279253452272595, 0.7082889481809873, 0.7780929833380806, 0.9110305760844113, 0.9408661965658305, 0.9421250185529444, 1.0, 0.4551613023839822, 0.9612141455329344, 1.0, 0.44149900857617347, 0.9490051755223876, 0.8786751961062007, 0.7562848677650553, 0.6727326405294691, 0.43394235442612644, 0.9642836546885394, 0.6780136262808578, 0.3325678372590227, 0.7597848917776782, 0.8847595446680929, 0.8810322736630851, 0.9329292922380341, 0.9750020534642385, 0.7804895955621882]
Finish training and take 40m
Namespace(log_name='./xcodeeval/2/soft3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='xcodeeval/2/soft3_codet5p_220m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=6, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'Fix', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' #include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': 'fixed program is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 6
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 182
  train_loss = 77.0153
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 75.91 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.91
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 363
  train_loss = 64.4365
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.39 	 Previous best codebleu 75.91
  ********************
 Achieve Best bleu:77.39
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 544
  train_loss = 51.5855
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.66 	 Previous best codebleu 77.39
  ********************
 Achieve Best bleu:77.66
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 725
  train_loss = 42.8163
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.74 	 Previous best codebleu 77.66
  ********************
 Achieve Best bleu:77.74
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 906
  train_loss = 36.0649
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.68 	 Previous best codebleu 77.74
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1087
  train_loss = 30.0287
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.57 	 Previous best codebleu 77.74
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 1268
  train_loss = 25.4957
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.87 	 Previous best codebleu 77.74
  ********************
 Achieve Best bleu:77.87
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 1449
  train_loss = 21.684
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.83 	 Previous best codebleu 77.87
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 1630
  train_loss = 18.7025
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.73 	 Previous best codebleu 77.87
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 1811
  train_loss = 16.9849
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.65 	 Previous best codebleu 77.87
  ********************
early stopping!!!
reload model from xcodeeval/2/soft3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 73.79 
  Total = 135 
  Exact Fixed = 6 
[27, 73, 88, 107, 117, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  ********************
  Total = 135 
  Exact Fixed = 6 
[27, 73, 88, 107, 117, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[100]
  codebleu = 73.79 
[0.39866608837580597, 0.5023065794143076, 0.9435265486810231, 0.8123415199409241, 0.6140107726309088, 0.9852691294317872, 0.9791134139687718, 0.7530784364090384, 0.9701508885525887, 0.7141494776867402, 0.24262656534211852, 0.5844229263024725, 0.904590355528839, 0.3325195752446274, 0.03281115469695418, 0.9690929327610214, 0.5566079466715802, 0.7482112321606759, 0.8840280108223306, 0.3503170107454441, 0.5123718159484094, 0.8465433723448936, 0.9780384047260811, 0.9617720754477719, 0.8823087892505581, 0.6201159722273546, 1.0, 0.947466474526101, 0.9420652287223139, 0.29476857896785025, 0.3878737250839708, 0.8605247897664112, 0.978658507917701, 0.9510814452964713, 0.6004280366320209, 0.5700473884960081, 0.4782027818671871, 0.9483958118402918, 0.21627756087184208, 0.596030258895899, 0.7219090712625591, 0.8207174296242723, 0.9251568164016057, 0.6406272585514035, 0.9659416709906494, 0.814497105957587, 0.9687829854371817, 0.940387748772268, 0.9840793393103271, 0.9747938432670278, 0.3047297501212361, 0.7162143583702325, 0.4657133305757337, 0.6110771271874474, 0.897868274842214, 0.9402139343391618, 0.8862841277509927, 0.9512940375189334, 0.4953415641367399, 0.7800574440621352, 0.7507584829026313, 0.9582778026878329, 0.9360674082443785, 0.556376903304558, 0.33070238333956836, 0.8886223888442533, 0.6691335802853828, 0.7510867123236671, 0.9768438379914091, 0.9622793098301157, 0.9409244155109817, 0.8196951483104229, 0.9715441710080255, 0.6423888919043657, 0.37276866377907425, 0.7852033214625844, 0.7496420585309386, 0.978606091610698, 0.9262642913741741, 0.9609336279140019, 0.31976874551780043, 0.30666666666666664, 0.7269618142997433, 0.7157249744648765, 0.45018476558326015, 0.6178500780511474, 0.35565802228866766, 0.993565204007768, 0.9393486136986724, 0.709805001952248, 0.6979780721244926, 0.9637360645128197, 0.8688277339527386, 0.9788259925881726, 0.8249919755332582, 0.5461719247723633, 0.9652541117011371, 0.8069755829126246, 0.7067236361805709, 0.9041842162151641, 0.4466796632469585, 0.5599929199988638, 0.4920880344612968, 0.28574466030417256, 0.98905613398207, 0.6951644390669244, 1.0, 0.4233020686260425, 0.9345121855559886, 0.4559058992495705, 0.7016795699979828, 0.7472051837618991, 0.8856566772820293, 0.7271540198222202, 0.9143685812083902, 0.9362813438627395, 1.0, 0.4335448943198981, 0.9615049129245392, 1.0, 0.3596831366137133, 0.9498733743967636, 0.8580298417070347, 0.8171220069513339, 0.6050860325789538, 0.5273981887723884, 0.9645896373785567, 0.5818842126352378, 0.2621892956480607, 0.7390132416145816, 0.7680752406649723, 0.9056265825509175, 0.9612737738433852, 0.9750949338196533, 0.7685193415839502]
Finish training and take 41m
