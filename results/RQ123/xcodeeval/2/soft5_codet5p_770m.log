Namespace(log_name='./xcodeeval/2/soft5_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='xcodeeval/2/soft5_codet5p_770m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=6, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 6
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 182
  train_loss = 67.1313
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.01 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.01
  ********************
Namespace(log_name='./xcodeeval/2/soft5_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='xcodeeval/2/soft5_codet5p_770m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=6, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 6
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 182
  train_loss = 67.1313
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 77.01 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.01
  ********************
Namespace(log_name='./xcodeeval/2/soft5_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='xcodeeval/2/soft5_codet5p_770m', data_dir='./data/xcodeeval/2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> long long a[100003],b[100003],x[100003]; int main() {     long long n,m,i,total=0,j;     scanf("%lld",&n);     a[0]=0;     b[0]=0;     for(i=1;i<=n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x[j]);     }     for(j=1;j<=m;j++)     {         for(i=1;i<=n;i++)         {             if(b[i]>=x[j])             {                 printf("%lld\\n",i);                 break;             }         }     }     return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': 'is buggy program', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is fixed program', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> long long a[200000],b[200000]; int main() {     long long n,i,m,total=0,x,j,start,end,mid;     scanf("%lld",&n);     for(i=0;i<n;i++)     {         scanf("%lld",&a[i]);         total=total+a[i];         b[i]=total;     }     scanf("%lld",&m);     for(j=1;j<=m;j++)     {         scanf("%lld",&x);         if(x<=b[0])         {             printf("1\\n");         }         else if(x>=b[n-1])         {             printf("%lld\\n",n);         }         else         {             start=0;             end=n-1;             while(start<=end)             {                 mid=(start+end)/2;                 if(b[mid]>=x&&b[mid-1]<x)                 {                     printf("%lld\\n",(mid+1));                     break;                 }                 else if(b[mid]<x)                 {                     start=mid+1;                 }                 else if(b[mid]>=x&&b[mid-1]>=x)                 {                     end=mid-1;                 }             }         }     }     return 0;  }'}]
***** Running training *****
  Num examples = 1082
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 272
  train_loss = 67.3751
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 75.99 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:75.99
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 543
  train_loss = 54.0858
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.24 	 Previous best codebleu 75.99
  ********************
 Achieve Best bleu:76.24
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 814
  train_loss = 39.2466
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.23 	 Previous best codebleu 76.24
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 1085
  train_loss = 27.9602
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.19 	 Previous best codebleu 76.24
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 1356
  train_loss = 19.335
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.39 	 Previous best codebleu 76.24
  ********************
 Achieve Best bleu:76.39
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1627
  train_loss = 13.1029
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 75.91 	 Previous best codebleu 76.39
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 1898
  train_loss = 8.7101
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.06 	 Previous best codebleu 76.39
  ********************

***** Running evaluation *****
  Num examples = 136
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 2169
  train_loss = 5.5214
  ********************
Previous best ppl:inf
BLEU file: ./data/xcodeeval/2/validation.jsonl
  codebleu-4 = 76.02 	 Previous best codebleu 76.39
  ********************
early stopping!!!
reload model from xcodeeval/2/soft5_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/xcodeeval/2/test.jsonl
  codebleu = 72.48 
  Total = 135 
  Exact Fixed = 5 
[73, 88, 107, 114, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 135 
  Exact Fixed = 5 
[73, 88, 107, 114, 120]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 72.48 
[0.4138834796801538, 0.5119460199048119, 0.9504928408158546, 0.837279164144823, 0.6149771354539022, 0.9852691294317872, 0.9550085942873607, 0.4618799345363792, 0.9413237717631426, 0.6574023320205616, 0.24210607807744522, 0.584899116778663, 0.9117781363015256, 0.07235026731022157, 0.03281115469695418, 0.9690470390182255, 0.5699124992618471, 0.7356931723051547, 0.7106363484445642, 0.40990352859103496, 0.51279090278026, 0.8919774150628842, 0.9780384047260811, 0.9617720754477719, 0.6882647766792233, 0.5655446737140681, 0.8848263842149353, 0.9589018418981516, 0.9486352770314925, 0.34542506968400094, 0.4222704774639776, 0.8653834749273801, 0.9617570994669966, 0.946665089615236, 0.5885359393675832, 0.6232841034718535, 0.4796238829408068, 0.9557465581089486, 0.5713412824644678, 0.5464689953930467, 0.6863757819221206, 0.8359122348190775, 0.9353176101067031, 0.6517624271254832, 0.9659404814269257, 0.9298931791730787, 0.9687829854371817, 0.940387748772268, 0.9840793393103271, 0.9545866260932565, 0.32151517522245066, 0.37582974298561705, 0.5047577306661156, 0.7823299287895147, 0.853063134088631, 0.9403863481322654, 0.8918142482329203, 0.9451848319937642, 0.49077560378130586, 0.607835221839913, 0.7191795355342101, 0.938605671540292, 0.8127489106826192, 0.5722769208847139, 0.3326127917717159, 0.7497872722734309, 0.7127500077175998, 0.571438355469486, 0.9768438379914091, 0.9671007961854172, 0.9430295145286021, 0.8369635941502973, 0.9715441710080255, 0.6438504303659041, 0.3932369549657331, 0.7878465415346224, 0.648054944728782, 0.979304849374673, 0.9278876679975507, 0.9511842736268326, 0.26676325783756394, 0.2923529411764706, 0.7269618142997433, 0.727419331945831, 0.40897025914278523, 0.6097416277258915, 0.3634121790752288, 0.9872114221460186, 0.9836247096600614, 0.6538001263697061, 0.7110836621866043, 0.9603861885822986, 0.8741900115980714, 0.9788259925881726, 0.7961104119141532, 0.712392973867872, 0.9586895873203385, 0.8558529762577309, 0.6181710046016234, 0.9070688315997795, 0.41250375321647825, 0.5767877917937355, 0.4668795825192024, 0.25916735484337194, 0.98905613398207, 0.6765153707439431, 1.0, 0.43480243860838597, 0.9371536949899508, 0.43063362247391185, 0.6943448106396939, 0.7738077478644633, 0.5905739657310716, 1.0, 0.9286542954941046, 0.9404076659244109, 0.8418743548836447, 0.5002115609865647, 0.9615049129245392, 1.0, 0.41784954647503847, 0.9498733743967636, 0.6949039715476987, 0.6628618028697012, 0.6212987985364005, 0.4247334619586536, 0.9645896373785567, 0.5210011744246135, 0.2873199576689667, 0.7594307197061102, 0.8716225800674122, 0.8778488047731399, 0.9642957518653632, 0.9750949338196533, 0.7829994808464917]
Finish training and take 1h19m
