Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00358
  global_step = 64
  train_loss = 3.5404
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00358
  ********************
BLEU file: ./data/defects4j/validation.jsonl
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00358
  global_step = 64
  train_loss = 3.5404
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00358
  ********************
BLEU file: ./data/defects4j/validation.jsonl
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=1024, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00358
  global_step = 64
  train_loss = 3.5404
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00358
  ********************
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 78.73 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:78.73
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00336
  global_step = 127
  train_loss = 2.2579
  ********************
Previous best ppl:1.00358
Achieve Best ppl:1.00336
  ********************
BLEU file: ./data/defects4j/validation.jsonl
Namespace(log_name='./defects4j/finetune_gptneo.log', model_name='EleutherAI/gpt-neo-1.3B', lang='java', output_dir='defects4j/finetune_gptneo', data_dir='./data/defects4j', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=1024, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: EleutherAI/gpt-neo-1.3B
model created!
Total 502 training instances 
***** Running training *****
  Num examples = 502
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00358
  global_step = 64
  train_loss = 3.5404
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00358
  ********************
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 77.37 	 Previous best codebleu 78.73
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00339
  global_step = 190
  train_loss = 1.8959
  ********************
Previous best ppl:1.00336
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 78.72 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:78.72
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00336
  global_step = 127
  train_loss = 2.2579
  ********************
Previous best ppl:1.00358
Achieve Best ppl:1.00336
  ********************
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 77.77 	 Previous best codebleu 78.73
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00345
  global_step = 253
  train_loss = 1.6229
  ********************
Previous best ppl:1.00336
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 77.39 	 Previous best codebleu 78.72
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00339
  global_step = 190
  train_loss = 1.8959
  ********************
Previous best ppl:1.00336
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 77.5 	 Previous best codebleu 78.73
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00351
  global_step = 316
  train_loss = 1.3601
  ********************
Previous best ppl:1.00336
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 77.76 	 Previous best codebleu 78.72
  ********************

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00345
  global_step = 253
  train_loss = 1.6229
  ********************
Previous best ppl:1.00336
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 75.16 	 Previous best codebleu 78.73
  ********************
reload model from defects4j/finetune_gptneo/checkpoint-best-bleu
BLEU file: ./data/defects4j/test.jsonl
  codebleu-4 = 77.5 	 Previous best codebleu 78.72
  ********************
  codebleu = 73.7 
  Total = 64 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 64 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 73.7 
[0.7977509497997374, 0.187138788372509, 0.7657110295787897, 0.8870317711504849, 0.8883667838108272, 0.6007163503048495, 0.6576252123917807, 0.8681777725936917, 0.7051146102158162, 0.8650423106205597, 0.7143513682140774, 0.6451687939127737, 0.8721125152975062, 0.877572165029932, 0.8866626937665041, 0.7369092647119393, 0.9032703781902429, 0.9228950337188102, 0.9617720754477719, 0.8459463132957461, 0.7457367319180978, 0.6897393400904446, 0.7906925479531332, 0.8693059718827794, 0.8734502199950254, 0.863021098885395, 0.2862125618794039, 0.7694729472672909, 0.6410378892354339, 0.8899407308130634, 0.7869230583782949, 0.5470596690445572, 0.7176189204658862, 0.8232523732318835, 0.8727340070600411, 0.8575737152700653, 0.8158461676559928, 0.7904665899488785, 0.9199440403895656, 0.6341247058119213, 0.8274985308226857, 0.3788321486588963, 0.8899407308130634, 0.3205070115741946, 0.6316733772548883, 0.6620374200206556, 0.8295509438760991, 0.8254084390841356, 0.8028172817807042, 0.9093967005232582, 0.8937343878768174, 0.8772101276126214, 0.8096645620006775, 0.6930827608711253, 0.9199440403895656, 0.5448048686005305, 0.5076904928072853, 0.8579593190519532, 0.04497272878124432, 0.8438720692633894, 0.595153561733246, 0.7756810045762287, 0.46390994156004267, 0.4898588856009749]
Finish training and take 30m

***** Running evaluation *****
  Num examples = 63
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00351
  global_step = 316
  train_loss = 1.3601
  ********************
Previous best ppl:1.00336
BLEU file: ./data/defects4j/validation.jsonl
  codebleu-4 = 75.19 	 Previous best codebleu 78.72
  ********************
reload model from defects4j/finetune_gptneo/checkpoint-best-bleu
BLEU file: ./data/defects4j/test.jsonl
  codebleu = 73.7 
  Total = 64 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 64 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 73.7 
[0.7977509497997374, 0.187138788372509, 0.7657110295787897, 0.8870317711504849, 0.8883667838108272, 0.6007163503048495, 0.6576252123917807, 0.8681777725936917, 0.7051146102158162, 0.8650423106205597, 0.7143513682140774, 0.6451687939127737, 0.8721125152975062, 0.877572165029932, 0.8866626937665041, 0.7369092647119393, 0.9032703781902429, 0.9228950337188102, 0.9617720754477719, 0.8459463132957461, 0.7457367319180978, 0.6897393400904446, 0.7906925479531332, 0.8693059718827794, 0.8734502199950254, 0.863021098885395, 0.2862125618794039, 0.7694729472672909, 0.6410378892354339, 0.8899407308130634, 0.7869230583782949, 0.5470596690445572, 0.7176189204658862, 0.8232523732318835, 0.8727340070600411, 0.8575737152700653, 0.8158461676559928, 0.7904665899488785, 0.9199440403895656, 0.6341247058119213, 0.8274985308226857, 0.3788321486588963, 0.8899407308130634, 0.3205070115741946, 0.6316733772548883, 0.6620374200206556, 0.8295509438760991, 0.8254084390841356, 0.8028172817807042, 0.9093967005232582, 0.8937343878768174, 0.8772101276126214, 0.8096645620006775, 0.6930827608711253, 0.9199440403895656, 0.5448048686005305, 0.5076904928072853, 0.8579593190519532, 0.04497272878124432, 0.8438720692633894, 0.595153561733246, 0.7756810045762287, 0.46390994156004267, 0.4898588856009749]
Finish training and take 29m
