class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {                 self.obj._get_axis_name(self.axis): index,                 "copy": False,                 "fill_value": fill_value,             }              return output.reindex(**d)
class HiveCommandClient(HiveClient):          if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table.lower() in stdout          else:
class BinGrouper(BaseGrouper):              ngroups,          )     @cache_readonly     def recons_codes(self):         return [np.r_[0, np.flatnonzero(self.bins[1:] != self.bins[:-1]) + 1]]      @cache_readonly      def result_index(self):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         dtypes = list(self.dtypes)         if self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0]):             dtype = dtypes[0]             arr_type = dtype.construct_array_type()             values = self.values             new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]             result = self._constructor(                 dict(zip(self.index, new_values)), index=self.columns             )         else:             new_values = self.values.T             if copy:                 new_values = new_values.copy()             result = self._constructor(                 new_values, index=self.columns, columns=self.index             )         return result.__finalize__(self)      T = property(transpose)
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=True, file_okay=True, dir_okay=False, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml,
class Sanic:                  if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].appendleft(middleware)      def middleware(self, middleware_or_request):
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 if is_sequence:                     output_generator = iter(generator)                 else:                     output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     if dilation_rate == (1, 1):         x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                    padding=padding,                                    data_format=tf_data_format)     else:         assert dilation_rate[0] == dilation_rate[1]         x = tf.nn.atrous_conv2d_transpose(             x, kernel, output_shape, dilation_rate[0], padding)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x
class XportReader(abc.Iterator):          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             self.filepath_or_buffer = filepath_or_buffer          self._read_header()
class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         if self.limit:             while len(self) >= self.limit:                 self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value)
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop)              return task     def start_serving(self):         if self.server:             try:                 return self.server.start_serving()             except AttributeError:                 raise NotImplementedError(                     "server.start_serving not available in this version "                     "of asyncio or uvloop."                 )     def serve_forever(self):         if self.server:             try:                 return self.server.serve_forever()             except AttributeError:                 raise NotImplementedError(                     "server.serve_forever not available in this version "                     "of asyncio or uvloop."                 )      def __await__(self):          task = asyncio.ensure_future(self.serve_coro)
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None     kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)      if output_padding is None:          if padding == 'valid':
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(val_data)                     validation_steps = validation_steps or len(val_data)                  else:                      val_enqueuer_gen = val_data              else:
class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path) or is_gcs_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]             if self.image_data_generator.preprocessing_function:                 x = self.image_data_generator.preprocessing_function(x)              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
class HTTPRequest(HTTPMessage):          )          headers = dict(self._orig.headers)         if 'Host' not in self._orig.headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value)
class TestProcessProtocol(protocol.ProcessProtocol):      def __init__(self):          self.deferred = defer.Deferred()         self.out = b''         self.err = b''          self.exitcode = None      def outReceived(self, data):
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         inf = np.isinf(values)         if inf.any():             values = np.where(inf, np.nan, values)          return values
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseUnaryOpsTests(BaseOpsUtil):     def test_invert(self, data):         s = pd.Series(data, name="name")         result = ~s         expected = pd.Series(~data, name="name")         self.assert_series_equal(result, expected)
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)          for task in tasks:             in_workers = (assistant and task.workers) or worker in task.workers              if task.status == 'RUNNING' and in_workers:
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     exclude_defaults=exclude_defaults,                     exclude_none=exclude_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',', ' ')     pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0      timezone, date_str = extract_timezone(date_str)
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.type, failure.value, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=encoding, newline=newline) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)"
def should_series_dispatch(left, right, op):          return True     if (is_datetime64_dtype(ldtype) and is_object_dtype(rdtype)) or (         is_datetime64_dtype(rdtype) and is_object_dtype(ldtype)     ):          return True      return False
def run_script(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     exit_status = proc.wait()     if exit_status != EXIT_SUCCESS:         raise FailedHookException(             "Hook script failed (exit status: %d)" % exit_status)  def run_script_with_context(script_path, cwd, context):
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             exc_info=failure_to_exc_info(f),                                             extra={'spider': spider}))          return d      def spider_is_idle(self, spider):
class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         full_info = formats_dict.get(representation_id, {}).copy()                         full_info.update(f)                         formats.append(full_info)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                     response_model_exclude_defaults=route.response_model_exclude_defaults,                     response_model_exclude_none=route.response_model_exclude_none,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,
def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",         "skew",          "kurt",          "quantile",      }
class BarPlot(MPLPlot):      def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))         if self.xticks is not None:             ax.set_xticks(np.array(self.xticks))         else:             ax.set_xticks(self.tick_pos)             ax.set_xticklabels(ticklabels)          if name is not None and self.use_index:              ax.set_xlabel(name)
def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = None      if len(cmds) >= 3:          machine = cmds[2]     startAllInstances = shells.and_("vagrant up", command.script)     if machine is None:          return startAllInstances     else:         return [ shells.and_("vagrant up " +  machine, command.script), startAllInstances]
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],                                      self.tasks)          else:             return six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())      def is_trivial_worker(self, state):
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1 and index:
def create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config__          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = create_cloned_field(f)          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head not in ["0", "_"] else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag
class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1]          assert data[0] == data[1]     def test_transpose(self, data):         df = pd.DataFrame({"A": data[:4], "B": data[:4]}, index=["a", "b", "c", "d"])         result = df.T         expected = pd.DataFrame(             {                 "a": type(data)._from_sequence([data[0]] * 2, dtype=data.dtype),                 "b": type(data)._from_sequence([data[1]] * 2, dtype=data.dtype),                 "c": type(data)._from_sequence([data[2]] * 2, dtype=data.dtype),                 "d": type(data)._from_sequence([data[3]] * 2, dtype=data.dtype),             },             index=["A", "B"],         )         self.assert_frame_equal(result, expected)         self.assert_frame_equal(np.transpose(np.transpose(df)), df)         self.assert_frame_equal(np.transpose(np.transpose(df[["A"]])), df[["A"]])
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '!!not-set!!')))          print(self.script)
def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             if not code.startswith('__'):                 msg = getattr(err_cls, code)                 return "[{code}] {msg}".format(code=code, msg=msg)             else:                 return super().__getattribute__(code)      return ErrorsWithCodes()
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(values, result)              if post_processing:                  result = post_processing(result, inferences)
def _isna_ndarraylike_old(obj):      return result def _isna_string_dtype(values: np.ndarray, dtype: np.dtype, old: bool) -> np.ndarray:     shape = values.shape     if is_string_like_dtype(dtype):         result = np.zeros(values.shape, dtype=bool)     else:         result = np.empty(shape, dtype=bool)         if old:             vec = libmissing.isnaobj_old(values.ravel())         else:             vec = libmissing.isnaobj(values.ravel())         result[...] = vec.reshape(shape)     return result  def notna(obj):      Detect non-missing values for an array-like object.
class _LocIndexer(_LocationIndexer):          if isinstance(labels, MultiIndex):             if (                 isinstance(key, str)                 and labels.levels[0]._supports_partial_string_indexing             ):                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))
class BracketTracker:         if (             self._lambda_argument_depths             and self._lambda_argument_depths[-1] == self.depth             and leaf.type == token.COLON         ):              self.depth -= 1             self._lambda_argument_depths.pop()              return True          return False
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):          if isinstance(maybe_slice, slice):              return self[maybe_slice]         return ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )      _can_hold_na = True      _na_value = NaT
def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return False  def _isna_old(obj):
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")         dti._set_freq("infer")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):             v = re.sub(r"\\'", "'", v[1:-1])         elif v.startswith("'"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\',
class Model(BaseModel):  class ModelSubclass(Model):      y: int     z: int = 0     w: int = None class ModelDefaults(BaseModel):     w: Optional[str] = None     x: Optional[str] = None     y: str = "y"     z: str = "z"  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1, z=0) @app.get(     "/exclude_unset", response_model=ModelDefaults, response_model_exclude_unset=True ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y") @app.get(     "/exclude_defaults",     response_model=ModelDefaults,     response_model_exclude_defaults=True, ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y") @app.get(     "/exclude_none", response_model=ModelDefaults, response_model_exclude_none=True ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y") @app.get(     "/exclude_unset_none",     response_model=ModelDefaults,     response_model_exclude_unset=True,     response_model_exclude_none=True, ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y")  client = TestClient(app)
default 'raise'              )          new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         freq = None         if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates[0])):             freq = self.freq         elif tz is None and self.tz is None:             freq = self.freq         return self._simple_new(new_dates, dtype=dtype, freq=freq)
class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.     copy : bool, default False         If True, return a copy of the combined index.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)  def _get_distinct_objs(objs: List[Index]) -> List[Index]:
class QuarterOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)  class BQuarterEnd(QuarterOffset):
class ImageDataGenerator(object):              The inputs, normalized.          if self.rescale:              x *= self.rescale          if self.samplewise_center:
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 if opname == "__ne__":                     ret[(self._codes == -1) & (other_codes == -1)] = True                 else:                     ret[mask] = False              return ret          if is_scalar(other):
def match(command, settings):      return _search(command.stderr) or _search(command.stdout) @wrap_settings({'fixlinecmd': '{editor} {file} +{line}',                 'fixcolcmd': None})  def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     if settings.fixcolcmd and 'col' in m.groupdict():         editor_call = settings.fixcolcmd.format(editor=os.environ['EDITOR'],                                                 file=m.group('file'),                                                 line=m.group('line'),                                                 col=m.group('col'))     else:         editor_call = settings.fixlinecmd.format(editor=os.environ['EDITOR'],                                                  file=m.group('file'),                                                  line=m.group('line'))      return shells.and_(editor_call, command.script)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, "same" or "valid".          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format             in inputs/kernels/outputs.         dilation_rate: tuple of 2 integers.          ValueError: if using an even kernel size with padding 'same'.
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)      def start_requests(self):         cls = self.__class__         if cls.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; it "                 "won't be called in future Scrapy releases. Please "                 "override Spider.start_requests method instead (see %s.%s)." % (                     cls.__module__, cls.__name__                 ),              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:         executor = ProcessPoolExecutor(max_workers=worker_count)     except OSError:         executor = None      try:          loop.run_until_complete(              schedule_formatting(
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError, OverflowError):                  pass              asdt, reso = parse_time_string(key, self.freq)
def base_url(url):  def urljoin(base, path):     if isinstance(path, bytes):         path = path.decode('utf-8')      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if isinstance(base, bytes):         base = base.decode('utf-8')     if not isinstance(base, compat_str) or not re.match(             r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path)
class _LocIndexer(_LocationIndexer):              return self._getbool_axis(key, axis=axis)          elif is_list_like_indexer(key):              if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):
def print_tensor(x, message=''):          The same tensor `x`, unchanged.     op = tf.print(message, x, output_stream=sys.stdout)     with tf.control_dependencies([op]):         return tf.identity(x)
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)         exact_match = (             len(arr_value.shape)             and arr_value.shape[0] == values.shape[0]             and arr_value.size == values.size         )          if is_empty_indexer(indexer, arr_value):              pass
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = safe_url_string(response.headers['location'])          redirected_url = urljoin(request.url, location)
except ImportError:  def _prepare_response_content(     res: Any,     *,     by_alias: bool = True,     exclude_unset: bool,     exclude_defaults: bool = False,     exclude_none: bool = False,  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(                 by_alias=by_alias,                 exclude_unset=exclude_unset,                 exclude_defaults=exclude_defaults,                 exclude_none=exclude_none,             )          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset, )      elif isinstance(res, list):          return [             _prepare_response_content(                 item,                 exclude_unset=exclude_unset,                 exclude_defaults=exclude_defaults,                 exclude_none=exclude_none,             )             for item in res          ]      elif isinstance(res, dict):          return {             k: _prepare_response_content(                 v,                 exclude_unset=exclude_unset,                 exclude_defaults=exclude_defaults,                 exclude_none=exclude_none,             )              for k, v in res.items()          }      return res
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 mask = level == key                 if not mask.any():                     raise ValueError(f"Key {key} not in level {level}")                 i = np.nonzero(level == key)[0][0]                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True         if total is None and iterable is not None:             try:                 total = len(iterable)             except (TypeError, AttributeError):                 total = None          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial             self.total = total              return          if kwargs:
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             if return_indexers:                 return multi_join_idx, lidx, ridx             else:                 return multi_join_idx          jl = list(overlap)[0]
class CategoricalBlock(ExtensionBlock):      def _holder(self):          return Categorical     def should_store(self, arr: ArrayLike):         return isinstance(arr, self._holder) and is_dtype_equal(self.dtype, arr.dtype)      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data          enqueuer = None          try:
def melt(          else:              value_vars = list(value_vars)             missing = Index(com.flatten(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, var_list=params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)
class YoutubeIE(YoutubeBaseInfoExtractor):          video_id = mobj.group(2)          return video_id     def _extract_chapters_from_json(self, webpage, video_id, duration):         if not webpage:             return         player = self._parse_json(             self._search_regex(                 r'RELATED_PLAYER_ARGS["\']\s*:\s*({.+})\s*,?\s*\n', webpage,                 'player args', default='{}'),             video_id, fatal=False)         if not player or not isinstance(player, dict):             return         watch_next_response = player.get('watch_next_response')         if not isinstance(watch_next_response, compat_str):             return         response = self._parse_json(watch_next_response, video_id, fatal=False)         if not response or not isinstance(response, dict):             return         chapters_list = try_get(             response,             lambda x: x['playerOverlays']                        ['playerOverlayRenderer']                        ['decoratedPlayerBarRenderer']                        ['decoratedPlayerBarRenderer']                        ['playerBar']                        ['chapteredPlayerBarRenderer']                        ['chapters'],             list)         if not chapters_list:             return         def chapter_time(chapter):             return float_or_none(                 try_get(                     chapter,                     lambda x: x['chapterRenderer']['timeRangeStartMillis'],                     int),                 scale=1000)         chapters = []         for next_num, chapter in enumerate(chapters_list, start=1):             start_time = chapter_time(chapter)             if start_time is None:                 continue             end_time = (chapter_time(chapters_list[next_num])                         if next_num < len(chapters_list) else duration)             if end_time is None:                 continue             title = try_get(                 chapter, lambda x: x['chapterRenderer']['title']['simpleText'],                 compat_str)             chapters.append({                 'start_time': start_time,                 'end_time': end_time,                 'title': title,             })         return chapters      @staticmethod     def _extract_chapters_from_description(description, duration):          if not description:              return None          chapter_lines = re.findall(
class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class         if classcell is not None:             new_attrs['__classcell__'] = classcell          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
class ListParameter(Parameter):         Ensure that struct is recursively converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index._values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name)     @classmethod     def _validate_dtype(cls, dtype: Dtype) -> None:         if dtype is None:             return         validation_metadata = {             "int64index": (is_signed_integer_dtype, "signed integer"),             "uint64index": (is_unsigned_integer_dtype, "unsigned integer"),             "float64index": (is_float_dtype, "float"),             "rangeindex": (is_signed_integer_dtype, "signed integer"),         }         validation_func, expected = validation_metadata[cls._typ]         if not validation_func(dtype):             msg = f"Incorrect `dtype` passed: expected {expected}, received {dtype}"             raise ValueError(msg)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]
def _urlencode(seq, enc):  def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)
class Request(object_ref):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ('://' not in self._url) and (not self._url.startswith('data:')):              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))
class EmptyLineTracker:                  return 0, 0             if is_decorator and self.previous_line and self.previous_line.is_comment:                 return 0, 0              newlines = 2              if current_line.depth:                  newlines -= 1
class DataFrame(NDFrame):          return new_data     def _combine_match_index(self, other: Series, func):          if ops.should_series_dispatch(self, other, func):
def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length]     ansi_present = bool(RE_ANSI.search(data)) while disp_len(data) > length:          data = data[:-1]     if ansi_present and bool(RE_ANSI.search(data)):         return data if data.endswith("\033[0m") else data + "\033[0m"      return data
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if v < val else v - 1 for v in clocs]              return result
def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     exclude_none=exclude_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = to_text(manifest['version'], errors='surrogate_or_strict')             if not hasattr(LooseVersion(version), 'version'):                 display.warning("Collection at '%s' does not have a valid version set, falling back to '*'. Found "                                 "version: '%s'" % (to_text(b_path), version))                 version = '*'              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version."
class LinuxHardware(Hardware):              mtab_entries.append(fields)          return mtab_entries     @staticmethod     def _replace_octal_escapes_helper(match):         return chr(int(match.group()[1:], 8))     def _replace_octal_escapes(self, value):         return self.OCTAL_ESCAPE_RE.sub(self._replace_octal_escapes_helper, value)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")         if not len(self._codes):             return self.dtype.na_value          good = self._codes != -1          if not good.all():              if skipna:
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_MANAGER_CLASS',                             settings.get('SPIDER_LOADER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())
logger = logging.getLogger(__name__)  title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_tag_regex = re.compile(r"(?<=<text).*?(?=>)") text_regex = re.compile(r"(?<=<text>).*(?=</text)")  info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,             exclude_defaults=exclude_defaults,             exclude_none=exclude_none,          )      else:          return jsonable_encoder(response_content)
class WebSocketHandler(tornado.web.RequestHandler):          .. versionadded:: 3.1         assert self.ws_connection is not None         self.ws_connection.set_nodelay(value)      def on_connection_close(self) -> None:          if self.ws_connection:
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         if 'KERAS_HOME' in os.environ:             cache_dir = os.environ.get('KERAS_HOME')         else:             cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5'
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     if not (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     ):         return False     if field.sub_fields:         if not all(is_scalar_field(f) for f in field.sub_fields):             return False     return True  def is_scalar_sequence_field(field: Field) -> bool:
def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["ever_failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}     set_tasks["failed"] = set_tasks["ever_failed"] - set_tasks["completed"]      set_tasks["scheduling_error"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}      set_tasks["still_pending_ext"] = {task for (task, status, ext) in task_history                                       if status == 'PENDING' and task not in set_tasks["ever_failed"] and task not in set_tasks["completed"] and not ext}      set_tasks["still_pending_not_ext"] = {task for (task, status, ext) in task_history                                           if status == 'PENDING' and task not in set_tasks["ever_failed"] and task not in set_tasks["completed"] and ext}      set_tasks["run_by_other_worker"] = set()      set_tasks["upstream_failure"] = set()      set_tasks["upstream_missing_dependency"] = set()
class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_task_value(param_name=param_name, task_name=task_name):                 return self.task_value(param_name=param_name, task_name=task_name)              elif self.is_bool:                  return False              elif self.is_list:
class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value: ArrayLike) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1          if not good.all():             if skipna and good.any():                  pointer = self._codes[good].min()              else:                  return np.nan
class RedirectMiddleware(BaseRedirectMiddleware):      def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in request.meta.get('handle_httpstatus_list', []) or                request.meta.get('handle_httpstatus_all', False)):              return response          if request.method == 'HEAD':
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         try:             command.script_parts.pop(upstream_option_index)         except IndexError:             pass      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts["2007"]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     use_sequence_api = is_sequence(generator)     if not use_sequence_api and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps is None:         if use_sequence_api:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator'
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         if callable(index) or is_dict_like(index):             return super().rename(index=index, **kwargs)         else:              return self._set_name(index, inplace=kwargs.get("inplace"))      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()         del IOLoop._ioloop_for_asyncio[self.asyncio_loop]      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block         if convert:             return [self.convert(numeric=False, copy=True)]          return self
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!         .. versionchanged:: 1.0.0            Returns an NA value on empty arrays          Raises          ------          TypeError
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             exclude_none=True,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape)     if data_format == 'channels_first' and dilation_rate != (1, 1):         force_transpose = True     else:         force_transpose = False     x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],
