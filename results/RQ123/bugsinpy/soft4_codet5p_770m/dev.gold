class APIRoute(routing.Route):          self.response_model_exclude = response_model_exclude          self.response_model_by_alias = response_model_by_alias          self.response_model_exclude_unset = response_model_exclude_unset         self.response_model_exclude_defaults = response_model_exclude_defaults         self.response_model_exclude_none = response_model_exclude_none          self.include_in_schema = include_in_schema          self.response_class = response_class
class Fish(Generic):      def info(self):         proc = Popen(['fish', '--version'],                       stdout=PIPE, stderr=DEVNULL)         version = proc.stdout.read().decode('utf-8').split()[-1]          return u'Fish Shell {}'.format(version)      def put_to_history(self, command):
def read_pickle(path, compression="infer"):      >>> import os      >>> os.remove("./dummy.pkl")     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(         filepath_or_buffer, compression=compression     )     if not isinstance(fp_or_buf, str) and compression == "infer":         compression = None     f, fh = get_handle(fp_or_buf, "rb", compression=compression, is_text=False)
class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph'))     prune_on_get_work = parameter.BoolParameter(default=False)  def fix_time(x):
patterns = (          '^lua: {file}:{line}:',         '^{file} \\(line {line}\\):',          '^{file}: line {line}: ',          '^{file}:{line}:{col}',         '^{file}:{line}:',          'at {file} line {line}',      )
class FeedExporter(object):          d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                              extra={'spider': spider}))          d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                             exc_info=failure_to_exc_info(f),                                             extra={'spider': spider}))          return d      def item_scraped(self, item, spider):
def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if is_sequence(val_data):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing)
default: :rc:`scatter.edgecolors`              - 'none': No patch boundary will be drawn.              - A color or sequence of colors.             For non-filled markers, *edgecolors* is ignored. Instead, the color             is determined like with 'face', i.e. from *c*, *colors*, or             *facecolors*.          plotnonfinite : bool, default: False              Set to plot points with nonfinite *c*, in conjunction with
def update_add(x, increment):          The variable `x` updated.     op = tf_state_ops.assign_add(x, increment)     with tf.control_dependencies([op]):         return tf.identity(x)  @symbolic
def js_to_json(code):              ([{,]\s*)              ("[^"]*"|\'[^\']*\'|[a-z0-9A-Z]+)              (:\s*)             ([0-9.]+|true|false|"[^"]*"|\'[^\']*\'|                 (?=\[|\{)             )      res = re.sub(r',(\s*\])', lambda m: m.group(1), res)      return res
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class LinuxHardware(Hardware):          pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))          maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT          for fields in mtab_entries:             fields = [self._replace_octal_escapes(field) for field in fields]              device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]
def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement      if src_txt[-1] != "\n":         src_txt += "\n"      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)          try:
def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                 exclude_defaults=response_model_exclude_defaults,                 exclude_none=response_model_exclude_none,                  is_coroutine=is_coroutine,              )              response = response_class(
def format_stdin_to_stdout(      `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to      :func:`format_file_contents`.     newline, encoding, src = prepare_input(sys.stdin.buffer.read())      dst = src      try:          dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
def srt_subtitles_timecode(seconds):  def dfxp2srt(dfxp_data):      LEGACY_NAMESPACES = ( (b'http: b'http: b'http: b'http:          ]), (b'http: b'http:          ]),      )
def match(command):  @git_support  def get_new_command(command):     upstream_option_index = -1     try:         upstream_option_index = command.script_parts.index('--set-upstream')     except ValueError:         pass     try:         upstream_option_index = command.script_parts.index('-u')     except ValueError:         pass     if upstream_option_index is not -1:         command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]     return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
import threading  import time  import traceback  import math from tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback  from tornado.log import app_log, gen_log
def _isna_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return False  _isna = _isna_new
def na_value_for_dtype(dtype, compat: bool = True):      if is_extension_array_dtype(dtype):          return dtype.na_value     if needs_i8_conversion(dtype):          return NaT      elif is_float_dtype(dtype):          return np.nan
class Sequential(Model):                                               use_multiprocessing=use_multiprocessing)      @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps=None,                            max_queue_size=10, workers=1,                            use_multiprocessing=False, verbose=0):
def assert_series_equal(          Compare datetime-like which is comparable ignoring dtype.      check_categorical : bool, default True          Whether to compare internal Categorical exactly.     check_category_order : bool, default True         Whether to compare category order of internal Categoricals         .. versionadded:: 1.0.2      obj : str, default 'Series'          Specify object name being compared, internally used to show appropriate          assertion message.
setup(              "options_test.cfg",              "static/robots.txt",              "static/dir/index.html",             "static_foo.txt",              "templates/utf8.html",              "test.crt",              "test.key",
def unescapeHTML(s):      assert type(s) == compat_str      return re.sub(         r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)  def get_subprocess_encoding():
def data(dtype):      return pd.array(make_data(), dtype=dtype) @pytest.fixture def data_for_twos(dtype):     return pd.array(np.ones(100), dtype=dtype) @pytest.fixture def data_missing(dtype):     return pd.array([np.nan, True], dtype=dtype) @pytest.fixture def data_for_sorting(dtype):     return pd.array([True, True, False], dtype=dtype) @pytest.fixture def data_missing_for_sorting(dtype):     return pd.array([True, np.nan, False], dtype=dtype) @pytest.fixture def na_cmp():     return lambda x, y: x is pd.NA and y is pd.NA @pytest.fixture def na_value():     return pd.NA @pytest.fixture def data_for_grouping(dtype):     b = True     a = False     na = np.nan     return pd.array([b, b, na, na, a, a, b], dtype=dtype) class TestDtype(base.BaseDtypeTests):     pass class TestInterface(base.BaseInterfaceTests):     pass class TestConstructors(base.BaseConstructorsTests):     pass class TestGetitem(base.BaseGetitemTests):     pass class TestSetitem(base.BaseSetitemTests):     pass class TestMissing(base.BaseMissingTests):     pass class TestArithmeticOps(base.BaseArithmeticOpsTests):     def check_opname(self, s, op_name, other, exc=None):         super().check_opname(s, op_name, other, exc=None)     def _check_op(self, s, op, other, op_name, exc=NotImplementedError):         if exc is None:             if op_name in ("__sub__", "__rsub__"):                 if _np_version_under1p14:                     pytest.skip("__sub__ does not yet raise in numpy 1.13")                 with pytest.raises(TypeError):                     op(s, other)                 return             result = op(s, other)             expected = s.combine(other, op)             if op_name in (                 "__floordiv__",                 "__rfloordiv__",                 "__pow__",                 "__rpow__",                 "__mod__",                 "__rmod__",             ):                 expected = expected.astype("Int8")             elif op_name in ("__truediv__", "__rtruediv__"):                 expected = s.astype(float).combine(other, op)             if op_name == "__rpow__":                 expected[result.isna()] = np.nan             self.assert_series_equal(result, expected)         else:             with pytest.raises(exc):                 op(s, other)     def _check_divmod_op(self, s, op, other, exc=None):         super()._check_divmod_op(s, op, other, None)     @pytest.mark.skip(reason="BooleanArray does not error on ops")     def test_error(self, data, all_arithmetic_operators):         pass class TestComparisonOps(base.BaseComparisonOpsTests):     def check_opname(self, s, op_name, other, exc=None):         super().check_opname(s, op_name, other, exc=None)     def _compare_other(self, s, data, op_name, other):         self.check_opname(s, op_name, other)     @pytest.mark.skip(reason="Tested in tests/arrays/test_boolean.py")      def test_compare_scalar(self, data, all_compare_operators):         pass     @pytest.mark.skip(reason="Tested in tests/arrays/test_boolean.py")      def test_compare_array(self, data, all_compare_operators):         pass class TestReshaping(base.BaseReshapingTests):     pass class TestMethods(base.BaseMethodsTests):     @pytest.mark.parametrize("na_sentinel", [-1, -2])     def test_factorize(self, data_for_grouping, na_sentinel):         labels, uniques = pd.factorize(data_for_grouping, na_sentinel=na_sentinel)         expected_labels = np.array(             [0, 0, na_sentinel, na_sentinel, 1, 1, 0], dtype=np.intp         )         expected_uniques = data_for_grouping.take([0, 4])         tm.assert_numpy_array_equal(labels, expected_labels)         self.assert_extension_array_equal(uniques, expected_uniques)     def test_combine_le(self, data_repeated):         orig_data1, orig_data2 = data_repeated(2)         s1 = pd.Series(orig_data1)         s2 = pd.Series(orig_data2)         result = s1.combine(s2, lambda x1, x2: x1 <= x2)         expected = pd.Series(             [a <= b for (a, b) in zip(list(orig_data1), list(orig_data2))],             dtype="boolean",          )         self.assert_series_equal(result, expected)         val = s1.iloc[0]         result = s1.combine(val, lambda x1, x2: x1 <= x2)         expected = pd.Series([a <= val for a in list(orig_data1)], dtype="boolean")         self.assert_series_equal(result, expected)     def test_searchsorted(self, data_for_sorting, as_series):         data_for_sorting = pd.array([True, False], dtype="boolean")         b, a = data_for_sorting         arr = type(data_for_sorting)._from_sequence([a, b])         if as_series:             arr = pd.Series(arr)         assert arr.searchsorted(a) == 0         assert arr.searchsorted(a, side="right") == 1         assert arr.searchsorted(b) == 1         assert arr.searchsorted(b, side="right") == 2         result = arr.searchsorted(arr.take([0, 1]))         expected = np.array([0, 1], dtype=np.intp)         tm.assert_numpy_array_equal(result, expected)         sorter = np.array([1, 0])         assert data_for_sorting.searchsorted(a, sorter=sorter) == 0     @pytest.mark.skip(reason="uses nullable integer")     def test_value_counts(self, all_data, dropna):         return super().test_value_counts(all_data, dropna) class TestCasting(base.BaseCastingTests):     pass class TestGroupby(base.BaseGroupbyTests):     def test_grouping_grouper(self, data_for_grouping):         df = pd.DataFrame(             {"A": ["B", "B", None, None, "A", "A", "B"], "B": data_for_grouping}         )         gr1 = df.groupby("A").grouper.groupings[0]         gr2 = df.groupby("B").grouper.groupings[0]         tm.assert_numpy_array_equal(gr1.grouper, df.A.values)         tm.assert_extension_array_equal(gr2.grouper, data_for_grouping)     @pytest.mark.parametrize("as_index", [True, False])     def test_groupby_extension_agg(self, as_index, data_for_grouping):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         result = df.groupby("B", as_index=as_index).A.mean()         _, index = pd.factorize(data_for_grouping, sort=True)         index = pd.Index(index, name="B")         expected = pd.Series([3, 1], index=index, name="A")         if as_index:             self.assert_series_equal(result, expected)         else:             expected = expected.reset_index()             self.assert_frame_equal(result, expected)     def test_groupby_extension_no_sort(self, data_for_grouping):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         result = df.groupby("B", sort=False).A.mean()         _, index = pd.factorize(data_for_grouping, sort=False)         index = pd.Index(index, name="B")         expected = pd.Series([1, 3], index=index, name="A")         self.assert_series_equal(result, expected)     def test_groupby_extension_transform(self, data_for_grouping):         valid = data_for_grouping[~data_for_grouping.isna()]         df = pd.DataFrame({"A": [1, 1, 3, 3, 1], "B": valid})         result = df.groupby("B").A.transform(len)         expected = pd.Series([3, 3, 2, 2, 3], name="A")         self.assert_series_equal(result, expected)     def test_groupby_extension_apply(self, data_for_grouping, groupby_apply_op):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         df.groupby("B").apply(groupby_apply_op)         df.groupby("B").A.apply(groupby_apply_op)         df.groupby("A").apply(groupby_apply_op)         df.groupby("A").B.apply(groupby_apply_op)     def test_groupby_apply_identity(self, data_for_grouping):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         result = df.groupby("A").B.apply(lambda x: x.array)         expected = pd.Series(             [                 df.B.iloc[[0, 1, 6]].array,                 df.B.iloc[[2, 3]].array,                 df.B.iloc[[4, 5]].array,             ],             index=pd.Index([1, 2, 3], name="A"),             name="B",          )         self.assert_series_equal(result, expected)     def test_in_numeric_groupby(self, data_for_grouping):         df = pd.DataFrame(             {                 "A": [1, 1, 2, 2, 3, 3, 1],                 "B": data_for_grouping,                 "C": [1, 1, 1, 1, 1, 1, 1],             }          )         result = df.groupby("A").sum().columns         if data_for_grouping.dtype._is_numeric:             expected = pd.Index(["B", "C"])         else:             expected = pd.Index(["C"])         tm.assert_index_equal(result, expected) class TestNumericReduce(base.BaseNumericReduceTests):     def check_reduce(self, s, op_name, skipna):         result = getattr(s, op_name)(skipna=skipna)         expected = getattr(s.astype("float64"), op_name)(skipna=skipna)         if np.isnan(expected):             expected = pd.NA         elif op_name in ("min", "max"):             expected = bool(expected)         tm.assert_almost_equal(result, expected) class TestBooleanReduce(base.BaseBooleanReduceTests):     pass class TestPrinting(base.BasePrintingTests):     pass class TestUnaryOps(base.BaseUnaryOpsTests):     pass
from pandas.core.dtypes.generic import ABCSeries  from pandas.core.dtypes.missing import isna  from pandas._typing import AnyArrayLike from pandas.core.algorithms import take_1d  from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs  import pandas.core.common as com  import pandas.core.indexes.base as ibase
def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if use_sequence_api:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
def dispatch_to_series(left, right, func, str_rep=None, axis=None):          assert right.index.equals(left.columns)         if right.dtype == "timedelta64[ns]":             right = np.asarray(right)             def column_op(a, b):                 return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}         else:             def column_op(a, b):                 return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}      elif isinstance(right, ABCSeries): assert right.index.equals(left.index)
def conv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) * stride - 2 * pad + filter_size def deconv_length(dim_size, stride_size, kernel_size, padding,                   output_padding, dilation=1):
class TestSeriesAnalytics:          assert s.is_monotonic is False          assert s.is_monotonic_decreasing is True      @pytest.mark.parametrize("func", [np.any, np.all])      @pytest.mark.parametrize("kwargs", [dict(keepdims=True), dict(out=object())])      @td.skip_if_np_lt("1.15")
from keras.utils.data_utils import validate_file  from keras import backend as K  pytestmark = pytest.mark.skipif(     K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,      reason='Temporarily disabled until the use_multiprocessing problem is solved')  if sys.version_info < (3,):
def uppercase_escape(s):      return re.sub(          r'\\U([0-9a-fA-F]{8})',          lambda m: compat_chr(int(m.group(1), base=16)), s) try:     struct.pack(u'!I', 0) except TypeError:     def struct_pack(spec, *args):         if isinstance(spec, compat_str):             spec = spec.encode('ascii')         return struct.pack(spec, *args)     def struct_unpack(spec, *args):         if isinstance(spec, compat_str):             spec = spec.encode('ascii')         return struct.unpack(spec, *args) else:     struct_pack = struct.pack     struct_unpack = struct.unpack
import logging  from six.moves.urllib.parse import urljoin from w3lib.url import safe_url_string  from scrapy.http import HtmlResponse  from scrapy.utils.response import get_meta_refresh  from scrapy.exceptions import IgnoreRequest, NotConfigured  logger = logging.getLogger(__name__)
class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if kind == "loc":             try:                 return self.categories._convert_scalar_indexer(key, kind=kind)             except TypeError:                 self._invalid_indexer("label", key)          return super()._convert_scalar_indexer(key, kind=kind)      @Appender(_index_shared_docs["_convert_list_indexer"])
class Conv2DTranspose(Conv2D):          out_height = conv_utils.deconv_length(height,                                                stride_h, kernel_h,                                                self.padding,                                               out_pad_h,                                               self.dilation_rate[0])          out_width = conv_utils.deconv_length(width,                                               stride_w, kernel_w,                                               self.padding,                                              out_pad_w,                                              self.dilation_rate[1])          if self.data_format == 'channels_first':              output_shape = (batch_size, self.filters, out_height, out_width)          else:
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]      if existing and not collection_info.force:         existing[0].add_requirement(parent, requirement)          collection_info = existing[0]      dep_map[to_text(collection_info)] = collection_info
class CannotSplit(Exception):  class WriteBack(Enum):      NO = 0      YES = 1
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0, base=10),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
class Model(Container):              validation_steps: Only relevant if `validation_data`                  is a generator. Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Optional for `Sequence`: if unspecified, will use                 the `len(validation_data)` as a number of steps.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Integer. Maximum size for the generator queue.
class MultiIndex(Index):                      indexer = self._get_level_indexer(key, level=level)                      new_index = maybe_mi_droplevels(indexer, [0], drop_level)                      return indexer, new_index             except (TypeError, InvalidIndexError):                  pass              if not any(isinstance(k, slice) for k in key):
def whitespace(leaf: Leaf) -> str:          ):              return NO         elif (             prevp.type == token.RIGHTSHIFT             and prevp.parent             and prevp.parent.type == syms.shift_expr             and prevp.prev_sibling             and prevp.prev_sibling.type == token.NAME             and prevp.prev_sibling.value == 'print'         ):             return NO      elif prev.type in OPENING_BRACKETS:          return NO
class BlockManager(PandasObject):                          convert=convert,                          regex=regex,                      )                     if m.any() or convert:                          new_rb = _extend_blocks(result, new_rb)                      else:                          new_rb.append(b)
class CollectionRequirement:                  requirement = req                  op = operator.eq             if parent and version == '*' and requirement != '*':                 display.warning("Failed to validate the collection requirement '%s:%s' for %s when the existing "                                 "install does not have a version set, the collection may not work."                                 % (to_text(self), req, parent))                 continue             elif requirement == '*' or version == '*':                 continue              if not op(LooseVersion(version), LooseVersion(requirement)):                  break
def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return      mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)      if mobj:
def get_elements_by_attribute(attribute, value, html, escape_value=True):      retlist = []          <([a-zA-Z0-9:._-]+)          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'|))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'|))*?          \s*>          (?P<content>.*?)          </\1>
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              return self         new_data = self._data.apply(operator.invert)         result = self._constructor(new_data).__finalize__(self)         return result      def __nonzero__(self):          raise ValueError(
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class MailSender(object):              msg = MIMEMultipart()          else:              msg = MIMENonMultipart(*mimetype.split('/', 1))         to = list(arg_to_iter(to))         cc = list(arg_to_iter(cc))          msg['From'] = self.mailfrom          msg['To'] = COMMASPACE.join(to)          msg['Date'] = formatdate(localtime=True)
class PagedList(object):  def uppercase_escape(s):     unicode_escape = codecs.getdecoder('unicode_escape')      return re.sub(          r'\\U[0-9a-fA-F]{8}',         lambda m: unicode_escape(m.group(0))[0],         s)  try:      struct.pack(u'!I', 0)
class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         if self.run_metadata:             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)         else:             fetched = self._callable_fn(*array_vals)          return fetched[:len(self.outputs)]      def _legacy_call(self, inputs):
class CSVLogger(Callback):          if not self.writer:              class CustomDialect(csv.excel):                  delimiter = self.sep             fieldnames = ['epoch'] + self.keys             if six.PY2:                 fieldnames = [unicode(x) for x in fieldnames]              self.writer = csv.DictWriter(self.csv_file,                                          fieldnames=fieldnames,                                          dialect=CustomDialect)              if self.append_header:                  self.writer.writeheader()
default: :rc:`scatter.edgecolors`          collection = mcoll.PathCollection(                  (path,), scales,                 facecolors=colors if marker_obj.is_filled() else 'none',                 edgecolors=edgecolors if marker_obj.is_filled() else colors,                  linewidths=linewidths,                  offsets=offsets,                  transOffset=kwargs.pop('transform', self.transData),
def standardize_weights(y,      Everything gets normalized to a single sample-wise (or timestep-wise)     weight array. If both `sample_weights` and `class_weights` are provided,     the weights are multiplied together.          y: Numpy array of model targets to be weighted.
class S3CopyToTable(rdbms.CopyToTable):          if '.' in self.table:              query = ("select 1 as table_exists "                       "from information_schema.tables "                      "where table_schema = lower(%s) and table_name = lower(%s) limit 1")          else:              query = ("select 1 as table_exists "                       "from pg_table_def "                      "where tablename = lower(%s) limit 1")          cursor = connection.cursor()          try:              cursor.execute(query, tuple(self.table.split('.')))
TEST_MODULES = [      'tornado.test.curl_httpclient_test',      'tornado.test.escape_test',      'tornado.test.gen_test',     'tornado.test.http1connection_test',      'tornado.test.httpclient_test',      'tornado.test.httpserver_test',      'tornado.test.httputil_test',
class BaseGrouper:              if mask.any():                  result = result.astype("float64")                  result[mask] = np.nan         elif (             how == "add"             and is_integer_dtype(orig_values.dtype)             and is_extension_array_dtype(orig_values.dtype)         ):             result = result.astype("int64")          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2
class _AxesBase(martist.Artist):              if right is None:                  right = old_right         if self.get_xscale() == 'log' and (left <= 0 or right <= 0):             old_left, old_right = self.get_xlim()              if left <= 0:                  cbook._warn_external(                      'Attempted to set non-positive left xlim on a '
class StackedRNNCells(Layer):                                   '`state_size` attribute. '                                   'received cells:', cells)          self.cells = cells         self.reverse_state_order = kwargs.pop('reverse_state_order', False)         if self.reverse_state_order:             warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '                           'will soon be deprecated. Please update the code to '                           'work with the natural order of states if you '                           'reply on the RNN states, '                           'eg `RNN(return_state=True)`.')          super(StackedRNNCells, self).__init__(**kwargs)      @property      def state_size(self):          state_size = []         for cell in self.cells[::-1] if self.reverse_state_order else self.cells:              if hasattr(cell.state_size, '__len__'):                  state_size += list(cell.state_size)              else:                  state_size.append(cell.state_size)          return tuple(state_size)     @property     def output_size(self):         if getattr(self.cells[-1], 'output_size', None) is not None:             return self.cells[-1].output_size         if hasattr(self.cells[-1].state_size, '__len__'):             return self.cells[-1].state_size[0]         else:             return self.cells[-1].state_size      def call(self, inputs, states, constants=None, **kwargs):          nested_states = []         for cell in self.cells[::-1] if self.reverse_state_order else self.cells:              if hasattr(cell.state_size, '__len__'):                  nested_states.append(states[:len(cell.state_size)])                  states = states[len(cell.state_size):]              else:                  nested_states.append([states[0]])                  states = states[1:]         if self.reverse_state_order:             nested_states = nested_states[::-1]          new_nested_states = []
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later.     if line.is_comment:          yield line          return
class TimedeltaIndex(                      result._set_freq("infer")              return result     def _fast_union(self, other, sort=None):          if len(other) == 0:              return self.view(type(self))
def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1)     expected["Group"] = expected["Group_obj"]      tm.assert_frame_equal(result, expected)
class BusinessHourMixin(BusinessMixin):              if bd != 0:                 if isinstance(self, _CustomMixin):                     skip_bd = CustomBusinessDay(                         n=bd,                         weekmask=self.weekmask,                         holidays=self.holidays,                         calendar=self.calendar,                     )                 else:                     skip_bd = BusinessDay(n=bd)                  if not self.next_bday.is_on_offset(other):                      prev_open = self._prev_opening_time(other)
except ImportError:      from ordereddict import OrderedDict  from luigi import six import logging logger = logging.getLogger('luigi-interface')  class TaskClassException(Exception):
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0, base=10),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
from pandas.core.dtypes.common import (      is_list_like,      is_object_dtype,      is_scalar,     pandas_dtype,  )  from pandas.core.dtypes.dtypes import register_extension_dtype  from pandas.core.dtypes.missing import isna
class TestBackend(object):          else:              assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)     def test_print_tensor(self, capsys):         for k in [KTH, KTF]:             x = k.placeholder((1, 1))             y = k.print_tensor(x, 'msg')             fn = k.function([x], [y])             _ = fn([np.ones((1, 1))])             out, err = capsys.readouterr()             assert out.replace('__str__ = ', '') == 'msg [[1.]]\n'          check_single_tensor_operation('print_tensor', (), WITH_NP)          check_single_tensor_operation('print_tensor', (2,), WITH_NP)      def test_elementwise_operations(self):          check_single_tensor_operation('max', (4, 2), WITH_NP)
class Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key)                  return self.iloc[indexer]              else:                 return self.iloc[key]          if isinstance(key, (list, tuple)):
class ExecutionEngine(object):          d = self._download(request, spider)          d.addBoth(self._handle_downloader_output, request, spider)          d.addErrback(lambda f: logger.info('Error while handling downloader output',                                            exc_info=failure_to_exc_info(f),                                            extra={'spider': spider}))          d.addBoth(lambda _: slot.remove_request(request))          d.addErrback(lambda f: logger.info('Error while removing request from slot',                                            exc_info=failure_to_exc_info(f),                                            extra={'spider': spider}))          d.addBoth(lambda _: slot.nextcall.schedule())          d.addErrback(lambda f: logger.info('Error while scheduling new request',                                            exc_info=failure_to_exc_info(f),                                            extra={'spider': spider}))          return d      def _handle_downloader_output(self, response, request, spider):
class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = label          self._labelkw = kw          self._set_label()
def crosstab(      from pandas import DataFrame      df = DataFrame(data, index=common_idx)     original_df_cols = df.columns      if values is None:          df["__dummy__"] = 0          kwargs = {"aggfunc": len, "fill_value": 0}
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or not np.isnan(right_value):                      return False              else:                 if np.any(left_value != right_value):                      return False          return True
def create_instance(objcls, settings, crawler, *args, **kwargs):      ``*args`` and ``**kwargs`` are forwarded to the constructors.      Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.     Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an     extension has not been implemented correctly).      if settings is None:          if crawler is None:              raise ValueError("Specify at least one of settings and crawler.")          settings = crawler.settings      if crawler and hasattr(objcls, 'from_crawler'):         instance = objcls.from_crawler(crawler, *args, **kwargs)         method_name = 'from_crawler'      elif hasattr(objcls, 'from_settings'):         instance = objcls.from_settings(settings, *args, **kwargs)         method_name = 'from_settings'      else:         instance = objcls(*args, **kwargs)         method_name = '__new__'     if instance is None:         raise TypeError("%s.%s returned None" % (objcls.__qualname__, method_name))     return instance  @contextmanager
class DataFrame(NDFrame):              other = other._convert(datetime=True, timedelta=True)              if not self.columns.equals(combined_columns):                  self = self.reindex(columns=combined_columns)         elif isinstance(other, list):             if not other:                 pass             elif not isinstance(other[0], DataFrame):                 other = DataFrame(other)                 if (self.columns.get_indexer(other.columns) >= 0).all():                     other = other.reindex(columns=self.columns)          from pandas.core.reshape.concat import concat
class Language(object):              kwargs = component_cfg.get(name, {})              kwargs.setdefault("batch_size", batch_size)              if not hasattr(pipe, "pipe"):                 docs = _pipe(docs, pipe, kwargs)              else:                  docs = pipe.pipe(docs, **kwargs)          for doc, gold in zip(docs, golds):
def count_leading_spaces(s):  def process_list_block(docstring, starting_point, section_end,                         leading_spaces, marker):      ending_point = docstring.find('\n\n', starting_point)     block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else                                       section_end)]      docstring_slice = docstring[starting_point:section_end].replace(block, marker)      docstring = (docstring[:starting_point]
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()         mat = numeric_df.astype(float, copy=False).to_numpy()          if method == "pearson":             correl = libalgos.nancorr(mat, minp=min_periods)          elif method == "spearman":             correl = libalgos.nancorr_spearman(mat, minp=min_periods)          elif method == "kendall" or callable(method):              if min_periods is None:                  min_periods = 1             mat = mat.T              corrf = nanops.get_corr_func(method)              K = len(cols)              correl = np.empty((K, K), dtype=float)
class tqdm(object):                      l_bar_user, r_bar_user = bar_format.split('{bar}')                     l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)                  else:                      return bar_format.format(**bar_args)
def check_required_arguments(argument_spec, module_parameters):              missing.append(k)      if missing:         msg = "missing required arguments: %s" % ", ".join(sorted(missing))          raise TypeError(to_native(msg))      return missing
class HTTP1Connection(httputil.HTTPConnection):              return connection_header != "close"          elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked"               or getattr(start_line, 'method', None) in ("HEAD", "GET")):              return connection_header == "keep-alive"          return False
def jsonable_encoder(              )          return jsonable_encoder(              obj_dict,             exclude_none=exclude_none,             exclude_defaults=exclude_defaults,              custom_encoder=encoder,              sqlalchemy_safe=sqlalchemy_safe,          )
from .generic import Generic  class Bash(Generic):      def app_alias(self, fuck):         alias = "alias {0}='TF_CMD=$(TF_ALIAS={0}" \                 " PYTHONIOENCODING=utf-8" \                 " TF_SHELL_ALIASES=$(alias)" \                 " thefuck $(fc -ln -1)) &&" \                  " eval $TF_CMD".format(fuck)          if settings.alter_history:
default: 'top'          from .tight_layout import (              get_renderer, get_subplotspec_list, get_tight_layout_figure)         from .cbook import _setattr_cm         from .backend_bases import RendererBase          subplotspec_list = get_subplotspec_list(self.axes)          if None in subplotspec_list:
def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):      with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None)     assert "'NoneType' object is not iterable" in to_native(te.value)
class WebSocketHandler(tornado.web.RequestHandler):          if not self._on_close_called:              self._on_close_called = True              self.on_close()             self._break_cycles()     def _break_cycles(self):         if self.get_status() != 101 or self._on_close_called:             super(WebSocketHandler, self)._break_cycles()      def send_error(self, *args, **kwargs):          if self.stream is None:
import re  from .common import InfoExtractor  from ..utils import (     fix_xml_ampersands,  )
class Scraper(object):                      spider=spider, exception=output.value)              else:                  logger.error('Error processing %(item)s', {'item': item},                              exc_info=failure_to_exc_info(output),                              extra={'spider': spider})          else:              logkws = self.logformatter.scraped(output, response, spider)              logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf  def get_future_imports(node: Node) -> Set[str]:     imports: Set[str] = set()     def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:         for child in children:             if isinstance(child, Leaf):                 if child.type == token.NAME:                     yield child.value             elif child.type == syms.import_as_name:                 orig_name = child.children[0]                 assert isinstance(orig_name, Leaf), "Invalid syntax parsing imports"                 assert orig_name.type == token.NAME, "Invalid syntax parsing imports"                 yield orig_name.value             elif child.type == syms.import_as_names:                 yield from get_imports_from_children(child.children)             else:                 assert False, "Invalid syntax parsing imports"      for child in node.children:          if child.type != syms.simple_stmt:              break
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      @cache_readonly      def _engine(self):         period = weakref.ref(self._values)          return self._engine_type(period, len(self))      @doc(Index.__contains__)
from scrapy.utils.ftp import ftp_makedirs_cwd  from scrapy.exceptions import NotConfigured  from scrapy.utils.misc import load_object  from scrapy.utils.python import get_func_args from scrapy.utils.log import failure_to_exc_info  logger = logging.getLogger(__name__)
def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)          return any(is_excluded(dtype) for is_excluded in is_excluded_checks)      return _is_dtype(arr_or_dtype, condition)
import time  import traceback  import math from tornado.concurrent import TracebackFuture, is_future, chain_future  from tornado.log import app_log, gen_log  from tornado.platform.auto import set_close_exec, Waker  from tornado import stack_context
class LSTMCell(Layer):                  inputs_f = inputs                  inputs_c = inputs                  inputs_o = inputs             x_i = K.dot(inputs_i, self.kernel_i)             x_f = K.dot(inputs_f, self.kernel_f)             x_c = K.dot(inputs_c, self.kernel_c)             x_o = K.dot(inputs_o, self.kernel_o)             if self.use_bias:                 x_i = K.bias_add(x_i, self.bias_i)                 x_f = K.bias_add(x_f, self.bias_f)                 x_c = K.bias_add(x_c, self.bias_c)                 x_o = K.bias_add(x_o, self.bias_o)              if 0 < self.recurrent_dropout < 1.:                  h_tm1_i = h_tm1 * rec_dp_mask[0]
class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl)             sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])             outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))             filename = expand_path(outtmpl).replace(sep, '') % template_dict
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 if is_sequence:                     output_generator = iter(generator)                 else:                     output_generator = generator              callback_model.stop_training = False
class DictParameter(Parameter):      tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related      values (like a database connection config).          Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.
class TestInsertIndexCoercion(CoercionBase):          )         msg = "cannot insert TimedeltaArray with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaArray with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, 1)
class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))          self._viewLim.intervalx = (left, right)         for ax in self._shared_x_axes.get_siblings(self):             ax._stale_viewlim_x = False          if auto is not None:              self._autoscaleXon = bool(auto)
class RetcodesTest(LuigiTestCase):          with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):              self.run_and_expect('RequiringTask', 0)              self.run_and_expect('RequiringTask --retcode-not-run 5', 5)     def test_retry_sucess_task(self):         class Foo(luigi.Task):             run_count = 0             def run(self):                 self.run_count += 1                 if self.run_count == 1:                     raise ValueError()             def complete(self):                 return self.run_count > 0         self.run_and_expect('Foo --scheduler-retry-delay=0', 0)         self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)         self.run_with_config(dict(task_failed='3'), 'Foo', 0)
class Tracer:          self._write(s)      def __enter__(self):         if DISABLED:             return          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame)         stack = self.thread_local.__dict__.setdefault(             'original_trace_functions', []         )          stack.append(sys.gettrace())          sys.settrace(self.trace)      def __exit__(self, exc_type, exc_value, exc_traceback):         if DISABLED:             return          stack = self.thread_local.original_trace_functions          sys.settrace(stack.pop())          calling_frame = inspect.currentframe().f_back
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:      srcbuf = io.BytesIO(src)      encoding, lines = tokenize.detect_encoding(srcbuf.readline)     if not lines:         return "", encoding, "\n"      newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"      srcbuf.seek(0)      with io.TextIOWrapper(srcbuf, encoding) as tiow:
def write_flv_header(stream, metadata):      stream.write(b'\x12')     stream.write(struct_pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00')      stream.write(metadata)
class Conv2DTranspose(Conv2D):              output_shape,              self.strides,              padding=self.padding,             data_format=self.data_format,             dilation_rate=self.dilation_rate)          if self.use_bias:              outputs = K.bias_add(
single_quoted = (  tabsize = 8 @dataclass(frozen=True) class TokenizerConfig:     async_is_reserved_keyword: bool = False  class TokenError(Exception): pass  class StopTokenizing(Exception): pass
class Tracer:          self.target_codes = set()          self.target_frames = set()          self.thread_local = threading.local()         if len(custom_repr) == 2 and not all(isinstance(x,                       pycompat.collections_abc.Iterable) for x in custom_repr):             custom_repr = (custom_repr,)         self.custom_repr = custom_repr      def __call__(self, function):         if DISABLED:             return function          self.target_codes.add(function.__code__)          @functools.wraps(function)
class SimpleRNNCell(Layer):          self.dropout = min(1., max(0., dropout))          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.state_size = self.units         self.output_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
class Task(object):          params_str = {}          params = dict(self.get_params())          for param_name, param_value in six.iteritems(self.param_kwargs):             params_str[param_name] = params[param_name].serialize(param_value)          return params_str
def _isna_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike(obj, old=True)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)      elif hasattr(obj, "__array__"):         return _isna_ndarraylike(np.asarray(obj), old=True)      else:          return False
def _unstack_multiple(data, clocs, fill_value=None):              result = data              for i in range(len(clocs)):                  val = clocs[i]                 result = result.unstack(val, fill_value=fill_value)                  clocs = [v if i > v else v - 1 for v in clocs]              return result
class Categorical(ExtensionArray, PandasObject):              if dtype == self.dtype:                  return self              return self._set_dtype(dtype)         if is_extension_array_dtype(dtype):             return array(self, dtype=dtype, copy=copy)          if is_integer_dtype(dtype) and self.isna().any():              msg = "Cannot convert float NaN to integer"              raise ValueError(msg)
def in_top_k(predictions, targets, k):  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None, dilation_rate=(1, 1)):      data_format = normalize_data_format(data_format)      x = _preprocess_conv2d_input(x, data_format)
class OffsiteMiddleware(object):          if not allowed_domains: return re.compile('') url_pattern = re.compile("^https?:         domains = []          for domain in allowed_domains:             if domain is None:                 continue             elif url_pattern.match(domain):                  message = ("allowed_domains accepts only domains, not URLs. "                             "Ignoring URL entry %s in allowed_domains." % domain)                  warnings.warn(message, URLWarning)             else:                 domains.append(re.escape(domain))          regex = r'^(.*\.)?(%s)$' % '|'.join(domains)          return re.compile(regex)
class GalaxyCLI(CLI):          obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]          inject_data = dict(             description='your {0} description'.format(galaxy_type),              ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          )          if galaxy_type == 'role':
def _unstack_multiple(data, clocs, fill_value=None):      index = data.index     if clocs in index.names:         clocs = [clocs]      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs]
