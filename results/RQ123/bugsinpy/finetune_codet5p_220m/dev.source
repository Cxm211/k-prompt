class TestBackend(object):          else:              assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)     def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (), WITH_NP)          check_single_tensor_operation('print_tensor', (2,), WITH_NP)         check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)         check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)      def test_elementwise_operations(self):          check_single_tensor_operation('max', (4, 2), WITH_NP)
class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)          return fetched[:len(self.outputs)]      def _legacy_call(self, inputs):
class TimedeltaIndex(                      result._set_freq("infer")              return result     def _fast_union(self, other):          if len(other) == 0:              return self.view(type(self))
from .generic import Generic  class Bash(Generic):      def app_alias(self, fuck):         alias = "TF_ALIAS={0}" \                 " alias {0}='PYTHONIOENCODING=utf-8" \                 " TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && " \                  " eval $TF_CMD".format(fuck)          if settings.alter_history:
class DictParameter(Parameter):      tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related      values (like a database connection config).         JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.          Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.
class APIRoute(routing.Route):          self.response_model_exclude = response_model_exclude          self.response_model_by_alias = response_model_by_alias          self.response_model_exclude_unset = response_model_exclude_unset          self.include_in_schema = include_in_schema          self.response_class = response_class
def js_to_json(code):              ([{,]\s*)              ("[^"]*"|\'[^\']*\'|[a-z0-9A-Z]+)              (:\s*)             ([0-9.]+|true|false|"[^"]*"|\'[^\']*\'|\[|\{)      res = re.sub(r',(\s*\])', lambda m: m.group(1), res)      return res
class BaseGrouper:              if mask.any():                  result = result.astype("float64")                  result[mask] = np.nan          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2
class PagedList(object):  def uppercase_escape(s):      return re.sub(          r'\\U[0-9a-fA-F]{8}',         lambda m: m.group(0).decode('unicode-escape'), s)  try:      struct.pack(u'!I', 0)
def in_top_k(predictions, targets, k):  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None):      data_format = normalize_data_format(data_format)      x = _preprocess_conv2d_input(x, data_format)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class Sequential(Model):                                               use_multiprocessing=use_multiprocessing)      @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                            use_multiprocessing=False, verbose=0):
TEST_MODULES = [      'tornado.test.curl_httpclient_test',      'tornado.test.escape_test',      'tornado.test.gen_test',      'tornado.test.httpclient_test',      'tornado.test.httpserver_test',      'tornado.test.httputil_test',
class Task(object):          params_str = {}          params = dict(self.get_params())          for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)          return params_str
def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]     return replace_argument(command.script, 'push', push_upstream)
import threading  import time  import traceback  import math import weakref from tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback  from tornado.log import app_log, gen_log
def whitespace(leaf: Leaf) -> str:          ):              return NO      elif prev.type in OPENING_BRACKETS:          return NO
def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0      mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)      if mobj:
default: :rc:`scatter.edgecolors`              - 'none': No patch boundary will be drawn.              - A color or sequence of colors.             For non-filled markers, the *edgecolors* kwarg is ignored and             forced to 'face' internally.          plotnonfinite : bool, default: False              Set to plot points with nonfinite *c*, in conjunction with
def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
class BlockManager(PandasObject):                          convert=convert,                          regex=regex,                      )                     if m.any():                          new_rb = _extend_blocks(result, new_rb)                      else:                          new_rb.append(b)
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:      srcbuf = io.BytesIO(src)      encoding, lines = tokenize.detect_encoding(srcbuf.readline)      newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"      srcbuf.seek(0)      with io.TextIOWrapper(srcbuf, encoding) as tiow:
class FeedExporter(object):          d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                              extra={'spider': spider}))          d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                             extra={'spider': spider, 'failure': f}))          return d      def item_scraped(self, item, spider):
class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph'))  def fix_time(x):
def read_pickle(path, compression="infer"):      >>> import os      >>> os.remove("./dummy.pkl")     path = stringify_path(path)     f, fh = get_handle(path, "rb", compression=compression, is_text=False)
class MailSender(object):              msg = MIMEMultipart()          else:              msg = MIMENonMultipart(*mimetype.split('/', 1))          msg['From'] = self.mailfrom          msg['To'] = COMMASPACE.join(to)          msg['Date'] = formatdate(localtime=True)
def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype)          return any(is_excluded(dtype) for is_excluded in is_excluded_checks)      return _is_dtype(arr_or_dtype, condition)
class Tracer:          self.target_codes = set()          self.target_frames = set()          self.thread_local = threading.local()      def __call__(self, function):          self.target_codes.add(function.__code__)          @functools.wraps(function)
patterns = (          '^lua: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}',      )
class DataFrame(NDFrame):              other = other._convert(datetime=True, timedelta=True)              if not self.columns.equals(combined_columns):                  self = self.reindex(columns=combined_columns)         elif isinstance(other, list) and not isinstance(other[0], DataFrame):             other = DataFrame(other)             if (self.columns.get_indexer(other.columns) >= 0).all():                 other = other.reindex(columns=self.columns)          from pandas.core.reshape.concat import concat
def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              )              response = response_class(
class Categorical(ExtensionArray, PandasObject):              if dtype == self.dtype:                  return self              return self._set_dtype(dtype)          if is_integer_dtype(dtype) and self.isna().any():              msg = "Cannot convert float NaN to integer"              raise ValueError(msg)
def _unstack_multiple(data, clocs, fill_value=None):              result = data              for i in range(len(clocs)):                  val = clocs[i]                 result = result.unstack(val)                  clocs = [v if i > v else v - 1 for v in clocs]              return result
def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]      if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement)          collection_info = existing[0]      dep_map[to_text(collection_info)] = collection_info
from keras.utils.data_utils import validate_file  from keras import backend as K  pytestmark = pytest.mark.skipif(     K.backend() == 'tensorflow',      reason='Temporarily disabled until the use_multiprocessing problem is solved')  if sys.version_info < (3,):
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def write_flv_header(stream, metadata):      stream.write(b'\x12')     stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00')      stream.write(metadata)
def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs]
except ImportError:      from ordereddict import OrderedDict  from luigi import six  class TaskClassException(Exception):
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              return self         arr = operator.inv(com.values_from_object(self))         return self.__array_wrap__(arr)      def __nonzero__(self):          raise ValueError(
setup(              "options_test.cfg",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",
class ExecutionEngine(object):          d = self._download(request, spider)          d.addBoth(self._handle_downloader_output, request, spider)          d.addErrback(lambda f: logger.info('Error while handling downloader output',                                            extra={'spider': spider, 'failure': f}))          d.addBoth(lambda _: slot.remove_request(request))          d.addErrback(lambda f: logger.info('Error while removing request from slot',                                            extra={'spider': spider, 'failure': f}))          d.addBoth(lambda _: slot.nextcall.schedule())          d.addErrback(lambda f: logger.info('Error while scheduling new request',                                            extra={'spider': spider, 'failure': f}))          return d      def _handle_downloader_output(self, response, request, spider):
class OffsiteMiddleware(object):          if not allowed_domains: return re.compile('') url_pattern = re.compile("^https?:          for domain in allowed_domains:             if url_pattern.match(domain):                  message = ("allowed_domains accepts only domains, not URLs. "                             "Ignoring URL entry %s in allowed_domains." % domain)                  warnings.warn(message, URLWarning)         domains = [re.escape(d) for d in allowed_domains if d is not None]          regex = r'^(.*\.)?(%s)$' % '|'.join(domains)          return re.compile(regex)
def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024] else "\n"         src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)          try:
def unescapeHTML(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)  def get_subprocess_encoding():
class tqdm(object):                      l_bar_user, r_bar_user = bar_format.split('{bar}')                     l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)                  else:                      return bar_format.format(**bar_args)
import re  from .common import InfoExtractor  from ..utils import (     fix_xml_all_ampersand,  )
def dispatch_to_series(left, right, func, str_rep=None, axis=None):          assert right.index.equals(left.columns)         def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}      elif isinstance(right, ABCSeries): assert right.index.equals(left.index)
def check_required_arguments(argument_spec, module_parameters):              missing.append(k)      if missing:         msg = "missing required arguments: %s" % ", ".join(missing)          raise TypeError(to_native(msg))      return missing
def get_elements_by_attribute(attribute, value, html, escape_value=True):      retlist = []          <([a-zA-Z0-9:._-]+)          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?          \s*>          (?P<content>.*?)          </\1>
class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)         version = proc.stdout.read().decode('utf-8').strip()          return u'Fish Shell {}'.format(version)      def put_to_history(self, command):
def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf  def get_future_imports(node: Node) -> Set[str]:     imports = set()      for child in node.children:          if child.type != syms.simple_stmt:              break
class LinuxHardware(Hardware):          pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))          maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT          for fields in mtab_entries:              device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]
def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):      with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None)         assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error)
class Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key)                  return self.iloc[indexer]              else:                 return self._get_values(key)          if isinstance(key, (list, tuple)):
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
class Tracer:          self._write(s)      def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame)         stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])          stack.append(sys.gettrace())          sys.settrace(self.trace)      def __exit__(self, exc_type, exc_value, exc_traceback):          stack = self.thread_local.original_trace_functions          sys.settrace(stack.pop())          calling_frame = inspect.currentframe().f_back
def _isna_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new
def format_stdin_to_stdout(      `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to      :func:`format_file_contents`.     src = sys.stdin.read()      dst = src      try:          dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
class Conv2DTranspose(Conv2D):              output_shape,              self.strides,              padding=self.padding,             data_format=self.data_format)          if self.use_bias:              outputs = K.bias_add(
class GalaxyCLI(CLI):          obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]          inject_data = dict(             description='your description',              ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          )          if galaxy_type == 'role':
class CollectionRequirement:                  requirement = req                  op = operator.eq                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LooseVersion(version), LooseVersion(requirement)):                  break
class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_scalar_indexer(key, kind=kind)          return super()._convert_scalar_indexer(key, kind=kind)      @Appender(_index_shared_docs["_convert_list_indexer"])
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
from scrapy.utils.ftp import ftp_makedirs_cwd  from scrapy.exceptions import NotConfigured  from scrapy.utils.misc import load_object  from scrapy.utils.python import get_func_args  logger = logging.getLogger(__name__)
class HTTP1Connection(httputil.HTTPConnection):              return connection_header != "close"          elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked"               or start_line.method in ("HEAD", "GET")):              return connection_header == "keep-alive"          return False
class SimpleRNNCell(Layer):          self.dropout = min(1., max(0., dropout))          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.state_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
class TestInsertIndexCoercion(CoercionBase):          )         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, 1)
class Model(Container):              validation_steps: Only relevant if `validation_data`                  is a generator. Total number of steps (batches of samples)                  to yield from `generator` before stopping.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Integer. Maximum size for the generator queue.
def count_leading_spaces(s):  def process_list_block(docstring, starting_point, section_end,                         leading_spaces, marker):      ending_point = docstring.find('\n\n', starting_point)     block = docstring[starting_point:(None if ending_point == -1 else                                       ending_point - 1)]      docstring_slice = docstring[starting_point:section_end].replace(block, marker)      docstring = (docstring[:starting_point]
from pandas.core.dtypes.common import (      is_list_like,      is_object_dtype,      is_scalar,  )  from pandas.core.dtypes.dtypes import register_extension_dtype  from pandas.core.dtypes.missing import isna
single_quoted = (  tabsize = 8  class TokenError(Exception): pass  class StopTokenizing(Exception): pass
class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl)             filename = expand_path(outtmpl % template_dict)
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
import time  import traceback  import math from tornado.concurrent import TracebackFuture, is_future  from tornado.log import app_log, gen_log  from tornado.platform.auto import set_close_exec, Waker  from tornado import stack_context
def update_add(x, increment):          The variable `x` updated.     return tf_state_ops.assign_add(x, increment)  @symbolic
class TestSeriesAnalytics:          assert s.is_monotonic is False          assert s.is_monotonic_decreasing is True     def test_unstack(self):         index = MultiIndex(             levels=[["bar", "foo"], ["one", "three", "two"]],             codes=[[1, 1, 0, 0], [0, 1, 0, 2]],         )         s = Series(np.arange(4.0), index=index)         unstacked = s.unstack()         expected = DataFrame(             [[2.0, np.nan, 3.0], [0.0, 1.0, np.nan]],             index=["bar", "foo"],             columns=["one", "three", "two"],         )         tm.assert_frame_equal(unstacked, expected)         unstacked = s.unstack(level=0)         tm.assert_frame_equal(unstacked, expected.T)         index = MultiIndex(             levels=[["bar"], ["one", "two", "three"], [0, 1]],             codes=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],         )         s = Series(np.random.randn(6), index=index)         exp_index = MultiIndex(             levels=[["one", "two", "three"], [0, 1]],             codes=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],         )         expected = DataFrame({"bar": s.values}, index=exp_index).sort_index(level=0)         unstacked = s.unstack(0).sort_index()         tm.assert_frame_equal(unstacked, expected)         idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])         ts = pd.Series([1, 2], index=idx)         left = ts.unstack()         right = DataFrame(             [[np.nan, 1], [2, np.nan]], index=[101, 102], columns=[np.nan, 3.5]         )         tm.assert_frame_equal(left, right)         idx = pd.MultiIndex.from_arrays(             [                 ["cat", "cat", "cat", "dog", "dog"],                 ["a", "a", "b", "a", "b"],                 [1, 2, 1, 1, np.nan],             ]         )         ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)         right = DataFrame(             [[1.0, 1.3], [1.1, np.nan], [np.nan, 1.4], [1.2, np.nan]],             columns=["cat", "dog"],         )         tpls = [("a", 1), ("a", 2), ("b", np.nan), ("b", 1)]         right.index = pd.MultiIndex.from_tuples(tpls)         tm.assert_frame_equal(ts.unstack(level=0), right)      @pytest.mark.parametrize("func", [np.any, np.all])      @pytest.mark.parametrize("kwargs", [dict(keepdims=True), dict(out=object())])      @td.skip_if_np_lt("1.15")
def conv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) * stride - 2 * pad + filter_size def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):
import logging  from six.moves.urllib.parse import urljoin  from scrapy.http import HtmlResponse  from scrapy.utils.response import get_meta_refresh from scrapy.utils.python import to_native_str  from scrapy.exceptions import IgnoreRequest, NotConfigured  logger = logging.getLogger(__name__)
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later.     if isinstance(line, UnformattedLines) or line.is_comment:          yield line          return
def standardize_weights(y,      Everything gets normalized to a single sample-wise (or timestep-wise)     weight array.          y: Numpy array of model targets to be weighted.
class BusinessHourMixin(BusinessMixin):              if bd != 0:                 skip_bd = BusinessDay(n=bd)                  if not self.next_bday.is_on_offset(other):                      prev_open = self._prev_opening_time(other)
def srt_subtitles_timecode(seconds):  def dfxp2srt(dfxp_data):      LEGACY_NAMESPACES = ( ('http: 'http: 'http: 'http:          ]), ('http: 'http:          ]),      )
class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label)          self._labelkw = kw          self._set_label()
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or not np.isnan(right_value):                      return False              else:                 if left_value != right_value:                      return False          return True
class S3CopyToTable(rdbms.CopyToTable):          if '.' in self.table:              query = ("select 1 as table_exists "                       "from information_schema.tables "                      "where table_schema = %s and table_name = %s limit 1")          else:              query = ("select 1 as table_exists "                       "from pg_table_def "                      "where tablename = %s limit 1")          cursor = connection.cursor()          try:              cursor.execute(query, tuple(self.table.split('.')))
class CannotSplit(Exception):     It holds the number of bytes of the prefix consumed before the format     control comment appeared.         unformatted_prefix = leaf.prefix[: self.consumed]         return Leaf(token.NEWLINE, unformatted_prefix) class FormatOn(FormatError):  class WriteBack(Enum):      NO = 0      YES = 1
class LSTMCell(Layer):                  inputs_f = inputs                  inputs_c = inputs                  inputs_o = inputs             x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i             x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f             x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c             x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o              if 0 < self.recurrent_dropout < 1.:                  h_tm1_i = h_tm1 * rec_dp_mask[0]
from pandas.core.dtypes.generic import ABCSeries  from pandas.core.dtypes.missing import isna  from pandas._typing import AnyArrayLike  from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs  import pandas.core.common as com  import pandas.core.indexes.base as ibase
class StackedRNNCells(Layer):                                   '`state_size` attribute. '                                   'received cells:', cells)          self.cells = cells          super(StackedRNNCells, self).__init__(**kwargs)      @property      def state_size(self):          state_size = []         for cell in self.cells[::-1]:              if hasattr(cell.state_size, '__len__'):                  state_size += list(cell.state_size)              else:                  state_size.append(cell.state_size)          return tuple(state_size)      def call(self, inputs, states, constants=None, **kwargs):          nested_states = []         for cell in self.cells[::-1]:              if hasattr(cell.state_size, '__len__'):                  nested_states.append(states[:len(cell.state_size)])                  states = states[len(cell.state_size):]              else:                  nested_states.append([states[0]])                  states = states[1:]         nested_states = nested_states[::-1]          new_nested_states = []
class CSVLogger(Callback):          if not self.writer:              class CustomDialect(csv.excel):                  delimiter = self.sep              self.writer = csv.DictWriter(self.csv_file,                                          fieldnames=['epoch'] + self.keys, dialect=CustomDialect)              if self.append_header:                  self.writer.writeheader()
def assert_series_equal(          Compare datetime-like which is comparable ignoring dtype.      check_categorical : bool, default True          Whether to compare internal Categorical exactly.      obj : str, default 'Series'          Specify object name being compared, internally used to show appropriate          assertion message.
class Scraper(object):                      spider=spider, exception=output.value)              else:                  logger.error('Error processing %(item)s', {'item': item},                              extra={'spider': spider, 'failure': output})          else:              logkws = self.logformatter.scraped(output, response, spider)              logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
class Language(object):              kwargs = component_cfg.get(name, {})              kwargs.setdefault("batch_size", batch_size)              if not hasattr(pipe, "pipe"):                 docs = _pipe(pipe, docs, kwargs)              else:                  docs = pipe.pipe(docs, **kwargs)          for doc, gold in zip(docs, golds):
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              callback_model.stop_training = False
class MultiIndex(Index):                      indexer = self._get_level_indexer(key, level=level)                      new_index = maybe_mi_droplevels(indexer, [0], drop_level)                      return indexer, new_index             except TypeError:                  pass              if not any(isinstance(k, slice) for k in key):
def data(dtype):      return pd.array(make_data(), dtype=dtype) def test_boolean_array_constructor():     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(values, mask)     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     with pytest.raises(TypeError, match="values should be boolean numpy array"):         BooleanArray(values.tolist(), mask)     with pytest.raises(TypeError, match="mask should be boolean numpy array"):         BooleanArray(values, mask.tolist())     with pytest.raises(TypeError, match="values should be boolean numpy array"):         BooleanArray(values.astype(int), mask)     with pytest.raises(TypeError, match="mask should be boolean numpy array"):         BooleanArray(values, None)     with pytest.raises(ValueError, match="values must be a 1D array"):         BooleanArray(values.reshape(1, -1), mask)     with pytest.raises(ValueError, match="mask must be a 1D array"):         BooleanArray(values, mask.reshape(1, -1)) def test_boolean_array_constructor_copy():     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(values, mask)     assert result._data is values     assert result._mask is mask     result = BooleanArray(values, mask, copy=True)     assert result._data is not values     assert result._mask is not mask def test_to_boolean_array():     expected = BooleanArray(         np.array([True, False, True]), np.array([False, False, False])     )     result = pd.array([True, False, True], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([True, False, True]), dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([True, False, True], dtype=object), dtype="boolean")     tm.assert_extension_array_equal(result, expected)     expected = BooleanArray(         np.array([True, False, True]), np.array([False, False, True])     )     result = pd.array([True, False, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([True, False, None], dtype=object), dtype="boolean")     tm.assert_extension_array_equal(result, expected) def test_to_boolean_array_all_none():     expected = BooleanArray(np.array([True, True, True]), np.array([True, True, True]))     result = pd.array([None, None, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([None, None, None], dtype=object), dtype="boolean")     tm.assert_extension_array_equal(result, expected) @pytest.mark.parametrize(     "a, b",     [         ([True, False, None, np.nan, pd.NA], [True, False, None, None, None]),         ([True, np.nan], [True, None]),         ([True, pd.NA], [True, None]),         ([np.nan, np.nan], [None, None]),         (np.array([np.nan, np.nan], dtype=float), [None, None]),     ], ) def test_to_boolean_array_missing_indicators(a, b):     result = pd.array(a, dtype="boolean")     expected = pd.array(b, dtype="boolean")     tm.assert_extension_array_equal(result, expected) @pytest.mark.parametrize(     "values",     [         ["foo", "bar"],         ["1", "2"],         [1, 2],         [1.0, 2.0],         pd.date_range("20130101", periods=2),         np.array(["foo"]),         np.array([1, 2]),         np.array([1.0, 2.0]),         [np.nan, {"a": 1}],     ], ) def test_to_boolean_array_error(values):     with pytest.raises(TypeError):         pd.array(values, dtype="boolean") def test_to_boolean_array_from_integer_array():     result = pd.array(np.array([1, 0, 1, 0]), dtype="boolean")     expected = pd.array([True, False, True, False], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([1, 0, 1, None]), dtype="boolean")     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected) def test_to_boolean_array_from_float_array():     result = pd.array(np.array([1.0, 0.0, 1.0, 0.0]), dtype="boolean")     expected = pd.array([True, False, True, False], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([1.0, 0.0, 1.0, np.nan]), dtype="boolean")     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected) def test_to_boolean_array_integer_like():     result = pd.array([1, 0, 1, 0], dtype="boolean")     expected = pd.array([True, False, True, False], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array([1, 0, 1, None], dtype="boolean")     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected) def test_coerce_to_array():     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(*coerce_to_array(values, mask=mask))     expected = BooleanArray(values, mask)     tm.assert_extension_array_equal(result, expected)     assert result._data is values     assert result._mask is mask     result = BooleanArray(*coerce_to_array(values, mask=mask, copy=True))     expected = BooleanArray(values, mask)     tm.assert_extension_array_equal(result, expected)     assert result._data is not values     assert result._mask is not mask     values = [True, False, None, False]     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(*coerce_to_array(values, mask=mask))     expected = BooleanArray(         np.array([True, False, True, True]), np.array([False, False, True, True])     )     tm.assert_extension_array_equal(result, expected)     result = BooleanArray(*coerce_to_array(np.array(values, dtype=object), mask=mask))     tm.assert_extension_array_equal(result, expected)     result = BooleanArray(*coerce_to_array(values, mask=mask.tolist()))     tm.assert_extension_array_equal(result, expected)     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     with pytest.raises(ValueError, match="values must be a 1D list-like"):         coerce_to_array(values.reshape(1, -1))     with pytest.raises(ValueError, match="mask must be a 1D list-like"):         coerce_to_array(values, mask=mask.reshape(1, -1)) def test_coerce_to_array_from_boolean_array():     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     arr = BooleanArray(values, mask)     result = BooleanArray(*coerce_to_array(arr))     tm.assert_extension_array_equal(result, arr)     assert result._data is arr._data     assert result._mask is arr._mask     result = BooleanArray(*coerce_to_array(arr), copy=True)     tm.assert_extension_array_equal(result, arr)     assert result._data is not arr._data     assert result._mask is not arr._mask     with pytest.raises(ValueError, match="cannot pass mask for BooleanArray input"):         coerce_to_array(arr, mask=mask) def test_coerce_to_numpy_array():     arr = pd.array([True, False, None], dtype="boolean")     result = np.array(arr)     expected = np.array([True, False, pd.NA], dtype="object")     tm.assert_numpy_array_equal(result, expected)     arr = pd.array([True, False, True], dtype="boolean")     result = np.array(arr)     expected = np.array([True, False, True], dtype="object")     tm.assert_numpy_array_equal(result, expected)     result = np.array(arr, dtype="bool")     expected = np.array([True, False, True], dtype="bool")     tm.assert_numpy_array_equal(result, expected)     arr = pd.array([True, False, None], dtype="boolean")     with pytest.raises(ValueError):         np.array(arr, dtype="bool") def test_to_boolean_array_from_strings():     result = BooleanArray._from_sequence_of_strings(         np.array(["True", "False", np.nan], dtype=object)     )     expected = BooleanArray(         np.array([True, False, False]), np.array([False, False, True])     )     tm.assert_extension_array_equal(result, expected) def test_to_boolean_array_from_strings_invalid_string():     with pytest.raises(ValueError, match="cannot be cast"):         BooleanArray._from_sequence_of_strings(["donkey"]) def test_repr():     df = pd.DataFrame({"A": pd.array([True, False, None], dtype="boolean")})     expected = "       A\n0   True\n1  False\n2   <NA>"     assert repr(df) == expected     expected = "0     True\n1    False\n2     <NA>\nName: A, dtype: boolean"     assert repr(df.A) == expected     expected = "<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean"     assert repr(df.A.array) == expected @pytest.mark.parametrize("box", [True, False], ids=["series", "array"]) def test_to_numpy(box):     con = pd.Series if box else pd.array     arr = con([True, False, True], dtype="boolean")     result = arr.to_numpy()     expected = np.array([True, False, True], dtype="object")     tm.assert_numpy_array_equal(result, expected)     arr = con([True, False, None], dtype="boolean")     result = arr.to_numpy()     expected = np.array([True, False, pd.NA], dtype="object")     tm.assert_numpy_array_equal(result, expected)     arr = con([True, False, None], dtype="boolean")     result = arr.to_numpy(dtype="str")     expected = np.array([True, False, pd.NA], dtype="<U5")     tm.assert_numpy_array_equal(result, expected)     arr = con([True, False, True], dtype="boolean")     result = arr.to_numpy(dtype="bool")     expected = np.array([True, False, True], dtype="bool")     tm.assert_numpy_array_equal(result, expected)     arr = con([True, False, None], dtype="boolean")     with pytest.raises(ValueError, match="cannot convert to 'bool'-dtype"):         result = arr.to_numpy(dtype="bool")     arr = con([True, False, None], dtype="boolean")     result = arr.to_numpy(dtype=object, na_value=None)     expected = np.array([True, False, None], dtype="object")     tm.assert_numpy_array_equal(result, expected)     result = arr.to_numpy(dtype=bool, na_value=False)     expected = np.array([True, False, False], dtype="bool")     tm.assert_numpy_array_equal(result, expected)     result = arr.to_numpy(dtype="int64", na_value=-99)     expected = np.array([1, 0, -99], dtype="int64")     tm.assert_numpy_array_equal(result, expected)     result = arr.to_numpy(dtype="float64", na_value=np.nan)     expected = np.array([1, 0, np.nan], dtype="float64")     tm.assert_numpy_array_equal(result, expected)     with pytest.raises(ValueError, match="cannot convert to 'int64'-dtype"):         arr.to_numpy(dtype="int64")     with pytest.raises(ValueError, match="cannot convert to 'float64'-dtype"):         arr.to_numpy(dtype="float64") def test_to_numpy_copy():     arr = pd.array([True, False, True], dtype="boolean")     result = arr.to_numpy(dtype=bool)     result[0] = False     tm.assert_extension_array_equal(         arr, pd.array([False, False, True], dtype="boolean")     )     arr = pd.array([True, False, True], dtype="boolean")     result = arr.to_numpy(dtype=bool, copy=True)     result[0] = False     tm.assert_extension_array_equal(arr, pd.array([True, False, True], dtype="boolean")) def test_astype():     arr = pd.array([True, False, None], dtype="boolean")     with pytest.raises(ValueError, match="cannot convert NA to integer"):         arr.astype("int64")     with pytest.raises(ValueError, match="cannot convert float NaN to"):         arr.astype("bool")     result = arr.astype("float64")     expected = np.array([1, 0, np.nan], dtype="float64")     tm.assert_numpy_array_equal(result, expected)     result = arr.astype("str")     expected = np.array(["True", "False", "<NA>"], dtype="object")     tm.assert_numpy_array_equal(result, expected)     arr = pd.array([True, False, True], dtype="boolean")     result = arr.astype("int64")     expected = np.array([1, 0, 1], dtype="int64")     tm.assert_numpy_array_equal(result, expected)     result = arr.astype("bool")     expected = np.array([True, False, True], dtype="bool")     tm.assert_numpy_array_equal(result, expected) def test_astype_to_boolean_array():     arr = pd.array([True, False, None], dtype="boolean")     result = arr.astype("boolean")     tm.assert_extension_array_equal(result, arr)     result = arr.astype(pd.BooleanDtype())     tm.assert_extension_array_equal(result, arr) def test_astype_to_integer_array():     arr = pd.array([True, False, None], dtype="boolean")     result = arr.astype("Int64")     expected = pd.array([1, 0, None], dtype="Int64")     tm.assert_extension_array_equal(result, expected) @pytest.mark.parametrize("na", [None, np.nan, pd.NA]) def test_setitem_missing_values(na):     arr = pd.array([True, False, None], dtype="boolean")     expected = pd.array([True, None, None], dtype="boolean")     arr[1] = na     tm.assert_extension_array_equal(arr, expected) @pytest.mark.parametrize(     "ufunc", [np.add, np.logical_or, np.logical_and, np.logical_xor] ) def test_ufuncs_binary(ufunc):     a = pd.array([True, False, None], dtype="boolean")     result = ufunc(a, a)     expected = pd.array(ufunc(a._data, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)     s = pd.Series(a)     result = ufunc(s, a)     expected = pd.Series(ufunc(a._data, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_series_equal(result, expected)     arr = np.array([True, True, False])     result = ufunc(a, arr)     expected = pd.array(ufunc(a._data, arr), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)     result = ufunc(arr, a)     expected = pd.array(ufunc(arr, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)     result = ufunc(a, True)     expected = pd.array(ufunc(a._data, True), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)     result = ufunc(True, a)     expected = pd.array(ufunc(True, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)     with pytest.raises(TypeError):         ufunc(a, "test") @pytest.mark.parametrize("ufunc", [np.logical_not]) def test_ufuncs_unary(ufunc):     a = pd.array([True, False, None], dtype="boolean")     result = ufunc(a)     expected = pd.array(ufunc(a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)     s = pd.Series(a)     result = ufunc(s)     expected = pd.Series(ufunc(a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_series_equal(result, expected) @pytest.mark.parametrize("values", [[True, False], [True, None]]) def test_ufunc_reduce_raises(values):     a = pd.array(values, dtype="boolean")     with pytest.raises(NotImplementedError):         np.add.reduce(a) class TestLogicalOps(BaseOpsUtil):     def test_numpy_scalars_ok(self, all_logical_operators):         a = pd.array([True, False, None], dtype="boolean")         op = getattr(a, all_logical_operators)         tm.assert_extension_array_equal(op(True), op(np.bool(True)))         tm.assert_extension_array_equal(op(False), op(np.bool(False)))     def get_op_from_name(self, op_name):         short_opname = op_name.strip("_")         short_opname = short_opname if "xor" in short_opname else short_opname + "_"         try:             op = getattr(operator, short_opname)         except AttributeError:             rop = getattr(operator, short_opname[1:])             op = lambda x, y: rop(y, x)         return op     def test_empty_ok(self, all_logical_operators):         a = pd.array([], dtype="boolean")         op_name = all_logical_operators         result = getattr(a, op_name)(True)         tm.assert_extension_array_equal(a, result)         result = getattr(a, op_name)(False)         tm.assert_extension_array_equal(a, result)     def test_logical_length_mismatch_raises(self, all_logical_operators):         op_name = all_logical_operators         a = pd.array([True, False, None], dtype="boolean")         msg = "Lengths must match to compare"         with pytest.raises(ValueError, match=msg):             getattr(a, op_name)([True, False])         with pytest.raises(ValueError, match=msg):             getattr(a, op_name)(np.array([True, False]))         with pytest.raises(ValueError, match=msg):             getattr(a, op_name)(pd.array([True, False], dtype="boolean"))     def test_logical_nan_raises(self, all_logical_operators):         op_name = all_logical_operators         a = pd.array([True, False, None], dtype="boolean")         msg = "Got float instead"         with pytest.raises(TypeError, match=msg):             getattr(a, op_name)(np.nan)     @pytest.mark.parametrize("other", ["a", 1])     def test_non_bool_or_na_other_raises(self, other, all_logical_operators):         a = pd.array([True, False], dtype="boolean")         with pytest.raises(TypeError, match=str(type(other).__name__)):             getattr(a, all_logical_operators)(other)     def test_kleene_or(self):         a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = a | b         expected = pd.array(             [True, True, True, True, False, None, True, None, None], dtype="boolean"         )         tm.assert_extension_array_equal(result, expected)         result = b | a         tm.assert_extension_array_equal(result, expected)         tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")         )     @pytest.mark.parametrize(         "other, expected",         [             (pd.NA, [True, None, None]),             (True, [True, True, True]),             (np.bool_(True), [True, True, True]),             (False, [True, False, None]),             (np.bool_(False), [True, False, None]),         ],     )     def test_kleene_or_scalar(self, other, expected):         a = pd.array([True, False, None], dtype="boolean")         result = a | other         expected = pd.array(expected, dtype="boolean")         tm.assert_extension_array_equal(result, expected)         result = other | a         tm.assert_extension_array_equal(result, expected)         tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")         )     def test_kleene_and(self):         a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = a & b         expected = pd.array(             [True, False, None, False, False, False, None, False, None], dtype="boolean"         )         tm.assert_extension_array_equal(result, expected)         result = b & a         tm.assert_extension_array_equal(result, expected)         tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")         )     @pytest.mark.parametrize(         "other, expected",         [             (pd.NA, [None, False, None]),             (True, [True, False, None]),             (False, [False, False, False]),             (np.bool_(True), [True, False, None]),             (np.bool_(False), [False, False, False]),         ],     )     def test_kleene_and_scalar(self, other, expected):         a = pd.array([True, False, None], dtype="boolean")         result = a & other         expected = pd.array(expected, dtype="boolean")         tm.assert_extension_array_equal(result, expected)         result = other & a         tm.assert_extension_array_equal(result, expected)         tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")         )     def test_kleene_xor(self):         a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = a ^ b         expected = pd.array(             [False, True, None, True, False, None, None, None, None], dtype="boolean"         )         tm.assert_extension_array_equal(result, expected)         result = b ^ a         tm.assert_extension_array_equal(result, expected)         tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")         )     @pytest.mark.parametrize(         "other, expected",         [             (pd.NA, [None, None, None]),             (True, [False, True, None]),             (np.bool_(True), [False, True, None]),             (np.bool_(False), [True, False, None]),         ],     )     def test_kleene_xor_scalar(self, other, expected):         a = pd.array([True, False, None], dtype="boolean")         result = a ^ other         expected = pd.array(expected, dtype="boolean")         tm.assert_extension_array_equal(result, expected)         result = other ^ a         tm.assert_extension_array_equal(result, expected)         tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")         )     @pytest.mark.parametrize(         "other", [True, False, pd.NA, [True, False, None] * 3],     )     def test_no_masked_assumptions(self, other, all_logical_operators):         a = pd.arrays.BooleanArray(             np.array([True, True, True, False, False, False, True, False, True]),             np.array([False] * 6 + [True, True, True]),         )         b = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         if isinstance(other, list):             other = pd.array(other, dtype="boolean")         result = getattr(a, all_logical_operators)(other)         expected = getattr(b, all_logical_operators)(other)         tm.assert_extension_array_equal(result, expected)         if isinstance(other, BooleanArray):             other._data[other._mask] = True             a._data[a._mask] = False             result = getattr(a, all_logical_operators)(other)             expected = getattr(b, all_logical_operators)(other)             tm.assert_extension_array_equal(result, expected) class TestComparisonOps(BaseOpsUtil):     def _compare_other(self, data, op_name, other):         op = self.get_op_from_name(op_name)         result = pd.Series(op(data, other))         expected = pd.Series(op(data._data, other), dtype="boolean")         expected[data._mask] = pd.NA         tm.assert_series_equal(result, expected)         s = pd.Series(data)         result = op(s, other)         expected = pd.Series(data._data)         expected = op(expected, other)         expected = expected.astype("boolean")         expected[data._mask] = pd.NA         tm.assert_series_equal(result, expected)      def test_compare_scalar(self, data, all_compare_operators):         op_name = all_compare_operators         self._compare_other(data, op_name, True)      def test_compare_array(self, data, all_compare_operators):         op_name = all_compare_operators         other = pd.array([True] * len(data), dtype="boolean")         self._compare_other(data, op_name, other)         other = np.array([True] * len(data))         self._compare_other(data, op_name, other)         other = pd.Series([True] * len(data))         self._compare_other(data, op_name, other)     @pytest.mark.parametrize("other", [True, False, pd.NA])     def test_scalar(self, other, all_compare_operators):         op = self.get_op_from_name(all_compare_operators)         a = pd.array([True, False, None], dtype="boolean")         result = op(a, other)         if other is pd.NA:             expected = pd.array([None, None, None], dtype="boolean")         else:             values = op(a._data, other)             expected = BooleanArray(values, a._mask, copy=True)         tm.assert_extension_array_equal(result, expected)         result[0] = None         tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")          )     def test_array(self, all_compare_operators):         op = self.get_op_from_name(all_compare_operators)         a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = op(a, b)         values = op(a._data, b._data)         mask = a._mask | b._mask         expected = BooleanArray(values, mask)         tm.assert_extension_array_equal(result, expected)         result[0] = None         tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")          )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")          ) class TestArithmeticOps(BaseOpsUtil):     def test_error(self, data, all_arithmetic_operators):         op = all_arithmetic_operators         s = pd.Series(data)         ops = getattr(s, op)         opa = getattr(data, op)         with pytest.raises(TypeError):             ops("foo")         with pytest.raises(TypeError):             ops(pd.Timestamp("20180101"))         if op not in ("__mul__", "__rmul__"):             with pytest.raises(TypeError):                 ops(pd.Series("foo", index=s.index))         result = opa(pd.DataFrame({"A": s}))         assert result is NotImplemented         with pytest.raises(NotImplementedError):             opa(np.arange(len(s)).reshape(-1, len(s))) @pytest.mark.parametrize("dropna", [True, False]) def test_reductions_return_types(dropna, data, all_numeric_reductions):     op = all_numeric_reductions     s = pd.Series(data)     if dropna:         s = s.dropna()     if op in ("sum", "prod"):         assert isinstance(getattr(s, op)(), np.int64)     elif op in ("min", "max"):         assert isinstance(getattr(s, op)(), np.bool_)     else:         assert isinstance(getattr(s, op)(), np.float64) @pytest.mark.parametrize(     "values, exp_any, exp_all, exp_any_noskip, exp_all_noskip",     [         ([True, pd.NA], True, True, True, pd.NA),         ([False, pd.NA], False, False, pd.NA, False),         ([pd.NA], False, True, pd.NA, pd.NA),         ([], False, True, False, True),     ], ) def test_any_all(values, exp_any, exp_all, exp_any_noskip, exp_all_noskip):     exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)     exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)     exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)     exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)     for con in [pd.array, pd.Series]:         a = con(values, dtype="boolean")         assert a.any() is exp_any         assert a.all() is exp_all         assert a.any(skipna=False) is exp_any_noskip         assert a.all(skipna=False) is exp_all_noskip         assert np.any(a.any()) is exp_any         assert np.all(a.all()) is exp_all @td.skip_if_no("pyarrow", min_version="0.15.0") def test_arrow_array(data):     import pyarrow as pa     arr = pa.array(data)     data_object = np.array(data, dtype=object)     data_object[data.isna()] = None     expected = pa.array(data_object, type=pa.bool_(), from_pandas=True)     assert arr.equals(expected) @td.skip_if_no("pyarrow", min_version="0.15.1.dev") def test_arrow_roundtrip():     import pyarrow as pa     data = pd.array([True, False, None], dtype="boolean")     df = pd.DataFrame({"a": data})     table = pa.table(df)     assert table.field("a").type == "bool"     result = table.to_pandas()     assert isinstance(result["a"].dtype, pd.BooleanDtype)     tm.assert_frame_equal(result, df) def test_value_counts_na():     arr = pd.array([True, False, pd.NA], dtype="boolean")     result = arr.value_counts(dropna=False)     expected = pd.Series([1, 1, 1], index=[True, False, pd.NA], dtype="Int64")     tm.assert_series_equal(result, expected)     result = arr.value_counts(dropna=True)     expected = pd.Series([1, 1], index=[True, False], dtype="Int64")     tm.assert_series_equal(result, expected) def test_diff():     a = pd.array(         [True, True, False, False, True, None, True, None, False], dtype="boolean"     )     result = pd.core.algorithms.diff(a, 1)     expected = pd.array(         [None, False, True, False, True, None, None, None, None], dtype="boolean"     )     tm.assert_extension_array_equal(result, expected)     s = pd.Series(a)     result = s.diff()     expected = pd.Series(expected)     tm.assert_series_equal(result, expected)
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1)     expected["Group"] = expected["Group_obj"].astype("category")      tm.assert_frame_equal(result, expected)
def jsonable_encoder(              )          return jsonable_encoder(              obj_dict,             include_none=include_none,              custom_encoder=encoder,              sqlalchemy_safe=sqlalchemy_safe,          )
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      @cache_readonly      def _engine(self):         period = weakref.ref(self)          return self._engine_type(period, len(self))      @doc(Index.__contains__)
def _isna_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):         return _isna_ndarraylike_old(np.asarray(obj))      else:          return False
class Conv2DTranspose(Conv2D):          out_height = conv_utils.deconv_length(height,                                                stride_h, kernel_h,                                                self.padding,                                               out_pad_h)          out_width = conv_utils.deconv_length(width,                                               stride_w, kernel_w,                                               self.padding,                                              out_pad_w)          if self.data_format == 'channels_first':              output_shape = (batch_size, self.filters, out_height, out_width)          else:
def na_value_for_dtype(dtype, compat: bool = True):      if is_extension_array_dtype(dtype):          return dtype.na_value     if (         is_datetime64_dtype(dtype)         or is_datetime64tz_dtype(dtype)         or is_timedelta64_dtype(dtype)         or is_period_dtype(dtype)     ):          return NaT      elif is_float_dtype(dtype):          return np.nan
def uppercase_escape(s):      return re.sub(          r'\\U([0-9a-fA-F]{8})',          lambda m: compat_chr(int(m.group(1), base=16)), s)
class RetcodesTest(LuigiTestCase):          with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):              self.run_and_expect('RequiringTask', 0)              self.run_and_expect('RequiringTask --retcode-not-run 5', 5)
def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing)
class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))          self._viewLim.intervalx = (left, right)          if auto is not None:              self._autoscaleXon = bool(auto)
default: :rc:`scatter.edgecolors`          collection = mcoll.PathCollection(                  (path,), scales,                 facecolors=colors,                 edgecolors=edgecolors,                  linewidths=linewidths,                  offsets=offsets,                  transOffset=kwargs.pop('transform', self.transData),
default: 'top'          from .tight_layout import (              get_renderer, get_subplotspec_list, get_tight_layout_figure)          subplotspec_list = get_subplotspec_list(self.axes)          if None in subplotspec_list:
def create_instance(objcls, settings, crawler, *args, **kwargs):      ``*args`` and ``**kwargs`` are forwarded to the constructors.      Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.      if settings is None:          if crawler is None:              raise ValueError("Specify at least one of settings and crawler.")          settings = crawler.settings      if crawler and hasattr(objcls, 'from_crawler'):         return objcls.from_crawler(crawler, *args, **kwargs)      elif hasattr(objcls, 'from_settings'):         return objcls.from_settings(settings, *args, **kwargs)      else:         return objcls(*args, **kwargs)  @contextmanager
class WebSocketHandler(tornado.web.RequestHandler):          if not self._on_close_called:              self._on_close_called = True              self.on_close()      def send_error(self, *args, **kwargs):          if self.stream is None:
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()         mat = numeric_df.values          if method == "pearson":             correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)          elif method == "spearman":             correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)          elif method == "kendall" or callable(method):              if min_periods is None:                  min_periods = 1             mat = ensure_float64(mat).T              corrf = nanops.get_corr_func(method)              K = len(cols)              correl = np.empty((K, K), dtype=float)
class _AxesBase(martist.Artist):              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left <= 0:                  cbook._warn_external(                      'Attempted to set non-positive left xlim on a '
def crosstab(      from pandas import DataFrame      df = DataFrame(data, index=common_idx)      if values is None:          df["__dummy__"] = 0          kwargs = {"aggfunc": len, "fill_value": 0}
