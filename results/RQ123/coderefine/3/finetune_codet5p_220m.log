Namespace(log_name='./coderefine/3/finetune_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='coderefine/3/finetune_codet5p_220m', data_dir='./data/coderefine/3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 523 training instances 
***** Running training *****
  Num examples = 523
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00038
  global_step = 66
  train_loss = 0.2356
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00038
  ********************
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 74.18 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.18
  ********************

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00036
  global_step = 131
  train_loss = 0.1435
  ********************
Previous best ppl:1.00038
Achieve Best ppl:1.00036
  ********************
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 74.75 	 Previous best codebleu 74.18
  ********************
 Achieve Best bleu:74.75
  ********************

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00035
  global_step = 196
  train_loss = 0.102
  ********************
Previous best ppl:1.00036
Achieve Best ppl:1.00035
  ********************
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 74.78 	 Previous best codebleu 74.75
  ********************
 Achieve Best bleu:74.78
  ********************

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00036
  global_step = 261
  train_loss = 0.0796
  ********************
Previous best ppl:1.00035
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 74.84 	 Previous best codebleu 74.78
  ********************
 Achieve Best bleu:74.84
  ********************

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00038
  global_step = 326
  train_loss = 0.061
  ********************
Previous best ppl:1.00035
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 74.57 	 Previous best codebleu 74.84
  ********************

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00041
  global_step = 391
  train_loss = 0.0452
  ********************
Previous best ppl:1.00035
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 74.6 	 Previous best codebleu 74.84
  ********************

***** Running evaluation *****
  Num examples = 65
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00043
  global_step = 456
  train_loss = 0.0367
  ********************
Previous best ppl:1.00035
BLEU file: ./data/coderefine/3/validation.jsonl
  codebleu-4 = 73.96 	 Previous best codebleu 74.84
  ********************
reload model from coderefine/3/finetune_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/coderefine/3/test.jsonl
  codebleu = 76.06 
  Total = 65 
  Exact Fixed = 1 
[21]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 65 
  Exact Fixed = 1 
[21]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 76.06 
[0.8114146497860415, 0.8511738523629877, 0.8249591016210021, 0.7709490070828129, 0.8487996180681285, 0.8363409602259708, 0.7709000729343136, 0.6204559138725771, 0.7998588350875768, 0.70926760466914, 0.748853825117711, 0.7075239702320301, 0.6094651919336411, 0.8890488806217476, 0.7862498146612795, 0.8363187078811223, 0.7122280471434537, 0.7202258343524188, 0.5852631285176734, 0.8882223493502044, 0.830004606415667, 0.8255914081198927, 0.7554664934124026, 0.7489847885942358, 0.8236488512415092, 0.6826750995383957, 0.7446694548111322, 0.7444445080237931, 0.8272047904656293, 0.845578753456268, 0.6813318902417225, 0.6974142146146509, 0.5677947661680006, 0.8039385856664942, 0.7746312155966322, 0.6298945625997883, 0.7407594667567519, 0.6534950189211068, 0.7319733117136124, 0.6579407445009879, 0.7633128779022263, 0.7922738397582165, 0.7630059082740179, 0.8110281732862805, 0.8262595919082771, 0.6386503838934359, 0.7449454385354564, 0.8748043250247002, 0.7644748991843855, 0.583099617149549, 0.8444372433296989, 0.6344340556808374, 0.7771473051828641, 0.7037908434637395, 0.861689146169665, 0.718168693013939, 0.8107689165434471, 0.7754542796544724, 0.8315815887018958, 0.9368787079450165, 0.7315806865860484, 0.8498496172039218, 0.8784078895034853, 0.8573171881204411, 0.5735283309621977]
Finish training and take 7m
