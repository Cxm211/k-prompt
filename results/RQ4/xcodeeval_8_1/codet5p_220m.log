Namespace(log_name='./RQ5/xcodeeval_8_1/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_8_1/codet5p_220m', data_dir='./data/RQ5/xcodeeval_8_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> #include <string.h> int main() {     char str[100000];     int i=0,k,l,j;     scanf("%s",str);     l=strlen(str);     while(str[i]!=\'\\0\')     {         if(str[i]==\'0\')         {             k=1;             break;         }         i++;     }     if(k==1)     {         for(j=i;j<l;j++)         {             str[j]=str[j+1];         }     }     else     {         for(i=0;i<l;i++)         {             str[i]=str[i+1];         }     }     puts(str);     return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> #include <string.h> int main() {     char str[100000];     int i=0,k=0,l,j;     scanf("%s",str);     l=strlen(str);     while(str[i]!=\'\\0\')     {         if(str[i]==\'0\')         {             k=1;             break;         }         i++;     }     if(k==1)     {         for(j=i;j<l;j++)         {             str[j]=str[j+1];         }     }     else     {         for(i=0;i<l;i++)         {             str[i]=str[i+1];         }     }     puts(str);     return 0; }'}]
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.936635966233584e+299
  global_step = 2
  train_loss = 176.4283
  ********************
Previous best ppl:inf
Achieve Best ppl:3.936635966233584e+299
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 25.67 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:25.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.1537237320510761e+264
  global_step = 3
  train_loss = 173.7045
  ********************
Previous best ppl:3.936635966233584e+299
Achieve Best ppl:1.1537237320510761e+264
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 64.11 	 Previous best codebleu 25.67
  ********************
 Achieve Best bleu:64.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.5381246432894724e+253
  global_step = 4
  train_loss = 116.1291
  ********************
Previous best ppl:1.1537237320510761e+264
Achieve Best ppl:1.5381246432894724e+253
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 69.25 	 Previous best codebleu 64.11
  ********************
 Achieve Best bleu:69.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 8.426824898072361e+266
  global_step = 5
  train_loss = 98.8693
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 70.58 	 Previous best codebleu 69.25
  ********************
 Achieve Best bleu:70.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.3588579722485035e+289
  global_step = 6
  train_loss = 84.2349
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 73.22 	 Previous best codebleu 70.58
  ********************
 Achieve Best bleu:73.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 70.5095
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 73.21 	 Previous best codebleu 73.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 61.4265
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 73.69 	 Previous best codebleu 73.22
  ********************
 Achieve Best bleu:73.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 58.4361
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 73.69 	 Previous best codebleu 73.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 51.8795
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 73.63 	 Previous best codebleu 73.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 49.5459
  ********************
Previous best ppl:1.5381246432894724e+253
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 73.63 	 Previous best codebleu 73.69
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_8_1/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_8_1/test.jsonl
  codebleu = 72.81 
  Total = 500 
  Exact Fixed = 5 
[54, 59, 188, 408, 455]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[268, 276]
  ********************
  Total = 500 
  Exact Fixed = 5 
[54, 59, 188, 408, 455]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[268, 276]
  codebleu = 72.81 
[0.6906751815123662, 0.4605463004182906, 0.8360786055940534, 0.9708205971615451, 0.9323764597381581, 0.9544741722900356, 0.9093088354127243, 0.2930837534053943, 0.28590084848815933, 0.5896551724137931, 0.33757627523685907, 0.8239908684888106, 0.8840280108223306, 0.9357255678321519, 0.8738548389086691, 0.9904619066549505, 0.8694464017012884, 0.8967411888266017, 0.8944958165491306, 0.8960614087063516, 0.8060262870788031, 0.896734987325442, 0.9888412502412365, 0.9120667022984088, 0.41165801720241146, 0.9657582590366391, 0.34393669628153556, 0.6927527844201661, 0.9552731235682033, 0.5800323580497375, 0.9609938513658753, 0.9540138870746515, 0.4669941875462229, 0.7138182723577209, 0.7801350289975862, 0.7480983802318328, 0.7810019348324313, 0.8209475165854486, 0.9513060711945898, 0.03914807619997354, 0.7869554753323493, 0.6439737215147783, 0.7646504374656518, 0.4262386448606621, 0.9028978279925957, 0.9673001684656128, 0.9722452161669002, 0.9607576277793455, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6751922228302627, 0.4096453519493349, 1.0, 0.034872658658241296, 0.2652100539123435, 0.779334003821368, 0.8817623929694763, 0.9753396087264441, 0.8481153341699572, 0.9837171835242919, 0.9447089104540864, 0.7569425689576943, 0.347162812695482, 0.9439321531160825, 0.4982318233525185, 0.31109151847105065, 0.9109341430050246, 0.9220017344612644, 0.4954735182274035, 0.7102686636556309, 0.3294036734712466, 0.6463782423660047, 0.7784759480546787, 0.4300770341785877, 0.4080703027889234, 0.6582969119790161, 0.9421953806476688, 0.37613929647805266, 0.8612986647888846, 0.9564084233825603, 0.8514062043486039, 0.1351038802052721, 0.9809329721714535, 0.9105346103428551, 0.9295279223019721, 0.8859407157558806, 0.5994398550450886, 0.4443730302862996, 0.7075727841380755, 0.9168021557818813, 0.03638680962604285, 0.9106033686427741, 0.3532450248136815, 0.8093822547341458, 0.49163705307857286, 0.47058078912383317, 0.5559045691482296, 0.36163873606499564, 0.48795243922324283, 0.8668858102395753, 0.8884437011415969, 0.8203495978509039, 0.9193864599685562, 0.8947330957271415, 0.5961510984329409, 0.5003909451993059, 0.6149849603928601, 0.9168539881373812, 0.5556620868852119, 0.8324183912964096, 0.14992380558663324, 0.7914278759686615, 0.9410126499841198, 0.9092903522106406, 0.9043869624480938, 0.28514244725937654, 0.9296362830963647, 0.41093966321126485, 0.7480407185735214, 0.9183782794156394, 0.8846521509197445, 0.4542071293267559, 0.9430357743776667, 0.7684232762284017, 0.7614331493899362, 0.9368495847081082, 0.976435199722677, 0.9656770287836146, 0.3292877947207648, 0.9416744734511042, 0.8165996742814894, 0.9739901642593853, 0.9631318837364407, 0.9678539444631555, 0.9427403770588954, 0.9798219468937557, 0.2984600211190037, 0.9560322423864296, 0.5739579759923696, 0.9732129467715989, 0.9059848678875759, 0.7332328509698608, 0.4885502139893544, 0.3157691625069721, 0.7846138466894739, 0.27086338909565627, 0.46622047742046324, 0.5652863984335796, 0.9299552251878043, 0.5010923309399078, 0.8267162447332495, 0.5031028066634594, 0.8624100358037953, 0.9435407234863082, 0.4775369858583672, 0.7223951844558967, 0.927982876787582, 0.9284993151613197, 0.9793040838341944, 0.7246670782300332, 0.6567874018519466, 0.7787797669196548, 0.3905993382006971, 0.3821796477166763, 0.40993941483876695, 0.9654461577493623, 0.8989320528341815, 0.7942417816647632, 0.782794014430714, 0.8815079864399178, 0.8649251515455869, 0.6052970083217083, 0.9384697992732594, 0.870281837636893, 0.3706598417143405, 0.8792228737352459, 0.9902911708211317, 0.345677647884599, 0.7065722131859199, 0.8042710150175236, 0.7318164187257492, 0.48082565379666875, 0.8992562005799716, 0.977833242375374, 0.9498158986114876, 0.8829948461632615, 0.946689158675359, 0.956301548538977, 0.9103506290794949, 0.827508035300462, 0.8585683608729555, 0.6163950321676452, 0.7755461446129143, 0.9315902507350824, 0.8241047036645743, 0.9260679831705607, 0.3415118170513282, 0.334794883347333, 0.9346720862236988, 0.9679884172683169, 0.4815374018033098, 0.8396431642664623, 0.9130555191813399, 0.9567759655767663, 0.825040224285887, 0.9048754517069983, 0.7150503754000699, 0.41474785475115006, 0.8401476187531387, 0.43419770508491723, 0.940598945729278, 0.522046098185854, 0.6895051363145968, 0.7251893956393104, 0.2632327198499751, 0.9589077111122837, 0.7276139660588652, 0.8535313467431398, 0.6386890640788627, 0.9699392701042773, 0.6894235860370095, 0.7470289772756651, 0.30629628128160413, 0.9425018741615083, 0.5313357348360503, 0.6728140986039998, 0.8898733743967635, 0.5784302566819072, 0.20906455911542446, 0.7836353492903962, 0.6547616013130733, 0.4183796987789526, 0.917570605188665, 0.6121051294344302, 0.578502292273178, 0.8207174296242723, 0.5831163274893049, 0.6916629272878606, 0.9076387847445839, 0.774674995957515, 0.6871897384788783, 0.26383991754845815, 0.2814867841617835, 0.5362395483988209, 0.843269692587079, 0.49095514733941775, 0.9802031318403173, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.9789628153220145, 0.4234097157305317, 0.9163819539750226, 0.6543858814635451, 0.9251375134812259, 0.9089815080638761, 0.737357448673426, 0.5177405757219062, 0.939230675483474, 0.38163008179290037, 0.41574277333701404, 0.6486630160206606, 0.8579701703713762, 0.6898800585173648, 0.9798514722492089, 0.37734919701699515, 0.9428491633077438, 0.9179853420852879, 0.6982229672813605, 0.8344780346578161, 0.9239858991360816, 0.9522217025290924, 0.23899729367746342, 0.27949142984930087, 0.9259714761461866, 0.927607533719276, 0.9143261256348318, 0.4155596630877777, 0.8533534577368376, 0.9335837927812758, 0.48228443707897417, 0.9352829048891074, 0.9596628528083495, 0.7253639326470425, 0.9322263155732342, 0.7056479811264925, 0.09240783929382662, 0.8102000924296275, 0.810126404694016, 0.8984188648676468, 0.9176349328120867, 0.9473491006263874, 0.8169372279002292, 0.9470722443623478, 0.8629713951467047, 0.1641943266187794, 0.9341110513082564, 0.49871330990748675, 0.7155662634597175, 0.7738062382871427, 0.5806003233954369, 0.775767676250582, 0.849729607166194, 0.8149571768480716, 0.857366514241093, 0.9121176482601447, 0.6283052858442292, 0.8302530908052741, 0.7485066002648197, 0.7493502972740398, 0.5992678325990608, 0.9881263545755623, 0.7637496742313488, 0.9820653195195825, 0.9616102999368312, 0.5012143983741278, 0.05535408155334448, 0.9664437619662871, 0.9147509231273736, 0.7766555135572375, 0.9627523265286426, 0.7972496829205722, 0.4140319065100101, 0.41243081325512787, 0.8861971446522757, 0.8154187738366516, 0.7015252862450747, 0.5896551724137931, 0.9707135589044302, 0.9100561171054498, 0.9608927757854613, 0.939821716744068, 0.7910429684270661, 0.7993280911383452, 0.8542311710040063, 0.97633392937918, 0.19832929234074576, 0.9406018863753673, 0.9887423993227424, 0.7993209869724084, 0.3698346382175046, 0.9829927321997591, 0.3273472980821144, 0.5074834355521683, 0.9648362508783428, 0.7772868173384335, 0.9332689017753089, 0.6688983661442429, 0.9672403188654792, 0.6516235135434673, 0.904647919869322, 0.8164468849403668, 0.4874181245052445, 0.7852033214625844, 0.9539726320203694, 0.9454632184934535, 0.27459370351787593, 0.9482157910035569, 0.6836831220105536, 0.6136084613959742, 0.40349767546797766, 0.8617675603039494, 0.48517394369404276, 0.8674701285162645, 0.05652271482930384, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.9469349502523363, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.6359084461047401, 0.7146371215272531, 0.40393690877945065, 0.9576736878812553, 0.8894563062341931, 0.6152342136628105, 0.6521898181329204, 0.8936598047708784, 0.3723588344016272, 0.9731496374915098, 0.8250877060561935, 0.3160565674525819, 0.7259849312989834, 0.8311449442914449, 0.9747348871931607, 0.9016204944152904, 0.3538483266922394, 0.8712420862261012, 0.40463236212571363, 0.9323453252450715, 0.7274120049815016, 0.461329065338275, 0.0864718984572822, 0.9018982179656195, 0.7900451252092993, 0.5135793600096521, 0.3901605146623929, 0.9177144543337801, 0.2546672438170512, 0.8834217951950976, 0.6549057600812135, 0.7706648767181559, 0.907558018935852, 0.8642224065245785, 1.0, 0.5588208186817714, 0.9157728873149173, 0.5266962094217575, 0.30200698386631886, 0.4329395571954886, 0.8961704370267427, 0.9027309916196755, 0.24631945218824464, 0.9714552656085744, 0.8502580434104035, 0.9732456940628658, 0.2579343193335908, 0.9654855521209342, 0.9133867250186578, 0.9128071327141931, 0.35645956062126843, 0.9668495847081082, 0.8019711204854889, 0.7348529689836785, 0.41399244311108085, 0.7153099526991196, 0.5864458370254797, 0.687542465552887, 0.9469672199403614, 0.5587912679655723, 0.05987019451564787, 0.9381573597220001, 0.8807613352737074, 0.9549636479816477, 0.6912905988814966, 0.8230038183585568, 0.7691694617413003, 0.9685698290675941, 0.3478242508876328, 0.9427008679312978, 0.9582017829875438, 0.6599202262871035, 0.3752162382299256, 0.15, 0.8055864673667326, 0.5405035784570433, 0.8598728735684844, 0.37333812510379816, 0.9117475844136459, 0.7472595462473427, 0.96804234407929, 0.9409121125706139, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5532704154064599, 0.9399257675249202, 0.9151731735749239, 0.8122136254293291, 0.9498733743967636, 0.9638457899322357, 0.7076164847141606, 0.816013750513627, 0.2973434798199742, 0.9526474078906453, 0.7657538252330831, 0.7771038806090877, 0.38230642541905163, 0.5833753791475811, 0.7294326952669937, 0.8896531297074333, 0.8391509636324979, 0.6569927900173904, 0.9086210618726622, 0.8828849822466518, 0.8664855589488811, 0.9417139803454657, 0.8483476509560168, 0.74313391043827, 0.38167413960632135, 0.576816427634421, 0.5564171303161877, 0.5763413299251462, 0.8637424870124135, 0.9154854838904751, 0.9860013241305441, 0.9126674102453449, 0.9758294505908078, 0.9166795416411146, 0.6008587815258186, 0.9064913890104815, 0.9767310708621901, 0.8270021204349962, 0.5272100939284846, 0.9177221125409464, 0.7099311392324364, 0.362136412185573]
Finish training and take 45m
