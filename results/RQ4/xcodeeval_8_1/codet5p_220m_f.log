Namespace(log_name='./RQ5/xcodeeval_8_1/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_8_1/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_8_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00043
  global_step = 2
  train_loss = 0.7137
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00043
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 16.09 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.09
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00042
  global_step = 3
  train_loss = 0.716
  ********************
Previous best ppl:1.00043
Achieve Best ppl:1.00042
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 46.45 	 Previous best codebleu 16.09
  ********************
 Achieve Best bleu:46.45
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00039
  global_step = 4
  train_loss = 0.5197
  ********************
Previous best ppl:1.00042
Achieve Best ppl:1.00039
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 48.78 	 Previous best codebleu 46.45
  ********************
 Achieve Best bleu:48.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00037
  global_step = 5
  train_loss = 0.4633
  ********************
Previous best ppl:1.00039
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 54.98 	 Previous best codebleu 48.78
  ********************
 Achieve Best bleu:54.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00036
  global_step = 6
  train_loss = 0.3515
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00036
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 61.1 	 Previous best codebleu 54.98
  ********************
 Achieve Best bleu:61.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00036
  global_step = 7
  train_loss = 0.3657
  ********************
Previous best ppl:1.00036
Achieve Best ppl:1.00036
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 65.34 	 Previous best codebleu 61.1
  ********************
 Achieve Best bleu:65.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00035
  global_step = 8
  train_loss = 0.3157
  ********************
Previous best ppl:1.00036
Achieve Best ppl:1.00035
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 66.79 	 Previous best codebleu 65.34
  ********************
 Achieve Best bleu:66.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00035
  global_step = 9
  train_loss = 0.3077
  ********************
Previous best ppl:1.00035
Achieve Best ppl:1.00035
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 68.85 	 Previous best codebleu 66.79
  ********************
 Achieve Best bleu:68.85
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00036
  global_step = 10
  train_loss = 0.2756
  ********************
Previous best ppl:1.00035
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 69.08 	 Previous best codebleu 68.85
  ********************
 Achieve Best bleu:69.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00036
  global_step = 11
  train_loss = 0.2527
  ********************
Previous best ppl:1.00035
BLEU file: ./data/RQ5/xcodeeval_8_1/validation.jsonl
  codebleu-4 = 69.34 	 Previous best codebleu 69.08
  ********************
 Achieve Best bleu:69.34
  ********************
reload model from RQ5/xcodeeval_8_1/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_8_1/test.jsonl
  codebleu = 70.22 
  Total = 500 
  Exact Fixed = 2 
[59, 359]
  Syntax Fixed = 1 
[228]
  Cleaned Fixed = 3 
[268, 276, 424]
  ********************
  Total = 500 
  Exact Fixed = 2 
[59, 359]
  Syntax Fixed = 1 
[228]
  Cleaned Fixed = 3 
[268, 276, 424]
  codebleu = 70.22 
[0.6906751815123662, 0.4605463004182906, 0.8702165828687908, 0.9289530895963891, 0.94502845638256, 0.9544741722900356, 0.8891914778698475, 0.2930837534053943, 0.28590084848815933, 0.5789473684210527, 0.30989767702652005, 0.7781235710525856, 0.8840280108223306, 0.8885734084433052, 0.5984835782442777, 0.9904619066549505, 0.8694464017012884, 0.8094536349262704, 0.8795795664077495, 0.8685842432291047, 0.28675490515154595, 0.896734987325442, 0.9888412502412365, 0.9120667022984088, 0.37648976095107356, 0.9545065027982036, 0.9900076373246287, 0.6895886374885628, 0.9770381149747873, 0.36688277713419926, 0.9417886615125295, 0.9266829495355062, 0.4669941875462229, 0.7044909929517218, 0.9324926543618852, 0.7480983802318328, 0.7515888524958438, 0.8437857090912297, 0.926574392539135, 0.03914807619997354, 0.5267515825171586, 0.6797481699178505, 0.7452955987559744, 0.4262386448606621, 0.9226227924173891, 0.9472011333202512, 0.9103068259298337, 0.7495741826264469, 0.7586086319061678, 0.8458233745010357, 0.7712904961153575, 0.6751922228302627, 0.4096453519493349, 0.961684085612954, 0.035053425263124334, 0.287051328308959, 0.779334003821368, 0.7470714894123271, 0.9753396087264441, 0.8481153341699572, 0.9837171835242919, 0.9167903900156287, 0.8113688567236648, 0.3066784807085365, 0.7016223905406086, 0.4982318233525185, 0.9829433066762787, 0.8924773415879181, 0.8312235875600145, 0.4936529526137271, 0.6347947120941286, 0.3294036734712466, 0.6463782423660047, 0.7676880486891622, 0.4218445404249763, 0.4072595760539546, 0.6229766942727039, 0.8693762249269474, 0.4388332956754316, 0.8139960896866854, 0.902227392888812, 0.8165897915483308, 0.763381172639277, 0.9809329721714535, 0.5838182742544287, 0.9295279223019721, 0.855038347382765, 0.5650232552120836, 0.4443730302862996, 0.7075727841380755, 0.9168021557818813, 0.04035349214255152, 0.8298976682212666, 0.3532450248136815, 0.7775157443525782, 0.49163705307857286, 0.48237959681980536, 0.5007722042578182, 0.9901350861027634, 0.484853818541481, 0.6495831784458661, 0.8543636012139875, 0.8203495978509039, 0.8760134136252631, 0.8947330957271415, 0.5757171601772311, 0.4999369426424631, 0.6110762010927913, 0.8913542339847487, 0.6375916648255536, 0.8324183912964096, 0.1535807038440256, 0.7914278759686615, 0.9617458190493637, 0.8999258106444634, 0.8528753639081896, 0.2727470035787713, 0.8892927565014872, 0.41236826061313914, 0.6893702256778609, 0.693353313900843, 0.9566283907946067, 0.4542071293267559, 0.9429302557017305, 0.766922089415516, 0.7614331493899362, 0.8818142221030589, 0.9610386256809607, 0.7143287228908768, 0.3389878729907265, 0.7687436519360417, 0.7697862988312114, 0.9739901642593853, 0.8660043964743054, 0.9678539444631555, 0.9131970537033065, 0.9798219468937557, 0.2984600211190037, 0.4067415296788903, 0.5739579759923696, 0.9645452834115844, 0.9236644367804976, 0.7366033348508686, 0.47428344360309693, 0.3121392853809564, 0.7846138466894739, 0.27086338909565627, 0.46622047742046324, 0.6626752830941247, 0.8874553434648698, 0.5010923309399078, 0.7915359360823997, 0.4997698113105318, 0.8624100358037953, 0.8812187328932486, 0.46104156185333434, 0.7223951844558967, 0.927982876787582, 0.9284993151613197, 0.9594655348368253, 0.48317659830974996, 0.6567874018519466, 0.7313130191686347, 0.3931548138311015, 0.38266547767619047, 0.40888908514404515, 0.9560198040025774, 0.7208418878517688, 0.3058355996530527, 0.7313790046779355, 0.7980269957235562, 0.8088898441066776, 0.6046409633357664, 0.9384697992732594, 0.5857414878263342, 0.39053634336136483, 0.8381733938908194, 0.97395721679217, 0.345677647884599, 0.7065722131859199, 0.7906346513811601, 0.5659999641778551, 0.517923330564907, 0.9462554006440358, 0.9673356569173506, 0.9709756774276397, 0.8324109597356506, 0.39656890392634686, 0.9490638577699793, 0.5890603091936865, 0.733460957643463, 0.63597476664485, 0.6163950321676452, 0.7522429154325503, 0.9466402786888075, 0.8478579993253472, 0.9260679831705607, 0.41447819544194736, 0.3475222634058708, 0.9179038691266712, 0.8891032490154938, 0.4815374018033098, 0.8396431642664623, 0.8094478998227173, 0.880749935653873, 0.825040224285887, 0.9036839434457644, 0.698568942963664, 0.41474785475115006, 0.8157403834958457, 0.43419770508491723, 0.940598945729278, 0.5315782939047562, 0.6895051363145968, 0.7945502142295664, 0.2632327198499751, 0.9468732114706748, 0.7334700763841198, 0.7818128797873793, 0.6386890640788627, 0.7706373616030229, 0.719035598602503, 0.7470289772756651, 0.31206551205083494, 0.9425018741615083, 0.5313357348360503, 0.6728140986039998, 0.9120837307732141, 0.5603126592475844, 0.20906455911542446, 0.6280565413194112, 0.6667550815530654, 0.42203823536431845, 0.9223899942569893, 0.6121051294344302, 0.5859400643182706, 0.7801747352106407, 0.5426315228433019, 0.6916629272878606, 0.8910304002143934, 0.7531581833114969, 0.6871897384788783, 0.8807436894790595, 0.28431929857133575, 0.4563152897311111, 0.8405463180578628, 0.4951906090706487, 0.8510490983872387, 0.8175867052313311, 0.915465568076623, 0.884094945342631, 0.7134429515280333, 0.4234097157305317, 0.8784019707307191, 0.6532073615386047, 0.6912599892441965, 0.8959462362143327, 0.713357448673426, 0.4872724912282442, 0.795954996852782, 0.38347177458286896, 0.46590620990723, 0.6814998932270734, 0.8070911076777747, 0.6653575999651403, 0.9798514722492089, 0.3842790621389744, 0.9428491633077438, 0.7417662583325522, 0.6982229672813605, 0.6237760530325492, 0.9239858991360816, 0.7407977949472733, 0.24038547115651787, 0.24742950191832275, 0.9259714761461866, 0.914831901042849, 0.8063181053553972, 0.4155596630877777, 0.7104623810012809, 0.9289929678685975, 0.48228443707897417, 0.9352829048891074, 0.8930257582055716, 0.7631695126965249, 0.9322263155732342, 0.7736692494156285, 0.089996975657093, 0.9462525423248562, 0.8102944719209066, 0.8984188648676468, 0.9176349328120867, 0.9473491006263874, 0.8169372279002292, 0.9162855444275764, 0.5883228774921581, 0.11465426030442766, 0.9306945788766983, 0.5066534145503407, 0.5750582617526012, 0.7738062382871427, 0.6113392265382966, 0.7743905786909704, 0.849729607166194, 0.8121978155567737, 0.7764408806548659, 0.842677577192681, 0.5659799968487437, 0.823836494500974, 0.7557968074254318, 0.7723662058843723, 0.49291175879973687, 0.970465351109826, 0.6960098931409482, 0.9726116006390206, 0.9165279116375977, 0.22878111871737256, 0.05479553230134265, 0.9747269061338388, 0.9147509231273736, 0.7424542859388822, 0.8796689205827106, 0.75573564821196, 0.39499392952802626, 0.4169073599388742, 0.8605133853244755, 0.8025665922220764, 0.7219059133242709, 0.5675675675675675, 0.9417960043391369, 0.8901784788422495, 0.9713651945608406, 0.8710529271820026, 0.9785132817927387, 0.6062523265015279, 0.8542311710040063, 0.9598145187525169, 0.19828703696081484, 0.9706304665534506, 0.9008704068195377, 0.8004514489168911, 0.3776268460097124, 0.9212143130575443, 0.3616444630348249, 0.5903480353298853, 0.9361840954190694, 0.7772868173384335, 0.9364608897087561, 0.6688983661442429, 0.9784080121481948, 0.6598038918531729, 0.9068495810967309, 0.8164468849403668, 0.4724181245052445, 0.7852033214625844, 0.17017806673421706, 0.8989987651636393, 0.27459370351787593, 0.9701742405810041, 0.6836831220105536, 0.615202664294525, 0.4314716423503189, 0.8531733285745522, 0.48517394369404276, 0.8674701285162645, 0.05914963589138415, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.7001975630219108, 0.7750614375718583, 0.4966154218075377, 0.8437436803649183, 0.5795301464266706, 0.6901826168267113, 0.31707345191643455, 0.9377852922161223, 0.8644503186595438, 0.5152647067985198, 0.6521898181329204, 0.8686598047708785, 0.38361540357357476, 0.9731496374915098, 0.8250877060561935, 0.33295797590328613, 0.2761108214401674, 0.8467013377193615, 0.9775679606773502, 0.868954402487071, 0.3623365084313141, 0.8712420862261012, 0.38482189876773926, 0.8889915329354385, 0.6598643301286921, 0.4546623986716084, 0.0864718984572822, 0.9030225195673945, 0.7900451252092993, 0.5272157236460158, 0.3860118883605762, 0.8803505779176037, 0.2499053390551464, 0.8803598841016611, 0.6465973188677442, 0.7629267814800605, 0.907558018935852, 0.8642224065245785, 0.9263286570076802, 0.49095602175976216, 0.8621057453081831, 0.5266962094217575, 0.30541322411928357, 0.3801054189042712, 0.897227698675753, 0.8497251305586286, 0.25160207241540583, 0.9714552656085744, 0.8776322774783345, 0.9732456940628658, 0.2579343193335908, 0.9654855521209342, 0.881460584546889, 0.9397626412560047, 0.9653175874774487, 0.9668495847081082, 0.7069428956668239, 0.7145240119690182, 0.38336013014635484, 0.6933604779837957, 0.5864458370254797, 0.687542465552887, 0.9139609569324616, 0.5587912679655723, 0.05901330852413053, 0.955153310850831, 0.8807613352737074, 0.978350857747293, 0.6912905988814966, 0.7697451983440904, 0.8132925569758128, 0.9685698290675941, 0.3478242508876328, 0.8572969093427243, 0.9833885445588753, 0.6582887637344705, 0.3737662111795117, 0.23928035982008997, 0.7780358142888271, 0.5498226092525664, 0.8598728735684844, 0.3702534090053835, 0.9117475844136459, 0.6626202226797823, 0.38730513293929447, 0.8932345101220579, 0.36168412495357327, 0.6690855766574787, 0.9296835011666059, 0.5740737169642371, 0.9632825394277797, 0.9151731735749239, 0.8122136254293291, 0.9498733743967636, 0.7480479830526708, 0.773611277512299, 0.42973358170255227, 0.28491251813329427, 0.9526474078906453, 0.7657538252330831, 0.785961729920259, 0.4628959302474374, 0.7582839388249971, 0.6053612547827409, 0.8896531297074333, 0.8436173548118322, 0.5105114891566459, 0.873375156140201, 0.8828849822466518, 0.8516095560468161, 0.9417139803454657, 0.8229555815364942, 0.7752855690103572, 0.38167413960632135, 0.576816427634421, 0.5574881988129607, 0.26600208430275746, 0.791926204735026, 0.9508336769881993, 0.9481560518823162, 0.9516072695606936, 0.9758294505908078, 0.9129766621063571, 0.5338105033844577, 0.9064913890104815, 0.939505437031215, 0.8739286595309739, 0.5259210844305199, 0.8241313418162175, 0.13150945238613837, 0.32507960814614334]
Finish training and take 1h3m
