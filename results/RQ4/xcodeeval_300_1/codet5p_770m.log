Namespace(log_name='./RQ5/xcodeeval_300_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_300_1/codet5p_770m', data_dir='./data/RQ5/xcodeeval_300_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> #include <string.h> #include <math.h> #include <stdlib.h>  int cmpfunc (const void * a, const void * b) {    return ( *(int*)a - *(int*)b ); }  int main() {     int n,i,sum = 0;     scanf("%d",&n);     int a[n];     for(i=0;i<n;i++){scanf("%d",a+i); sum += a[i];}     qsort(a, n, sizeof(int), cmpfunc);      int ans = sum;     int j = 0;     for(j=0;j<n-1;j++)     {         ans += a[j];         sum -= a[j];         ans += sum;     }     printf("%d",ans);      return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> #include <string.h> #include <math.h> #include <stdlib.h>  int cmpfunc (const void * a, const void * b) {    return ( *(int*)a - *(int*)b ); }  int main() {     int n,i;     long long int sum = 0;     scanf("%d",&n);     int a[n];     for(i=0;i<n;i++){scanf("%d",a+i); sum += a[i];}     qsort(a, n, sizeof(int), cmpfunc);      long long int ans = sum;     int j = 0;     for(j=0;j<n-1;j++)     {         ans += a[j];         sum -= a[j];         ans += sum;     }     printf("%lld",ans);      return 0; }'}]
***** Running training *****
  Num examples = 300
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.1343691599906374e+298
  global_step = 76
  train_loss = 70.9207
  ********************
Previous best ppl:inf
Achieve Best ppl:1.1343691599906374e+298
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.75 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 151
  train_loss = 53.7492
  ********************
Previous best ppl:1.1343691599906374e+298
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.64 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 226
  train_loss = 37.0655
  ********************
Previous best ppl:1.1343691599906374e+298
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.55 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.7547100823975747e+294
  global_step = 301
  train_loss = 25.707
  ********************
Previous best ppl:1.1343691599906374e+298
Achieve Best ppl:2.7547100823975747e+294
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.29 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 8.526242250932951e+293
  global_step = 376
  train_loss = 18.3951
  ********************
Previous best ppl:2.7547100823975747e+294
Achieve Best ppl:8.526242250932951e+293
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.25 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.148304944490169e+274
  global_step = 451
  train_loss = 11.1626
  ********************
Previous best ppl:8.526242250932951e+293
Achieve Best ppl:2.148304944490169e+274
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.03 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 7.74824668966325e+278
  global_step = 526
  train_loss = 7.4855
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 73.7 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 5.722609301910694e+285
  global_step = 601
  train_loss = 4.9592
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 73.98 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 3.209134208939412e+294
  global_step = 676
  train_loss = 3.5769
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.36 	 Previous best codebleu 74.75
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_300_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_300_1/test.jsonl
  codebleu = 74.54 
  Total = 500 
  Exact Fixed = 6 
[54, 59, 111, 188, 322, 477]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[254, 268, 276, 359, 424, 455]
  ********************
  Total = 500 
  Exact Fixed = 6 
[54, 59, 111, 188, 322, 477]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[254, 268, 276, 359, 424, 455]
  codebleu = 74.54 
[0.6906751815123662, 0.4658094583130275, 0.8702165828687908, 0.9799907063810385, 0.94502845638256, 0.9544741722900356, 0.9334580759528754, 0.2930837534053943, 0.28590084848815933, 0.23685164088784277, 0.33757627523685907, 0.8239908684888106, 0.8840280108223306, 0.9357255678321519, 0.9320028922042589, 0.9904619066549505, 0.8694464017012884, 0.9149769232414114, 0.8944958165491306, 0.8960614087063516, 0.8193236358223526, 0.896734987325442, 0.9888412502412365, 0.9714741186824503, 0.3705477276166575, 0.9657582590366391, 0.9900076373246287, 0.7042466967023711, 0.9552731235682033, 0.5800323580497375, 0.9609938513658753, 0.9540138870746515, 0.4669941875462229, 0.726665460092766, 0.9735441531161189, 0.7480983802318328, 0.7810019348324313, 0.7958032019228529, 0.9513060711945898, 0.03914807619997354, 0.8036605977879999, 0.6439737215147783, 0.7646504374656518, 0.4262386448606621, 0.9226227924173891, 0.9673001684656128, 0.9722452161669002, 0.9620084641589606, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6932485940047782, 0.40973768494010554, 1.0, 0.035053425263124334, 0.26630536517678816, 0.8267820318104704, 0.8817623929694763, 0.9753396087264441, 0.8827108485449091, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.347162812695482, 0.9821809817650384, 0.4982318233525185, 0.9829433066762787, 0.934387097307501, 0.9220017344612644, 0.4954735182274035, 0.8668947531709692, 0.3294036734712466, 0.6463782423660047, 0.7784759480546787, 0.4284201796189373, 0.4080703027889234, 0.6666962650203826, 0.9421953806476688, 0.3885946899772229, 0.8297197174204636, 0.9564084233825603, 0.9738014890069033, 0.7365455054053898, 0.9809329721714535, 0.8832690859009091, 0.9295279223019721, 0.8859407157558806, 0.5604977044094772, 0.4443730302862996, 0.7075727841380755, 0.9168021557818813, 0.028789548341145818, 0.9106033686427741, 0.3532450248136815, 0.8375795025499561, 0.49643876062226794, 0.4420093605524046, 0.48670907292950416, 0.9659330238290955, 0.48795243922324283, 0.8668858102395753, 0.8702366655053949, 0.8203495978509039, 0.9193864599685562, 0.8947330957271415, 0.5961510984329409, 0.5003909451993059, 0.7833636144245735, 0.9168539881373812, 0.5609883778095345, 1.0, 0.14992380558663324, 0.7914278759686615, 0.961472991307396, 0.9092903522106406, 0.9268555414820475, 0.2849664187950911, 0.9728421777783798, 0.41093966321126485, 0.7480407185735214, 0.9374523007440652, 0.9566283907946067, 0.4542071293267559, 0.9717916167891165, 0.7684232762284017, 0.7614331493899362, 0.922651602330101, 0.976435199722677, 0.9656770287836146, 0.33945728624618854, 0.9416744734511042, 0.8165996742814894, 0.9739901642593853, 0.9631318837364407, 0.9678539444631555, 0.9798800402828953, 0.9798219468937557, 0.3058015211465632, 0.9560322423864296, 0.5739579759923696, 0.9732129467715989, 0.9150833522873736, 0.7332328509698608, 0.47428344360309693, 0.3226943735057258, 0.7846138466894739, 0.27086338909565627, 0.4501887204811095, 0.6877917301640358, 0.9299552251878043, 0.5010923309399078, 0.8326115328890102, 0.5031028066634594, 0.872087455158634, 0.9435407234863082, 0.47392833193653083, 0.7223951844558967, 0.927982876787582, 0.9602403186628621, 0.9793040838341944, 0.7110307145936695, 0.6567874018519466, 0.7787797669196548, 0.3931548138311015, 0.3821796477166763, 0.408600022348932, 0.9654461577493623, 0.9598135790579945, 0.7942417816647632, 0.782794014430714, 0.8815079864399178, 0.8649251515455869, 0.6052970083217083, 0.9384697992732594, 0.9842551012740206, 0.3711041873826805, 0.8792228737352459, 0.9902911708211317, 0.3723443145512656, 0.7065722131859199, 0.8042710150175236, 0.7656202397674886, 0.5370318012864018, 0.8992562005799716, 0.9931785684115373, 0.9709756774276397, 0.8829948461632615, 0.946689158675359, 0.956301548538977, 0.9172004740789947, 0.981793787791766, 0.8789900106153626, 0.6163950321676452, 0.526474668670368, 0.9744119964056819, 0.8908110243776604, 0.5492010360849566, 0.41395796030346144, 0.3475222634058708, 0.9346720862236988, 0.9679884172683169, 0.4841195614277229, 0.8396431642664623, 0.9130555191813399, 0.9757745973711676, 0.825040224285887, 0.9080441504666455, 0.7207550074832697, 0.41474785475115006, 0.8712547010751163, 0.43419770508491723, 0.940598945729278, 0.5315782939047562, 0.6895051363145968, 0.7709540621971285, 0.2632327198499751, 0.9542362270693294, 0.7507483205316109, 0.9187091832462004, 0.6386890640788627, 0.9699392701042773, 0.9840867555998587, 0.8198387442649027, 0.30629628128160413, 0.9425018741615083, 0.5313357348360503, 0.6297029892537214, 0.8898733743967635, 0.5784302566819072, 0.20906455911542446, 0.7836353492903962, 0.5439420078074064, 0.42497916943931774, 0.9345121855559886, 0.6121051294344302, 0.578502292273178, 0.8207174296242723, 0.5831163274893049, 0.6916629272878606, 0.9533257646563149, 0.774674995957515, 0.6871897384788783, 0.8980438351581705, 0.28759777369338146, 0.5362395483988209, 0.9059801416992133, 0.4920928783231675, 0.9802031318403173, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.9789628153220145, 0.4234097157305317, 0.9445928919196247, 0.6922592432779943, 0.9251375134812259, 0.9089815080638761, 0.737357448673426, 0.5177405757219062, 0.939230675483474, 0.3755076328133085, 0.41574277333701404, 0.6486630160206606, 0.8703232021938421, 0.6928192511783815, 0.9798514722492089, 0.38035740917095934, 0.9428491633077438, 0.9179853420852879, 0.9399969282795693, 0.8344780346578161, 0.9239858991360816, 0.9522217025290924, 0.23899729367746342, 0.27949142984930087, 0.9259714761461866, 0.927607533719276, 0.9286279274366336, 0.4155596630877777, 0.8533534577368376, 0.9581930467118089, 0.48228443707897417, 0.9352829048891074, 0.9596628528083495, 0.7253639326470425, 0.9322263155732342, 0.7729580558472922, 0.7110639912399511, 0.9770479547618027, 0.8588461682692663, 0.8984188648676468, 0.9238932796791619, 0.9473491006263874, 0.9747232059340507, 0.9470722443623478, 0.8856423851883901, 0.1645276846716329, 0.9341110513082564, 0.5157443236412498, 0.5750582617526012, 0.7738062382871427, 0.5790261988125887, 0.8710393395645104, 0.849729607166194, 0.8121978155567737, 0.857366514241093, 0.9121176482601447, 0.6283052858442292, 0.8302530908052741, 0.7485066002648197, 0.7624146378614128, 0.8086811749495997, 0.9881263545755623, 0.7698315590133648, 0.980118408318158, 0.9616102999368312, 0.5012143983741278, 0.05231760479519017, 0.9664437619662871, 0.9147509231273736, 0.8071763268259384, 1.0, 0.938595895727768, 0.40613716966790486, 0.41243081325512787, 0.8861971446522757, 0.8154187738366516, 0.6821985572659508, 0.18507851983776646, 0.9707135589044302, 0.9100561171054498, 0.9713651945608406, 0.939821716744068, 0.9785132817927387, 0.8171220069513339, 0.8542311710040063, 0.97633392937918, 0.19832929234074576, 0.9406018863753673, 0.9887423993227424, 0.7711795825442882, 0.3776268460097124, 0.9829927321997591, 0.3273472980821144, 0.5074834355521683, 0.9872106081152878, 0.7772868173384335, 0.9332689017753089, 0.6688983661442429, 0.9784080121481948, 0.6516235135434673, 0.904647919869322, 0.8164468849403668, 0.4874181245052445, 0.7852033214625844, 0.972521995777744, 0.9454632184934535, 0.27459370351787593, 0.9580342317904542, 0.704137667465099, 0.6136084613959742, 0.47541291345050607, 0.8617675603039494, 0.4532830736687707, 0.8674701285162645, 0.05652271482930384, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.9728742867739708, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.6359084461047401, 0.7146371215272531, 0.3028445555596091, 0.9459298954511661, 0.933792626439619, 0.5507463552897676, 0.6567079107412255, 0.8936598047708784, 0.3723588344016272, 0.9731496374915098, 0.8250877060561935, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9747348871931607, 0.8858102874808143, 0.3538483266922394, 0.8712420862261012, 0.4198136333318135, 0.9323453252450715, 0.9006848455688443, 0.461329065338275, 0.0864718984572822, 0.9176516704321582, 0.7900451252092993, 0.5135793600096521, 0.3902680246863285, 0.9686198599811506, 0.2718871669900238, 0.8834217951950976, 0.6597338140559376, 0.7706648767181559, 0.907558018935852, 0.8642224065245785, 0.9778801105259323, 0.5588208186817714, 0.9157728873149173, 0.5452287811589471, 0.30387397503062613, 0.43343274350759264, 0.8961704370267427, 0.9027309916196755, 0.2555812048385557, 0.9714552656085744, 0.9190922894214792, 0.9732456940628658, 0.2579343193335908, 0.9654855521209342, 0.8384459213032074, 0.9128071327141931, 0.9541895950679766, 0.9668495847081082, 0.8019711204854889, 0.7348529689836785, 0.3532301871931892, 0.7042526158020281, 0.5864458370254797, 0.687542465552887, 0.9469672199403614, 0.5587912679655723, 0.05987019451564787, 0.9630540715975315, 0.8807613352737074, 0.9549636479816477, 0.6912905988814966, 0.8230038183585568, 0.8216258903091462, 0.9717033980223364, 0.3478242508876328, 0.9427008679312978, 0.9582017829875438, 0.7141494776867402, 0.3752162382299256, 0.38538461538461544, 0.8055864673667326, 0.5475329309937669, 0.8598728735684844, 0.37333812510379816, 0.9117475844136459, 0.9635847916815603, 0.96804234407929, 0.9306848398433412, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5649828078733281, 0.9399257675249202, 0.9205724657465093, 0.8122136254293291, 0.9498733743967636, 0.9714834981320903, 0.7488923610303595, 0.8728375908477501, 0.2973434798199742, 0.9526474078906453, 0.7657538252330831, 0.796166197021682, 0.38230642541905163, 0.587190039861028, 0.8804232565396799, 0.8896531297074333, 0.8920643294874622, 0.6575319963393351, 1.0, 0.9078336364448021, 0.8664855589488811, 0.9417139803454657, 0.8757845494319172, 0.7752855690103572, 0.3824686492796324, 0.576816427634421, 0.5446884529932624, 0.5763413299251462, 0.8698366814058456, 0.9152601179282849, 0.9860013241305441, 0.9819992034986089, 0.9758294505908078, 0.9166795416411146, 0.6008587815258186, 0.9064913890104815, 0.9788598843598264, 0.8270021204349962, 0.535011993521429, 0.9403437700960667, 0.8190595233656908, 0.3559429663587866]
Finish training and take 1h13m
Namespace(log_name='./RQ5/xcodeeval_300_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_300_1/codet5p_770m', data_dir='./data/RQ5/xcodeeval_300_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> #include <string.h> #include <math.h> #include <stdlib.h>  int cmpfunc (const void * a, const void * b) {    return ( *(int*)a - *(int*)b ); }  int main() {     int n,i,sum = 0;     scanf("%d",&n);     int a[n];     for(i=0;i<n;i++){scanf("%d",a+i); sum += a[i];}     qsort(a, n, sizeof(int), cmpfunc);      int ans = sum;     int j = 0;     for(j=0;j<n-1;j++)     {         ans += a[j];         sum -= a[j];         ans += sum;     }     printf("%d",ans);      return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> #include <string.h> #include <math.h> #include <stdlib.h>  int cmpfunc (const void * a, const void * b) {    return ( *(int*)a - *(int*)b ); }  int main() {     int n,i;     long long int sum = 0;     scanf("%d",&n);     int a[n];     for(i=0;i<n;i++){scanf("%d",a+i); sum += a[i];}     qsort(a, n, sizeof(int), cmpfunc);      long long int ans = sum;     int j = 0;     for(j=0;j<n-1;j++)     {         ans += a[j];         sum -= a[j];         ans += sum;     }     printf("%lld",ans);      return 0; }'}]
***** Running training *****
  Num examples = 300
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.1343691599906374e+298
  global_step = 76
  train_loss = 70.9207
  ********************
Previous best ppl:inf
Achieve Best ppl:1.1343691599906374e+298
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.73 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 151
  train_loss = 53.7492
  ********************
Previous best ppl:1.1343691599906374e+298
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.62 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 226
  train_loss = 37.0655
  ********************
Previous best ppl:1.1343691599906374e+298
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.53 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.7547100823975747e+294
  global_step = 301
  train_loss = 25.707
  ********************
Previous best ppl:1.1343691599906374e+298
Achieve Best ppl:2.7547100823975747e+294
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.27 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 8.526242250932951e+293
  global_step = 376
  train_loss = 18.3951
  ********************
Previous best ppl:2.7547100823975747e+294
Achieve Best ppl:8.526242250932951e+293
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.24 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.148304944490169e+274
  global_step = 451
  train_loss = 11.1626
  ********************
Previous best ppl:8.526242250932951e+293
Achieve Best ppl:2.148304944490169e+274
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.01 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 7.74824668966325e+278
  global_step = 526
  train_loss = 7.4855
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 73.69 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 5.722609301910694e+285
  global_step = 601
  train_loss = 4.9592
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 73.97 	 Previous best codebleu 74.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 3.209134208939412e+294
  global_step = 676
  train_loss = 3.5769
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.35 	 Previous best codebleu 74.73
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_300_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_300_1/test.jsonl
  codebleu = 74.54 
  Total = 500 
  Exact Fixed = 6 
[54, 59, 111, 188, 322, 477]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[254, 268, 276, 359, 424, 455]
  ********************
  Total = 500 
  Exact Fixed = 6 
[54, 59, 111, 188, 322, 477]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[254, 268, 276, 359, 424, 455]
  codebleu = 74.54 
[0.6906751815123662, 0.4658094583130275, 0.8702165828687908, 0.9799907063810385, 0.94502845638256, 0.9544741722900356, 0.9334580759528754, 0.2930837534053943, 0.28590084848815933, 0.23685164088784277, 0.33757627523685907, 0.8239908684888106, 0.8840280108223306, 0.9357255678321519, 0.9320028922042589, 0.9904619066549505, 0.8694464017012884, 0.9149769232414114, 0.8944958165491306, 0.8960614087063516, 0.8193236358223526, 0.896734987325442, 0.9888412502412365, 0.9714741186824503, 0.3705477276166575, 0.9657582590366391, 0.9900076373246287, 0.7042466967023711, 0.9552731235682033, 0.5800323580497375, 0.9609938513658753, 0.9540138870746515, 0.4669941875462229, 0.726665460092766, 0.9735441531161189, 0.7480983802318328, 0.7810019348324313, 0.7958032019228529, 0.9513060711945898, 0.03914807619997354, 0.8036605977879999, 0.6439737215147783, 0.7646504374656518, 0.4262386448606621, 0.9226227924173891, 0.9673001684656128, 0.9722452161669002, 0.9620084641589606, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6932485940047782, 0.40973768494010554, 1.0, 0.035053425263124334, 0.26630536517678816, 0.8267820318104704, 0.8817623929694763, 0.9753396087264441, 0.8827108485449091, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.347162812695482, 0.9821809817650384, 0.4982318233525185, 0.9829433066762787, 0.934387097307501, 0.9220017344612644, 0.4954735182274035, 0.8668947531709692, 0.3294036734712466, 0.6463782423660047, 0.7784759480546787, 0.4284201796189373, 0.4080703027889234, 0.6666962650203826, 0.9421953806476688, 0.3885946899772229, 0.8612986647888846, 0.9564084233825603, 0.9738014890069033, 0.7365455054053898, 0.9809329721714535, 0.8832690859009091, 0.9295279223019721, 0.8859407157558806, 0.5683924412515824, 0.4443730302862996, 0.7075727841380755, 0.9168021557818813, 0.028789548341145818, 0.9106033686427741, 0.3532450248136815, 0.8375795025499561, 0.49643876062226794, 0.4420093605524046, 0.48670907292950416, 0.9659330238290955, 0.48795243922324283, 0.8668858102395753, 0.8702366655053949, 0.8203495978509039, 0.9193864599685562, 0.8947330957271415, 0.5961510984329409, 0.5003909451993059, 0.7833636144245735, 0.9168539881373812, 0.5609883778095345, 1.0, 0.14992380558663324, 0.7914278759686615, 0.961472991307396, 0.9092903522106406, 0.9268555414820475, 0.2849664187950911, 0.9728421777783798, 0.41093966321126485, 0.7480407185735214, 0.9374523007440652, 0.9566283907946067, 0.4542071293267559, 0.9717916167891165, 0.7684232762284017, 0.7614331493899362, 0.922651602330101, 0.976435199722677, 0.9656770287836146, 0.33945728624618854, 0.9416744734511042, 0.8165996742814894, 0.9739901642593853, 0.9631318837364407, 0.9678539444631555, 0.9798800402828953, 0.9798219468937557, 0.3058015211465632, 0.9560322423864296, 0.5739579759923696, 0.9732129467715989, 0.9150833522873736, 0.7332328509698608, 0.47428344360309693, 0.3226943735057258, 0.7846138466894739, 0.27086338909565627, 0.4501887204811095, 0.6877917301640358, 0.9299552251878043, 0.5010923309399078, 0.8326115328890102, 0.5031028066634594, 0.8624100358037953, 0.9435407234863082, 0.47392833193653083, 0.7223951844558967, 0.927982876787582, 0.9602403186628621, 0.9793040838341944, 0.7110307145936695, 0.6567874018519466, 0.7787797669196548, 0.3931548138311015, 0.3821796477166763, 0.408600022348932, 0.9654461577493623, 0.9598135790579945, 0.7942417816647632, 0.782794014430714, 0.8815079864399178, 0.8649251515455869, 0.6052970083217083, 0.9384697992732594, 0.9842551012740206, 0.3711041873826805, 0.8792228737352459, 0.9902911708211317, 0.3723443145512656, 0.7065722131859199, 0.8042710150175236, 0.7656202397674886, 0.5370318012864018, 0.8992562005799716, 0.9931785684115373, 0.9709756774276397, 0.8829948461632615, 0.946689158675359, 0.956301548538977, 0.9172004740789947, 0.981793787791766, 0.8856566772820293, 0.6163950321676452, 0.526474668670368, 0.9744119964056819, 0.8908110243776604, 0.5492010360849566, 0.41395796030346144, 0.3475222634058708, 0.9346720862236988, 0.9679884172683169, 0.4841195614277229, 0.8396431642664623, 0.9130555191813399, 0.9757745973711676, 0.825040224285887, 0.9080441504666455, 0.7207550074832697, 0.41474785475115006, 0.8712547010751163, 0.43419770508491723, 0.940598945729278, 0.5315782939047562, 0.6895051363145968, 0.7709540621971285, 0.2632327198499751, 0.9542362270693294, 0.7507483205316109, 0.9187091832462004, 0.6386890640788627, 0.9699392701042773, 0.9840867555998587, 0.8198387442649027, 0.30629628128160413, 0.9425018741615083, 0.5313357348360503, 0.6297029892537214, 0.8898733743967635, 0.5784302566819072, 0.20906455911542446, 0.7836353492903962, 0.5439420078074064, 0.42497916943931774, 0.9345121855559886, 0.6121051294344302, 0.578502292273178, 0.8207174296242723, 0.5831163274893049, 0.6916629272878606, 0.9533257646563149, 0.774674995957515, 0.6871897384788783, 0.8980438351581705, 0.28759777369338146, 0.5362395483988209, 0.9059801416992133, 0.4920928783231675, 0.9802031318403173, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.9789628153220145, 0.4234097157305317, 0.9445928919196247, 0.6922592432779943, 0.9251375134812259, 0.9089815080638761, 0.737357448673426, 0.5177405757219062, 0.939230675483474, 0.3755076328133085, 0.41574277333701404, 0.6486630160206606, 0.8703232021938421, 0.6928192511783815, 0.9798514722492089, 0.38035740917095934, 0.9428491633077438, 0.9179853420852879, 0.9399969282795693, 0.8344780346578161, 0.9239858991360816, 0.9522217025290924, 0.23899729367746342, 0.27949142984930087, 0.9259714761461866, 0.927607533719276, 0.9286279274366336, 0.4155596630877777, 0.8533534577368376, 0.9581930467118089, 0.48228443707897417, 0.9352829048891074, 0.9596628528083495, 0.7253639326470425, 0.9322263155732342, 0.7729580558472922, 0.7110639912399511, 0.9770479547618027, 0.8588461682692663, 0.8984188648676468, 0.9238932796791619, 0.9473491006263874, 0.9747232059340507, 0.9470722443623478, 0.8856423851883901, 0.16640268467163288, 0.9341110513082564, 0.5157443236412498, 0.5750582617526012, 0.7738062382871427, 0.5790261988125887, 0.8710393395645104, 0.849729607166194, 0.8121978155567737, 0.857366514241093, 0.9121176482601447, 0.6283052858442292, 0.8302530908052741, 0.7485066002648197, 0.7624146378614128, 0.8086811749495997, 0.9881263545755623, 0.7698315590133648, 0.980118408318158, 0.9616102999368312, 0.5012143983741278, 0.05231760479519017, 0.9664437619662871, 0.9147509231273736, 0.8196763268259384, 1.0, 0.938595895727768, 0.3982424328257996, 0.41243081325512787, 0.8861971446522757, 0.8154187738366516, 0.6821985572659508, 0.18507851983776646, 0.9707135589044302, 0.9100561171054498, 0.9713651945608406, 0.939821716744068, 0.9785132817927387, 0.8171220069513339, 0.8542311710040063, 0.97633392937918, 0.19832929234074576, 0.9406018863753673, 0.9887423993227424, 0.7711795825442882, 0.3776268460097124, 0.9829927321997591, 0.3273472980821144, 0.5074834355521683, 0.9872106081152878, 0.7772868173384335, 0.9332689017753089, 0.6688983661442429, 0.9784080121481948, 0.6516235135434673, 0.904647919869322, 0.8164468849403668, 0.45741812450524455, 0.7852033214625844, 0.972521995777744, 0.9454632184934535, 0.27459370351787593, 0.9580342317904542, 0.704137667465099, 0.6136084613959742, 0.47541291345050607, 0.8617675603039494, 0.4532830736687707, 0.8674701285162645, 0.05652271482930384, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.9728742867739708, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.6359084461047401, 0.7146371215272531, 0.3028445555596091, 0.9459298954511661, 0.933792626439619, 0.5507463552897676, 0.6567079107412255, 0.8936598047708784, 0.3723588344016272, 0.9731496374915098, 0.8250877060561935, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9747348871931607, 0.8858102874808143, 0.3538483266922394, 0.8712420862261012, 0.4198136333318135, 0.9323453252450715, 0.9006848455688443, 0.461329065338275, 0.0864718984572822, 0.9176516704321582, 0.7900451252092993, 0.5135793600096521, 0.3902680246863285, 0.9686198599811506, 0.267125262228119, 0.8834217951950976, 0.6597338140559376, 0.7706648767181559, 0.907558018935852, 0.8642224065245785, 0.9778801105259323, 0.5588208186817714, 0.9157728873149173, 0.5452287811589471, 0.30387397503062613, 0.43343274350759264, 0.8961704370267427, 0.9027309916196755, 0.2555812048385557, 0.9714552656085744, 0.9190922894214792, 0.9732456940628658, 0.2579343193335908, 0.9654855521209342, 0.8384459213032074, 0.9128071327141931, 0.9541895950679766, 0.9668495847081082, 0.8019711204854889, 0.7348529689836785, 0.3532301871931892, 0.7042526158020281, 0.5864458370254797, 0.687542465552887, 0.9469672199403614, 0.5587912679655723, 0.05987019451564787, 0.9630540715975315, 0.8807613352737074, 0.9549636479816477, 0.6912905988814966, 0.8144323897871283, 0.8216258903091462, 0.9717033980223364, 0.3478242508876328, 0.9427008679312978, 0.9582017829875438, 0.7141494776867402, 0.3752162382299256, 0.38538461538461544, 0.8055864673667326, 0.5475329309937669, 0.8598728735684844, 0.37333812510379816, 0.9117475844136459, 0.9635847916815603, 0.96804234407929, 0.9306848398433412, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5649828078733281, 0.9399257675249202, 0.9205724657465093, 0.8122136254293291, 0.9498733743967636, 0.9714834981320903, 0.7488923610303595, 0.8728375908477501, 0.2973434798199742, 0.9526474078906453, 0.7657538252330831, 0.796166197021682, 0.38230642541905163, 0.587190039861028, 0.8804232565396799, 0.8896531297074333, 0.8920643294874622, 0.6575319963393351, 1.0, 0.9078336364448021, 0.8664855589488811, 0.9417139803454657, 0.8757845494319172, 0.7752855690103572, 0.3824686492796324, 0.576816427634421, 0.5446884529932624, 0.5763413299251462, 0.8698366814058456, 0.9152601179282849, 0.9860013241305441, 0.9819992034986089, 0.9758294505908078, 0.9166795416411146, 0.6008587815258186, 0.9064913890104815, 0.9788598843598264, 0.8270021204349962, 0.535011993521429, 0.9403437700960667, 0.8190595233656908, 0.3559429663587866]
Finish training and take 1h12m
Namespace(log_name='./RQ5/xcodeeval_300_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_300_1/codet5p_770m', data_dir='./data/RQ5/xcodeeval_300_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> #include <string.h> #include <math.h> #include <stdlib.h>  int cmpfunc (const void * a, const void * b) {    return ( *(int*)a - *(int*)b ); }  int main() {     int n,i,sum = 0;     scanf("%d",&n);     int a[n];     for(i=0;i<n;i++){scanf("%d",a+i); sum += a[i];}     qsort(a, n, sizeof(int), cmpfunc);      int ans = sum;     int j = 0;     for(j=0;j<n-1;j++)     {         ans += a[j];         sum -= a[j];         ans += sum;     }     printf("%d",ans);      return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> #include <string.h> #include <math.h> #include <stdlib.h>  int cmpfunc (const void * a, const void * b) {    return ( *(int*)a - *(int*)b ); }  int main() {     int n,i;     long long int sum = 0;     scanf("%d",&n);     int a[n];     for(i=0;i<n;i++){scanf("%d",a+i); sum += a[i];}     qsort(a, n, sizeof(int), cmpfunc);      long long int ans = sum;     int j = 0;     for(j=0;j<n-1;j++)     {         ans += a[j];         sum -= a[j];         ans += sum;     }     printf("%lld",ans);      return 0; }'}]
***** Running training *****
  Num examples = 300
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.1343691599906374e+298
  global_step = 76
  train_loss = 70.9207
  ********************
Previous best ppl:inf
Achieve Best ppl:1.1343691599906374e+298
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.75 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 151
  train_loss = 53.7492
  ********************
Previous best ppl:1.1343691599906374e+298
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.64 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 226
  train_loss = 37.0655
  ********************
Previous best ppl:1.1343691599906374e+298
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.55 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.7547100823975747e+294
  global_step = 301
  train_loss = 25.707
  ********************
Previous best ppl:1.1343691599906374e+298
Achieve Best ppl:2.7547100823975747e+294
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.29 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 8.526242250932951e+293
  global_step = 376
  train_loss = 18.3951
  ********************
Previous best ppl:2.7547100823975747e+294
Achieve Best ppl:8.526242250932951e+293
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.25 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.148304944490169e+274
  global_step = 451
  train_loss = 11.1626
  ********************
Previous best ppl:8.526242250932951e+293
Achieve Best ppl:2.148304944490169e+274
  ********************
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 74.03 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 7.74824668966325e+278
  global_step = 526
  train_loss = 7.4855
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 73.7 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 5.722609301910694e+285
  global_step = 601
  train_loss = 4.9592
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
  codebleu-4 = 73.98 	 Previous best codebleu 74.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 3.209134208939412e+294
  global_step = 676
  train_loss = 3.5769
  ********************
Previous best ppl:2.148304944490169e+274
BLEU file: ./data/RQ5/xcodeeval_300_1/validation.jsonl
