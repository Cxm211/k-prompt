Namespace(log_name='./RQ5/tfix_1_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_1_3/codet5p_770m', data_dir='./data/RQ5/tfix_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "String.prototype.supplant = function (o) {     'use strict';", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "function supplant (str, o) {     'use strict';"}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.8450467731918902e+273
  global_step = 2
  train_loss = 33.5488
  ********************
Previous best ppl:inf
Achieve Best ppl:1.8450467731918902e+273
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 16.57 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.208669854242096e+272
  global_step = 3
  train_loss = 35.7317
  ********************
Previous best ppl:1.8450467731918902e+273
Achieve Best ppl:3.208669854242096e+272
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 20.24 	 Previous best codebleu 16.57
  ********************
 Achieve Best bleu:20.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.5006876486869613e+264
  global_step = 4
  train_loss = 16.5167
  ********************
Previous best ppl:3.208669854242096e+272
Achieve Best ppl:2.5006876486869613e+264
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 20.13 	 Previous best codebleu 20.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.599546646534119e+254
  global_step = 5
  train_loss = 12.3978
  ********************
Previous best ppl:2.5006876486869613e+264
Achieve Best ppl:2.599546646534119e+254
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 18.8 	 Previous best codebleu 20.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.5115047720753995e+245
  global_step = 6
  train_loss = 5.4182
  ********************
Previous best ppl:2.599546646534119e+254
Achieve Best ppl:1.5115047720753995e+245
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 17.12 	 Previous best codebleu 20.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.738959667195665e+239
  global_step = 7
  train_loss = 2.0643
  ********************
Previous best ppl:1.5115047720753995e+245
Achieve Best ppl:1.738959667195665e+239
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 16.7 	 Previous best codebleu 20.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 3.296984753291216e+236
  global_step = 8
  train_loss = 1.5098
  ********************
Previous best ppl:1.738959667195665e+239
Achieve Best ppl:3.296984753291216e+236
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 18.13 	 Previous best codebleu 20.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 5.00417348060191e+234
  global_step = 9
  train_loss = 0.5996
  ********************
Previous best ppl:3.296984753291216e+236
Achieve Best ppl:5.00417348060191e+234
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 20.27 	 Previous best codebleu 20.24
  ********************
 Achieve Best bleu:20.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.681224706491495e+234
  global_step = 10
  train_loss = 0.5019
  ********************
Previous best ppl:5.00417348060191e+234
Achieve Best ppl:1.681224706491495e+234
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 20.43 	 Previous best codebleu 20.27
  ********************
 Achieve Best bleu:20.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 9.90254370757866e+233
  global_step = 11
  train_loss = 0.2704
  ********************
Previous best ppl:1.681224706491495e+234
Achieve Best ppl:9.90254370757866e+233
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 20.67 	 Previous best codebleu 20.43
  ********************
 Achieve Best bleu:20.67
  ********************
reload model from RQ5/tfix_1_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_3/test.jsonl
  codebleu = 17.78 
  Total = 500 
  Exact Fixed = 1 
[156]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 1 
[156]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 17.78 
[0.585102864635543, 0.0, 0.002158273381294964, 0.04224211916301699, 0.3283932798717467, 0.330509613994594, 0.201427250591917, 0.0001899481171821221, 0.011127529866982887, 0.042234743887362244, 0.1940732516314112, 0.30085344628423183, 0.0, 0.025703577136141492, 0.10350896434718548, 0.21637647734291668, 0.23325177740286585, 0.00019343334438189566, 0.0025952187031010243, 0.33803488565372747, 0.016120108820421617, 0.2189454310335113, 0.10932755035684345, 0.41816152981665267, 0.0, 0.0, 0.3632525898627351, 0.01753028101973829, 0.060812189879517614, 0.00020049706525262763, 0.007822027087156971, 0.194720133720785, 0.006035090942979248, 0.011764705882352941, 0.00016188325867657913, 0.15483870967741933, 0.0058823529411764705, 0.7673553277505802, 0.00018223069264086325, 0.3578842793350559, 0.005433815680228977, 0.03461538461538462, 0.012124174442153118, 0.0, 0.007782724176325438, 0.23478339987468247, 0.0, 0.0, 0.09322245322245322, 0.04829166280214689, 0.2481570915456958, 0.07098901098901098, 0.003947368421052631, 0.19213501065723898, 0.0, 0.0001900874151807054, 0.7802374490936517, 0.4829420792282159, 0.00013245370412823493, 0.12025719977324295, 0.0, 0.08161129581395413, 0.0008241532952327512, 0.00015336130495433888, 0.006030043962642721, 0.046751207146736175, 0.7130131929911753, 0.00015457861088906486, 0.00013255847719915588, 0.29503170840016946, 0.0005079245795588133, 0.00013516397788880798, 0.5909537849503046, 0.6221350448232155, 0.017910447761194027, 0.0001698050034502829, 0.08787235021773841, 0.3480215393054998, 0.24541048678633057, 0.004559147975074382, 0.16054802087960013, 0.3096757191037311, 0.007142857142857142, 0.008911022364827685, 0.0, 0.12, 0.05454545454545454, 0.0049572711734276916, 0.00033047301478323694, 0.06348486137763744, 0.00012920655239395548, 0.0, 0.0001925127216444437, 0.1204162256751731, 0.390092880592218, 0.0, 0.23307783392530348, 0.0, 0.032798683889844166, 0.19875715514341388, 0.0, 0.12011832841857897, 0.14693174578093166, 0.05331364711914735, 0.00013345390122496118, 0.0, 0.2249491363758039, 0.008531788028503184, 0.006474820143884892, 0.016690707360134407, 0.00015608817821895546, 0.001818365369563136, 0.000249558328500792, 0.0011816038614058154, 0.027906976744186046, 0.013526139440150289, 0.0010348206717397577, 0.11384562327023805, 0.1349038006503105, 0.00020364677714104575, 0.0, 0.05393246391210912, 0.5571400506383467, 0.0, 0.8607522141611879, 0.42690063952058444, 0.0931363519682846, 0.12461592402453842, 0.38610222616390355, 0.3531872222331906, 0.03708704366061093, 0.5798616156502202, 0.2894559597295894, 0.7270054295385486, 0.0, 0.4019627617245256, 0.2604226726006128, 0.229336118692598, 0.0, 0.29313414926412906, 0.5456360433180683, 0.0003724706485591179, 0.23080552463002063, 0.44593493775495774, 0.26620300302570415, 0.008846144484970914, 0.01828296523374187, 0.5410098196503339, 0.407745320828121, 0.3495669480135687, 0.5626095322000642, 0.37231516050251334, 0.6549466844721648, 0.0, 0.010714285714285713, 0.8249365300761395, 0.1772392955523947, 0.6238915578606445, 0.0036413277208484597, 0.3186348606518658, 0.41028226134499773, 0.3617833201408547, 0.4086539371045919, 0.21396844755210426, 0.06216150774956633, 0.202357487018685, 0.008633093525179856, 0.026221467227695937, 0.003529411764705882, 0.0001310965738787073, 0.6336131842675646, 0.19135331825615537, 0.03732969446330587, 0.2366673354887133, 0.2625727272913627, 0.21443493896635907, 0.5882210413806648, 0.004867954327771064, 0.13680999007492306, 0.22977601070037967, 0.008485589152757093, 0.00931693249963025, 0.04230831584201597, 0.22829131652661067, 0.11953399203911007, 0.004279394044546169, 0.2704547810372747, 0.6318643422506911, 0.048353900425107614, 0.31182463504543145, 0.00015868583070269182, 0.5728633222115624, 0.15207365485302582, 0.18352815831576694, 0.12182868514706813, 0.06674371851605768, 0.16092924352753898, 0.3251454563881895, 0.010308731468133386, 0.12, 0.0, 0.00909090909090909, 0.3075813597892745, 0.00016265511167863126, 0.004838709677419354, 0.006918490112082697, 0.2877305110887664, 0.0, 0.2638097345655506, 0.00019080257648941624, 0.18665344789927385, 0.00037247065594336117, 0.016949217256014157, 0.19896373056994818, 0.7081488603774551, 0.2922366880304672, 0.003508771929824561, 0.3159537722422461, 0.17314679839523286, 0.3393061591353758, 0.3293107871739681, 0.12, 0.351318257697565, 0.40274616459711726, 0.20852772714979856, 0.45977927815147646, 0.31038705831869967, 0.1589229926336191, 0.8384080208837095, 0.0044444444444444444, 0.0, 0.22444811530541917, 0.5895516209682825, 0.005128205128205128, 0.473975598175717, 0.32043341856227847, 0.014891958710834671, 0.2109899463928066, 0.43265338520565433, 0.011016747628977492, 0.2690081875061954, 0.7217311029543599, 0.0032728751830962046, 0.1466237140261809, 0.2211620487864034, 0.0, 0.00022649787155168532, 0.0583479241898411, 0.005128205128205128, 0.0643989760335704, 0.0, 0.6542318594873184, 0.5854322219348195, 0.1636363636363636, 0.0037525435212045983, 0.054172690032566966, 0.0, 0.008928571428571428, 0.0, 0.037653610904624484, 0.18505843362805113, 0.3162809129028983, 0.49868527387513895, 0.17872864341846018, 0.0, 0.0, 0.5902578903872038, 0.00013474838522751912, 0.0040749011407994725, 0.07317073170731707, 0.49225095888748427, 0.6359377782575386, 0.1201508542463377, 0.0, 0.13636363636363635, 0.15, 0.004847070655626918, 0.765628994806113, 0.007692307692307692, 0.6443118248022499, 0.3817167348013304, 0.4426023770077935, 0.1662162162162162, 0.05763410602583487, 0.5731797372813252, 0.011737329536738556, 0.2000067973624204, 0.5115257011117617, 0.1889330989302217, 0.012310638862827883, 0.08012361787926375, 0.0004003150385325254, 0.007017543859649122, 0.010067114093959731, 0.1200774861489188, 0.021791405915364114, 0.09288355864591581, 0.010588235294117647, 0.11569872913642396, 0.2692467833670816, 0.07251752936905524, 0.20600051382812706, 0.05236363636363637, 0.08094428175346141, 0.0, 0.0, 0.061093245778763405, 0.0, 0.25356777150600074, 0.6375544013161896, 0.231334423946577, 0.07555396613918164, 0.007653061224489796, 0.30823089742926635, 0.4327647482582093, 0.24033531194746147, 0.4857464133443863, 0.41923534665800505, 0.5110548480656569, 0.00019080257648941643, 0.16025628111289864, 0.27901992148463217, 0.08944453274465158, 0.09295425247117978, 0.00033295038121324894, 0.2009693100021718, 0.0, 0.00019187611917723368, 0.31229027935986226, 0.24484810278607505, 0.0, 0.422200726654675, 0.19931174660682704, 0.008586596365087514, 0.4614376438599185, 0.050332492624856194, 0.3301320611367892, 0.3176470588235294, 0.2574591649779647, 0.23825399172583628, 0.1487100775506775, 0.14280244959602764, 0.1366831633640569, 0.39092436753297244, 0.006, 0.2717469487659808, 0.2868111046432684, 0.4069678317246005, 0.28952165806922325, 0.11162790697674418, 0.8207892056769182, 0.01054897350063757, 0.30409667500754217, 0.43238646453521734, 0.2529115871818854, 8.642613367712977e-05, 0.003666405945113348, 0.7587131821517394, 0.1487618572869025, 0.00014564151328921233, 0.02553191489361702, 0.059327113530056666, 0.18729908115021043, 0.004981278525867286, 0.22008263634390313, 0.2987638514830326, 0.004023875063633121, 0.30880870747611, 0.0, 0.15268083796316478, 0.004060628343301426, 0.28394629042996317, 0.25037949981791796, 0.47645011233143497, 0.00014564151252288173, 0.19958599619306805, 0.0, 0.23220647287338098, 0.3105263157894737, 0.004901642761425339, 0.1364836723838286, 0.008065383466531673, 0.47824018351133285, 0.000159730386018824, 0.06567164179104477, 0.199552272990658, 0.00014600144260433338, 0.006264376223271548, 0.024969698344327232, 0.21428571428571427, 0.018169742030241885, 0.006666666666666667, 0.388339568647702, 0.5905138339920948, 0.006404588677257217, 0.2010832549316533, 0.23094175467455957, 0.003543307086614173, 0.007690963704574267, 0.14958159536370128, 0.10705124183932324, 0.00030167388302357106, 0.48278651920990034, 0.003529411764705882, 0.2736493260842873, 0.005487787338024488, 0.22091070523495315, 0.00013607200814496357, 0.2515767150440241, 0.3285791399524984, 0.007058823529411764, 0.3953872638996019, 0.0003792869309970828, 0.15970766164830896, 0.0001323068336182539, 0.06955118539180007, 0.00017563531890448658, 0.2355881516211435, 0.1959592547677507, 0.0, 0.0, 0.007279971179040475, 0.0, 0.813658845007212, 0.31555911105933393, 0.2650814836352948, 0.000138422087404115, 0.25021509966866795, 0.00043609013817570046, 0.31783183799205156, 0.0, 0.641979242240402, 0.018282918854612626, 0.03841012185231035, 0.175, 0.026850933174619583, 0.08802796508478244, 0.04228824881293848, 0.3788764292844347, 0.009555454336416178, 0.011426302505406516, 0.09183337448473106, 0.765537139984148, 0.09511626702233655, 0.062207651156779176, 0.010729242218138682, 0.2721425200848805, 0.028571428571428567, 0.0, 0.19559933757885611, 0.3116064028483914, 0.12050535389531492, 0.12258633465438172, 0.06610563977789846, 0.2997201150962668, 0.16889771903580877, 0.0, 0.3337610326395716, 0.30900332127893965, 0.007266988248739224, 0.0001908025736605804, 0.2892823643291921, 0.00014769101972633735, 0.004204674116422587, 0.006729315486401777, 0.015525013718906073, 0.0839888020080717, 0.0, 0.0002648908509573129, 0.0, 0.03518751775050407, 0.8846351414044087, 0.355067633384039, 0.14535062853958475, 0.0, 0.4157142776706557, 0.20746950775010964, 0.011748735285175201, 0.2437082051352898, 0.0, 0.09884759959246951, 0.0, 0.020682941518282802, 0.037186066828349386, 0.28760549273178004, 0.08062264578611415, 0.225087443269327, 0.2937922490170073, 0.007322482667852599, 0.44262023105466686, 0.21221173681527372, 0.00014769101972633735, 0.6712154286229404, 0.6071258603602052, 0.6873513855155244, 0.4700548037736054, 0.42481388355934147, 0.6003151139561633, 0.18730498997584244, 0.05163080402089658]
Finish training and take 1h45m
