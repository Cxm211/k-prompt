Namespace(log_name='./RQ5/tfix_1_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_1_3/codet5p_220m_f', data_dir='./data/RQ5/tfix_1_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01306
  global_step = 1
  train_loss = 3.416
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01306
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 9.57 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.01306
  global_step = 1
  train_loss = 2.9103
  ********************
Previous best ppl:1.01306
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 9.57 	 Previous best codebleu 9.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.01306
  global_step = 1
  train_loss = 3.2627
  ********************
Previous best ppl:1.01306
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 9.57 	 Previous best codebleu 9.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.01306
  global_step = 1
  train_loss = 2.1903
  ********************
Previous best ppl:1.01306
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 9.57 	 Previous best codebleu 9.57
  ********************
reload model from RQ5/tfix_1_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_3/test.jsonl
  codebleu = 9.41 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 9.41 
[0.0, 0.0, 0.21564381117992515, 0.09777777777777777, 0.06, 0.0002286996199303223, 0.09483550011571311, 0.0, 0.5862595419847327, 0.1056689342403628, 0.045, 0.075, 0.5953125, 0.1056689342403628, 0.09641967621419675, 0.00024662639742040155, 0.4668089058392709, 0.00022674223712915148, 0.24814342402842832, 0.075, 0.043436442423132535, 0.0, 0.03, 0.3744260276298309, 0.00018430397879578664, 0.10096153846153846, 0.03333333333333333, 0.02, 0.00018593745266052813, 0.0, 0.036, 0.0, 0.10344827586206896, 0.17777777777777776, 0.0, 0.0, 0.1248296787891282, 0.08, 0.08329638410473932, 0.03, 0.3244155399451255, 0.0, 0.04, 0.0, 0.00025718554291555854, 0.026135337751586853, 0.3581730290608439, 0.003870967741935484, 0.0, 0.07256909848954332, 0.37574615451212967, 0.314937888294931, 0.15066246273817807, 0.11515625, 0.0, 0.05139518038321647, 0.04, 0.0, 0.00032773993919673613, 0.11597222222222223, 0.061818181818181814, 0.0, 0.0, 0.0, 0.045, 0.020689655172413793, 0.21661100205026934, 0.3001402995428433, 0.0, 0.018513485865774136, 0.0, 0.09028956520915066, 0.0002282027212960606, 0.07625893728855906, 0.0, 0.0, 0.0, 0.12087056768698731, 0.03693839323403049, 0.4781583336244614, 0.06214285714285714, 0.06254195772475732, 0.14845360824742268, 0.072, 0.07370288389139262, 0.0431636435644878, 0.053164556962025315, 0.07584470489613841, 0.0007561846043305587, 0.17470046082949306, 0.0, 0.05737704918032786, 0.0002043957995161953, 0.29, 0.0, 0.0, 0.06428571428571428, 0.0, 0.1201814058956916, 0.18, 0.0, 0.2002746026626076, 0.20225437507727764, 0.0, 0.07381854588666761, 0.14689655172413793, 0.015168234128403317, 0.021428571428571425, 0.22930013347957595, 0.003303153058418963, 0.0, 0.00035479984220565145, 0.057142857142857134, 0.0, 0.0, 0.0, 0.000238717422175849, 0.30019520905618796, 0.0, 0.0, 0.0, 0.04168006892382794, 0.22961609311456155, 0.0, 0.1592331517374133, 0.15, 0.05753424657534246, 0.1422458638879903, 0.09246723137617321, 0.0002573308374541707, 0.04046813737961993, 0.008737864077669903, 0.0, 0.0947913761412752, 0.0, 0.11552471285000954, 0.2310316228695416, 0.21074170091178698, 0.29117647058823526, 0.015145469372368037, 0.169075340550358, 0.5978152094472327, 0.03599123227277386, 0.006231508915072083, 0.32962204716734156, 0.028571428571428567, 0.00019951775206848235, 0.0, 0.3002282149800296, 0.06, 0.021, 0.23683183145908915, 0.11230553536330781, 0.0, 0.1201814058956916, 0.0, 0.0, 0.2872340425531915, 0.035586993637826754, 0.03, 0.0, 0.04, 0.07522820623123269, 0.08019968524964156, 0.11636336985920465, 0.29, 0.04285714285714285, 0.01651919948962148, 0.0, 0.00016062610658746672, 0.04, 0.24022065723224867, 0.04277267671113655, 0.2950183526329466, 0.04, 0.06029836362199902, 0.0, 0.0, 0.08836479805042363, 0.04, 0.0, 0.06741734798229843, 0.05357142857142857, 0.27940085111028534, 0.058471685406431925, 0.035416666666666666, 0.0904470086958443, 0.08, 0.0, 0.0, 0.0, 0.04285714285714285, 0.10046082949308754, 0.02425733084609426, 0.0, 0.006453032558132183, 0.29843698708195987, 0.0, 0.04184302552318947, 0.02222222222222222, 0.0, 0.10069444444444443, 0.09230918370967607, 0.0, 0.08884892086330934, 0.02593620828214533, 0.19, 0.0006477741618237837, 0.2544738249993563, 0.0, 0.17391010504073967, 0.0, 0.0, 0.06666666666666667, 0.09630957032599267, 0.0, 0.0004766121624080292, 0.09, 0.12672462994274936, 0.07168628575275282, 0.31181046926274997, 0.11813117441708759, 0.09246656716061036, 0.0645434879811014, 0.4421575624545749, 0.0226177628380741, 0.06551369134469681, 0.15918996147007258, 0.2839017295611836, 0.23628622984248537, 0.054631188496426224, 0.10964867329166894, 0.0, 0.07555555555555556, 0.057473801396518434, 0.09720462183454204, 0.005825242718446601, 0.0, 0.0002778305898955002, 0.0, 0.18074569931823542, 0.09045742474320197, 0.0005192759410041587, 0.07728291794658737, 0.1348122827644548, 0.00039286931312219566, 0.48251032220503876, 0.04, 0.09999999999999999, 0.049033384972911195, 0.05043025289508164, 0.04, 0.26736669330307516, 0.4904214477164926, 0.11783407419538058, 0.00034580715211209175, 0.0, 0.02, 0.5834320360531986, 0.0661294198167601, 0.118562874251497, 0.3060589442440897, 0.3222337651142304, 0.12273900805661564, 0.18, 0.0619449726079773, 0.0, 0.2516572804069188, 0.0, 0.12279813772340233, 0.00019239981563718193, 0.0, 0.1502950791823698, 0.0001756352113293001, 0.0, 0.15916295353775126, 0.02, 0.022245290908634203, 0.0, 0.0005054898389350869, 0.0, 0.04285714285714285, 0.18701491673620702, 0.03194910810933169, 0.2885045506010429, 0.2724758438393521, 0.19, 0.06264620266040126, 0.06, 0.0992560288958351, 0.20676454977844497, 0.0, 0.04, 0.08620593895783285, 0.0002963000403489922, 0.002243110661158092, 0.12213606232036155, 0.09642857142857143, 0.25718226408713823, 0.30759671298580726, 0.1198426197529785, 0.30018093668038276, 0.0, 0.301526222774285, 0.3627702594767873, 0.075, 0.11778825292202998, 0.0010375319885476753, 0.061538461538461535, 0.34160532057185033, 0.31237401763383743, 0.0, 0.17078774291536372, 0.3065097627027401, 0.0, 0.06, 0.24395400009731755, 0.34787152583664266, 0.14155103452351114, 0.39957028361095315, 0.08077117577942763, 0.09614358681384716, 0.0, 0.3001545932965874, 0.10980777704496315, 0.3190755519109386, 0.18786546632277684, 0.040184646489905336, 0.07754858173887434, 0.057142857142857134, 0.00019952268886887114, 0.0, 0.30035637299979645, 0.5956262708816733, 0.04514055497134398, 0.05031700806494349, 0.10389805097451274, 0.0, 0.0, 0.14331145063048056, 0.22912815755919103, 0.04, 0.10023546620261521, 0.3, 0.06, 0.00821917808219178, 0.00027895652545934023, 0.07817339382137184, 0.2816290711338978, 0.0, 0.05217391304347826, 0.075, 0.00024996984477168187, 0.0, 0.2539347348266064, 0.19885490323400956, 0.30027139487800136, 0.07536781208797567, 0.0, 0.05473734781205051, 0.33749999999999997, 0.00022674222970049606, 0.020199517997906452, 0.0782364051590788, 0.00027767524274151587, 0.02, 0.12278104612403773, 0.10953821361758971, 0.1483090302786475, 0.12934424255617566, 0.1056689342403628, 0.0, 0.032791619591029886, 0.06, 0.2650156181956267, 0.0, 0.0, 0.054707447472731804, 0.06, 0.03, 0.03086756146950141, 0.17774997125567393, 0.12549100058194634, 0.07758620689655173, 0.0, 0.4814084846649497, 0.10710947491031785, 0.06, 0.0, 0.075, 0.04285714285714285, 0.1044778314412517, 0.000701082767288406, 0.33003095744382094, 0.04244161212165373, 0.012103787724135541, 0.143829468960359, 0.09236579932431288, 0.0, 0.00022674223712915148, 0.044677940580735036, 0.3029296132316057, 0.10846345364769469, 0.12024258834830494, 0.08188615559668039, 0.0940668202764977, 0.06, 0.0, 0.0, 0.11140653429572188, 0.0002280775734315799, 0.04, 0.15639959629996475, 0.06270032851344694, 0.00027138030019156696, 0.3168486812301297, 0.0020720323417875233, 0.0, 0.4049553640234519, 0.00023349611539724187, 0.0, 0.0692156862745098, 0.0, 0.0003399335368547793, 0.03214285714285714, 0.0, 0.0, 0.04285714285714285, 0.0, 0.12056128008441266, 0.0, 0.30022820347454077, 0.0, 0.3022817823689665, 0.04, 0.2859816025767916, 0.07349376314283496, 0.00035496143519283917, 0.02475995473740944, 0.10158366966667101, 0.0, 0.06, 0.0, 0.10571260323513912, 0.05521310611696377, 0.08843268037760457, 0.08025284426747367, 0.0, 0.0, 0.053369272237196765, 0.07815866322128541, 0.04, 0.03214285714285714, 0.0, 0.03639455782312925, 0.05454545454545454, 0.03354447536826719, 0.3014162473313179, 0.08203650479399927, 0.0, 0.0, 0.04285714285714285, 0.0, 0.00038150055352924875, 0.020298349915443315, 0.060118498216044265, 0.0, 0.0001756352113293001, 0.0, 0.0, 0.0, 0.06, 0.0, 0.04, 0.0, 0.0, 0.0, 0.027067594729781277, 0.35189853232419344, 0.0, 0.09449511808409536, 0.0, 0.11190587138863002, 0.0, 0.0, 0.06, 0.02222222222222222, 0.048257330991903054, 0.3079481382638432, 0.10938626526898508, 0.2816058687400842, 0.00014769101972633721, 0.02727272727272727, 0.00020218985684104196, 0.2992306299461637, 0.41538461538461535, 0.022674897153027968, 0.6054351506913523, 0.08372439486549133, 0.0]
Finish training and take 52m
