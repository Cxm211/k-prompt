Namespace(log_name='./RQ5/tfix_1_3/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_1_3/codet5p_220m', data_dir='./data/RQ5/tfix_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "String.prototype.supplant = function (o) {     'use strict';", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "function supplant (str, o) {     'use strict';"}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 5.341953087608712e+294
  global_step = 2
  train_loss = 50.257
  ********************
Previous best ppl:inf
Achieve Best ppl:5.341953087608712e+294
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 10.74 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 2.8649228030535065e+288
  global_step = 3
  train_loss = 43.9367
  ********************
Previous best ppl:5.341953087608712e+294
Achieve Best ppl:2.8649228030535065e+288
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 16.58 	 Previous best codebleu 10.74
  ********************
 Achieve Best bleu:16.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.1283780516781636e+286
  global_step = 4
  train_loss = 23.6904
  ********************
Previous best ppl:2.8649228030535065e+288
Achieve Best ppl:2.1283780516781636e+286
  ********************
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 17.66 	 Previous best codebleu 16.58
  ********************
 Achieve Best bleu:17.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.6654019479406037e+286
  global_step = 5
  train_loss = 13.441
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 20.16 	 Previous best codebleu 17.66
  ********************
 Achieve Best bleu:20.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 2.389645241950699e+288
  global_step = 6
  train_loss = 9.777
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 18.99 	 Previous best codebleu 20.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 8.288467072260702e+290
  global_step = 7
  train_loss = 5.1529
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 19.75 	 Previous best codebleu 20.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 4.1987310013325515e+293
  global_step = 8
  train_loss = 6.2977
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 21.84 	 Previous best codebleu 20.16
  ********************
 Achieve Best bleu:21.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 4.0133752595288363e+295
  global_step = 9
  train_loss = 3.4047
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 21.53 	 Previous best codebleu 21.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.2365114422496543e+297
  global_step = 10
  train_loss = 2.6604
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 22.65 	 Previous best codebleu 21.84
  ********************
 Achieve Best bleu:22.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 5.31281902071226e+297
  global_step = 11
  train_loss = 2.9217
  ********************
Previous best ppl:2.1283780516781636e+286
BLEU file: ./data/RQ5/tfix_1_3/validation.jsonl
  codebleu-4 = 22.71 	 Previous best codebleu 22.65
  ********************
 Achieve Best bleu:22.71
  ********************
reload model from RQ5/tfix_1_3/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_3/test.jsonl
  codebleu = 24.68 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 2 
[389, 495]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 2 
[389, 495]
  Cleaned Fixed = 0 
[]
  codebleu = 24.68 
[0.1511004082425128, 0.3972571601859192, 0.5072961412874616, 0.029245783724367154, 0.33464712583596873, 0.38407303579117646, 0.2702512834863934, 0.060908330969006746, 0.19126523811877372, 0.011044617384321324, 0.1940732516314112, 0.06435438494478167, 0.5032679142479244, 0.015789473684210523, 0.09958896640765287, 0.2740017448340758, 0.039053212437938155, 0.24750164378859374, 0.20197784515073813, 0.2949308076612111, 0.269339809874528, 0.14288368977705457, 0.10749999999999998, 0.40173739460936, 0.302319921398574, 0.16950560465994519, 0.1416158103095349, 0.043322201428364904, 0.29920705481262444, 0.0007040381794326505, 0.15002821724577348, 0.15175158948041068, 0.38886315280864736, 0.23243106621555115, 0.015152068763952298, 0.4800807681423667, 0.18365088971663446, 0.773790868417212, 0.0194686098285704, 0.34705164022536145, 0.1949354267150458, 0.12585365853658537, 0.040006904438560476, 0.0, 0.05417662489070242, 0.046391383528519015, 0.0, 0.2630868325315998, 0.02903700073511394, 0.06169607084013775, 0.39270439405271795, 0.13968191912625666, 0.14552352601893828, 0.25215788458096267, 0.0, 0.5943204938549332, 0.30988288435276967, 0.545883384622377, 0.3712959708047634, 0.004976121921967943, 0.16234800246236186, 0.49577691591092843, 0.07554460759784506, 0.2153109682796138, 0.25909220057693383, 0.06596711701876352, 0.2718098408443422, 0.5306678487779758, 0.0, 0.1540854972272831, 0.00026456579371815454, 0.14285318339937003, 0.061255360282305896, 0.1938351402013458, 0.0, 0.0002274353083496761, 0.5809934928368095, 0.19481146032694535, 0.5527002027747032, 0.00222582712097816, 0.3560653527099435, 0.5748297722527274, 0.059901315789473684, 0.4453423391373998, 0.5921552741480063, 0.3291587203866318, 0.041379310344827586, 0.2503179313399953, 0.043640358682583605, 0.31550205546708565, 0.014832430347069386, 0.5888512052539021, 0.0006792702475327395, 0.21924426741773384, 0.2997545984367861, 0.19373302351360033, 0.47915636688184804, 0.0005687420346882253, 0.02341283170977494, 0.1988297097213063, 0.0, 0.16700302849941395, 0.12357468026660261, 0.04, 0.0007297039180488304, 0.0, 0.09844736129236636, 0.12515078713132993, 0.24499798240485715, 0.5657835394460773, 0.28312673487783796, 0.00038840371916893865, 0.0907704980849997, 0.14210526315789473, 0.2265822162946517, 0.5136883090152139, 0.2589400038283298, 0.08137457003028836, 0.06610023041474654, 0.0007946930211790324, 0.0005836115639003902, 0.006035756749413492, 0.5845995014417215, 0.0, 0.20484283187197203, 0.5572579751256154, 0.0, 0.08605721532650187, 0.2268371484778424, 0.4731716191316424, 0.19889087383666917, 0.07471219825745665, 0.5777131912598308, 0.22452750754070605, 0.1780821917808219, 0.08769514822007017, 0.18332206607844959, 0.09604993685153809, 0.29655397253520543, 0.09515686415088993, 0.45171691830486294, 0.5942713899858554, 0.2888213671527027, 0.013719057932523258, 0.2536619252941014, 0.4850749717055899, 0.23553719008264462, 0.5268699198734281, 0.31504238351782327, 0.29655169870530246, 0.6572987350239475, 0.49147816852634196, 0.6774074093694246, 0.12165933536718052, 0.47575088855099124, 0.026424875358677317, 0.2290851908394655, 0.5152761431716149, 0.29604019681512517, 0.08299707203731399, 0.3114102311733935, 0.54555206105229, 0.19549338259307059, 0.4727470116500335, 0.35826636117829713, 0.3109961935108119, 0.2025380604294563, 0.2518485278725144, 0.16951102588686479, 0.0, 0.20732025151597722, 0.10463296294476036, 0.0128839672933789, 0.29856314505274484, 0.31062239145069376, 0.23066989694434925, 0.5915450082441134, 0.03232889675125068, 0.26523844774257216, 0.4756984318364834, 0.2504970626222174, 0.49664676948868286, 0.6542114238404432, 0.563457094179104, 0.09398513232303897, 0.1912184186056957, 0.11906563900477152, 0.6609378536551012, 0.29589041095890406, 0.4090125340573332, 0.01606861654689227, 0.5805949473322493, 0.6540180681051748, 0.2045298979675928, 0.14104632623109278, 0.045335461959045364, 0.2488028803915237, 0.27268877429380345, 0.2543608360838398, 0.12, 0.0003725601465217323, 0.3, 0.002486697380107197, 0.12879497725581265, 0.25442583314879563, 0.44869546142944583, 0.15160178066493785, 0.0008393965144923618, 0.4307250470563342, 0.000910391539172086, 0.0882659249503231, 0.29425688553661344, 0.47627650584102754, 0.5825186696629865, 0.0933249826721918, 0.0, 0.7022283102773357, 0.2399918682556229, 0.2686439442563223, 0.29533088182757095, 0.30982555867169914, 0.12029807548606203, 0.45592552261433705, 0.2476606548638816, 0.14878335837378642, 0.4918664160195385, 0.21351094414261984, 0.15943401330721915, 0.39511531098600977, 0.20828084100231042, 0.19895624428182337, 0.3748138835593414, 0.2638609849731891, 0.258762236937865, 0.09107733086384849, 0.32043341856227847, 0.36790329269129957, 0.31229696980922833, 0.18867941146631037, 0.10717406684990652, 0.24977741827542616, 0.5913484343027366, 0.005555555555555555, 0.1394650824038857, 0.0012804051579682698, 0.00019866129993099848, 0.4476195925029415, 0.09504551020494262, 0.22763228707705435, 0.08721930243669374, 0.0031865782971695444, 0.5315860989375352, 0.28468918133276466, 0.5010060194267428, 0.0, 0.0005822625528377322, 0.0, 0.10341106478775242, 0.0, 0.056628958483147945, 0.0, 0.29704601416140414, 0.17639243833520385, 0.4514219202067661, 0.00394106577973726, 0.02727272727272727, 0.0002995686822302629, 0.27490122054380045, 0.04770260054915678, 0.20040723663369653, 0.4225395044110835, 0.2520005976064002, 0.07467442824100598, 0.0, 0.14233576642335766, 0.43111993709706165, 0.009153430661835826, 0.0758995711376316, 0.06666666666666667, 0.6765602671392215, 0.3879826800002675, 0.4067122268827078, 0.14333231831995016, 0.08201368865751629, 0.7676737014816164, 0.4054712056891644, 0.3005622964503088, 0.21221302546459886, 0.31056105739384526, 0.19108509286107686, 0.49971976026564013, 0.03821771168817468, 0.4072261322958437, 0.5115054361830695, 0.5470075710514086, 0.021791405915364114, 0.15032916846544697, 0.7154519429433581, 0.23639979033335262, 0.4276477489441399, 0.08513326912097077, 0.428887908516038, 0.0495049504950495, 0.0012768349849813531, 0.0, 0.0831633570833785, 0.11938080540891852, 0.0, 0.4109629370839422, 0.43091146569185657, 0.3232998028978382, 0.07460966295417314, 0.5436703867155002, 0.30964224999265727, 0.3799683196126244, 0.2437273851037481, 0.22173905036978447, 0.346914294910274, 0.5110548480656569, 0.356649466819273, 0.20244963031349722, 0.3650842341854927, 0.061773915174217, 0.145359649879123, 0.013728627430961506, 0.3159258326001334, 0.5069881044162146, 0.00040400731793577364, 0.27534525251800535, 0.3764598709939443, 0.5152948440343424, 0.310134646496747, 0.007347161301094975, 0.0032525574741478908, 0.3484457945142521, 0.060647016510458944, 0.48658319923564053, 0.20189493901762168, 0.3003700320271683, 0.2561977782963108, 0.39298424086229033, 0.07381155210375316, 0.43783167254853783, 0.12849643794525684, 0.48463212495967134, 0.5288724788208753, 0.19775935366100122, 0.11779805723051455, 0.3627875783358533, 0.229521920940327, 0.8207892056769182, 0.3062464119023305, 0.3043022671152214, 0.43238646453521734, 0.2564267966943183, 0.20264682685546298, 0.5388066712686259, 0.7202057266895355, 0.23715778401450918, 0.35507062375835347, 0.11428571428571427, 0.5546744440448658, 0.29769727645125277, 0.0011811023622047244, 0.23411097442537826, 0.17821799126557575, 0.2448806523417843, 0.22066161960770186, 0.0, 0.15545352814136082, 0.00964609419696983, 0.29523607763797766, 0.3701691697830054, 0.3201577748635403, 0.2222291224106058, 0.08761356495365917, 0.41888744362959685, 0.0854170910814719, 0.40125408527698747, 0.35355234013662706, 0.392840805134023, 0.18599584650067016, 0.5274038897054159, 0.0004922197245859136, 0.11137724550898202, 0.44173601097129783, 0.0002550764565091358, 0.31539985080653044, 0.8406708688455613, 0.5758104701870441, 0.3697658935662226, 0.006117845723438082, 0.10024668459109945, 0.2517135125226922, 0.008069587490669562, 0.6492320565450098, 0.3580195630509062, 0.2167404189516547, 0.007690963704574267, 0.17142857142857143, 0.06349232285040483, 0.11222747378304343, 0.055116788845643284, 0.10645927066184609, 0.6265310763217098, 0.06216552701270855, 0.8024154659334062, 0.0, 0.25420468823565245, 0.5797998279470696, 0.021367968330508205, 0.1201932679692306, 0.11965948962634958, 0.04003147089870683, 0.3277753390139937, 0.23727914593451493, 0.2934551999017699, 0.05844523581557126, 0.4989988391204995, 0.02531511745029084, 0.0755563689774105, 0.3088777298853096, 0.18768407222285893, 0.32296071316097247, 0.3116998851363171, 0.37338260962912295, 0.0002791071696597906, 0.10270604834583072, 0.0003423124262958443, 0.5207350135988427, 0.270593149890702, 0.7903253497771447, 0.10364365020397118, 0.017045131045938498, 0.08635794743429287, 0.1488656218343849, 0.584808093104174, 0.5336849797403102, 0.6168269612469919, 0.37691974429656216, 0.0, 0.4588423650486511, 0.6953716106188446, 0.10461380920659498, 0.5770820794986339, 0.468137810617965, 0.2457673022138464, 0.0, 0.49973307121206856, 0.5704268576082554, 0.15030012247759014, 0.3090873786407767, 0.2026797528054671, 0.12141749351251294, 0.2936834822645697, 0.35071771168817467, 0.00860737694361129, 0.30754796786155236, 0.1097352672799288, 0.23104214814827978, 0.00011873998298196074, 0.8240471167935453, 0.143383828870156, 0.08901931012562479, 0.0011857707509881422, 0.006639647896814851, 0.2148628978872794, 0.0, 0.23664913660446746, 0.2929751856867093, 0.03482955126026931, 0.239185696833259, 0.22213114578115284, 0.26068086973728666, 0.0, 0.6032092553953932, 0.24144656651187926, 0.008170527003119814, 0.5383931420587988, 0.07404663460062587, 0.15350258615583065, 0.0, 0.36393172467891854, 0.0, 0.39886099390069074, 0.09196455149506105, 0.22869434859412388, 0.22248633195246992, 0.2237754593029431, 0.44262023105466686, 0.23540510557795602, 0.005425508162785448, 0.6450668479571828, 0.3039997422265547, 0.968179283050743, 0.4412206770176272, 0.092333006735127, 0.3763346126583449, 0.09156897739047971, 0.3571446984837483]
Finish training and take 49m
