Namespace(log_name='./RQ5/javascript_1/soft0_1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft0_1_codet5p_770m', data_dir='./data/RQ5/javascript_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' }     return  (InlineEditor.ColorSwatch._constructor());   }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' no-undef', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '}     return  (ColorSwatch._constructor());   }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 28.6415
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 12.79 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 30.8997
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 21.99 	 Previous best codebleu 12.79
  ********************
 Achieve Best bleu:21.99
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 11.6618
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 24.33 	 Previous best codebleu 21.99
  ********************
 Achieve Best bleu:24.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 2.8128
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 29.8 	 Previous best codebleu 24.33
  ********************
 Achieve Best bleu:29.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 1.1404
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 32.43 	 Previous best codebleu 29.8
  ********************
 Achieve Best bleu:32.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 6.7281
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 34.22 	 Previous best codebleu 32.43
  ********************
 Achieve Best bleu:34.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 0.5009
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 35.87 	 Previous best codebleu 34.22
  ********************
 Achieve Best bleu:35.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 0.1452
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 35.16 	 Previous best codebleu 35.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.2179
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 35.57 	 Previous best codebleu 35.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 0.1145
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 35.42 	 Previous best codebleu 35.87
  ********************
early stopping!!!
reload model from RQ5/javascript_1/soft0_1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_1/test.jsonl
  codebleu = 34.26 
  Total = 500 
  Exact Fixed = 3 
[69, 392, 419]
  Syntax Fixed = 1 
[112]
  Cleaned Fixed = 2 
[112, 356]
  ********************
  Total = 500 
  Exact Fixed = 3 
[69, 392, 419]
  Syntax Fixed = 1 
[112]
  Cleaned Fixed = 2 
[112, 356]
  codebleu = 34.26 
[0.2979970788997593, 0.20190957445563623, 0.37910266588066427, 0.5260608358590962, 0.07836454458053776, 0.24022228112960337, 0.21945729551505624, 0.28992869072362454, 0.4538756369009079, 0.023941692372521955, 0.6190047773975725, 0.4831173059853664, 0.25255157651202054, 0.2570436783222022, 0.36167683642860954, 0.589856007482291, 0.48959810148659855, 0.23572720749452142, 0.0, 0.811311525645722, 0.6670296163053906, 0.001642008090590286, 0.01524949670461367, 0.5135214793849547, 0.34712574420862263, 0.53049277925119, 0.18202819659386654, 0.35809327892058224, 0.34650852962940437, 0.3, 0.21554552303606905, 0.15368131468897903, 0.5596924101375398, 0.4239400589672744, 0.5436070173591719, 0.45946831016537293, 0.6070188572501798, 0.35197493470696306, 0.2910234110442321, 0.43985271067894743, 0.8573464528613979, 0.5448655253853507, 0.6338532730110269, 0.4922855403148038, 0.010009956645084516, 0.07633450323767893, 0.7314771093546691, 0.40585726070295775, 0.5095616509650316, 0.11012461081133956, 0.6030310689625527, 0.31531437306324683, 0.06335454681849449, 0.33039047093608204, 0.3533755248262469, 0.3428571428571428, 0.13333333333333333, 0.16643592822031655, 0.433946745450431, 0.5834705634342923, 0.3940329840180277, 0.15664539942118683, 0.4568323161707849, 0.31189206979101625, 0.13476708160930279, 0.5977205237418233, 0.5714532400808212, 0.3015443532509886, 0.8249365300761395, 0.5529934966412883, 0.6437526708015762, 0.6116767472564975, 0.46636591793889975, 0.0, 0.0, 0.15, 0.006826503896445559, 0.5007655170569006, 0.09054591715094637, 0.3893077532044019, 0.5668276635701768, 0.0, 0.22109556419170986, 0.3341548092412445, 0.15, 0.6716182502055232, 0.5055629631421744, 0.4167756877189603, 0.44535065830823106, 0.12620880885688693, 0.0, 0.42088250860057536, 0.5392499922398382, 0.33950844343036213, 0.6124299284874438, 0.6902635248390409, 0.07876652267182907, 0.5429696459533631, 0.6798803763106699, 0.13502714779310215, 0.48151833528931587, 0.7089551634180106, 0.029401582477283354, 0.14140285003624808, 0.09659690861220327, 0.2570454666669611, 0.4934146094367222, 0.21493084311587032, 0.5901844133905401, 0.29661241920780745, 0.5532136440138307, 0.9491673847910091, 0.1974188475788684, 0.604498905545545, 0.0, 0.0, 0.48839502083117775, 0.30848626712658656, 0.6135020754637419, 0.23915018366608948, 0.4270786016483965, 0.13735068167534337, 0.013792410875289297, 0.12243970118183252, 0.1631819263831593, 0.5015395325573263, 0.11895541456122424, 0.29855109608400165, 0.680348676033865, 0.1189667775175813, 0.11456463735979497, 0.23302727997642692, 0.753611364541793, 0.29963353620961874, 0.2465085296294044, 0.045185702248963316, 0.118125, 0.39713222042710916, 0.4748351538692779, 0.42430386249654006, 0.23130418833762034, 0.7053311537188118, 0.15, 0.14953378289231495, 0.0, 0.25895968232407623, 0.23426055438611637, 0.39698010391968397, 0.1899566385116471, 0.15657381237195916, 0.03606856837889304, 0.7032116639620007, 0.2933192603414, 0.5137426030245367, 0.41317061818305634, 0.24097819173437954, 0.14091464832767508, 0.45671395267298265, 0.21170458518859758, 0.29230769230769227, 0.0, 0.39299909599314586, 0.773553137645282, 0.075, 0.35354789291433764, 0.43770919151791166, 0.4026968889437066, 0.7227128879652001, 0.15136791218324006, 0.7019482498243361, 0.2854172631448252, 0.32087832434232316, 0.24413635155290278, 0.09590997803101786, 0.49420165235842395, 0.6066748621480897, 0.198034900064599, 0.08330782523331828, 0.11274239501676267, 0.15, 0.4242214411967119, 0.22837626477693762, 0.6653221738295125, 0.3945031518770046, 0.46829198157228114, 0.29477207253021476, 0.2907572975094218, 0.3004668501501153, 0.6759301173378023, 0.20672167224829094, 0.2634969901733584, 0.0, 0.08700388639901563, 0.43717836870822446, 0.2067960383668118, 0.01661942668662242, 0.4311907477389383, 0.09536402469274179, 0.5959243398932159, 0.5279047552323763, 0.23536740820110297, 0.612170286630372, 0.23983016784239036, 0.0029925864127697795, 0.6489807027038881, 0.3808116768355446, 0.6242164128013139, 0.22572932067936138, 0.5898749067454394, 0.24856677472146582, 0.6209493257069808, 0.0, 0.06326177272506203, 0.22344843081546217, 0.6121787547146431, 0.44125663598630405, 0.025001214399131652, 0.43904778312700155, 0.31899152072100645, 0.0, 0.37802953328815464, 0.48302688177324754, 0.0857142857142857, 0.5174535440940147, 0.2532470286193939, 0.12703744955126642, 0.3682611169807786, 0.21796143727338885, 0.31089018182496003, 0.4957103430154332, 0.3016883047041743, 0.24877006380454947, 0.20498619335678087, 0.5550030364877768, 0.34272438732934996, 0.0, 0.7300104059997974, 0.6276684581081834, 0.41217182957726184, 0.24598619571760771, 0.6531006455765043, 0.6283385453126689, 0.2920915623309564, 0.11318743913115802, 0.34895017176440035, 0.013792410875289297, 0.13476708160930279, 0.305028069101137, 0.29260836375203764, 0.8141572281239757, 0.40087093505002125, 0.2433339497978736, 0.20866354767478065, 0.3116030871687355, 0.6392291939170431, 0.4338491014511952, 0.1999579770306643, 0.2995190022650262, 0.0, 0.32199875290117663, 0.2623858345678357, 0.25617525309446004, 0.7224752378803548, 0.017343057283937858, 0.8396794437685136, 0.1737755479780052, 0.40197126343648293, 0.3508892888435109, 0.5105423483283629, 0.2015372100944879, 0.6643842626707708, 0.18091337367104826, 0.6018165317337315, 0.5358226704562313, 0.5808241804341914, 0.3250572171854944, 0.778785113836511, 0.33036017439379955, 0.36086658471199967, 0.3243203545289008, 0.0645920050429355, 0.2049348688909128, 0.5131751962960711, 0.0, 0.0922382617563554, 0.14045152099337604, 0.31906723961609396, 0.3430866894021251, 0.0, 0.19225990978714969, 0.09230769230769231, 0.31636872806380223, 0.22856128882154264, 0.6642647227570666, 0.0, 0.008127149387347866, 0.6908294091497738, 0.494086029894227, 0.0, 0.6462319640830607, 0.3439240447738373, 0.4814374506953589, 0.2457354197214469, 0.3068616296354167, 0.0, 0.4453640305672881, 0.1792194713678828, 0.17762044711961827, 0.5194637253863024, 0.18836073304732065, 0.7158643298974992, 0.023975218851315705, 0.634739134738433, 0.5202654083610733, 0.0, 0.25888127890102786, 0.7846480691073847, 0.12, 0.041527976087742405, 0.21818181818181817, 0.32668554816249035, 0.3413880220598309, 0.5619131609071588, 0.6688334121502666, 0.4965580647670747, 0.7671396677809144, 0.595947608997915, 0.4851774242229803, 0.5979877649013252, 0.15074469564080756, 0.30545008728923306, 0.0, 0.5261072559244404, 0.42789825755242816, 0.09455516551375628, 0.16019041514852225, 0.20391689105206734, 0.5706825274213014, 0.4955381010025495, 0.05177652945440095, 0.5665926606221665, 0.022657627559156193, 0.8566207081945381, 0.27698048876809767, 0.11369371748147741, 0.28879371674791376, 0.5859523403181391, 0.017733069674444876, 0.55638849227698, 0.6373329930404128, 0.2878518069964033, 0.6844784511922568, 0.4471602262362091, 0.5923083558363026, 0.0, 0.6775244294693408, 0.48839502083117775, 0.20908946218904467, 0.21163668691063853, 0.34064715873077267, 0.014430743472087264, 0.34931405693875783, 0.4312652820919249, 0.808221529667446, 0.3223875854141642, 0.11871360299690586, 0.7104690849699224, 0.0, 0.1248785438106199, 0.36209152792271854, 0.2618389734292904, 0.2244912096707368, 0.13938873255811476, 0.3345188110745876, 0.654196725926155, 0.19237285318203282, 0.33742322052420143, 0.3461524229887739, 0.3833681124813819, 0.6153514626022043, 0.24454353381340382, 0.6914505394374126, 0.17101223173877847, 0.458072910245714, 0.023312580185318302, 0.5197109370915457, 0.33708319799649966, 0.3508081453961951, 0.19180810376586305, 0.48676871792267157, 0.1745808333803876, 1.0, 0.2046646816360657, 0.4016960879051724, 0.09453360085481802, 0.4777601556679766, 0.2650018011029693, 0.5257763776271088, 0.8488854229397131, 0.1544200149559103, 0.017031113995997202, 0.21032782044278986, 0.25362775096092977, 0.12283693344527488, 0.07117154929281683, 0.511108883263512, 0.3046035634054043, 0.6796482736019971, 0.5784073255024652, 0.1418190102191073, 0.6749865294460549, 0.1757603931147898, 0.020714617475938255, 0.6259496800747891, 0.40680275874194305, 0.5933438246961875, 0.0, 0.30046321105062235, 0.9891483218006352, 0.0, 0.32661262436276534, 0.0, 0.0, 0.46034910007318686, 0.8020054046730669, 0.7223477086206853, 0.0, 0.15604727567342397, 0.5238276667088327, 0.44226618905800374, 0.22747819076126363, 0.1018903872973364, 0.42071161283773484, 0.34289299814414176, 0.8228963827959196, 0.4510910587246674, 0.41171570345980213, 0.18669350847458108, 0.025944605278236044, 0.08775375390483889, 0.5671350186058814, 0.45598065967273066, 0.08693636075549857, 0.4379886359927012, 0.14935392912026235, 0.4241979893956649, 0.0, 0.0, 0.38320596924269007, 0.09999999999999999, 0.15676181693655206, 0.4661190376875927, 0.5857343158984987, 0.7576851784475747, 0.075, 0.29221098287512304, 0.4706448004786508, 0.0, 0.4837629325594314, 0.0, 0.10307297361799285, 0.4959513365429262, 0.3171833207728648, 0.13448200990261447, 0.46575232448771275, 0.23623355125614362, 0.2478737274849781, 0.5549829336831389, 0.9495565333799167, 0.5096467024829745, 0.43201024220111967, 0.2301752002983829, 0.0, 0.6841007521231434, 0.3793744711262238, 0.2423858097213784, 0.7253305249945152, 0.1223157836011612, 0.16903981537205748, 0.0, 0.4702562381374042, 0.20436692013296331, 0.5951539110268794, 0.39024518379467305, 0.3655661806456132, 0.0, 0.2377990281106999, 0.15351688848083994, 0.32993947651284805, 0.3072696319255147, 0.414436274930823, 0.43766406455343976, 0.11383536835035626, 0.4664465621721662, 0.5870443567699026, 0.26690284210464893, 0.6036873999148171, 0.22532223858608097, 0.22639874222714212, 0.616231859887385]
Finish training and take 1h40m
Namespace(log_name='./RQ5/javascript_1/soft0_1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft0_1_codet5p_770m', data_dir='./data/RQ5/javascript_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '}     return  (InlineEditor.ColorSwatch._constructor());   }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '}     return  (ColorSwatch._constructor());   }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 38.4269
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 16.24 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 34.6197
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 23.91 	 Previous best codebleu 16.24
  ********************
 Achieve Best bleu:23.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 12.9719
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 25.84 	 Previous best codebleu 23.91
  ********************
 Achieve Best bleu:25.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 3.6475
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 28.82 	 Previous best codebleu 25.84
  ********************
 Achieve Best bleu:28.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 8.1417
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 33.65 	 Previous best codebleu 28.82
  ********************
 Achieve Best bleu:33.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 1.0926
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 36.91 	 Previous best codebleu 33.65
  ********************
 Achieve Best bleu:36.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 1.1047
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 38.26 	 Previous best codebleu 36.91
  ********************
 Achieve Best bleu:38.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 1.8578
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 38.55 	 Previous best codebleu 38.26
  ********************
 Achieve Best bleu:38.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.2217
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 37.51 	 Previous best codebleu 38.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 0.1289
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 36.76 	 Previous best codebleu 38.55
  ********************
reload model from RQ5/javascript_1/soft0_1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_1/test.jsonl
  codebleu = 35.69 
  Total = 500 
  Exact Fixed = 5 
[129, 215, 392, 399, 419]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[69, 163, 174]
  ********************
  Total = 500 
  Exact Fixed = 5 
[129, 215, 392, 399, 419]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[69, 163, 174]
  codebleu = 35.69 
[0.29224905049958727, 0.2009280453499171, 0.41772702879788204, 0.2996950354360158, 0.11281196306830271, 0.30239035348277205, 0.5530145652307843, 0.45946616359127257, 0.5303543300556994, 0.019067239616093933, 0.3084974645257407, 0.6128412130964106, 0.25255157651202054, 0.5865078319553579, 0.7815675571575538, 0.589856007482291, 0.49458515552807747, 0.3154840539987193, 0.0, 0.6448598858735686, 0.6670296163053906, 0.19374824525689965, 0.01524949670461367, 0.2551807951133311, 0.0, 0.5957256378556044, 0.3, 0.35809327892058224, 0.29457844157048185, 0.4666666666666667, 0.0, 0.2588572863046423, 0.684204830148586, 0.5052177258922845, 0.5436070173591719, 0.5249978749099593, 0.09230351964490242, 0.0, 0.3340047425620204, 0.3629139032602967, 0.7448566561657683, 0.5448655253853507, 0.5662433039655248, 0.5612514672260462, 0.00821917808219178, 0.4163821814174854, 0.7314771093546691, 0.6423991042001886, 0.5095616509650316, 0.11910795826911655, 0.7161255757487717, 0.2562299234416274, 0.0845300876885447, 0.17361060447459234, 0.3533755248262469, 0.3428571428571428, 0.19999999999999998, 0.16724081781425765, 0.30547230097765815, 0.22074275726406972, 0.4950121951818397, 0.2537032822665313, 0.6185841231388776, 0.24421935131616732, 0.15468178060273022, 0.6001712768755635, 0.5376168680269278, 0.4041375933720921, 0.6132387151836265, 0.01485529595854132, 0.6847482616910893, 0.32318145721937186, 0.21485239216303215, 0.0, 0.5271554381111617, 0.06486486486486487, 0.006826503896445559, 0.3385142588469919, 0.0857142857142857, 0.47683113255278453, 0.2603650200107874, 0.0, 0.1648694812262581, 0.38355628719913637, 0.15, 0.6310642904184962, 0.3, 0.4167756877189603, 0.47996086480687705, 0.25630093738625903, 0.06, 0.4446724306281599, 0.5392499922398382, 0.3694186072847828, 0.5207031973789977, 0.6902635248390409, 0.0857142857142857, 0.5942674499276168, 0.6798803763106699, 0.3111069937307952, 0.44447326411167487, 0.6981996210480456, 0.029401582477283354, 0.03595365954392629, 0.2525698806286212, 0.7440188528481942, 0.39915375517163815, 0.0, 0.4710884845462473, 0.3741250087960465, 0.5023425659061871, 0.6278440751703782, 0.23813949765549175, 0.604498905545545, 0.0, 0.0, 0.48839502083117775, 0.3364076169889719, 0.7224576287641187, 0.32927024968042806, 0.2011446278610293, 0.5926549364499356, 0.012506797362420424, 0.28515328647075866, 0.16616215076894816, 0.9491673847910091, 0.2974610638433307, 0.5856672680560794, 1.0, 0.0, 0.14224981392389122, 0.25520039903915914, 0.6696938642867756, 0.36962993614338396, 0.3079510025390523, 0.0006965415624715231, 0.05454545454545454, 0.47168601836630664, 0.6121458910881179, 0.41950057620957193, 0.15, 0.7053311537188118, 0.0, 0.12538390009388026, 0.27561318069050394, 0.4828810656992366, 0.23426055438611637, 0.22642773182532988, 0.20754837265854736, 0.14046243543663928, 0.03606856837889304, 0.6115436614201126, 0.2933192603414, 0.46900818750619533, 0.45642024598159237, 0.17201244606609029, 0.5937938465893167, 0.5383363493124113, 0.21170458518859758, 0.09999999999999999, 0.02514545638818947, 0.38441618785709153, 0.8431082915068202, 0.075, 0.45359326906298236, 0.1240513915987411, 0.4026968889437066, 0.21325242520222448, 0.3035131907129758, 0.4765514201137756, 0.7031368919494505, 0.6411869310447431, 0.2421395234947894, 0.6186654844638931, 0.49420165235842395, 0.6066748621480897, 0.198034900064599, 0.16058948244980703, 0.13693706889509483, 0.3480698440793539, 0.6051294155160957, 0.00032402115960723056, 0.6556400685407089, 0.0, 0.2566828330671295, 0.6387868261482768, 0.11189055219184274, 0.21289227544802397, 0.5863478985641175, 0.22930560995488647, 0.4885926413190037, 0.0, 0.20563599948280353, 0.43717836870822446, 0.12653107886558418, 0.43276216504296444, 0.7504004004704901, 0.3431162932885432, 0.6490563958711548, 0.519856007482291, 0.5738884298635611, 0.612170286630372, 0.24085956163586794, 0.0029925864127697795, 0.6489807027038881, 0.5867126509610628, 0.6301086386659165, 0.15061548779827955, 0.47520404719253106, 0.4632844671732036, 0.2591451158303522, 0.18700067546350158, 0.06018485678336463, 0.25404200148064193, 1.0, 0.5278563658424991, 0.22768072479798956, 0.0, 0.5523445081871601, 0.3234066372075278, 0.43944855055804594, 0.48302688177324754, 0.2508693845453372, 0.1301929297135142, 0.3518283206854355, 0.37501599800764257, 0.3682611169807786, 0.4956201859150352, 0.3615239988249709, 0.4957103430154332, 0.6930428146435975, 0.08998290412300283, 0.46013112345018237, 0.3, 0.34272438732934996, 0.2970059457705779, 0.7502328541813553, 0.6276684581081834, 0.08770398979400075, 0.204788220424696, 0.1608077145285827, 0.3, 0.2920915623309564, 0.0, 0.34895017176440035, 0.2933167576926855, 0.13770527935561802, 0.2435627304141043, 0.6029787489449101, 0.8141572281239757, 0.40087093505002125, 0.17617429467587584, 0.20866354767478065, 0.026244279557747184, 0.42452026437807655, 0.4338491014511952, 0.1999579770306643, 0.17893060548067247, 0.6150282557010782, 0.015391386947946768, 0.2623858345678357, 0.29950150725481295, 0.25496676744460994, 0.12900357665007425, 0.8396794437685136, 0.6762833633423001, 0.40197126343648293, 0.182733699567381, 0.38203304370679314, 0.2015372100944879, 0.6670296163053906, 0.17715572499641738, 0.6990258562552629, 0.37810237819050224, 0.24711505477790438, 0.3250572171854944, 0.778785113836511, 0.2515455658760476, 0.37048938294614847, 0.3243203545289008, 0.09345129391872986, 0.6990072675569636, 0.5131751962960711, 0.0, 0.0922382617563554, 0.30345313652582323, 0.31906723961609396, 0.36716738066932275, 0.0, 0.41216218061492155, 0.0, 0.29679028413581576, 0.4154806474750553, 0.6642647227570666, 0.0, 0.16615384615384615, 0.26002119614812386, 0.01147135285865619, 0.0, 0.6462319640830607, 0.3922768913024998, 0.4814374506953589, 0.2457354197214469, 0.5981649658092772, 0.0, 0.40174984549512993, 0.30975863275220417, 0.3786383928257664, 0.5194637253863024, 0.18836073304732065, 0.7158643298974992, 0.023975218851315705, 0.40470616284056093, 0.6777375191678128, 0.32077070655931117, 0.41597213880576306, 0.7678888855390539, 0.31221022737533, 0.041527976087742405, 0.21818181818181817, 0.0, 0.4302831533536506, 0.5992019158206943, 0.18792001115942947, 0.4965580647670747, 0.7671396677809144, 0.075, 0.47591449496046334, 0.7025820495340969, 0.0, 0.30545008728923306, 0.0, 0.10588235294117647, 0.14594241303126726, 0.49610310732316865, 0.6624137095416934, 0.26668480956597557, 0.490591429806986, 0.15713915918965934, 0.0, 0.3904576858400934, 0.3654842591679414, 0.8566207081945381, 0.0, 0.41756427241598104, 0.09999999999999999, 0.5712997683665408, 0.08181946886807157, 0.55638849227698, 0.7251381451714287, 0.44867577877089326, 0.39739573504004655, 0.8173086142015562, 0.0, 0.0, 0.5057622511461156, 0.48839502083117775, 0.4992375935295651, 0.0, 0.35493287301648696, 0.014430743472087264, 0.16277537094614922, 0.34188267737990674, 0.808221529667446, 0.2045929207836538, 0.22325991295455372, 0.5243648309257957, 0.3070773759693087, 0.0, 0.5257692995272405, 0.3026919961001559, 0.12409054461617691, 0.4482901213379712, 0.3242577008663058, 0.654196725926155, 0.34650852962940437, 0.5702743778074135, 0.7504004004704901, 0.3833681124813819, 0.6956232288530841, 0.23772535199522205, 0.8288120051119818, 0.3, 0.458072910245714, 0.023312580185318302, 0.5197109370915457, 0.19954533603284416, 0.3508081453961951, 0.04608347932253265, 0.15080787013247496, 0.1745808333803876, 1.0, 0.20396389360539482, 0.2406277883763611, 0.0, 0.0, 0.426804987569981, 0.6939655337688696, 1.0, 0.29636120573848534, 0.1478229866289921, 0.2099753868614004, 0.13356096931645173, 0.18968968840934997, 0.09514097505435423, 0.6222383916353247, 0.34981995376794145, 0.5271926235758367, 0.630659729746583, 0.17256656474185916, 0.6915721650456463, 0.23919068294046847, 0.5478260869565217, 0.6259496800747891, 0.47875874704124055, 0.5170731287100256, 0.17145077207464213, 0.30046321105062235, 0.9891483218006352, 0.212989484471527, 0.33983413635321175, 0.10873086345749489, 0.025555238068023588, 0.46034910007318686, 0.41926050398668113, 0.14107142045889115, 0.3143520734736618, 0.12267691083181458, 0.5237030996295007, 0.5759548166707876, 0.21971811602790056, 0.1018903872973364, 0.38247330256782786, 0.48427822208425986, 0.8228963827959196, 0.39109105872466743, 0.3255933189479775, 0.4747262001981727, 0.6125113060325476, 0.5929330292933186, 0.4444467746375167, 0.07186732186732187, 0.08693636075549857, 0.35972776642748383, 0.290467081015921, 0.5973517012085765, 0.0, 0.7038050470349024, 0.38320596924269007, 0.12875003091568185, 0.13784712570223168, 0.7450720368693571, 0.5857343158984987, 0.7576851784475747, 0.06, 0.8395135383339902, 0.4832381152599351, 0.6225119968630071, 0.5051915039880028, 0.20129994237561438, 0.0926115299070165, 0.5087142393682708, 0.39426585289217186, 0.13448200990261447, 0.336068568378893, 0.23335538277936863, 0.4616760548423344, 0.29971986266150674, 0.7877237574075306, 0.2814021446756408, 0.43201024220111967, 0.2531322115099076, 0.15042453538462128, 0.6841007521231434, 0.4458545906620257, 0.025959569403851163, 0.7253305249945152, 0.1223157836011612, 0.1731056143966373, 0.2187147879677449, 0.4702562381374042, 0.03194910810933169, 0.4024187938221744, 0.39024518379467305, 0.6624009877085362, 0.29248220444385864, 0.1049076358834401, 0.15351688848083994, 0.3474736208780839, 0.32828576165764645, 0.414436274930823, 0.43766406455343976, 0.09313539560014188, 0.4391738348994389, 0.3, 0.044893938381347684, 0.2401795301926496, 0.5825591001976738, 0.17408800166879834, 0.5996395391679379]
Finish training and take 1h42m
