Namespace(log_name='./RQ5/javascript_1/soft7_2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft7_2_codet5p_770m', data_dir='./data/RQ5/javascript_1_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
Namespace(log_name='./RQ5/javascript_1/soft7_2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft7_2_codet5p_770m', data_dir='./data/RQ5/javascript_1_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' if(!!unVisibleCols.length) {                     pagingHandler(groupHeader, {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' no-extra-boolean-cast', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if(visibleCols.length < groupCols) {                     pagingHandler(groupHeader, {'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 54.9761
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 12.8 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 44.3857
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 16.43 	 Previous best codebleu 12.8
  ********************
 Achieve Best bleu:16.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 21.5927
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 19.03 	 Previous best codebleu 16.43
  ********************
 Achieve Best bleu:19.03
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 13.9969
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 22.08 	 Previous best codebleu 19.03
  ********************
 Achieve Best bleu:22.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 5.9445
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 25.69 	 Previous best codebleu 22.08
  ********************
 Achieve Best bleu:25.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 3.0814
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 30.52 	 Previous best codebleu 25.69
  ********************
 Achieve Best bleu:30.52
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 1.5634
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 34.35 	 Previous best codebleu 30.52
  ********************
 Achieve Best bleu:34.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 1.2574
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 35.47 	 Previous best codebleu 34.35
  ********************
 Achieve Best bleu:35.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.8747
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 36.47 	 Previous best codebleu 35.47
  ********************
 Achieve Best bleu:36.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 1.9748
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 38.78 	 Previous best codebleu 36.47
  ********************
 Achieve Best bleu:38.78
  ********************
reload model from RQ5/javascript_1/soft7_2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_2/test.jsonl
  codebleu = 37.81 
  Total = 500 
  Exact Fixed = 2 
[61, 405]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 2 
[61, 405]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 37.81 
[0.47951708928204084, 0.6733521221670304, 0.41772702879788204, 0.16224049508515925, 0.0010363221775209538, 0.03376487223872229, 0.6863884298635612, 0.43828831385677636, 0.446119949729612, 0.5489251904522303, 0.5988935122709037, 0.4091069095851213, 0.2002114828319985, 0.6329739871337788, 0.3172078780799674, 0.3900153332066243, 0.22760389447236906, 0.5347828370323281, 0.0, 0.4251750701054166, 0.5920296163053906, 0.28178507258525926, 0.5371341534916789, 0.24022911833942617, 0.42435105095281245, 0.5958753345853993, 0.31219943047827065, 0.4396977885911743, 0.1814794487481174, 0.5914285714285714, 0.24520387101505192, 0.6649091634909969, 0.5001139210576769, 0.13403056013042883, 0.4212896845094789, 0.213927143608791, 0.36241724833293765, 0.4744362749308231, 0.24101185060239005, 0.3, 0.8027794523576361, 0.702226723380104, 0.700590168990773, 0.5663997925253785, 0.6332527923879377, 0.5871175358311811, 0.834350770839696, 0.8178607431385985, 0.21445862147356637, 0.2984197902445935, 0.019464594328050915, 0.23687370837680893, 0.20492615076885407, 0.5264886858900315, 0.12300616590626301, 0.3155459171509464, 0.12, 0.13771239743403269, 0.5989262750990256, 0.5908278317428526, 1.0, 0.2605255056885018, 0.6420978076980255, 0.8321296454239947, 0.7158229243802021, 0.19482137163084995, 0.430875728161367, 0.6983338153069474, 0.15289869917282814, 0.8171627322865166, 0.2382699164847879, 0.22013430448015572, 0.6313659179388997, 0.2539573161622403, 0.6445931574752105, 0.15, 0.4985557593391261, 0.377251685355458, 0.11311767022706631, 0.34032700269089244, 0.40741368947460466, 0.6571741663707173, 0.29776183102133325, 0.22730156000330948, 0.18062245588097034, 0.6745684305999443, 0.40817894487578654, 0.39481725179866956, 0.47996086480687705, 0.4268839003347389, 0.075, 0.317517459692158, 0.5641854303244904, 0.3433062204607786, 0.5992709938548251, 0.7485536690812331, 0.1596283361029218, 0.4461837111433119, 0.31944778269522933, 0.3202035712229998, 0.48151833528931587, 0.6621093954841358, 0.011253970178350756, 0.019499058380229292, 0.2624222248790914, 0.7697223311159012, 0.10550369478577849, 0.5102043912123392, 0.7929193860906751, 0.3822904043504285, 0.5048738245831823, 0.25504467770493533, 0.3381394976554918, 0.714297617187952, 0.8455292927646232, 0.5899676509171733, 0.5418035126585234, 0.31644883180771544, 0.7643938999758249, 0.3282399655988043, 0.7030716541219081, 0.26095731127406935, 0.3289630818290568, 0.000568952951437459, 0.003669677045151612, 0.20532521668799822, 0.10541936614367986, 0.1557552911425139, 0.5977175089807298, 0.5568870225660189, 0.14224981392389122, 0.24104389658838232, 0.5702335155106275, 0.35513223603278815, 0.19726027397260273, 0.7686479189761937, 0.09999999999999999, 0.7308835793300246, 0.24472641295104425, 0.42430386249654006, 0.53238103074832, 0.7232871590915826, 0.4812246100706432, 0.1508808394358597, 0.6441840566053633, 0.47209653681517216, 0.1483488149910871, 0.25027467370199613, 0.41324238658516804, 0.22347253502951772, 0.6429326815734496, 0.6507190856640116, 0.23480432950792463, 0.2903264184962796, 0.464920048657917, 0.21765336707209076, 0.22520271851844903, 0.4501922135425479, 0.0382177547839275, 0.10355234013662709, 0.26772438732935, 0.5538392357757506, 0.6005419819099078, 0.13182275420461528, 0.1473254967064994, 0.16552344444928055, 0.36991190450457334, 0.7943824886639701, 0.17458792563302522, 0.42976656372499167, 0.1609255738614917, 0.5945531884512912, 0.19374830173038918, 0.27593324680536846, 0.3605132142494443, 0.6560247841578373, 0.009055538614448026, 0.5250881277610704, 0.343798626738156, 0.5279905343601452, 0.4945086314330902, 0.00013399847711978655, 0.8422067364356236, 0.07825011689776729, 0.4978339243889879, 0.1775949732984588, 0.401706244308338, 0.24256878532422088, 0.35866555769957775, 0.6263670487950274, 0.03816539143631262, 0.7237392159029667, 0.25444134162478105, 0.3280608799990351, 0.13661640022621738, 0.5119214456799607, 0.7504004004704901, 0.09374082671761248, 0.23349332619240096, 0.6042557187958627, 0.2018375892948912, 0.6197425129870276, 0.8127440275555333, 0.4529011142002214, 0.5780486740734356, 0.7596480665497221, 0.3525022299119591, 0.13996139057212667, 0.3433254960033112, 0.6407534718346111, 0.27515927402956025, 0.6860528808610047, 0.10220433395410801, 0.11837376155760526, 0.5956439852323709, 0.46625663598630407, 0.17186483697427668, 0.4798127473721585, 0.8304054224244419, 0.7586380858578841, 0.2013391028134342, 0.4833678859742482, 0.029577104225799546, 0.352337941937802, 0.36746017659504293, 0.060190556369917374, 0.3782721856909792, 0.1552417442871904, 0.6502973371873174, 0.3944716986584166, 0.43378752751168215, 0.3514400440634492, 0.5406990993602754, 0.005039981735037458, 0.25042597337667905, 0.1576033048231314, 0.30693310215651903, 0.07609548325990335, 0.4215676481643553, 0.25550954714385304, 0.6179527809215893, 0.3600988738495229, 0.364968013784138, 0.29002738878969714, 0.4453710621987834, 0.005634271240224073, 0.09180149734125959, 0.46697146828894043, 0.5093871904084899, 0.6865929391266208, 0.10131060227593218, 0.19329414510767018, 0.08907633414462596, 0.6859894670707158, 0.15436177999393216, 0.07708905562397844, 0.041474654377880185, 0.4825529411708366, 0.6293308099815368, 0.006421934492309619, 0.23669321919655517, 0.7180974714554764, 0.7511200104643803, 0.2550135523481469, 0.871149014981726, 0.09311129058983796, 0.4486590401007454, 0.7517178594135414, 0.4992152818917487, 0.11456171617810418, 0.5920296163053906, 0.17750858593612567, 0.6897507239122158, 0.2662285900895401, 0.16752227518985324, 0.17584276410256192, 0.778785113836511, 0.17245210557386054, 0.1636017699703288, 0.36494569883285416, 0.0006201071487303424, 0.6861035411299102, 0.5031152927307899, 0.29347922450832814, 0.0468605206747464, 0.311318278270378, 0.13037012380078397, 0.3262784076445664, 0.593757569162549, 0.7222372725220607, 0.09, 0.39356323276364047, 0.4654788500276994, 0.7093846190645419, 0.00013395995497303924, 0.18533181300247914, 0.7418551910912801, 0.7799696154227835, 0.6358048322836624, 0.26123371730081063, 0.449631211483896, 0.09215693261848987, 0.7790401685486271, 0.6194196708682929, 0.0, 0.7540514287200638, 0.6042260895170799, 0.3807256007926331, 0.4668521863383327, 0.14162562171255766, 0.3231363572763577, 0.37833824586548837, 0.419436844605607, 0.5488628862515236, 0.2527062238969392, 0.2381742713669553, 0.1875563716305789, 0.18246123585808374, 0.041527976087742405, 0.32641839065175154, 0.6919111149149633, 0.6470007964694864, 0.5992019158206943, 0.761020252272177, 0.13398680050067013, 0.28412951149315907, 0.7718584711568836, 0.3724377127841062, 0.7889116194329123, 0.2792267242623299, 0.24200542891946875, 0.0, 0.26215184527872726, 0.27485579794713694, 0.5077588224993747, 0.050156765379359224, 0.12310113653232592, 0.9117063738487072, 0.15682247450687786, 0.19822154671766765, 0.2346211104647077, 0.045806284167299254, 0.8022530852680203, 0.6345042541575315, 0.16313802986883552, 0.2006354448443511, 0.38362922921410825, 0.465523972643262, 0.5566478602423253, 0.6540474892905961, 0.5755663420939352, 0.7170015046833873, 0.8515059601346342, 0.3564008359993149, 0.0, 0.4769725409613548, 0.314477076757666, 0.1838713538480249, 0.3595713930432408, 0.4174033698702468, 0.008694754573109073, 0.12250746869225945, 0.12768744137231708, 0.28230814125863857, 0.7696296454239946, 0.5897926741330912, 0.4408019104574141, 0.2593809659321378, 0.12793150008632492, 0.36587342440580645, 0.04557092843465621, 0.2627166511815171, 0.5228832644008625, 0.10616614973095514, 0.8184276974159628, 0.15, 0.6073917520388044, 0.7504004004704901, 0.00020548564156522265, 0.17490162229181855, 0.2646183374858615, 0.7906009505253491, 0.0008436533550241122, 0.12788871973346425, 0.15043977012404672, 0.4227057950850569, 0.215013302191103, 0.1709442817534614, 0.27560685302182575, 0.30327200251735614, 0.16294434829805365, 0.4925466491076139, 0.26324955857729304, 0.2997732308822231, 0.30219202318486355, 0.3205074332348342, 0.2749931744824224, 0.4012880030099427, 0.30101731793973274, 0.24790682600276043, 0.0004012128405815345, 0.4813238395454885, 0.3654166918252179, 0.0023669587806221743, 1.0, 0.17677026729123674, 0.1516548486236823, 0.34993365111225555, 0.33357262605060956, 0.23730510763939283, 0.5469424819871818, 0.40939599215703437, 0.0005388425628224694, 0.6863884298635612, 0.30425803664401, 0.2205712483541965, 0.19129264116313002, 0.2830669532735086, 0.2426290502531993, 0.9437985500759485, 0.06927670363972456, 0.0006806869875650191, 0.003225477090058566, 0.21651718548842924, 0.0027522935779816515, 0.7271751139759535, 0.3275887968144088, 0.20750429107968574, 0.3952562381374042, 0.6799870881890929, 0.7452644986878588, 0.13041411888940263, 0.5979500588071365, 0.380038038773427, 0.5959263320186408, 0.9009945546583267, 0.5579165668327014, 0.3995320173290975, 0.22297517580320061, 0.2810612649280416, 0.6965770485533604, 0.4930282512535298, 0.25311193401315524, 0.3415904602219504, 0.3057990091375193, 0.49882645355127553, 0.06046169384081876, 0.7498018405233149, 0.14941086985078172, 0.22291614065172188, 0.11975671785859504, 0.14335966168551179, 0.7106391824574765, 0.1743945318386763, 0.07578947368421053, 0.2680963881729128, 0.3373775655290533, 0.7420554014474346, 0.5498394497606403, 0.12238553116020154, 0.04898609592251211, 0.3497770149147582, 0.27603653191301736, 0.5510585507100338, 0.2822149863601122, 0.2284744452370564, 0.11358515788111218, 0.648187959576259, 0.766768914846239, 0.3229510208039455, 0.0, 0.12368827869807053, 0.43316565609736535, 0.43392783617316655, 0.18608115106316922, 0.0563991919244969, 0.23148533913467942, 0.2382915953335315, 0.7442951894445113, 0.3044318210319886, 0.16180347712357745, 0.39675296426202156, 0.373287992008415, 0.1838447177061216, 0.782939066295371, 0.3165806866380918, 0.18035442771931492, 0.0025952185577196923, 0.5898305084745763, 0.25828576165764644, 0.22614576840946915, 0.5949436010430027, 0.5299522285347575, 0.1828370380928119, 0.754237295389593, 0.2502381845322978, 0.21492537313432833, 0.6090749086635656, 0.7891681140896578, 0.6553732715648326]
Finish training and take 1h31m
