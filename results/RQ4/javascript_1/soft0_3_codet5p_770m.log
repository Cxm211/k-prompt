Namespace(log_name='./RQ5/javascript_1/soft0_3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft0_3_codet5p_770m', data_dir='./data/RQ5/javascript_1_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if (!!mResult) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if (mResult) {'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 20.5227
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 16.24 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 21.5082
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 18.58 	 Previous best codebleu 16.24
  ********************
 Achieve Best bleu:18.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 4.5631
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 22.73 	 Previous best codebleu 18.58
  ********************
 Achieve Best bleu:22.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 2.3101
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 24.97 	 Previous best codebleu 22.73
  ********************
 Achieve Best bleu:24.97
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 1.6754
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 26.76 	 Previous best codebleu 24.97
  ********************
 Achieve Best bleu:26.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 4.4667
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 25.42 	 Previous best codebleu 26.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 0.2145
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 25.49 	 Previous best codebleu 26.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 0.1361
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 24.16 	 Previous best codebleu 26.76
  ********************
early stopping!!!
reload model from RQ5/javascript_1/soft0_3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_3/test.jsonl
  codebleu = 23.93 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 23.93 
[0.6731716191316424, 0.024992782940525985, 0.13263605482565088, 0.5294913703178445, 0.14655142011377564, 0.045786652312548234, 0.19396239338088345, 0.3287884324003159, 0.40329774094701853, 0.0375, 0.0, 0.02215651810904784, 0.0226075224308729, 0.4430386884866492, 0.4706490889030359, 0.05429084638967137, 0.02976130371644454, 0.039324324324324325, 0.0, 0.4882296960047106, 0.2659954911340934, 0.0, 0.5746450339771512, 0.34155521819911794, 0.4406204374015982, 0.0, 0.732774825407631, 0.3741881978703363, 0.0, 0.5142857142857142, 0.09603004382977383, 0.02068701102847838, 0.2559369128272438, 0.11126164383693861, 0.3, 0.4561609047550288, 0.02394949843453903, 0.3847752094509008, 0.3340047425620204, 0.4094550226833914, 0.5769230243983176, 0.4023073752942379, 0.25576201370975676, 0.5639043976970031, 0.5993361036209655, 0.5061377315997734, 0.6719742251881117, 0.11226570582806134, 0.0, 0.11657154641724346, 0.19357540718557575, 0.5410029250288506, 0.0007854849557074536, 0.3756287861000467, 0.0, 0.0, 0.06, 0.11028263651614215, 0.6080942405507963, 0.0, 0.12394169237252195, 0.18751450531926955, 0.009012966972574579, 0.10838274917609722, 0.15477107165667556, 0.5108190831812653, 0.0, 0.4573033544425907, 0.0, 0.7855787771981987, 0.6847482616910893, 0.6075544699256141, 0.0, 0.0, 0.04408517371708687, 0.0, 0.006118445937348138, 0.17472746508580017, 0.075, 0.0, 0.149339380368078, 0.5020712406112477, 0.02913215362273379, 0.061538461538461535, 0.0006860933845000622, 0.6477418742765129, 0.3934566395644027, 0.15854271756860228, 0.09999999999999999, 0.23461021255170733, 0.0, 0.46321851560994864, 0.5793435451110831, 0.26666666666666666, 0.001311641221919523, 0.6176434288973713, 0.0226075224308729, 0.0036515950322592123, 0.43395085652132837, 0.24350696197190003, 0.0, 0.7089551634180106, 0.0, 0.0, 0.7064454728337657, 0.2746515395852575, 0.05454545454545454, 0.74021074690812, 0.32950881933599857, 0.12179309151202622, 0.09999999999999999, 0.1910708376275108, 0.10434782608695652, 0.4738018684644263, 0.8803662308912057, 0.6885015519766482, 0.0, 0.21368652688884832, 0.458885336254912, 0.3282399655988043, 0.0, 0.4149534511066882, 0.001639077096578131, 0.00108134634417014, 0.0148889232165562, 0.2777840005665938, 0.0, 0.0, 0.04155521819911797, 0.16561264251020869, 0.14224981392389122, 0.28027716860816454, 0.431287471110104, 0.2906083590391886, 0.0857142857142857, 0.07201937062239372, 0.075, 0.02215651810904784, 0.10966554667599986, 0.0857142857142857, 0.0226075224308729, 0.720032539996653, 0.011319083598953018, 0.0, 0.38418097856141586, 0.4607974118436553, 0.024992782940525985, 0.7492027451538551, 0.33503177394682454, 0.1295448621543013, 0.30112168935005806, 0.05391043154723463, 0.19723791641638322, 0.35000000000000003, 0.06090639179940133, 0.23632077478601077, 0.16975590985793634, 0.5865265109839976, 0.0, 0.05752965674900004, 0.0, 0.10138675024053767, 0.0, 0.0809222070277578, 0.12655142011377563, 0.29687502082618045, 0.3764020331459349, 0.7451158442407357, 0.2992286298565113, 0.0, 0.7860594922037922, 0.0, 0.02215651810904784, 0.0468605206747464, 0.5441070669348561, 0.6433851802167154, 0.11219121170202062, 0.47016879391277366, 0.24999278294052596, 0.1875, 0.0910158494817324, 0.04111084163384836, 0.7970284626812179, 0.0, 0.4857250514124658, 0.3660129971089868, 0.0, 0.19590162174624512, 0.06688879533529177, 0.30633721785518786, 0.0, 0.7237392159029667, 0.09807571912207222, 0.359100732288422, 0.15, 0.21960456298678693, 0.1544798640200008, 0.3500221175710181, 0.7014995993790445, 0.14792779478289653, 0.049999999999999996, 0.32987636425456257, 0.13733601542463347, 0.0, 0.6323386190190008, 0.10898674928505511, 0.22499999999999998, 0.15, 0.4826602189222059, 0.10909090909090909, 0.5764795139892516, 0.020213339037811265, 0.22996835183968561, 0.09638149986045377, 0.0, 0.44629097285515595, 0.2074841111688946, 0.5833069501526021, 0.09999999999999999, 0.0, 0.5540531920184177, 0.0226075224308729, 0.04662232904253158, 0.1005467160613551, 0.0, 0.020846591562191125, 0.3344550226833914, 0.013043478260869565, 0.0, 0.3660608358590962, 0.3810136766023814, 0.3738019563050906, 0.03445502268339141, 0.0, 0.049999999999999996, 0.024992782940525985, 0.7884806282644992, 0.0857142857142857, 0.3982286110739164, 0.28969288756749023, 0.09978753734569693, 0.37217202547907424, 0.3373940575497127, 0.024992782940525985, 0.2812485832347605, 0.15663264913220276, 0.06666666666666667, 0.3, 0.0, 0.7279657833697224, 0.14215651810904784, 0.19329414510767018, 0.15961650941538447, 0.7155729917407292, 0.012154741201956665, 0.09054591715094637, 0.0, 0.021546987431699048, 0.3415384850527624, 0.0, 0.0, 0.5582883138567764, 0.4584726890613361, 0.23037062904176595, 0.871149014981726, 0.5516253321338307, 0.4486590401007454, 0.1871503808225305, 0.2943881797263131, 0.02913215362273379, 0.2659954911340934, 0.023182513656863794, 0.609180978561416, 0.4683462077478211, 0.6657004497903911, 0.0, 0.43986388514485997, 0.0, 0.1251733468474147, 0.09999999999999999, 0.02913215362273379, 0.18341658623740298, 0.04111084163384836, 0.2727272727272727, 0.02913215362273379, 0.49724635203805456, 0.12394169237252195, 0.3068947476220048, 0.5679077958442817, 0.39678313625068495, 0.0, 0.48714365903318313, 0.4229226590692534, 0.0, 0.04913407317897768, 0.15752708973136217, 0.11407325163141122, 0.02020829362367807, 0.17969021011291694, 0.7940416292738874, 0.37829774094701857, 0.0, 0.5087537711369159, 0.09999999999999999, 0.0, 0.02149387944305044, 0.28927418909753955, 0.12499278294052599, 0.02526254177670713, 0.22499999999999998, 0.746832332548443, 0.37833824586548837, 0.1714285714285714, 0.5488628862515236, 0.6011697895341449, 0.2943789248026033, 0.2446440550876814, 0.0, 0.041527976087742405, 0.1714285714285714, 0.5309558188383698, 0.023941692372521955, 0.5159503405396788, 0.7071332849165511, 0.12179309151202622, 0.7131326407225315, 0.0, 0.26666666666666666, 0.000247327504387602, 0.24527048494297776, 0.3361519735310733, 0.0, 0.13492657657695584, 0.4102973371873174, 0.1289503798031426, 0.043621189914860366, 0.1858071622471934, 0.53682237084508, 0.24436474119908835, 0.02913215362273379, 0.16315789473684209, 0.21132701411959331, 0.8022530852680203, 0.6345042541575315, 0.42312344481178027, 0.015668124844007404, 0.17184798271893187, 0.01268342520959569, 0.09999999999999999, 0.7444602584270059, 0.501793075204031, 0.3272727272727272, 0.021243180820230228, 0.0, 0.0, 0.0, 0.12913215362273378, 0.1005467160613551, 0.29290447212874376, 0.34387644052144434, 0.0, 0.0332212457512744, 0.09360557763800133, 0.808221529667446, 0.17280829034635492, 0.2344550226833914, 0.0, 0.3545942395656087, 0.006541068871646702, 0.30672447121247104, 0.3, 0.2957325462250059, 0.6162050754771118, 0.15, 0.6377558294804022, 0.09680232558139534, 0.6452718649838225, 0.1544798640200008, 0.08020484732060233, 0.28178264951256843, 0.0, 0.8040142249358395, 0.0, 0.146936317150369, 0.024842797352915724, 0.36335989500965044, 0.0, 0.0, 0.12700262984655553, 0.19999999999999998, 0.19507247304661363, 0.09999999999999999, 0.45143902282553305, 0.2997732308822231, 0.026551420113775634, 0.18, 0.27747861876990987, 0.03379608036298831, 0.0, 0.24790682600276043, 0.1640230588296479, 0.4729307418555062, 0.04414174369298079, 0.0, 0.021243180820230228, 0.20082062018706492, 0.27976130371644453, 0.0, 0.630659729746583, 0.16609444720383254, 0.5821154266022259, 0.23919068294046847, 0.0, 0.5334642652876687, 0.5464075798999237, 0.09999999999999999, 0.12394169237252195, 0.09999999999999999, 0.09806242944188129, 0.15132541084960927, 0.35297176132225583, 0.0, 0.029339380368078014, 0.4463414634146341, 0.1714285714285714, 0.7271751139759535, 0.30058064588892736, 0.2578275533196176, 0.09999999999999999, 0.2212431808202302, 0.7493558989607656, 0.021243180820230228, 0.7928486444658168, 0.0857142857142857, 0.0, 0.38301426644885805, 0.026885026527377665, 0.3995320173290975, 0.708628838180964, 0.43996242707328115, 0.0, 0.5309575999632741, 0.0, 0.34828357272468946, 0.18834643231794143, 0.04068353540488429, 0.1691306151806585, 0.7293212151712596, 0.06269775719630426, 0.024992782940525985, 0.0, 0.17647058823529413, 0.6156286994696712, 0.0, 0.075, 0.30662364850887314, 0.38458036272712603, 0.02068701102847838, 0.6085062507204752, 0.22179140591536411, 0.3393331232708163, 0.0, 0.7281013979705606, 0.526210757266397, 0.029580569926963382, 0.27456170472172553, 0.021423359923506444, 0.198279504603191, 0.27289365012144207, 0.02215651810904784, 0.0, 0.0, 0.2792985133497866, 0.39492127561739643, 0.3504269667930605, 0.10889679937114949, 0.48004664910761385, 0.22493824052469036, 0.5533972709394317, 0.2190030005919364, 0.1714285714285714, 0.5814500037030309, 0.12655142011377563, 0.31959950876963034, 0.20292203403187364, 0.11367315124100329, 0.12194044061165557, 0.026551420113775634, 0.12260752243087289, 0.22373276960130628, 0.0, 0.0, 0.02215651810904784, 0.3443863239068763, 0.31408262185746816, 0.0773019105810793, 0.13333333333333333, 0.21793483238157646, 0.23084259771663657, 0.5737699458175705]
Finish training and take 1h0m
