Namespace(log_name='./RQ5/javascript_1/soft0_2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft0_2_codet5p_770m', data_dir='./data/RQ5/javascript_1_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if(!!unVisibleCols.length) {                     pagingHandler(groupHeader, {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if(visibleCols.length < groupCols) {                     pagingHandler(groupHeader, {'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 51.74
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
Namespace(log_name='./RQ5/javascript_1/soft0_2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft0_2_codet5p_770m', data_dir='./data/RQ5/javascript_1_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if(!!unVisibleCols.length) {                     pagingHandler(groupHeader, {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if(visibleCols.length < groupCols) {                     pagingHandler(groupHeader, {'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 47.9068
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 16.25 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 46.0631
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 22.45 	 Previous best codebleu 16.25
  ********************
 Achieve Best bleu:22.45
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 17.1808
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 25.83 	 Previous best codebleu 22.45
  ********************
 Achieve Best bleu:25.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 6.1845
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 30.87 	 Previous best codebleu 25.83
  ********************
 Achieve Best bleu:30.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 3.2097
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 39.07 	 Previous best codebleu 30.87
  ********************
 Achieve Best bleu:39.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 0.9455
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 43.39 	 Previous best codebleu 39.07
  ********************
 Achieve Best bleu:43.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 1.3718
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 45.1 	 Previous best codebleu 43.39
  ********************
 Achieve Best bleu:45.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 0.3299
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 43.45 	 Previous best codebleu 45.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.4265
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 44.54 	 Previous best codebleu 45.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 0.1999
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_2/validation.jsonl
  codebleu-4 = 43.94 	 Previous best codebleu 45.1
  ********************
early stopping!!!
reload model from RQ5/javascript_1/soft0_2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_2/test.jsonl
  codebleu = 44.66 
  Total = 500 
  Exact Fixed = 5 
[1, 61, 129, 174, 390]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 5 
[1, 61, 129, 174, 390]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 44.66 
[0.9891483218006352, 0.06826184131408944, 0.4499615708051955, 0.6896475098120338, 0.08436692013296332, 0.04568223564396978, 0.6530550965302279, 0.3287884324003159, 0.4934099939845978, 0.5541354777257445, 0.30848301068070133, 0.8696847134058581, 0.13203660364007316, 0.6329739871337788, 0.3169967481883137, 0.6452148813321528, 0.44194930875697946, 0.4701430729789151, 0.0, 0.4513345815168244, 0.39515421638127773, 0.28178507258525926, 0.5440729541780828, 0.7158229243802021, 0.41109531123785636, 0.528394176396658, 0.30802732662134724, 0.396349572956634, 0.19999999999999998, 0.5142857142857142, 0.18805540216402733, 0.6774284282374794, 0.705387299307066, 0.8570961003195992, 0.5607942439151187, 0.5696431220857536, 0.6239155542346693, 0.4744362749308231, 0.3340047425620204, 0.3442048888914765, 0.5397334206577995, 0.702226723380104, 0.700590168990773, 0.5639043976970031, 0.5993361036209655, 0.5909604298691042, 0.7672890127001166, 0.8178607431385985, 0.25617082803866614, 0.2574138233688298, 0.7517692380063994, 0.7358863528261614, 0.6248885220437987, 0.5264886858900315, 0.05379903235460271, 0.13333333333333333, 0.06, 0.5338054518326395, 0.5638156502975669, 0.6580241834790774, 1.0, 0.3033081154166797, 0.6420978076980255, 0.8321296454239947, 0.7158229243802021, 0.5674789691832558, 0.44190513992607283, 0.6983338153069474, 0.15289869917282814, 0.7855787771981987, 0.6847482616910893, 0.669492796583623, 0.6313659179388997, 0.2539573161622403, 0.5236538106877768, 0.13846153846153847, 0.27972102006711447, 0.44694573249540037, 0.059311806604741016, 0.35409094231279425, 0.3644340828501731, 0.17130364986239272, 0.29776183102133325, 0.002243110661158092, 0.012954629162367337, 0.7065485068370296, 0.506912284423967, 0.3990579162958894, 0.43986388514485997, 0.6492561474202864, 0.075, 0.42273091745181135, 0.5793435451110831, 0.7327471563983727, 0.7298304277609075, 0.772199578216531, 0.6980472624402649, 0.6871621017950617, 0.8899929854363868, 0.3111069937307952, 0.2494236029639056, 0.7089551634180106, 0.5627200451626766, 0.0, 0.2704629321304923, 0.6336892692235006, 0.4917813099919539, 0.5102043912123392, 0.7929193860906751, 0.4637721020304535, 0.5169403506925052, 0.5765939546949644, 0.31083397506733595, 0.714297617187952, 0.8868935042137447, 0.18234110643341622, 0.5418035126585234, 0.6584286884561462, 0.7643938999758249, 0.2619896427763203, 0.5329641979757095, 0.6449996471154618, 0.23623009890372684, 0.28515328647075866, 0.16286579711468366, 0.9090858388287586, 0.06300819652962071, 0.1506769339968601, 1.0, 0.30580421153820125, 0.14224981392389122, 0.6269940846421196, 0.7117160621404106, 0.5463957566504889, 0.11554591715094636, 0.10159151219476173, 0.075, 0.551199747781906, 0.5463874253477488, 0.42430386249654006, 0.2858639086357819, 0.21284189002660295, 0.5109210160727382, 0.14575681137379307, 0.5892647227570667, 0.5026610132484245, 0.21815183352893158, 0.47565661352901145, 0.06336484743071628, 0.8313560704120448, 0.5342294154189744, 0.4841187907059259, 0.1826838908725193, 0.526797617187952, 0.5591532522333204, 0.23632077478601077, 0.902700490932961, 0.4536889087324454, 0.0453425253981304, 0.23049138297301558, 0.005904096612562088, 0.5774263257292502, 0.33246351414044084, 0.09822108307670613, 0.48624193424630757, 0.45517285439292154, 0.3759439648211699, 0.7943824886639701, 0.17121847382214372, 0.8020311540826717, 0.7860594922037922, 0.4556514557354716, 0.8991129363138861, 1.0, 0.5441070669348561, 0.6560247841578373, 0.007019293955659427, 0.47016879391277366, 0.748363176537869, 0.5279905343601452, 0.28604415722679444, 0.19035071477803323, 0.7970284626812179, 0.14329322794034355, 0.41740086764536444, 0.3660129971089868, 0.401706244308338, 0.43535311981078373, 0.5931493104194671, 0.6763670487950273, 0.3573011355166036, 0.7997827167911244, 0.2248912291925236, 0.38518768881016113, 0.48633400213704503, 0.4528412049217675, 0.5139267529993974, 0.3083871672565479, 0.7014995993790445, 0.5531179266383359, 0.4368097396455818, 0.674524582150827, 0.4771566026662144, 0.6751705176140901, 0.5780486740734356, 0.5577395904012914, 0.44547259471014233, 0.48633400213704503, 0.5667624863780527, 0.204042748727021, 0.6707027207781995, 0.6860528808610047, 0.06552572456572223, 0.23907442706273568, 0.004790911047663172, 0.5278563658424991, 0.2074841111688946, 0.5326968889437065, 0.4985805903416326, 0.4177464707575192, 0.5691230775634993, 0.47521488133215284, 0.25704272418535706, 0.352337941937802, 0.33052026896990144, 0.46904473191093105, 0.4342914059153641, 0.06138010776885055, 0.28780965999884256, 0.16228175059257646, 0.812812273439955, 0.1728420700463256, 0.021791405915364114, 0.21950946384695552, 0.564179104477612, 0.396838405481522, 0.9142445273649933, 0.6765950349550134, 0.2703415737700678, 0.5913354370230705, 0.7170126220080686, 0.372866746136746, 0.3373940575497127, 0.27558314725722455, 0.4453710621987834, 0.6381054355852964, 0.7158229243802021, 0.46697146828894043, 0.6496441836400741, 0.8231779991158767, 0.4250417623577092, 0.17649475021297867, 0.17107118515962755, 0.766682914692771, 0.45437024693324646, 0.2379911542742785, 0.196275735434565, 0.48246354538431135, 0.4814800945236143, 0.3665582680402964, 0.09632560384114891, 0.2988064132910565, 0.37298599539553945, 0.46489474286703747, 0.871149014981726, 0.7127774792622994, 0.4486590401007454, 0.3150645388642504, 0.37519430792926095, 0.015582806812327689, 0.39515421638127773, 0.17681255192642123, 0.7289642331466002, 0.6509106257652548, 0.776554029901346, 0.31921085441808716, 0.778785113836511, 0.09456149082451723, 0.8378634002940782, 0.25415788060972533, 0.11666737327895797, 0.552865128981459, 0.5571258435884442, 0.3528279653407508, 0.11582292438020222, 0.49724635203805456, 0.7545909332472028, 0.5822109370915456, 0.6504518448874317, 0.33675543480310277, 0.10588235294117647, 0.3721335658104542, 0.4229226590692534, 0.5943157831857282, 0.0, 0.591569940896651, 0.8016440493564279, 0.6663482067610581, 0.4114103048223333, 0.8947836446510709, 0.472020996722366, 0.512283842723568, 0.6594810589932428, 0.6194196708682929, 0.0, 0.44149298837832585, 0.5517865908584089, 0.3207256007926331, 0.4633087211631649, 0.16693216161874924, 0.5904095224507812, 0.37833824586548837, 0.4337572146077191, 0.4248592016818845, 0.578936860503413, 0.4969617815012778, 0.4443440228112808, 0.6259403648918203, 0.0536616865191997, 0.192, 0.8387796716807858, 0.6678228983889894, 0.5159503405396788, 0.878163401167724, 0.5339102500446592, 0.8256475917744592, 0.47515510931955607, 0.23453422619733333, 0.6007491426321363, 0.48275794232794217, 0.3361519735310733, 0.0, 0.29379269442695755, 0.4102973371873174, 0.27342999143227303, 0.7125108877355164, 0.30831848424488534, 0.6953602200025533, 0.5780365272269377, 0.1759424130312673, 0.40156651918683534, 0.4005326816439736, 0.7272531677393268, 0.6345042541575315, 0.4905278186463043, 0.39095917695500915, 0.7512083692594183, 0.465523972643262, 0.38201024220111973, 0.6701337704536606, 0.5969803377115616, 0.7170015046833873, 0.8515059601346342, 0.6570865573743658, 0.0, 0.34515475868460394, 0.18136591793889978, 0.5134582507008687, 0.5307707065593111, 0.11140974030126737, 0.008694754573109073, 0.557719307720078, 0.16857207258475373, 0.808221529667446, 0.35251198221477176, 0.3344550226833914, 0.31536802347096193, 0.30512218636080235, 0.07991602774254361, 0.36587342440580645, 0.014231742501286922, 0.3464886858900315, 0.5228832644008625, 0.10616614973095514, 0.8184276974159628, 0.19999999999999998, 0.9701564725523897, 0.5139267529993974, 0.0031323668437758896, 0.7431351738566481, 0.14674643804276, 0.2164865431895339, 0.029089419117327243, 0.20559854502990993, 0.15091102173533677, 0.5578607431385983, 0.18127539955741684, 0.1814794487481174, 0.3015333886374011, 0.7151187056012045, 0.18409501165701359, 0.5243648309257957, 0.45143902282553305, 0.2420311700345064, 0.5242575244377461, 0.4777601556679766, 0.4837282150616216, 0.5783036814100584, 0.5008670073433122, 0.24790682600276043, 0.1650567204976689, 0.4813238395454885, 0.12925360445018286, 0.0038861645719320075, 0.6749861219639539, 0.22978757180445614, 0.27976130371644453, 0.34993365111225555, 0.7047281527834622, 0.7165113845863448, 0.8403116829613333, 0.17774818725435676, 0.026198875192873054, 0.5245257436334638, 0.47875874704124055, 0.5309537492920497, 0.25204267246334444, 0.13535104804853892, 0.308801196177701, 0.9437985500759485, 0.35297176132225583, 0.011161450000535185, 0.030390470936082048, 0.4664355844276247, 0.7154557474788577, 0.7271751139759535, 0.3181760473859727, 0.4626618362643713, 0.18747183518174532, 0.5779094888932341, 0.7933471438083787, 0.47132482093408723, 0.7928486444658168, 0.380038038773427, 0.3362509693916087, 0.9009945546583267, 0.25940226811350725, 0.46619868399576414, 0.743345352381902, 0.4077637026519637, 0.23090232075255773, 0.531086204090959, 0.06360241691711237, 0.34277291518460246, 0.307730249353226, 0.5926763596850908, 0.039342623924934365, 0.45280676035784423, 0.30927607150859204, 0.12036061202697737, 0.13903854307791302, 0.822378125936329, 0.6156286994696712, 0.6130918321069074, 0.075, 0.7280553527107964, 0.31679730828866415, 0.6762599787610002, 0.3605305991138357, 0.6371808959162841, 0.11268026698847815, 0.30844528188570425, 0.37797410960263045, 0.5010585507100339, 0.0002258189481489238, 0.3809966405126346, 0.2327667644787686, 0.198279504603191, 0.9495565333799167, 0.2819878751178625, 0.21080723102368715, 0.24118710506028768, 0.5107073303553274, 0.4514987733151654, 0.17871426713710534, 0.6586275761728796, 0.30118364130611386, 0.2382915953335315, 0.6944579044728878, 0.4155372107618692, 0.2827099304198406, 0.39675296426202156, 0.9039342742606371, 0.7115187153144993, 0.7174759810668236, 0.31237328339586123, 0.5359050276195838, 0.06813781061796503, 0.5204959945815166, 0.25828576165764644, 0.22614576840946915, 0.0023733831964681176, 0.6574634481857526, 0.5744100321686132, 0.6432530992618458, 0.45759530908884194, 0.7629326815734496, 0.6222244648066615, 0.49000278074157855, 0.7028202058240542]
Finish training and take 1h28m
