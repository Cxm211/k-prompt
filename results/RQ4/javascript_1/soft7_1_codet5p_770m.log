Namespace(log_name='./RQ5/javascript_1/soft7_1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft7_1_codet5p_770m', data_dir='./data/RQ5/javascript_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
Namespace(log_name='./RQ5/javascript_1/soft7_1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/soft7_1_codet5p_770m', data_dir='./data/RQ5/javascript_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' }     return  (InlineEditor.ColorSwatch._constructor());   }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' no-undef', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '}     return  (ColorSwatch._constructor());   }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 35.1038
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 12.81 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 32.3586
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 19.73 	 Previous best codebleu 12.81
  ********************
 Achieve Best bleu:19.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 9.489
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 22.63 	 Previous best codebleu 19.73
  ********************
 Achieve Best bleu:22.63
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 5.8838
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 30.38 	 Previous best codebleu 22.63
  ********************
 Achieve Best bleu:30.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 2.7902
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 32.21 	 Previous best codebleu 30.38
  ********************
 Achieve Best bleu:32.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 0.6934
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 32.17 	 Previous best codebleu 32.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 0.6799
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 32.74 	 Previous best codebleu 32.21
  ********************
 Achieve Best bleu:32.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 1.3027
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 32.1 	 Previous best codebleu 32.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.0953
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 33.59 	 Previous best codebleu 32.74
  ********************
 Achieve Best bleu:33.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 0.2286
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_1_1/validation.jsonl
  codebleu-4 = 33.7 	 Previous best codebleu 33.59
  ********************
 Achieve Best bleu:33.7
  ********************
reload model from RQ5/javascript_1/soft7_1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_1/test.jsonl
  codebleu = 28.15 
  Total = 500 
  Exact Fixed = 5 
[163, 392, 399, 419, 477]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[69, 356]
  ********************
  Total = 500 
  Exact Fixed = 5 
[163, 392, 399, 419, 477]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[69, 356]
  codebleu = 28.15 
[0.30743430994489374, 0.21000164378859376, 0.45023754093460366, 0.16606083585909626, 0.0, 0.017733069674444876, 0.2284930804770449, 0.41188247223622954, 0.4404140984393694, 0.019067239616093933, 0.2571428571428571, 0.42065878646489874, 0.3210882926276888, 0.288461852719701, 0.31540730447787274, 0.0, 0.21125003109104193, 0.31026138348249366, 0.0, 0.5153049567790734, 0.3205519550792775, 0.0012230375941953103, 0.01524949670461367, 0.5150192378669725, 0.0, 0.53049277925119, 0.3151684580802643, 0.35809327892058224, 0.3297947365831559, 0.37422942601645603, 0.1078347864287916, 0.23017391721296732, 0.07729224368432165, 0.4698162219402159, 0.5703247762183352, 0.09999999999999999, 0.7451384298635613, 0.36322493470696304, 0.35582292438020224, 0.09999999999999999, 0.7448566561657683, 0.2620130386975501, 0.6223345154446425, 0.0, 0.0, 0.0, 0.7068493203876209, 0.6423991042001886, 0.5095616509650316, 0.11895673365479492, 0.010816785171904045, 0.12, 0.3319152882000146, 0.4382542079790537, 0.3533755248262469, 0.3, 0.3, 0.40213548118497394, 0.2714385963017088, 0.6113876719227276, 0.4950121951818397, 0.24071789128867366, 0.15, 0.18657487211341686, 0.15057661251461443, 0.6001712768755635, 0.31906723961609396, 0.0, 0.6132387151836265, 0.01485529595854132, 0.6437526708015762, 0.31830806090043573, 0.6313659179388997, 0.0, 0.14680851063829786, 0.15, 0.006826503896445559, 0.016918638755598202, 0.09054591715094637, 0.3205519550792775, 0.5669789782517316, 0.0, 0.11971050017868241, 0.0, 0.04989396901868597, 0.016368728063802206, 0.49803343789861654, 0.1680684833369525, 0.44535065830823106, 0.23190202205911903, 0.06, 0.025555238068023588, 0.36483896192326826, 0.3398771706050855, 0.01349826040542033, 0.6902635248390409, 0.0, 0.5406335844577743, 0.3, 0.3014535712229998, 0.5716694424800955, 0.3761555910733458, 0.5178550177671022, 0.03759940104027143, 0.1709021304259643, 0.5721968891317245, 0.39915375517163815, 0.0, 0.760294910396554, 0.2021523538193693, 0.5023425659061871, 0.3188194347416491, 0.22619887519287302, 0.714297617187952, 0.0, 0.0, 0.0, 0.34099821306793376, 0.016155591073345806, 0.1464486562573952, 0.1541177889744038, 0.44376655075260274, 0.0, 0.18945528892333208, 0.16616215076894816, 0.6626759840224594, 0.14436225365497407, 0.5856672680560794, 0.680348676033865, 0.0, 0.11456463735979497, 0.5493832945103956, 0.7488327444711602, 0.3444626216540379, 0.3079510025390523, 0.010554145460383137, 0.09230769230769231, 0.2571428571428571, 0.6121458910881179, 0.43126528209192483, 0.0, 0.7232871590915826, 0.0, 0.12607041406667352, 0.29736364249072084, 0.5026610132484245, 0.23426055438611637, 0.12, 0.1315730987102095, 0.1717604526677455, 0.03606856837889304, 0.31581255827205035, 0.38267489701492063, 0.5137426030245367, 0.5191340091934508, 0.22897632048817324, 0.19233967971839472, 0.5445844635634268, 0.21170458518859758, 0.0857142857142857, 0.029794736583155908, 0.20062397678867971, 1.0, 0.075, 0.12, 0.2552381366060073, 0.4026968889437066, 0.20690805221197597, 0.0, 0.4765514201137756, 0.21367036685097146, 0.3, 0.24970525941193863, 0.0, 0.18, 0.17266529230445898, 0.18624690106638714, 0.0, 0.3144558830168195, 0.075, 0.19285714285714284, 0.013596626149569732, 0.282479224938717, 0.0, 0.3651124756030656, 0.6087150812881095, 0.014839462812013007, 0.3045018868888758, 0.5961413966756324, 0.5601224913106935, 0.43058076126361805, 0.0, 0.16017142534102521, 0.43717836870822446, 0.22423625743053316, 0.01661942668662242, 0.4311907477389383, 0.09536402469274179, 0.6161956499352204, 0.5279047552323763, 0.633737575456493, 0.4804289638224861, 0.06666666666666667, 0.0, 0.6489807027038881, 0.2973876642205491, 0.6242164128013139, 0.23153571111875548, 0.47708733335912445, 0.4632844671732036, 0.03126422583976078, 0.0, 0.04244669838760902, 0.0, 0.6121787547146431, 0.4170618496084528, 0.025001214399131652, 0.15, 0.0, 0.7648852879293249, 0.43944855055804594, 0.4634885561059724, 0.03152182385621825, 0.11835630425164798, 0.32182461679598845, 0.026244279557747184, 0.3682611169807786, 0.15507111509391258, 0.33231876020331524, 0.4957103430154332, 0.3016883047041743, 0.34884164987385075, 0.16211617594276972, 0.3, 0.3297947365831559, 0.2977439540681126, 0.7502328541813553, 0.6765950349550134, 0.13048068584429287, 0.3622241122035698, 0.09352593996309425, 0.3412050525686657, 0.14011833018409797, 0.0, 0.34895017176440035, 0.0, 0.10731921108844397, 0.34025628111289863, 0.6207404894895134, 0.8141572281239757, 0.40087093505002125, 0.09775460105127004, 0.20866354767478065, 0.2820730521241227, 0.08541343000500026, 0.4338491014511952, 0.2661180216547285, 0.26686357379622305, 0.0, 0.38322493470696306, 0.0476734197185939, 0.025760393114789816, 0.7382312160616913, 0.06375830811976467, 0.01476087873831729, 0.6762833633423001, 0.212306674329528, 0.4161281884847685, 0.22267489701492066, 0.2002760018923899, 0.6643842626707708, 0.6711990235727783, 0.6391498942229566, 0.3, 0.7750356508147567, 0.4339213351685698, 0.011546265654748703, 0.33036017439379955, 0.3854768639683604, 0.2958166302124694, 0.03352607746512763, 0.0, 0.46272438732934995, 0.0, 0.0922382617563554, 0.07822786691126289, 0.27621009675895103, 0.5242943096485244, 0.0, 0.19718977111187735, 0.09999999999999999, 0.5771357802561726, 0.39637246808542226, 0.6642647227570666, 0.0, 0.008127149387347866, 0.12, 0.5242687349350936, 0.0, 0.5738381947944386, 0.0, 0.512283842723568, 0.2457354197214469, 0.6000751040940313, 0.0, 0.22883156338048122, 0.3266545291585887, 0.0, 0.5194637253863024, 0.4637488527944089, 0.3197639404629975, 0.023975218851315705, 0.4045805699269634, 0.0, 0.0, 0.44454356737719164, 0.7846480691073847, 0.5142512715522436, 0.047335073018980586, 0.1458563535911602, 0.0, 0.34222090249057624, 0.2571428571428571, 0.7576795659964204, 0.4965580647670747, 0.0, 0.6388047518550579, 0.2867234270698205, 0.0006197284944694396, 0.0, 0.30545008728923306, 0.0, 0.36, 0.4408391421014253, 0.017733069674444876, 0.0, 0.03606856837889304, 0.3612943448784868, 0.09683836674915444, 0.0, 0.2498970016021973, 0.022657627559156193, 0.7538840073722015, 0.0, 0.41756427241598104, 0.0, 0.3, 0.09921254611970604, 0.55638849227698, 0.6463982054703639, 0.37548755551356516, 0.40594274358705507, 0.1714285714285714, 0.593590844328102, 0.0, 0.6775244294693408, 0.0, 0.5178434585130358, 0.3007144222413223, 0.36921858730220125, 0.010582675732934282, 0.010293507377813755, 0.11529485564124908, 0.808221529667446, 0.1536671076179856, 0.20325991295455373, 0.5849464258957323, 0.06426392418941672, 0.13021191090275874, 0.5098686912125754, 0.1869969629668011, 0.2244912096707368, 0.0976964273059128, 0.3242577008663058, 0.01567246452625196, 0.34650852962940437, 0.4041201952645965, 0.02177207628470134, 0.24051096962423907, 0.15, 0.24454353381340382, 0.8288120051119818, 0.20910746983401654, 0.447298452667235, 0.109026865899604, 0.5578607431385983, 0.2560260050021163, 0.3508081453961951, 0.04488756919664031, 0.0, 0.2088017714661632, 1.0, 0.34995276001995573, 0.2575361331096032, 0.0, 0.0, 0.05208475380480242, 0.580040285921767, 1.0, 0.0, 0.0, 0.20975138154279746, 0.23388196305947806, 0.12283693344527488, 0.09690055115200145, 0.6222383916353247, 0.34981995376794145, 0.3425450884101482, 0.5784073255024652, 0.30872596763440474, 0.6749865294460549, 0.015434003542254326, 0.026198875192873054, 0.6259496800747891, 0.47875874704124055, 0.5120343616573666, 0.0, 0.19310553878591902, 0.9891483218006352, 0.0, 0.32661262436276534, 0.0, 0.0, 0.0, 0.41926050398668113, 0.6638599176393867, 0.0, 0.30302157796687523, 0.5237030996295007, 0.38759445958817595, 0.222319907982865, 0.6041851450349092, 0.42528686189915166, 0.4973931270915492, 0.8228963827959196, 0.0, 0.4886857976019322, 0.3822475966547003, 0.022160462978181618, 0.17775375390483888, 0.24144959378741382, 0.3, 0.08693636075549857, 0.35972776642748383, 0.12328207287840023, 0.5890011576185719, 0.0, 0.0, 0.4605568025736869, 0.11908185070981978, 0.0, 0.5979289023965351, 0.5857343158984987, 0.3408188745659053, 0.075, 0.19280858781167154, 0.5081448004786508, 0.0, 0.4916576694015367, 0.0, 0.21533888628138323, 0.5888934767779554, 0.01830806090043577, 0.015437990270907974, 0.42591449496046335, 0.23335538277936863, 0.2478737274849781, 0.025001214399131652, 0.8341247388270204, 0.24922543887344006, 0.43201024220111967, 0.2531322115099076, 0.16877656124603424, 0.6841007521231434, 0.4580350417898452, 0.075, 1.0, 0.1223157836011612, 0.4527117073931767, 0.3802860793491076, 0.4702562381374042, 0.20436692013296331, 0.36624883020548643, 0.39024518379467305, 0.2409185711784925, 0.0, 0.23544922788578437, 0.021676967357347608, 0.3, 0.19578576165764644, 0.414436274930823, 0.024514163027911554, 0.05084156611248943, 0.0, 0.37454545454545457, 0.0, 0.15, 0.20981903635285157, 0.22639874222714212, 0.0]
Finish training and take 53m
