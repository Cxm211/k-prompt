Namespace(log_name='./RQ5/javascript_1/f3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_1/f3_codet5p_770m', data_dir='./data/RQ5/javascript_1_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0157
  global_step = 1
  train_loss = 5.7634
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0157
  ********************
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 12.04 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.0157
  global_step = 1
  train_loss = 3.7593
  ********************
Previous best ppl:1.0157
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 12.04 	 Previous best codebleu 12.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0157
  global_step = 1
  train_loss = 4.6879
  ********************
Previous best ppl:1.0157
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 12.04 	 Previous best codebleu 12.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0157
  global_step = 1
  train_loss = 3.652
  ********************
Previous best ppl:1.0157
BLEU file: ./data/RQ5/javascript_1_3/validation.jsonl
  codebleu-4 = 12.04 	 Previous best codebleu 12.04
  ********************
reload model from RQ5/javascript_1/f3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_1_3/test.jsonl
  codebleu = 14.12 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 14.12 
[0.17419354838709677, 0.000869259408931731, 0.23531832664919433, 0.059018081940947584, 0.08345982397674775, 0.03143465525212748, 0.15110251695697172, 0.24143422629954425, 0.1131257278939988, 0.04769346142512824, 0.18968350274095308, 0.47228756414146944, 0.0, 0.5988566865003426, 0.2752835639821334, 0.026943793911007024, 0.2515390694295902, 0.2505538382116617, 0.0, 0.13655172413793104, 0.06071428571428571, 0.0, 0.0002802705940310813, 0.0, 0.0945272424288741, 0.0, 0.013743904474597137, 0.0, 0.0, 0.32175555571722514, 0.0381307252542209, 0.05241780046130159, 0.26754570360051794, 0.010841741977347002, 0.5982043390941232, 0.011538461538461539, 0.058315412857897354, 0.19517107329049246, 0.0, 0.5950270247920245, 0.5880983897931813, 0.17381483141059642, 0.27916793007717544, 0.1808469848083733, 0.022276853963602565, 0.23586841950172832, 0.2363193027870422, 0.0, 0.14598226921136728, 0.119620038894526, 0.04979806041215509, 0.27550469058594856, 0.013788459973280143, 0.17218312913222686, 0.023225806451612905, 0.17326203208556148, 0.021052631578947368, 0.035606741301283586, 0.2281879154530409, 0.14623695225861968, 0.1660821758885206, 0.14482617521238944, 0.14306078158901867, 0.11175339626175157, 0.27121303444195904, 0.24345235302661955, 0.08235098344366469, 0.0, 0.04285714285714285, 0.30374236155356654, 0.0, 0.057630006879553365, 0.2793103448275862, 0.19999999999999998, 0.5925669042178512, 0.04077669902912621, 0.00021104323332092574, 0.3740897920111252, 0.08210396525212119, 0.05925925925925926, 0.4056253938582504, 0.17665452501972473, 0.11668331430125017, 0.43344571407989535, 0.029539228958889288, 0.150359569962666, 0.5011384235593304, 0.08558838563771996, 0.0, 0.011404474543819078, 0.04310863031343924, 0.0, 0.0, 0.4443554299873676, 0.5824237352008099, 0.08887047158003276, 0.07058559907029913, 0.007697151916860044, 0.2788932731076479, 0.1277183867180186, 0.06770989145797929, 0.09566035571277498, 0.0006882290431401457, 0.03431459701689811, 0.08566530699989099, 0.23178109727771, 0.16397515527950307, 0.1365269533568196, 0.14029997171169123, 0.013534061226923442, 0.2969418036519161, 0.30272637652789425, 0.13284876941871415, 0.5949273683143212, 0.16362176782626447, 0.09931245225362872, 0.002960938342930922, 0.075, 0.08127980295676776, 0.12303433456958704, 0.33859858713658664, 0.09238018839598947, 0.048508007158106894, 0.04285714285714285, 0.16280953397578396, 0.05338717531705556, 0.00015561794847387438, 0.0, 0.0002772673347058908, 0.07076763493963092, 0.0, 0.26123651345519056, 0.04285714285714285, 0.35094658052938454, 0.0, 0.00166561451935193, 0.11666666666666667, 0.24417953456223518, 0.2413230177822317, 0.40705428577207897, 0.10031734945166793, 0.07045309758458879, 0.1623951418675351, 0.15109882926050225, 0.0, 0.0, 0.5544105758936645, 0.10557791834387578, 0.05141709313930286, 0.06507116011177887, 0.3433155080213903, 0.0002001125907533803, 0.04285714285714285, 0.04285714285714285, 0.4675330357846878, 0.3002570775931024, 0.05084649499333197, 0.132441080865395, 0.23194910810933167, 0.07178235032840419, 0.14466492325208455, 0.1308191755520723, 0.11049769268394974, 0.10951695463443631, 0.006060606060606061, 0.1669856699439995, 0.18640930164708192, 0.2074871998377286, 0.0, 0.2466491783958218, 0.004555862997001096, 0.121875, 0.24204948978064433, 0.18635304850758372, 0.4293278064375352, 0.20801726206393706, 0.1050228250387708, 0.0753660820626865, 0.09774212715389186, 0.12343945982457306, 0.023076923076923078, 0.00042110430988806367, 0.19690228368963333, 0.39094318430291364, 0.18622316890470417, 0.264396514333872, 0.021597121602749375, 0.052081158027712036, 0.13200444022017022, 0.013761467889908258, 0.09325750474557945, 0.18474469561264678, 0.19, 0.4145814057264603, 0.2863942863086679, 0.0, 0.011646563301429338, 0.06667014287204087, 0.06692227574739927, 0.06863629582939093, 0.24011593732691383, 0.0379980802542729, 0.11735405229674951, 0.1459775580068467, 0.20871810009377154, 0.08355497229927153, 0.023225806451612905, 0.0020382540257626964, 0.46237536271796653, 0.0, 0.18040283433667617, 0.17787303715513436, 0.06364225417822336, 0.1109833315489229, 0.3001402995428433, 0.5880112443839685, 0.20673052268229186, 0.33689839572192515, 0.10017251293847038, 0.1550287549676081, 0.12649951642249443, 0.30578488575020324, 0.03783301402313949, 0.034890599883489526, 0.18939311477137696, 0.12086857797833384, 0.19699718434510716, 0.051523109243697476, 0.003529411764705882, 0.0560759310543093, 0.10993666129912853, 0.06554743742225837, 0.04285714285714285, 0.06279069767441861, 0.35857988165680477, 0.0772682875809522, 0.0, 0.14609662171454274, 0.07142861331207336, 0.14931472065979476, 0.2927041114040754, 0.08629490712706922, 0.0, 0.0002462475965736148, 0.09553225843628335, 0.1612427747631145, 0.16430459408432976, 0.41826589822680227, 0.19148126463700232, 0.2864784956686989, 0.1646326291227876, 0.0, 0.04845145973842423, 0.10086518687566541, 0.0612653997520687, 0.010863880188646445, 0.03432076022271853, 0.032407153752575135, 0.43997117062823204, 0.015039304226837288, 0.3, 0.29718557778526433, 0.21445168308141238, 0.07804118745140812, 0.2094946203898729, 0.0027027027027027024, 0.00032680289919742855, 0.20818566815747772, 0.3018744523824208, 0.04285714285714285, 0.03529411764705882, 0.17772753726343646, 0.20451519371115137, 0.06415094339622641, 0.0, 0.11761397949857358, 0.5887105107701678, 0.003914836198452628, 0.3094800676642932, 0.18313120885169043, 0.0044494382244639405, 0.1336754499303652, 0.5873015873015873, 0.006081081081081081, 0.0, 0.16875619615939888, 0.04285714285714285, 0.0, 0.12985058194192547, 0.30599547511312214, 0.0, 0.5520752192698588, 0.140965825677096, 0.00818181818181818, 0.003985904720346576, 0.15346560934574602, 0.0, 0.14056652044253085, 0.15354229497974548, 0.2824262111393489, 0.26351832095059685, 0.0, 0.019095251841278815, 0.32750778074630116, 0.0005045398847921957, 0.21910125272544004, 0.19744936579809028, 0.03868181091463042, 0.07049008795785153, 0.08105134695477, 0.18285914016755972, 0.16070633890350874, 0.4158789331942421, 0.13806525840914294, 0.0, 0.1792692136187554, 0.20964292925866815, 0.10370482613027934, 0.06495022446408602, 0.2211764705882353, 0.34795945546627105, 0.1126771152447602, 0.0, 0.22061090380292692, 0.17091086703885644, 0.3168835180162405, 0.24383244486554548, 0.42418860671962977, 0.11092105987949163, 0.10327618173575764, 0.23934426229508196, 0.00023950569867991155, 0.5967214589508401, 0.0, 0.12173471117678675, 0.003896103896103896, 0.19503478501864038, 0.3180593442821573, 0.1065888022241202, 0.0003713974244248666, 0.0, 0.09710446857203277, 0.07320995944127032, 0.004767322472078818, 0.14133989754821866, 0.22930419472438324, 0.0, 0.1377976250692072, 0.10017251293847038, 0.18995944453782468, 0.159, 0.1502373018248877, 0.06602136181575434, 0.0039888757381542515, 0.06294135763153356, 0.009361517032345754, 0.0, 0.16161755871336778, 0.0, 0.32189914637219685, 0.016366797029855055, 0.1440401926788704, 0.09218657159833629, 0.0, 0.0, 0.23985271067894742, 0.12513337723369183, 0.11298642280214008, 0.11906116642958747, 0.09414818310523833, 0.07313131313131313, 0.14781043444955183, 0.0, 0.05620323249720435, 0.11333542833352375, 0.11137426091429825, 0.33830526044569414, 0.0, 0.0, 0.11471094627886746, 0.0, 0.04285714285714285, 0.0, 0.04285714285714285, 0.18348973607038122, 0.0, 0.5753987818053847, 0.0, 0.03599861504120888, 0.09642857142857142, 0.0, 0.005405405405405405, 0.2043455736189358, 0.0, 0.11310247878880861, 0.15625, 0.03268054207660595, 0.0, 0.0008134819994388358, 0.0825126623865569, 0.00024219784672214076, 0.213810236893761, 0.024360417814326508, 0.06245214636658998, 0.26985988435669717, 0.0, 0.18505964934456245, 0.06375, 0.010933241410467809, 0.17252611902403664, 0.4328696770954552, 0.0, 0.0, 0.15050897523118825, 0.0, 0.01139240506329114, 0.0867078306164616, 0.16129032258064516, 0.059900934374023426, 0.316845833799588, 0.08898817951053667, 0.0, 0.05158804975542083, 0.07248826291079812, 0.5981874969480654, 0.06453825760159164, 0.30203511372755315, 0.12889932691494152, 0.4803294329997505, 0.1408664540205553, 0.14821679116750555, 0.2146981460150726, 0.4222568734571158, 0.17647058823529413, 0.05691638197789536, 0.16746126033176037, 0.07479709039962679, 0.1419022856933126, 0.0, 0.2144401458071805, 0.24034592550919753, 0.048961756960929206, 0.13755003166804589, 0.16592682067827685, 0.1713578207715427, 0.580492486373541, 0.2835650089980833, 0.145769303086841, 0.2253685818472838, 0.11674655903247219, 0.04302196206581876, 0.0, 0.20892018668671714, 0.002678571428571428, 0.04203821656050955, 0.10557791834387578, 0.005910253429435014, 0.15664816008943103, 0.0, 0.2842883243314728, 0.050367336449626515, 0.08531947897774218, 0.014048930050138807, 0.058165530496297316, 0.0033212043403435653, 0.16559210526315787, 0.08753067819743415, 0.024895563904695286, 0.08346703157130356, 0.0025755930704985408, 0.253728022378497, 0.1202175557426667, 0.1605294273570493, 0.00020037427061667188, 0.1819769162474105, 0.025655417080420233, 0.00791483687135004, 0.00017032351956087805, 0.17620959076525222, 0.23845318571315466, 0.4950622714736742, 0.2511961257043217, 0.5628144707241545, 0.0927663734115347, 0.1551419007991198, 0.10040423327186758, 0.21274618320214422, 0.0, 0.1840187967096329, 0.20657428656151414, 0.0, 0.0, 0.09508131414306002, 0.209660041622485, 0.3563636363636364, 0.03926255044095242, 0.5981875793698279, 0.30721408119235455, 0.2036468674426391, 0.2665524729649817]
Finish training and take 1h0m
