Namespace(log_name='./RQ5/sstubs_1_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/sstubs_1_3/codet5p_770m', data_dir='./data/RQ5/sstubs_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '* @see <a href="http:       */      public ResponseList<Status> getMentions() throws TwitterException {         return Status.constructStatuses(get(getBaseURL() + "statuses/mentions.json",                  null, true));      }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '* @see <a href="http:       */      public ResponseList<Status> getMentions() throws TwitterException {         return Status.createStatuseList(get(getBaseURL() + "statuses/mentions.json",                  null, true));      }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 4.0186263752048405e+282
  global_step = 2
  train_loss = 55.5359
  ********************
Previous best ppl:inf
Achieve Best ppl:4.0186263752048405e+282
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 19.48 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 6.119444689918921e+280
  global_step = 3
  train_loss = 72.0569
  ********************
Previous best ppl:4.0186263752048405e+282
Achieve Best ppl:6.119444689918921e+280
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 25.87 	 Previous best codebleu 19.48
  ********************
 Achieve Best bleu:25.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 3.587228098713259e+277
  global_step = 4
  train_loss = 20.5575
  ********************
Previous best ppl:6.119444689918921e+280
Achieve Best ppl:3.587228098713259e+277
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 30.51 	 Previous best codebleu 25.87
  ********************
 Achieve Best bleu:30.51
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 6.962005864605939e+277
  global_step = 5
  train_loss = 11.2803
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 31.09 	 Previous best codebleu 30.51
  ********************
 Achieve Best bleu:31.09
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.1962253320718205e+282
  global_step = 6
  train_loss = 6.2256
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 37.47 	 Previous best codebleu 31.09
  ********************
 Achieve Best bleu:37.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 4.371684124438164e+287
  global_step = 7
  train_loss = 2.5205
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 41.56 	 Previous best codebleu 37.47
  ********************
 Achieve Best bleu:41.56
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 4.6197889522408815e+291
  global_step = 8
  train_loss = 2.0478
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 41.59 	 Previous best codebleu 41.56
  ********************
 Achieve Best bleu:41.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 4.9451656867059075e+294
  global_step = 9
  train_loss = 1.0402
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 42.72 	 Previous best codebleu 41.59
  ********************
 Achieve Best bleu:42.72
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 2.4020067995747998e+296
  global_step = 10
  train_loss = 0.9499
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 42.87 	 Previous best codebleu 42.72
  ********************
 Achieve Best bleu:42.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 8.772043172813185e+296
  global_step = 11
  train_loss = 2.5809
  ********************
Previous best ppl:3.587228098713259e+277
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 44.83 	 Previous best codebleu 42.87
  ********************
 Achieve Best bleu:44.83
  ********************
reload model from RQ5/sstubs_1_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_3/test.jsonl
  codebleu = 41.02 
  Total = 500 
  Exact Fixed = 4 
[117, 244, 449, 497]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 4 
[117, 244, 449, 497]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 41.02 
[0.8680694265767652, 0.9444789331909464, 0.8805861838544717, 0.8542596709868013, 0.2280683199611704, 0.0, 0.3532741940671328, 0.20478681307637792, 0.15596821196255345, 0.5738383526395311, 0.25142859485406344, 0.26165795504346523, 0.3431620359200348, 0.6333015868665175, 0.6114278249038432, 0.4069671371303185, 0.6402246530393667, 0.1050934649606318, 0.24389135310860968, 0.20478681307637792, 0.31403883867933374, 0.09999999999999999, 0.3, 0.3304732228982677, 0.30903986397026484, 0.3065908241707331, 0.07694811584255411, 0.3065908241707331, 0.3425928916380074, 0.8800909780386468, 0.0, 0.2280172454092323, 0.09999999999999999, 0.22747100407401857, 0.07787343508832104, 0.2519348293443251, 0.0025676917166032607, 0.4502285737503047, 0.4502285737503047, 0.3003918938927313, 0.05090311986863711, 0.11756978653530377, 0.603999928776363, 0.3783976315706759, 0.3726458146241048, 0.30970664040112905, 0.0, 0.3498316870940007, 0.0035202563515678645, 0.3065908241707331, 0.3780231479398861, 0.3387462434710216, 0.4502285737503047, 0.18819265943452473, 0.8199384087829295, 0.9210904702787386, 0.08448902357979535, 0.15275728777949046, 0.18819265943452473, 0.737237894846401, 0.914769900117343, 0.5605030161400112, 0.09999999999999999, 0.0, 0.4757281628270304, 0.9545187036711082, 0.0, 0.0, 0.3746956397034723, 0.35052048105945494, 0.0028087627219396593, 0.8748733743967635, 0.3042012729022377, 0.6758201572595013, 0.4643823313091531, 0.3065908241707331, 0.3308103709069264, 0.3269562794349121, 0.0, 0.4502285737503047, 0.33983783881445984, 0.8493581773872141, 0.5724136752259128, 0.23095786956007458, 0.4740536144654707, 0.4283817385609757, 0.8736632501454265, 0.2519348293443251, 0.2519348293443251, 0.3425928916380074, 0.3162068218501836, 0.8499565581538413, 0.8076571944458698, 0.3177272727272727, 0.7744540852360562, 0.1120766403617833, 0.7073139730147537, 0.737237894846401, 0.8291686685028647, 0.5738383526395311, 0.6042319975269723, 0.8417670412930416, 0.0, 0.12300476825473933, 0.09999999999999999, 0.32496214308346116, 0.35559170775817595, 0.8967400300083177, 0.9338562346624684, 0.3065908241707331, 0.2834512961771026, 0.4502285737503047, 0.2633095568857501, 0.012553640857816352, 0.0, 0.3042012729022377, 1.0, 0.4782106617327956, 0.9422672588258918, 0.20478681307637792, 0.07787343508832104, 0.0, 0.18944603112728348, 0.4430605804385013, 0.0, 0.3082096833495514, 0.0, 0.8188383904432455, 0.3685341728251086, 0.25543916494947055, 0.3042012729022377, 0.7370588327908355, 0.39278955706265506, 0.0053254437869822485, 0.3065907841220216, 0.7473755522701948, 0.03110099976557007, 0.28111918198772134, 0.3080686524780538, 0.8951610826398968, 0.9100894982546051, 0.4502285737503047, 0.8561340786437819, 0.0, 0.6348398904455226, 0.9489198753148058, 0.4502285737503047, 0.25142859485406344, 0.5518897894393091, 0.075, 0.8434383869084117, 0.3065908241707331, 0.3065908241707331, 0.8499565581538413, 0.3065908241707331, 0.3065908241707331, 0.3065908241707331, 0.27977117467702656, 0.6027466911477039, 0.4502285737503047, 0.34177044285679853, 0.4502285737503047, 0.854980352330124, 0.11859931596531861, 0.26636452072400113, 0.3065908241707331, 0.8323846658888985, 0.8800909780386468, 0.31408899902816534, 0.7755456605996771, 0.2098303411473198, 0.3065908241707331, 0.2280683199611704, 0.3339129935963955, 0.4469223452755743, 0.3065908241707331, 0.618370006091282, 0.2926988532144709, 0.8667343878768172, 0.18819265943452473, 0.0, 0.18461538461538463, 0.00246753684588946, 0.08289431607821632, 0.09999999999999999, 0.5665756328866424, 0.5665756328866424, 0.32423555617940925, 0.7028350894361455, 0.6789308978755005, 0.8247755941320389, 0.20478681307637792, 0.25543916494947055, 0.8895951078706568, 0.7928742897926486, 0.0, 0.9077668318893848, 0.0, 0.0, 0.20478681307637792, 0.8579853689231081, 0.14121070535236663, 0.7203179233030433, 0.2535458513723986, 0.321428032986284, 0.07353503272424458, 0.26113840918940007, 0.28482429749043864, 0.09999999999999999, 0.3065908241707331, 0.5193293392530346, 0.7368107852764039, 0.48363996500568973, 0.27977117467702656, 0.9655267507815484, 0.36477464826881784, 0.737237894846401, 0.2791483765885513, 0.16668659256704474, 0.26200542367960955, 0.8601446358962308, 0.6156084233246197, 0.4502285737503047, 0.32423555617940925, 0.3425928916380074, 0.3018306051379399, 0.4643823313091531, 0.6866682694251139, 0.9812222067898357, 0.3821024306889538, 0.9812222067898357, 0.48363996500568973, 0.737237894846401, 0.7198136379345541, 0.25333090764015026, 0.30697565800017695, 0.0, 0.9176898202751427, 0.09999999999999999, 0.4203353617875786, 0.3042012729022377, 0.15561736468453066, 0.2750633503903068, 1.0, 0.0, 0.35052048105945494, 0.684673674666875, 0.5356214197413983, 0.0, 0.9583676774082095, 0.18048781692638513, 0.6195874596005413, 0.22109001613948767, 0.0, 0.07499870920660848, 0.0, 0.30962602466863887, 0.9100894982546051, 0.3065907841220216, 0.2873727863187473, 0.3188726047893403, 0.14464843649871714, 0.5874139141588355, 0.9309124444004095, 0.4502285737503047, 0.36444245116764035, 0.740459763198945, 0.29801355983232525, 0.2465922174052709, 0.0, 0.3256165132667337, 0.740459763198945, 0.09999999999999999, 0.33998600236828735, 0.3173880774585838, 0.26502681291794156, 0.8431259056719794, 0.3115726025491742, 0.11593639498969233, 0.42211613384074176, 0.3065907841220216, 0.3065908241707331, 0.17943165476179027, 0.8825179208478791, 0.3065908241707331, 0.6389509693923223, 0.8839120495878086, 0.5988930138770876, 0.0, 0.2983506828155873, 0.6027466911477039, 0.9878208831507505, 0.8581776804980471, 0.0, 0.3080686524780538, 0.0, 0.9245037448286011, 0.8951610826398968, 0.8142751664827657, 0.0, 0.22109001613948767, 0.2917953464708157, 0.24389135310860968, 0.4430605804385013, 0.42735043943774065, 0.0, 0.36477464826881784, 0.8944053780614853, 0.6200207915028593, 0.007100591715976331, 0.7261959721262421, 0.09999999999999999, 0.4502285737503047, 0.15326970561901615, 0.09999999999999999, 0.8951610826398968, 0.32005655020488, 0.4502285737503047, 0.740459763198945, 0.9120249200237338, 0.19043467392192354, 0.8884536857519452, 0.904674617514176, 0.8988572379895367, 0.3065907841220216, 0.35264844547997204, 0.41938495852617996, 0.3096660937246946, 0.4643823313091531, 0.3068086580323969, 0.0, 0.32406005784827524, 0.2852726177528931, 0.30697565800017695, 0.8223979815438567, 0.6462297532623966, 0.3065908241707331, 0.4166527427796821, 0.6485506609368317, 0.0, 0.19043467392192354, 0.22109001613948767, 0.3065908241707331, 0.14423474126346514, 0.2994546913438551, 0.0, 0.0, 0.8933342731653321, 0.3046402169883104, 0.6402246530393667, 0.0011464020482973832, 0.7570226067909419, 0.7427028689549173, 0.9036631069313947, 0.055929871492915006, 0.07201491153619795, 0.18819265943452473, 0.5287105993036408, 0.0, 0.3243589874393973, 0.8357727343475672, 0.22109001613948767, 0.8686637841686108, 0.4643823313091531, 0.3042012729022377, 0.025403949547211466, 0.9178555811265612, 0.1531776188155235, 0.3396069864053338, 0.32423555617940925, 0.6814911742694971, 0.8002558693517027, 0.05991283027635263, 0.9134269603213818, 0.9358330212941763, 0.07012090342704061, 0.4502285737503047, 0.7028350894361455, 0.0, 0.9197527805210692, 0.7200552189599008, 0.8805861838544717, 0.0025396135631503293, 0.25543916494947055, 0.3127339204207301, 0.3068086580323969, 0.4502285737503047, 0.3065908241707331, 0.8945187036711082, 0.48941032276635743, 0.2920631021369105, 0.48363996500568973, 0.4502285737503047, 0.3717305654672707, 0.09999999999999999, 0.0, 0.8309111894015125, 0.3065908241707331, 0.5868347090447624, 0.30404614212985187, 0.09999999999999999, 0.9021932715994789, 0.28825619194092195, 0.28111918198772134, 0.3065907841220216, 0.0, 0.12140444441994167, 0.9120249200237338, 0.19999999999999998, 0.3065908241707331, 0.12, 0.3065907841220216, 0.8800909780386468, 0.15643006545699747, 0.8757862586884326, 0.5857199632839989, 0.2270883544341227, 0.30995682185018364, 0.19999999999999998, 0.3425928916380074, 0.5287105993036408, 0.3, 0.1589743349146922, 0.08061506798475729, 0.4502285737503047, 0.5591789359122503, 0.89275106261774, 0.8800909780386468, 0.19999999999999998, 0.33931394611229354, 0.0, 0.28111918198772134, 0.3396069864053338, 0.31329171823942004, 0.6399013847840145, 0.9036631069313947, 0.3994808979356458, 0.0, 0.3396069864053338, 0.3065908241707331, 0.3111426027752081, 0.2519348293443251, 0.816180603357824, 0.3065908241707331, 0.0, 0.7028350894361455, 0.9141413403930432, 0.6629996292208165, 0.9532616077763247, 0.19043467392192354, 0.8357727343475672, 0.3065907841220216, 0.3245623704150716, 0.3228080593812368, 0.3090235364489762, 0.6967069174250534, 0.0035202563515678645, 0.7979750254435045, 0.0, 0.3065908241707331, 0.0, 0.4502285737503047, 0.22747100407401857, 0.675841012569804, 0.3065908241707331, 0.4502285737503047, 0.9120249200237338, 0.3065908241707331, 0.8721616356449857, 0.6769643632322668, 0.24507106276248442, 0.7001951047216559, 0.8039259375320511, 0.15225178047758692, 0.8757862586884326, 0.15225178047758692, 0.0, 0.2609752162203199, 0.01028595937826659, 0.9272945586059962, 0.48363996500568973, 0.9005026293768383, 0.3409090909090909, 0.11830985915492956, 0.17687354317895287, 0.23740941490449086, 0.15194430131002226, 0.21152798083082328, 0.0, 0.1321982706565745, 0.33931394611229354, 0.0472529299021513, 0.09999999999999999, 0.9090101051758518, 0.8640162312181812, 0.3065908241707331, 1.0, 0.0, 0.09999999999999999, 0.20068052636125644]
Finish training and take 1h36m
