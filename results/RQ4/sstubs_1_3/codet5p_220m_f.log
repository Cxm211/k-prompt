Namespace(log_name='./RQ5/sstubs_1_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_1_3/codet5p_220m_f', data_dir='./data/RQ5/sstubs_1_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00339
  global_step = 1
  train_loss = 1.6941
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00339
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 6.44 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:6.44
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00339
  global_step = 1
  train_loss = 1.401
  ********************
Previous best ppl:1.00339
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 6.44 	 Previous best codebleu 6.44
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00339
  global_step = 1
  train_loss = 1.5521
  ********************
Previous best ppl:1.00339
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 6.44 	 Previous best codebleu 6.44
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00339
  global_step = 1
  train_loss = 2.1034
  ********************
Previous best ppl:1.00339
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 6.44 	 Previous best codebleu 6.44
  ********************
reload model from RQ5/sstubs_1_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_3/test.jsonl
  codebleu = 6.68 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 6.68 
[0.04814814814814814, 0.007523684798783523, 0.0005928039101636814, 0.031395348837209305, 0.1623581271356293, 0.0, 0.02957746478873239, 0.029268292682926828, 0.3217707491891386, 0.02278481012658228, 0.22557650573686017, 0.02454353363581084, 0.10260509268983112, 0.030769230769230767, 0.3313669937984834, 0.05755775877758856, 0.02669411216520866, 0.0, 0.04090824158255012, 0.029268292682926828, 0.0475609756097561, 0.05384615384615384, 0.00020276794409418948, 0.053164556962025315, 0.029268292682926828, 0.029268292682926828, 0.09260463439529389, 0.029268292682926828, 0.30400008357477093, 0.49117267506052176, 0.02692307692307692, 0.2318033616122117, 0.007692307692307692, 0.42821810620430634, 0.2534308804687, 0.31027258667291957, 0.16959689847821202, 0.030769230769230767, 0.029268292682926828, 0.29998126103652833, 0.015384615384615384, 0.05217391304347826, 0.04827586206896552, 0.05420181675072045, 0.054878048780487805, 0.023032448222124503, 0.0423076923076923, 0.33514517059327, 0.21802672292305583, 0.029268292682926828, 0.06296296296296296, 0.04177215189873417, 0.029268292682926828, 0.029268292682926828, 0.01923076923076923, 0.008028739281982047, 0.29421765938334654, 0.30826359853997354, 0.029268292682926828, 0.04275686430330017, 0.03068181818181818, 0.04285714285714285, 0.05384615384615384, 0.2834945973317957, 0.09563011549301503, 0.027464652654898978, 0.017857142857142856, 0.09385763622185966, 0.2233297585861607, 0.053901483039974586, 0.0, 0.022469387178553648, 0.029268292682926828, 0.0, 0.029268292682926828, 0.02553191489361702, 0.044916538273583745, 0.05804686724049914, 0.020689655172413793, 0.029268292682926828, 0.03506493506493506, 0.30629730257378907, 0.0, 0.1053078629568936, 0.03814965652757861, 0.029268292682926828, 0.027164403323315718, 0.31027258667291957, 0.31027258667291957, 0.30400008357477093, 0.046788970266111195, 0.0004735667696382707, 0.030769230769230767, 0.13214285714285715, 0.017857142857142856, 0.013036344226062105, 0.000693744597268347, 0.037928865810863306, 0.017045454545454544, 0.02278481012658228, 0.06477272727272727, 0.0004990567491135724, 0.3285321824598165, 0.05669326398673455, 0.0, 0.029268292682926828, 0.059459459459459456, 0.0007454166516354986, 0.0737634575951237, 0.029268292682926828, 0.026503080394519648, 0.029268292682926828, 0.027777777777777776, 0.014814814814814814, 0.2834945973317957, 0.029268292682926828, 0.00022151826558816694, 0.026109443095010468, 0.00950878396462554, 0.024999999999999998, 0.2534308804687, 0.01081081081081081, 0.02608695652173913, 0.49114844535802243, 0.01, 0.029268292682926828, 0.010344827586206896, 0.017045454545454544, 0.07692307692307691, 0.04169142104541396, 0.029268292682926828, 0.056106531325470194, 0.025609756097560978, 0.010588235294117647, 0.029268292682926828, 0.015584415584415584, 0.0341991341991342, 0.030008946881712194, 0.029629629629629627, 0.0007454166516354986, 0.018885332361693055, 0.029268292682926828, 0.36699270690173047, 0.0, 0.006818181818181818, 0.006058822185834247, 0.029268292682926828, 0.22557650573686017, 0.025292664144017835, 0.007594936708860759, 0.017045454545454544, 0.029268292682926828, 0.02553191489361702, 0.0004735667696382707, 0.029268292682926828, 0.029268292682926828, 0.029268292682926828, 0.045864604887731926, 0.03494892479073041, 0.029268292682926828, 0.3176944808118819, 0.029268292682926828, 0.0, 0.024789488709326526, 0.2700949973686811, 0.029268292682926828, 0.017045454545454544, 0.0005642876553915427, 0.2537240922607609, 0.051219512195121955, 0.007594936708860759, 0.029268292682926828, 0.1623581271356293, 0.046153846153846156, 0.02263786628019011, 0.029268292682926828, 0.007986800677472232, 0.026373626373626374, 0.17156325045420134, 0.029268292682926828, 0.21400442664005223, 0.2448566758089149, 0.0688964080282791, 0.0, 0.02219624587345279, 0.051626820631371924, 0.051626820631371924, 0.029268292682926828, 0.043623521068170495, 0.03534998720007273, 0.039285714285714285, 0.029268292682926828, 0.04169142104541396, 0.02443097375870013, 0.02222222222222222, 0.2906206714795637, 0.03487083503627407, 0.26873754872181765, 0.2834945973317957, 0.024999999999999998, 0.02692307692307692, 0.05202980449261052, 0.08414634146341464, 0.03666666666666667, 0.008594114001468643, 0.007792958429651773, 0.031903439990932765, 0.01923076923076923, 0.046153846153846156, 0.029268292682926828, 0.23604090688242418, 0.04588235294117647, 0.029268292682926828, 0.045864604887731926, 0.03693213997827833, 0.01923076923076923, 0.04275686430330017, 0.07028319399519614, 0.029268292682926828, 0.011009174311926606, 0.03143013023776703, 0.014634146341463414, 0.029268292682926828, 0.029268292682926828, 0.30400008357477093, 0.007142857142857142, 0.029268292682926828, 0.045, 0.013834908690357585, 0.016213771099795004, 0.013834908690357585, 0.029268292682926828, 0.04275686430330017, 0.027906976744186046, 0.014233724572109151, 0.029268292682926828, 0.018292682926829267, 0.04468098309226761, 0.06799999999999999, 0.03854482266507449, 0.029268292682926828, 0.007142857142857142, 0.042527362000154986, 0.0002613129707879179, 0.007317073170731707, 0.053901483039974586, 0.00024067989794191588, 0.054024586848597546, 0.2788308472076162, 0.000241420243847319, 0.22073568466444485, 0.028571428571428567, 0.03850336716308983, 0.010714285714285713, 0.0, 0.3285321824598165, 0.0067415730337078645, 0.018885332361693055, 0.029268292682926828, 0.02386062103392467, 0.061538461538461535, 0.04499437971470593, 0.04193548387096774, 0.007166761550717767, 0.029268292682926828, 0.03157894736842105, 0.05384615384615384, 0.008235129488787733, 0.027872973289042428, 0.2906206714795637, 0.025609756097560978, 0.05384615384615384, 0.037037037037037035, 0.07930171756483195, 0.005263157894736842, 0.008118614364619544, 0.11949426058551593, 0.03034936487225139, 0.007641408802558065, 0.08810280341865927, 0.029268292682926828, 0.029268292682926828, 0.030769230769230767, 0.01976169633206894, 0.029268292682926828, 0.023595505617977526, 0.06, 0.03732863815533257, 0.029268292682926828, 0.003113072601214736, 0.03494892479073041, 0.00024209037944225868, 0.058536585365853655, 0.2834945973317957, 0.029629629629629627, 0.07373206285471859, 0.019251099359421736, 0.0007921764713150371, 0.038288014863933, 0.017241379310344827, 0.03850336716308983, 0.20222252503043972, 0.04090824158255012, 0.49114844535802243, 0.1647193177407218, 0.26873754872181765, 0.01923076923076923, 0.0008283313594340638, 0.033707865168539325, 0.02625, 0.005553358291656445, 0.05454545454545454, 0.029268292682926828, 0.3026229544679663, 0.007434558733205778, 0.0007921764713150371, 0.19929482502899576, 0.029268292682926828, 0.05384615384615384, 0.01606708236751034, 0.029268292682926828, 0.02413793103448276, 0.007543775844013259, 0.0007482966283516011, 0.029268292682926828, 0.10657894736842105, 0.30310863874345545, 0.058777531007782186, 0.029268292682926828, 0.03195127741443901, 0.003157894736842105, 0.06331027373845304, 0.04647118087067515, 0.029268292682926828, 0.016418619290209876, 0.09545454545454544, 0.029268292682926828, 0.09225503935429241, 0.007126613532136443, 0.07402792594338825, 0.029268292682926828, 0.03850336716308983, 0.029268292682926828, 0.0, 0.007317073170731707, 0.07555021901096573, 0.007317073170731707, 0.0, 0.0, 0.02669411216520866, 0.01957039793113119, 0.0004469614785577372, 0.04, 0.008377329157259845, 0.2564075515817131, 0.02608695652173913, 0.029268292682926828, 0.03214285714285714, 0.3045160141340286, 0.0, 0.030769230769230767, 0.03850336716308983, 0.048026794247195144, 0.029268292682926828, 0.029268292682926828, 0.2646501833792942, 0.01815043392077402, 0.11486507847309313, 0.030769230769230767, 0.029268292682926828, 0.017502512689975186, 0.018292682926829267, 0.28221954307160385, 0.007057803610471459, 0.015392641361815708, 0.025839773318570076, 0.029268292682926828, 0.043623521068170495, 0.0, 0.04227920506749927, 0.01875, 0.0005928039101636814, 0.14662441193824194, 0.04169142104541396, 0.03846153846153846, 0.03195127741443901, 0.029268292682926828, 0.029268292682926828, 0.03129292158114976, 0.029268292682926828, 0.031395348837209305, 0.029268292682926828, 0.029268292682926828, 0.04875, 0.4542305675769438, 0.01081081081081081, 0.03837209302325582, 0.029268292682926828, 0.0, 0.08215000806392052, 0.16554054054054054, 0.0007517077483145859, 0.03666666666666667, 0.030008946881712194, 0.029268292682926828, 0.011842105263157893, 0.054929513409214314, 0.01606708236751034, 0.27701049691885155, 0.029268292682926828, 0.05202550667764185, 0.029268292682926828, 0.0005207067228498577, 0.03159302744697758, 0.014634146341463414, 0.0862474222445365, 0.025454746275679962, 0.03732863815533257, 0.03417721518987341, 0.30400008357477093, 0.03214285714285714, 0.007317073170731707, 0.30897618472573485, 0.018855327295662656, 0.029268292682926828, 0.0002721714374326977, 0.03476032279777461, 0.0005642876553915427, 0.03417721518987341, 0.030769230769230767, 0.16023540197392272, 0.030008946881712194, 0.030769230769230767, 0.018180426475703296, 0.034918753527738225, 0.008377329157259845, 0.015789473684210523, 0.046357419086086565, 0.030769230769230767, 0.029268292682926828, 0.2036975612423323, 0.31027258667291957, 0.02980460261290444, 0.029268292682926828, 0.03232611535517477, 0.043623521068170495, 0.0387047542439348, 0.02967032967032967, 0.00565068214255087, 0.029268292682926828, 0.030769230769230767, 0.029268292682926828, 0.01154365433812873, 0.12169865572341872, 0.038666739812472505, 0.025609756097560978, 0.02282608695652174, 0.007594936708860759, 0.2834945973317957, 0.029268292682926828, 0.032926829268292684, 0.029268292682926828, 0.42821810620430634, 0.02195121951219512, 0.029268292682926828, 0.029268292682926828, 0.01606708236751034, 0.029268292682926828, 0.04772727272727272, 0.2828737275210399, 0.05917873484360295, 0.003571428571428571, 0.07066230411219265, 0.2082513844410077, 0.014634146341463414, 0.2082513844410077, 0.26262648786374165, 0.053164556962025315, 0.0, 0.008169893448168632, 0.029268292682926828, 0.01452460852889218, 0.05217391304347826, 0.0, 0.025609756097560978, 0.029986765685888237, 0.10145562909676539, 0.021322256836275526, 0.0, 0.014634146341463414, 0.030769230769230767, 0.02608695652173913, 0.01396830998940639, 0.0008241107787393142, 0.03817761404917069, 0.029268292682926828, 0.0002613129707879179, 0.027027027027027025, 0.4542305675769438, 0.19919413006252876]
Finish training and take 47m
