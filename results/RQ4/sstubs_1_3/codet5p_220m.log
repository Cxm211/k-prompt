Namespace(log_name='./RQ5/sstubs_1_3/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_1_3/codet5p_220m', data_dir='./data/RQ5/sstubs_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '* @see <a href="http:       */      public ResponseList<Status> getMentions() throws TwitterException {         return Status.constructStatuses(get(getBaseURL() + "statuses/mentions.json",                  null, true));      }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '* @see <a href="http:       */      public ResponseList<Status> getMentions() throws TwitterException {         return Status.createStatuseList(get(getBaseURL() + "statuses/mentions.json",                  null, true));      }'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 4.658501960673558e+299
  global_step = 2
  train_loss = 69.309
  ********************
Previous best ppl:inf
Achieve Best ppl:4.658501960673558e+299
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 18.46 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:18.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 9.349124457749257e+292
  global_step = 3
  train_loss = 70.291
  ********************
Previous best ppl:4.658501960673558e+299
Achieve Best ppl:9.349124457749257e+292
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 27.91 	 Previous best codebleu 18.46
  ********************
 Achieve Best bleu:27.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 4.208716567807634e+287
  global_step = 4
  train_loss = 37.5564
  ********************
Previous best ppl:9.349124457749257e+292
Achieve Best ppl:4.208716567807634e+287
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 28.44 	 Previous best codebleu 27.91
  ********************
 Achieve Best bleu:28.44
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 6.918215561051928e+284
  global_step = 5
  train_loss = 20.2787
  ********************
Previous best ppl:4.208716567807634e+287
Achieve Best ppl:6.918215561051928e+284
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 29.11 	 Previous best codebleu 28.44
  ********************
 Achieve Best bleu:29.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 4.521804129961413e+281
  global_step = 6
  train_loss = 14.1613
  ********************
Previous best ppl:6.918215561051928e+284
Achieve Best ppl:4.521804129961413e+281
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 28.61 	 Previous best codebleu 29.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 4.134047805781028e+279
  global_step = 7
  train_loss = 9.4643
  ********************
Previous best ppl:4.521804129961413e+281
Achieve Best ppl:4.134047805781028e+279
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 28.17 	 Previous best codebleu 29.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 5.775496741223046e+278
  global_step = 8
  train_loss = 4.6766
  ********************
Previous best ppl:4.134047805781028e+279
Achieve Best ppl:5.775496741223046e+278
  ********************
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 25.83 	 Previous best codebleu 29.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 2.9086048825134904e+279
  global_step = 9
  train_loss = 4.2302
  ********************
Previous best ppl:5.775496741223046e+278
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 23.41 	 Previous best codebleu 29.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.2330669208857337e+280
  global_step = 10
  train_loss = 3.0268
  ********************
Previous best ppl:5.775496741223046e+278
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 27.59 	 Previous best codebleu 29.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 2.770561520185161e+280
  global_step = 11
  train_loss = 2.1754
  ********************
Previous best ppl:5.775496741223046e+278
BLEU file: ./data/RQ5/sstubs_1_3/validation.jsonl
  codebleu-4 = 26.78 	 Previous best codebleu 29.11
  ********************
early stopping!!!
reload model from RQ5/sstubs_1_3/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_3/test.jsonl
  codebleu = 28.91 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 28.91 
[0.32485454973382605, 0.3169303268199098, 0.4875287787948384, 0.3401040722562194, 0.2824394263506065, 0.5924475066042644, 0.3477282248590317, 0.20553968339446563, 0.1550173814191771, 0.0, 0.12412987292652902, 0.25554221543919886, 0.3086945552077479, 0.31656893003837777, 0.3256361663433542, 0.2985376972763043, 0.24122059144836297, 0.007450943463840344, 0.2511873868768069, 0.20553968339446563, 0.2738461898890448, 0.3450212664701092, 0.25526620935318883, 0.3217944138338333, 0.33428602168406535, 0.1876571383382391, 0.11143477824409148, 0.1876803572131687, 0.24845580554025212, 0.4722149414194885, 0.31565575186175354, 0.23227917278787633, 0.26620120502800687, 0.20393451549618824, 0.25594201636760316, 0.26914028976280824, 0.30787540591427215, 0.1876803572131687, 0.1876571383382391, 0.3655988863524907, 0.3227654313821924, 0.26429098736943585, 0.34848002125377686, 0.3652424993173432, 0.30644280123850165, 0.2637054524289886, 0.07577675585184776, 0.3263301352958554, 0.1931136782474239, 0.1876571383382391, 0.34561849285441837, 0.32845032528001905, 0.1876571383382391, 0.1876571383382391, 0.35893810116973196, 0.2234704712550814, 0.24862989494299348, 0.21430491487657147, 0.1876571383382391, 0.3616529211999628, 0.33535217658372596, 0.3139068113265492, 0.3450212664701092, 0.34795685384325375, 0.1775230976070838, 0.3060530787742789, 0.3208025695041648, 0.3029473311689351, 0.3432068642134224, 0.3436505547973299, 0.5971720813643862, 0.25766479209497717, 0.1876574378565481, 0.5328522761377882, 0.2040801686086763, 0.1876571383382391, 0.333845670302285, 0.3149373731635938, 0.2264439730451018, 0.1876571383382391, 0.34359509314217285, 0.4697232182449972, 0.5955630663572451, 0.2317878747541064, 0.2631917802782824, 0.18765698865659905, 0.35489919686781957, 0.26914028976280824, 0.26914028976280824, 0.24845580554025212, 0.23621578887821623, 0.4667375684488244, 0.29515251410966503, 0.42681610144745286, 0.3112879168375532, 0.29155452080132677, 0.6078870292154335, 0.3738619829555347, 0.3324638909080563, 0.0, 0.266626654328438, 0.5008038902987748, 0.3290913726565643, 0.27752735655706706, 0.4021506454346485, 0.1898284866309536, 0.3658544338359049, 0.9493239030379825, 0.37977961937659066, 0.1876803572131687, 0.2671328512186051, 0.18512716572380702, 0.24499457480748646, 0.015622955674664814, 0.34795685384325375, 0.1876574378565481, 0.17779097880064487, 0.4685184829438127, 0.05813409662330446, 0.20553968339446563, 0.25594201636760316, 0.3194422853699078, 0.232137856009684, 0.47197968522230177, 0.2599610492111576, 0.20408053059119619, 0.27214752164945233, 0.7516477780137727, 0.36155745951153584, 0.23888847882115447, 0.1876574378565481, 0.9111462462300539, 0.23840501627599223, 0.012340838639976107, 0.18765698865659905, 0.3571349792683839, 0.25530977683583506, 0.22991787830184596, 0.33181581050293246, 0.4950827624179488, 0.29703981937749147, 0.1876571383382391, 0.17267767070260936, 0.5800583054556783, 0.2240364697369626, 0.5665802189291513, 0.1876803572131687, 0.12412987292652902, 0.3053500126818865, 0.3091364975812744, 0.3042083250340084, 0.18512716572380702, 0.1876571383382391, 0.4667375684488244, 0.1876571383382391, 0.18512716572380702, 0.18512716572380702, 0.31659149807502174, 0.23432964300772832, 0.1876571383382391, 0.3490309688218874, 0.1876571383382391, 0.4735761180474042, 0.2896114203214502, 0.30808754191303095, 0.1876571383382391, 0.25444248910599765, 0.47110187316139696, 0.30846272878434877, 0.34971487258054085, 0.20848734842804387, 0.1876571383382391, 0.2824394263506065, 0.21437801350048216, 0.2340964707191575, 0.1876803572131687, 0.2855286117201881, 0.3194133821676884, 0.22096821323287008, 0.1876571383382391, 0.39460712841683465, 0.6007966513335891, 0.3836502911408098, 0.2418490049793659, 0.7938680124927537, 0.33716445043915283, 0.33716445043915283, 0.1898284866309536, 0.5278565120437972, 0.26784085809947605, 0.3408767015727569, 0.20553968339446563, 0.23888847882115447, 0.3259790336768483, 0.32856116471579155, 0.286537299002996, 0.3408924077176022, 0.3358338268423236, 0.34795685384325375, 0.20553968339446563, 0.3268944984117019, 0.30510433614124777, 0.07639661128705903, 0.2535760950322077, 0.21498732265157683, 0.20332467191596898, 0.24885970503261992, 0.24551378125141887, 0.0, 0.1876803572131687, 0.27033858075361117, 0.3344155798181457, 0.20553968339446563, 0.31659149807502174, 0.35101699559298066, 0.3089401814603217, 0.37069438790931947, 0.2790080625374185, 0.1876571383382391, 0.2267763714719113, 0.28962492507902854, 0.3122012063798685, 0.1876803572131687, 0.1898284866309536, 0.24845580554025212, 0.3032322862610733, 0.2040801686086763, 0.2417271952069293, 0.32776587459902334, 0.1871963363292262, 0.32776587459902334, 0.20553968339446563, 0.37069438790931947, 0.3041655905143851, 0.22763476120157772, 0.20408053059119619, 0.30663144323920555, 0.3571723661840906, 0.6215651594137355, 0.20081325426438928, 0.1876574378565481, 0.19068079687751185, 0.3339366226896449, 0.20916065147828317, 0.3056471656023309, 0.3436505547973299, 0.12194030716512014, 0.39590361755504333, 0.3826290510271121, 0.22062381107600176, 0.28636213532716065, 0.33202445358491395, 0.23872511545928998, 0.3042733590840471, 0.24307167596138, 0.3290913726565643, 0.30078896231339186, 0.29703981937749147, 0.18765698865659905, 0.282709079139486, 0.3177623433623744, 0.28378697792273494, 0.2829398829368194, 0.31762166849220624, 0.1876803572131687, 0.31343910585194035, 0.3365167134240933, 0.2706854734923318, 0.22992221425032433, 0.286537299002996, 0.33093045795670484, 0.34521524385043667, 0.0035573122529644263, 0.28168173424023446, 0.3157621335219956, 0.2919782077391381, 0.37820575171584303, 0.24611399756741964, 0.1614065731318908, 0.4149868312740004, 0.18765698865659905, 0.1876571383382391, 0.2055399097631272, 0.3234526390361888, 0.18512716572380702, 0.30240805660359626, 0.3294513370661671, 0.2343572156616956, 0.3250652161580977, 0.07604706217052662, 0.23432964300772832, 0.08310299629462556, 0.2657877882889486, 0.34795685384325375, 0.33181581050293246, 0.3826290510271121, 0.3868463380679426, 0.4729797365370805, 0.2459684847455241, 0.31339580702912634, 0.23872511545928998, 0.0, 0.2511873868768069, 0.47197968522230177, 0.0, 0.3358338268423236, 0.3089401814603217, 0.4754072683077991, 0.31232624153714783, 0.22196719858371536, 0.298924285293647, 0.3635672299745197, 0.1876571383382391, 0.25197948551922994, 0.17315235264868553, 0.4729797365370805, 0.31647618106448394, 0.1876571383382391, 0.34521524385043667, 0.2735456239556795, 0.1898284866309536, 0.2669667093858023, 0.31904206503556154, 0.5023588709094758, 0.18765698865659905, 0.35246047555515997, 0.3181484344508046, 0.29028009991478, 0.2040801686086763, 0.22734839455816358, 0.5967588765848731, 0.31324855056186146, 0.2782841323712554, 0.20408053059119619, 0.26854440942769986, 0.38278607847323537, 0.1876571383382391, 0.3421745259511628, 0.6671136162345271, 0.2952770913884809, 0.1898284866309536, 0.23872511545928998, 0.1876571383382391, 0.25159825867914243, 0.2717825556199732, 0.3826290510271121, 0.3042139866312394, 0.6198926039236974, 0.22579147804625732, 0.24122059144836297, 0.3233528886842079, 0.5080083193394263, 0.3029911825632435, 0.18961435999434367, 0.2912202762938008, 0.3312506540052634, 0.1876571383382391, 0.41196483704782505, 0.3378344383737225, 0.29731749930056195, 0.31983708710520636, 0.23872511545928998, 0.368545783583072, 0.2040801686086763, 0.1876574378565481, 0.25404793280014626, 0.2805993155906735, 0.25486539800484337, 0.3166058172774651, 0.1898284866309536, 0.2923157469324282, 0.10129820290106722, 0.312056426019265, 0.2582510024277435, 0.3108257668447203, 0.2418490049793659, 0.1876571383382391, 0.5278565120437972, 0.3700117928090917, 0.38219112520467846, 0.15852580252885548, 0.4875287787948384, 0.2609191118540996, 0.23888847882115447, 0.3486039569408482, 0.22734839455816358, 0.1876571383382391, 0.1876803572131687, 0.26013787913481545, 0.2055403161396916, 0.29522527652631886, 0.2040802912893467, 0.1876571383382391, 0.3837950237139193, 0.7262638399990768, 0.3194422853699078, 0.3504880800129796, 0.1876571383382391, 0.4703406173660968, 0.2824259546758144, 0.32670965293107324, 0.5030691177839028, 0.2943191531689024, 0.22991787830184596, 0.18765698865659905, 0.3067867546822812, 0.27715824740010997, 0.2735456239556795, 0.29619045378899184, 0.1876803572131687, 0.3866309947587458, 0.18765698865659905, 0.47078875208572823, 0.31971450436541865, 0.2898895624562234, 0.6238456161281777, 0.2507462479527662, 0.2343572156616956, 0.3232977223575305, 0.24845580554025212, 0.41196483704782505, 0.27648433577951576, 0.18574950868920714, 0.23456957412391927, 0.1876571383382391, 0.5388526965394469, 0.33640820804037896, 0.47110187316139696, 0.3232977223575305, 0.3166058172774651, 0.25955748541209367, 0.22991787830184596, 0.3166058172774651, 0.31505062743874074, 0.32460229821304215, 0.18961435999434367, 0.3223382075021538, 0.019039572465137367, 0.3166058172774651, 0.18512716572380702, 0.30337592959823384, 0.26914028976280824, 0.2907956101498957, 0.1876571383382391, 0.3364533208820164, 0.5278565120437972, 0.09416765379511362, 0.331888737488432, 0.28056258647845356, 0.1898284866309536, 0.31983708710520636, 0.18765698865659905, 0.284750033445585, 0.12156699043073001, 0.3041393448492401, 0.024200149662472165, 0.32521051014261004, 0.2678922444035586, 0.34795685384325375, 0.18512716572380702, 0.3280229776466231, 0.1876571383382391, 0.20393451549618824, 0.2307916114306265, 0.1876803572131687, 0.1876803572131687, 0.2735456239556795, 0.1876571383382391, 0.7813999140874204, 0.3114008044838491, 0.24212339970870744, 0.3029440065772892, 0.22969564345676674, 0.32613662378731223, 0.2914834805967322, 0.32613662378731223, 0.3831639710751872, 0.2579131625989625, 0.29155452080132677, 0.31666463580442394, 0.2040802912893467, 0.3466911034205318, 0.3106781145572946, 0.3043926207540578, 0.18250786834069793, 0.2322323891801078, 0.35749882749509015, 0.3060188103968118, 0.2896114203214502, 0.19061756487735887, 0.3166058172774651, 0.3312506540052634, 0.01871019067568271, 0.47889967039915926, 0.3363893893785139, 0.1876571383382391, 0.20916065147828317, 0.22166426053020358, 0.7262638399990768, 0.13438739362322596]
Finish training and take 57m
