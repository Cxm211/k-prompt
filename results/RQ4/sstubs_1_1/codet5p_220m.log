Namespace(log_name='./RQ5/sstubs_1_1/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_1_1/codet5p_220m', data_dir='./data/RQ5/sstubs_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'Class<?> c = makeClass(className, proceedOnExceptions);          if (c != null) {            if (!classes.add(c)) {             LOG.error("Ignoring duplicate class " + className);            }          }        }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'Class<?> c = makeClass(className, proceedOnExceptions);          if (c != null) {            if (!classes.add(c)) {             LOG.warn("Ignoring duplicate class " + className);            }          }        }'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 7.622326975141684e+299
  global_step = 2
  train_loss = 45.8239
  ********************
Previous best ppl:inf
Achieve Best ppl:7.622326975141684e+299
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 18.73 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:18.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 9.86714639275101e+295
  global_step = 3
  train_loss = 42.8045
  ********************
Previous best ppl:7.622326975141684e+299
Achieve Best ppl:9.86714639275101e+295
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 20.95 	 Previous best codebleu 18.73
  ********************
 Achieve Best bleu:20.95
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 5.1450495582464204e+289
  global_step = 4
  train_loss = 18.4949
  ********************
Previous best ppl:9.86714639275101e+295
Achieve Best ppl:5.1450495582464204e+289
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 25.42 	 Previous best codebleu 20.95
  ********************
 Achieve Best bleu:25.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 8.065489485864051e+280
  global_step = 5
  train_loss = 11.0375
  ********************
Previous best ppl:5.1450495582464204e+289
Achieve Best ppl:8.065489485864051e+280
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 27.19 	 Previous best codebleu 25.42
  ********************
 Achieve Best bleu:27.19
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.4426744046547195e+273
  global_step = 6
  train_loss = 7.4673
  ********************
Previous best ppl:8.065489485864051e+280
Achieve Best ppl:1.4426744046547195e+273
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 29.47 	 Previous best codebleu 27.19
  ********************
 Achieve Best bleu:29.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.6181285535876899e+267
  global_step = 7
  train_loss = 6.733
  ********************
Previous best ppl:1.4426744046547195e+273
Achieve Best ppl:1.6181285535876899e+267
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 31.45 	 Previous best codebleu 29.47
  ********************
 Achieve Best bleu:31.45
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 9.484223831278453e+262
  global_step = 8
  train_loss = 3.1123
  ********************
Previous best ppl:1.6181285535876899e+267
Achieve Best ppl:9.484223831278453e+262
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 33.78 	 Previous best codebleu 31.45
  ********************
 Achieve Best bleu:33.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 3.6131189636804697e+260
  global_step = 9
  train_loss = 2.6998
  ********************
Previous best ppl:9.484223831278453e+262
Achieve Best ppl:3.6131189636804697e+260
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 34.98 	 Previous best codebleu 33.78
  ********************
 Achieve Best bleu:34.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 2.358115181269394e+259
  global_step = 10
  train_loss = 2.1809
  ********************
Previous best ppl:3.6131189636804697e+260
Achieve Best ppl:2.358115181269394e+259
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 36.07 	 Previous best codebleu 34.98
  ********************
 Achieve Best bleu:36.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 7.955227959224307e+258
  global_step = 11
  train_loss = 2.7971
  ********************
Previous best ppl:2.358115181269394e+259
Achieve Best ppl:7.955227959224307e+258
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 36.45 	 Previous best codebleu 36.07
  ********************
 Achieve Best bleu:36.45
  ********************
reload model from RQ5/sstubs_1_1/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_1/test.jsonl
  codebleu = 36.87 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 36.87 
[0.28808908193062716, 0.32194771719677234, 0.31543977103564314, 0.3190706908264328, 0.3201631569990538, 0.05257703053377982, 0.2695238907934408, 0.2500466500473914, 0.2530655945319447, 0.26624847961528697, 0.05769393871410812, 0.5733689877416202, 0.8239175302468416, 0.31623508208530354, 0.13715087234147832, 0.32969470765127745, 0.3167892855194842, 0.31569226993860616, 0.8455326290822458, 0.31374014914294934, 0.37747974083739827, 0.5384435061143487, 0.28781118776931974, 0.11027410992515466, 0.4976373823302641, 0.25400986316942925, 0.39321828767163036, 0.246368902301264, 0.5897967879349078, 0.3113673396318115, 0.753539840620477, 0.31623508208530354, 0.5633942975446201, 0.18340736236597388, 0.06483679585696527, 0.19418196336150378, 0.32698267042537665, 0.6986226053496382, 0.31374004603950256, 0.5809115503579649, 0.6602776852783424, 0.8091674114194076, 0.22036227165212022, 0.3238358758014815, 0.3175504642381779, 0.32194771719677234, 0.31623508208530354, 0.317961713611274, 0.2500466500473914, 0.321849014900452, 0.9261420415269246, 0.31756232396831896, 0.7809449492238212, 0.32449621053495126, 0.13520414734215067, 0.31415365907389187, 0.7278862186819539, 0.31569226993860616, 0.246368902301264, 0.30165045074135366, 0.3229085910772827, 0.25164269272948486, 0.15318697242088353, 0.15243076951414536, 0.31374014914294934, 0.2559215750813122, 0.23340342581077367, 0.3161786627608356, 0.31990120205677625, 0.3229085910772827, 0.4876465813283564, 0.34582452347839154, 0.22979928912078307, 0.6039670821555738, 0.19418196336150378, 0.9307233460793374, 0.31521845490247496, 0.3334757737249952, 0.5733689877416202, 0.31871385143880904, 0.32491226446121185, 0.31623508208530354, 0.22979928912078307, 0.8101853697811778, 0.32226461033597226, 0.3845705812636465, 0.31415365907389187, 0.31543977103564314, 0.26421410161981457, 0.31544012422281176, 0.2831414331486569, 0.2500466500473914, 0.3175504642381779, 0.5071460579579538, 0.3238358758014815, 0.3177456701420423, 0.3167892855194842, 0.8778637712545649, 0.3015516299221399, 0.32194771719677234, 0.31415365907389187, 0.35826449535738314, 0.124417983361038, 0.5740160734511758, 0.2503361108592342, 0.3167892855194842, 0.29770035827743685, 0.2944690101865559, 0.3520428138277214, 0.3210550838189261, 0.810098459231861, 0.31374014914294934, 0.2572139446808701, 0.31569226993860616, 0.34077595567456637, 0.753539840620477, 0.6976830901995327, 0.3167892855194842, 0.312636739281399, 0.3520428138277214, 0.4148031006284764, 0.3126483562035793, 0.25976326520784626, 0.46394044743801477, 0.31374014914294934, 0.44272617739740255, 0.31415365907389187, 0.3520428138277214, 0.2740470733794834, 0.21187659356306415, 0.2635365992289306, 0.30503635880043545, 0.7026648823501653, 0.3951564992761264, 0.4696285590971926, 0.4557259450753391, 0.31976473100081315, 0.18340736236597388, 0.25056768078798547, 0.32171452167648085, 0.24504683475754555, 0.3345584552776828, 0.2938919257321953, 0.3167892855194842, 0.18230871261186563, 0.25565978891931396, 0.3694820060335342, 0.3173081558087734, 0.2546175221254776, 0.32376449519181855, 0.956063796736657, 0.1992187059039082, 0.2116270772043513, 0.1520814660604104, 0.010452878492404106, 0.24383015674145328, 0.2816038037222292, 0.5185183012309345, 0.7760455465795291, 0.317961713611274, 0.16282233987646844, 0.28693327161320475, 0.3332744617897194, 0.6589353648117229, 0.3167892855194842, 0.3113673396318115, 0.16120598148617538, 0.3169935641459558, 0.31544012422281176, 0.24860885906230715, 0.3779602850702001, 0.31732395274309494, 0.44272617739740255, 0.29468768321766414, 0.1563356879799786, 0.3177456701420423, 0.3167892855194842, 0.0509056220229485, 0.2345697238787122, 0.4375162892303258, 0.3677179613561373, 0.7468487216789661, 0.19418196336150378, 0.1578558090310955, 0.8612736644066057, 0.31543977103564314, 0.631265797129656, 0.4532454061842627, 0.5982231653986002, 0.29109114947587833, 0.3166472358595306, 0.5108720144143741, 0.7569919041597843, 0.5107428946143708, 0.20321637594170538, 0.3173081558087734, 0.315439716538697, 0.48420646679074963, 0.6698432429804781, 0.3389076434258737, 0.26306687834110143, 0.026362272674762995, 0.3479812171710367, 0.31543977103564314, 0.1695404001561423, 0.8101853697811778, 0.5449863434493945, 0.2921757392902686, 0.3595257914774273, 0.5933852677970027, 0.3479812171710367, 0.2942502263383905, 0.2584316059266456, 0.3177456701420423, 0.31623508208530354, 0.5121173445783539, 0.8943617249504292, 0.2836816388514768, 0.3366143590169026, 0.08559010427953291, 0.13520414734215067, 0.33915912747691834, 0.5173249919833754, 0.16312520274019593, 0.4154920891206787, 0.15908758865413175, 0.8646399562441716, 0.31543977103564314, 0.2534797347176153, 0.25649631306388165, 0.2463378402511162, 0.19494709204223643, 0.31543977103564314, 0.7915025041176507, 0.3452618919432795, 0.3245277676270618, 0.32171452167648085, 0.17436575952422978, 0.3167892855194842, 0.3187280497203133, 0.19494709204223643, 0.24655481287640924, 0.884137748772268, 0.1807446541489878, 0.30055018888621315, 0.3229085910772827, 0.2534605086303466, 0.31569226993860616, 0.31990120205677625, 0.29739718388466385, 0.31575269442379017, 0.31954274617476186, 0.3609612510777236, 0.3194814914244992, 0.3167892855194842, 0.31871385143880904, 0.3104843170354144, 0.23343075037490327, 0.18230871261186563, 0.7831358545297717, 0.6553829440964383, 0.31732395274309494, 0.1992187059039082, 0.6469704789377048, 0.24383015674145328, 0.4976373823302641, 0.4976373823302641, 0.3186266006108396, 0.34530795416990206, 0.2503361108592342, 0.29703465556744657, 0.4067735104817894, 0.4148031006284764, 0.28833233328546337, 0.49990320173584096, 0.5210443845282675, 0.7772405328859977, 0.28265498823245705, 0.3138234706425919, 0.22979928912078307, 0.3209729162308761, 0.2455078601892251, 0.4819039645074892, 0.5806897342185793, 0.2802993166518153, 0.16002864997781363, 0.19418196336150378, 0.26982296498343034, 0.5233323188463777, 0.6462248480103507, 0.3173081558087734, 0.10400844916353984, 0.5960257192662304, 0.060641324832659466, 0.2116270772043513, 0.31235286792791866, 0.31681719787452534, 0.31841794197699486, 0.2839409059171802, 0.2752539558620541, 0.34111091470684723, 0.3285212705686977, 0.32644531730142085, 0.7787804994109263, 0.8721507316445842, 0.31732395274309494, 0.5740160734511758, 0.17436575952422978, 0.3225489055074292, 0.4617203571693407, 0.317961713611274, 0.35826449535738314, 0.12819837571663928, 0.3467659841921274, 0.24593166411148454, 0.28781118776931974, 0.5351461501345471, 0.18230871261186563, 0.1992187059039082, 0.3234123138322617, 0.32265026708940964, 0.317961713611274, 0.3210218109211347, 0.3044677425778906, 0.49961064781679937, 0.34323059028895586, 0.2116270772043513, 0.3366143590169026, 0.32624740610604586, 0.47574294740670114, 0.3463224777522874, 0.8283639275824604, 0.3173536695094426, 0.7895036297547358, 0.3520428138277214, 0.3296100856425881, 0.3167892855194842, 0.20321637594170538, 0.3241552064410432, 0.2520135069376199, 0.3193718488542723, 0.31374014914294934, 0.29142228732248115, 0.3138234706425919, 0.31569226993860616, 0.1992187059039082, 0.3201631569990538, 0.011390703606337697, 0.32146225391936045, 0.31543977103564314, 0.1592803496323653, 0.5733689877416202, 0.5687713089928935, 0.7920211748253955, 0.24383015674145328, 0.24313320493026852, 0.23343075037490327, 0.3945879964753264, 0.25123393964184565, 0.31543977103564314, 0.5204552762216812, 0.5889894488475713, 0.3050479994088755, 0.3281455140187832, 0.3280833990781272, 0.9797958193928082, 0.3257043663673568, 0.8576767641936891, 0.2905064350666231, 0.31906231971715704, 0.4528295912438961, 0.4962971115985111, 0.2802993166518153, 0.3384941908337464, 0.3167892855194842, 0.6035697226689571, 0.17727810433140692, 0.5233323188463777, 0.29142228732248115, 0.2285609682720839, 0.8577558528669129, 0.28832074129268415, 0.31569226993860616, 0.3031154120992848, 0.8091674114194076, 0.7760455465795291, 0.6546772550732723, 0.5806897342185793, 0.31374014914294934, 0.2913245242148896, 0.7080852951659315, 0.5750101161672544, 0.6469704789377048, 0.31543977103564314, 0.2465754076877737, 0.6650911312870743, 0.7073644096550431, 0.5930220092037559, 0.5740160734511758, 0.31374014914294934, 0.31415365907389187, 0.3366143590169026, 0.5223823073043945, 0.6358176545920248, 0.5533446311424253, 0.011390703606337697, 0.31374004603950256, 0.3416506333889378, 0.8377246692948361, 0.8101853697811778, 0.24845524399630814, 0.2742641612977985, 0.2106697322858553, 0.31221904213634366, 0.31544012422281176, 0.5578212196919117, 0.20609820653795624, 0.3307419673221354, 0.3217639525881357, 0.20118425492899925, 0.21035978666216046, 0.317961713611274, 0.16602574608879797, 0.05138625365249735, 0.3193718488542723, 0.3463145067993097, 0.3595257914774273, 0.2323478803046005, 0.16377690238676365, 0.18229976890970662, 0.496946152155715, 0.6228763454694435, 0.3175504642381779, 0.3553321206096246, 0.27733760609621055, 0.0509056220229485, 0.22247187597412793, 0.27256046077900586, 0.32688995042102453, 0.27334599513488994, 0.5026057471290608, 0.2550640714905056, 0.7501355486757559, 0.31654517058319603, 0.3366143590169026, 0.7250240964747626, 0.2553778181273504, 0.31990120205677625, 0.21035978666216046, 0.5683537040740728, 0.2546175221254776, 0.4637998634277968, 0.8655056700329364, 0.31569226993860616, 0.4692085613288536, 0.051982656339130566, 0.3380393642935883, 0.26982296498343034, 0.25164269272948486, 0.1893572143251887, 0.05138625365249735, 0.48289966316506616, 0.31374004603950256, 0.8816015466501335, 0.31415365907389187, 0.1992187059039082, 0.31415365907389187, 0.058189241251175784, 0.31415365907389187, 0.315439716538697, 0.32631136881864603, 0.21162466686283854, 0.6775647022201422, 0.27221260902660704, 0.31374014914294934, 0.6370381584898323, 0.31543977103564314, 0.42930065539376183, 0.29932354718060544, 0.26613840366235353, 0.333795253713295, 0.8377246692948361, 0.17230148169262824, 0.7080852951659315, 0.496946152155715, 0.12045512345275088, 0.2546175221254776, 0.32265026708940964, 0.3709639085688395, 0.13174176917230165, 0.35057310124841423, 0.2972584912468632, 0.564980985206958, 0.31543977103564314, 0.29617599129622935, 0.05257703053377982, 0.3138234706425919, 0.31543977103564314, 0.3129843001805516, 0.6358176545920248]
Finish training and take 56m
