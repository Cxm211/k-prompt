Namespace(log_name='./RQ5/sstubs_1_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/sstubs_1_1/codet5p_770m', data_dir='./data/RQ5/sstubs_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'Class<?> c = makeClass(className, proceedOnExceptions);          if (c != null) {            if (!classes.add(c)) {             LOG.error("Ignoring duplicate class " + className);            }          }        }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'Class<?> c = makeClass(className, proceedOnExceptions);          if (c != null) {            if (!classes.add(c)) {             LOG.warn("Ignoring duplicate class " + className);            }          }        }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 4.877351802025219e+285
  global_step = 2
  train_loss = 43.9489
  ********************
Previous best ppl:inf
Achieve Best ppl:4.877351802025219e+285
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 19.66 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 2.5685593669659974e+272
  global_step = 3
  train_loss = 36.5276
  ********************
Previous best ppl:4.877351802025219e+285
Achieve Best ppl:2.5685593669659974e+272
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 26.19 	 Previous best codebleu 19.66
  ********************
 Achieve Best bleu:26.19
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 8.091293214791307e+260
  global_step = 4
  train_loss = 17.5415
  ********************
Previous best ppl:2.5685593669659974e+272
Achieve Best ppl:8.091293214791307e+260
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 29.46 	 Previous best codebleu 26.19
  ********************
 Achieve Best bleu:29.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.2097902220853012e+250
  global_step = 5
  train_loss = 7.3217
  ********************
Previous best ppl:8.091293214791307e+260
Achieve Best ppl:2.2097902220853012e+250
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 38.18 	 Previous best codebleu 29.46
  ********************
 Achieve Best bleu:38.18
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.9496168153977645e+247
  global_step = 6
  train_loss = 5.8911
  ********************
Previous best ppl:2.2097902220853012e+250
Achieve Best ppl:1.9496168153977645e+247
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 42.97 	 Previous best codebleu 38.18
  ********************
 Achieve Best bleu:42.97
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 9.211516137689719e+261
  global_step = 7
  train_loss = 3.1523
  ********************
Previous best ppl:1.9496168153977645e+247
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 49.01 	 Previous best codebleu 42.97
  ********************
 Achieve Best bleu:49.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 5.466337492964337e+267
  global_step = 8
  train_loss = 1.2774
  ********************
Previous best ppl:1.9496168153977645e+247
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 50.8 	 Previous best codebleu 49.01
  ********************
 Achieve Best bleu:50.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 9.340281533073469e+269
  global_step = 9
  train_loss = 0.9727
  ********************
Previous best ppl:1.9496168153977645e+247
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 53.65 	 Previous best codebleu 50.8
  ********************
 Achieve Best bleu:53.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 6.83150553480969e+270
  global_step = 10
  train_loss = 0.5399
  ********************
Previous best ppl:1.9496168153977645e+247
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 54.96 	 Previous best codebleu 53.65
  ********************
 Achieve Best bleu:54.96
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.1366861767516863e+271
  global_step = 11
  train_loss = 0.7858
  ********************
Previous best ppl:1.9496168153977645e+247
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 55.65 	 Previous best codebleu 54.96
  ********************
 Achieve Best bleu:55.65
  ********************
reload model from RQ5/sstubs_1_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_1/test.jsonl
  codebleu = 54.3 
  Total = 500 
  Exact Fixed = 1 
[302]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 1 
[302]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 54.3 
[0.8478582006667373, 0.32284721876405115, 0.5950215145911428, 0.8947836446510709, 0.562966429349787, 0.5878913720043559, 0.0003543348452298378, 0.16896677751758132, 0.10892181684181305, 0.29722769064318355, 0.16896677751758132, 0.42946128690534435, 0.8239175302468416, 0.5950215145911428, 0.24969428666820487, 0.2512532244148443, 0.5957205512329915, 0.5950215145911428, 0.8455326290822458, 0.5950215145911428, 0.17146927957032215, 0.12, 0.2699550867931218, 0.5348472970691327, 0.4998489003265194, 0.6647490526414709, 0.4116376358941889, 0.2549601132699395, 0.6210369135999132, 0.4802539600398662, 0.753539840620477, 0.5950215145911428, 0.6704083149750388, 0.6325215145911429, 0.16896677751758132, 0.5950215145911428, 0.3459126097108203, 0.6986226053496382, 0.5957205512329915, 0.3685916259938049, 0.7101359479538825, 0.7469276559607714, 0.236897992876745, 0.5523809117705359, 0.533018104833857, 0.32284721876405115, 0.5950215145911428, 0.5950215145911428, 0.33441975900301213, 0.5105315885289852, 0.9261420415269246, 0.6325215145911429, 0.8041662314864719, 0.9172498814207914, 0.28658523154541005, 0.5950215145911428, 0.6824725059663808, 0.5950215145911428, 0.2549601132699395, 0.12912654310259034, 0.7452043432596527, 0.25676294547178413, 0.16941778183940637, 0.16896677751758132, 0.5950215145911428, 0.31806382643638925, 0.9216202708461565, 0.8480582060293878, 0.6189868975194304, 0.7452043432596527, 0.6171440469886148, 0.3484455655437157, 0.09122442817655818, 0.002362204724409449, 0.5950215145911428, 0.9307233460793374, 0.3184890284919435, 0.3518761601518421, 0.42946128690534435, 0.531309110509413, 0.3476853343839889, 0.5950215145911428, 0.09122442817655818, 0.5295103130909388, 0.7264473511338186, 0.6905410794052483, 0.5950215145911428, 0.5990462366148999, 0.6225189361190878, 0.5726698478229096, 0.6421449483444079, 0.16896677751758132, 0.533018104833857, 0.39651669918964183, 0.5523809117705359, 0.6192599727483976, 0.5957205512329915, 0.8756149344112303, 0.8384965070852362, 0.32284721876405115, 0.5962050762899251, 0.3114309623659668, 0.16896677751758132, 0.8711768984739177, 0.25314308006521397, 0.5957205512329915, 0.30563791011158864, 0.09999999999999999, 0.34157739516172103, 0.5950215145911428, 0.9498733743967636, 0.5950215145911428, 0.3387921514667241, 0.5950215145911428, 0.5522165745266051, 0.753539840620477, 0.8166896576725897, 0.5957205512329915, 0.8036920331680233, 0.34157739516172103, 0.23853146301468592, 0.8290895082075922, 0.10987557436050548, 0.5826680042740902, 0.5950215145911428, 0.4553342278127285, 0.5950215145911428, 0.34157739516172103, 0.2774956980742575, 0.16896677751758132, 0.16941778183940637, 0.713506794192561, 0.8615578602079338, 0.7915504495394027, 0.46978817996765165, 0.4579004252904564, 0.9002056775473146, 0.6325215145911429, 0.25211712841326195, 0.6198948417269856, 0.16896677751758132, 0.8020102181123085, 0.29691815143813993, 0.5957205512329915, 0.2538790127193298, 0.8286663362541027, 0.3627466309227687, 0.5950215145911428, 0.16896677751758132, 0.8044037027487478, 0.7128197283736033, 0.5507778269895944, 0.2589234645346758, 0.16896677751758132, 0.33441975900301213, 0.519243734490203, 0.7425824046042754, 0.0381678195461318, 0.8806751756510949, 0.5950215145911428, 0.33406733871359523, 0.7662034194574154, 0.4320267000400019, 0.7184863543690603, 0.5957205512329915, 0.4802539600398662, 0.2449884043152637, 0.8078472700244907, 0.5726698478229096, 0.16896677751758132, 0.6357878078427105, 0.6198948417269856, 0.4553342278127285, 0.29468768321766414, 0.599768812683, 0.6232145298455622, 0.5957205512329915, 0.5708154695261138, 0.773218252799355, 0.7150559700806929, 0.3828384533168713, 0.3776923039477813, 0.5950215145911428, 0.6233044200330857, 0.0035502958579881655, 0.5950215145911428, 0.8873411867760128, 0.45534971577058236, 0.8664266028701744, 0.7920823763459031, 0.45384307830816417, 0.455688919111743, 0.8933075122679861, 0.2958924381148021, 0.5836654007948345, 0.5950215145911428, 0.5957205512329915, 0.4858833206998275, 0.709228362422202, 0.7996739980047762, 0.35005382316064043, 0.30182657292622894, 0.313906331839957, 0.5950215145911428, 0.16896677751758132, 0.5295103130909388, 0.7549863434493944, 0.9216202708461565, 0.313657554282433, 0.39971530151806506, 0.313906331839957, 0.390710742998124, 0.6749499456881326, 0.6192599727483976, 0.5950215145911428, 0.7604292763386018, 0.23542553096336377, 0.16896677751758132, 0.6345363823129714, 0.63826194766857, 0.28658523154541005, 0.877477165904387, 0.27307483485963496, 0.329145648024187, 0.47329908826231076, 0.27900827663796895, 0.8646399562441716, 0.5950215145911428, 0.16941778183940637, 0.8868198930118008, 0.16941778183940637, 0.5950215145911428, 0.5950215145911428, 0.8434391566054807, 0.40191256875430315, 0.8078472700244907, 0.6198948417269856, 0.5950215145911428, 0.5957205512329915, 0.5950215145911428, 0.5950215145911428, 0.16896677751758132, 0.884137748772268, 0.2509949774595946, 0.2811364699192088, 0.7452043432596527, 0.16896677751758132, 0.5950215145911428, 0.6189868975194304, 0.16941778183940637, 0.7695185525662089, 0.6192599727483976, 0.007296541009866084, 0.562966429349787, 0.5957205512329915, 0.531309110509413, 0.34256116790871755, 0.5469160590898346, 0.2538790127193298, 0.804233341010764, 0.6262267536198174, 0.6198948417269856, 0.5507778269895944, 0.6634809001122681, 0.519243734490203, 0.4998489003265194, 0.4998489003265194, 0.6714367115312883, 0.34530795416990206, 0.25314308006521397, 0.6167273710690397, 0.9048884805959156, 0.23853146301468592, 0.44323384125797544, 0.693827678544888, 0.5929106314180244, 0.5823356691289201, 0.8599929854363868, 0.5368098410322794, 0.09122442817655818, 0.6263972816776822, 0.7012036049950872, 0.3099260871630786, 0.7595178459004281, 0.8423705070523915, 0.5464761951300527, 0.5950215145911428, 0.3205108280512538, 0.5175469933783539, 0.7016265496309229, 0.5950215145911428, 0.8947836446510709, 0.28189011655846047, 0.5874729721263494, 0.2589234645346758, 0.7219579317566772, 0.7574370226475247, 0.6192599727483976, 0.6518385261334172, 0.4929483315634773, 0.7125049064935683, 1.0, 0.9088140261756905, 0.8801000781179811, 0.8235685091530658, 0.6198948417269856, 0.8598037983556437, 0.5950215145911428, 0.6453579693489011, 0.7026615485512439, 0.5950215145911428, 0.3114309623659668, 0.7418554698250603, 0.6831198107101925, 0.24814054815050665, 0.2699550867931218, 0.8080635670355532, 0.2538790127193298, 0.5507778269895944, 0.3558584105129584, 0.5950215145911428, 0.5950215145911428, 0.6192599727483976, 0.5825332304962951, 0.6452916133611851, 0.865339844665981, 0.2589234645346758, 0.36277845833422745, 0.6680164578329885, 0.47574294740670114, 0.9303743065148351, 0.7539305337188108, 0.5950215145911428, 0.44751744999784426, 0.34157739516172103, 0.28949243867069524, 0.5957205512329915, 0.5836654007948345, 0.5292581655482093, 0.16941778183940637, 0.729652944883701, 0.5950215145911428, 0.2938567485815512, 0.5368098410322794, 0.5950215145911428, 0.5507778269895944, 0.562966429349787, 0.21034294566035727, 0.8795367404028516, 0.5950215145911428, 0.06090444102214733, 0.39857995047164474, 0.3505059785571712, 0.6882921098464296, 0.519243734490203, 0.9155137465591443, 0.5469160590898346, 0.8975561631678814, 0.2648024234089775, 0.5950215145911428, 0.007100591715976331, 0.5641647559248762, 0.5567687933109712, 0.9041734668686527, 0.6400569972642296, 0.7800573233848309, 0.8575497269459637, 0.8296519967730851, 0.16896677751758132, 0.5990462366148999, 0.7976894923134883, 0.8672757060777059, 0.8423705070523915, 0.9287546369958737, 0.5957205512329915, 0.6047560373913667, 0.17718959489913924, 0.5175469933783539, 0.2938567485815512, 0.580390092530563, 0.339652470723793, 0.6682737652903339, 0.5950215145911428, 0.6077229906490504, 0.7469276559607714, 0.8806751756510949, 0.7800696385956964, 0.7595178459004281, 0.5950215145911428, 0.28716431500314904, 0.7080852951659315, 0.6851513300845844, 0.6634809001122681, 0.5950215145911428, 0.3004810458332089, 0.7250911312870743, 0.7073644096550431, 0.940598945729278, 0.8711768984739177, 0.5950215145911428, 0.5962050762899251, 0.6345363823129714, 0.6854556366233495, 0.583116155480702, 0.3513687359109236, 0.21034294566035727, 0.5957205512329915, 0.6343607823893095, 0.8767099682404029, 0.5295103130909388, 0.10088683875669788, 0.6745548638486198, 0.8302942569423217, 0.7360005100107223, 0.5726698478229096, 0.78246345492775, 0.16941778183940637, 0.8676078881152269, 0.8034074925147732, 0.5663328228425174, 0.5735145780063077, 0.5950215145911428, 0.6325215145911429, 0.7487051332341205, 0.729652944883701, 0.8964512388990802, 0.313657554282433, 0.15100275651092848, 0.15911658505163193, 0.7810580026281693, 0.8535087661549325, 0.4348876942979004, 0.533018104833857, 0.6185205701331067, 0.8174862646946779, 0.5708154695261138, 0.2449884043152637, 0.28006071860529475, 0.8200204593729716, 0.9186249397078678, 0.4946949488280725, 0.16896677751758132, 0.5390332881965515, 0.6814442242058667, 0.36277845833422745, 0.8234778751522029, 0.5728350771365776, 0.6189868975194304, 0.5735145780063077, 0.6315113217789707, 0.16896677751758132, 0.47292832209883573, 0.8069555747668429, 0.5950215145911428, 0.47224149188812414, 0.17307692307692307, 0.8941528307478339, 0.3205108280512538, 0.25676294547178413, 0.18691392347542038, 0.7487051332341205, 0.09765726534033564, 0.5957205512329915, 0.8816015466501335, 0.5950215145911428, 0.5507778269895944, 0.5962050762899251, 0.5378416796706287, 0.5962050762899251, 0.5957205512329915, 0.7760410989129443, 0.6356664843177978, 0.6775647022201422, 0.8276442284466219, 0.5950215145911428, 0.7129632918608437, 0.5950215145911428, 0.21157157245435812, 0.5189041158273829, 0.27435439804151907, 0.32297051960681217, 0.8767099682404029, 0.6600534786024624, 0.7080852951659315, 0.8535087661549325, 0.2482760564740458, 0.16896677751758132, 0.5950215145911428, 0.9207265089204273, 0.19247974200936507, 0.8338897865083772, 0.16941778183940637, 0.9105144663231395, 0.5950215145911428, 0.3458161462270667, 0.5878913720043559, 0.5368098410322794, 0.5950215145911428, 0.6699563805952584, 0.583116155480702]
Finish training and take 1h19m
