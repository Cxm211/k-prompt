Namespace(log_name='./RQ5/sstubs_1_1/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_1_1/codet5p_220m_f', data_dir='./data/RQ5/sstubs_1_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00318
  global_step = 1
  train_loss = 0.9232
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00318
  ********************
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 5.32 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:5.32
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00318
  global_step = 1
  train_loss = 1.0024
  ********************
Previous best ppl:1.00318
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 5.32 	 Previous best codebleu 5.32
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00318
  global_step = 1
  train_loss = 1.6445
  ********************
Previous best ppl:1.00318
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 5.32 	 Previous best codebleu 5.32
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00318
  global_step = 1
  train_loss = 1.2518
  ********************
Previous best ppl:1.00318
BLEU file: ./data/RQ5/sstubs_1_1/validation.jsonl
  codebleu-4 = 5.32 	 Previous best codebleu 5.32
  ********************
reload model from RQ5/sstubs_1_1/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_1/test.jsonl
  codebleu = 5.4 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 5.4 
[0.0244146951306953, 0.2082513844410077, 0.029268292682926828, 0.01922853487865821, 0.03732863815533257, 0.02669411216520866, 0.2800389607132805, 0.0, 0.25475647135474866, 0.17216603117669524, 0.03232611535517477, 0.022469387178553648, 0.03375, 0.02553191489361702, 0.00024209037944225868, 0.07582417582417582, 0.029268292682926828, 0.029268292682926828, 0.014634146341463414, 0.029268292682926828, 0.0, 0.000259690543957315, 0.01974510798901144, 0.027872973289042428, 0.0004990567491135724, 0.03, 0.0, 0.007669587519459089, 0.0, 0.03214285714285714, 0.05384615384615384, 0.029268292682926828, 0.006896551724137931, 0.029268292682926828, 0.0, 0.029268292682926828, 0.34716106396968055, 0.008169893448168632, 0.029268292682926828, 0.0, 0.08414634146341464, 0.008594114001468643, 0.18106635154850986, 0.04090824158255012, 0.030008946881712194, 0.2082513844410077, 0.029268292682926828, 0.029268292682926828, 0.025839773318570076, 0.012025767717584283, 0.03837209302325582, 0.029268292682926828, 0.024324324324324326, 0.034482758620689655, 0.015334159344500005, 0.029268292682926828, 0.04875, 0.029268292682926828, 0.007669587519459089, 0.09385763622185966, 0.030769230769230767, 0.007769194828452498, 0.2751865268479001, 0.0, 0.029268292682926828, 0.03964963255458119, 0.02688976829597673, 0.015384615384615384, 0.029268292682926828, 0.030769230769230767, 0.20222252503043972, 0.34108940358705814, 0.00024067989794191588, 0.0069767441860465115, 0.029268292682926828, 0.05030872025688913, 0.3056000284447953, 0.000241420243847319, 0.022469387178553648, 0.029986765685888237, 0.30400008357477093, 0.02553191489361702, 0.00024067989794191588, 0.3808472136513605, 0.029268292682926828, 0.04425544978556004, 0.029268292682926828, 0.029268292682926828, 0.026503080394519648, 0.029268292682926828, 0.054878048780487805, 0.0, 0.030008946881712194, 0.0, 0.04090824158255012, 0.029268292682926828, 0.029268292682926828, 0.010714285714285713, 0.06853146853146853, 0.2082513844410077, 0.029268292682926828, 0.26873754872181765, 0.14662441193824194, 0.022469387178553648, 0.007006594960252806, 0.029268292682926828, 0.3015719218758112, 0.005454545454545454, 0.2834945973317957, 0.029268292682926828, 0.0, 0.029268292682926828, 0.0, 0.029268292682926828, 0.053901483039974586, 0.05384615384615384, 0.02957746478873239, 0.029268292682926828, 0.32702208443582825, 0.2834945973317957, 0.42821810620430634, 0.0075213984322905145, 0.015384615384615384, 0.01875, 0.029268292682926828, 0.0004735667696382707, 0.029268292682926828, 0.2834945973317957, 0.018816373738748265, 0.06406488038204017, 0.054523068616471757, 0.22246268846739337, 0.08354430379746834, 0.0, 0.0007517077483145859, 0.49117267506052176, 0.005263157894736842, 0.029268292682926828, 0.25744782447428344, 0.029268292682926828, 0.013036344226062105, 0.018292682926829267, 0.018292682926829267, 0.029268292682926828, 0.008377329157259845, 0.26734454207733355, 0.07105263157894737, 0.029268292682926828, 0.0, 0.03417721518987341, 0.02715075789722889, 0.030637390150243894, 0.008028739281982047, 0.0, 0.02454353363581084, 0.04169142104541396, 0.011018934521238333, 0.043623521068170495, 0.013793103448275862, 0.029268292682926828, 0.02045454545454545, 0.01592920353982301, 0.015189873417721518, 0.049999999999999996, 0.029268292682926828, 0.03214285714285714, 0.00023139937480654103, 0.030769230769230767, 0.029268292682926828, 0.07594767016946943, 0.045613592865295545, 0.029268292682926828, 0.0004735667696382707, 0.00023457024054606413, 0.009677419354838708, 0.024999999999999998, 0.029268292682926828, 0.03143013023776703, 0.18774529255568273, 0.00047473358805527583, 0.37168007079782717, 0.07692307692307691, 0.029268292682926828, 0.030379746835443037, 0.007853822069603255, 0.029268292682926828, 0.029022000080363153, 0.0004915506694683193, 0.02526315789473684, 0.07741429900783918, 0.019354838709677417, 0.0005207067228498577, 0.04588235294117647, 0.014233724572109151, 0.042221587047334246, 0.029268292682926828, 0.029268292682926828, 0.0005438470226390179, 0.018180426475703296, 0.054878048780487805, 0.029032258064516127, 0.0, 0.3176944808118819, 0.029268292682926828, 0.0, 0.0, 0.0, 0.02386062103392467, 0.31027258667291957, 0.0002627616317097737, 0.3176944808118819, 0.2837403939181237, 0.018918918918918916, 0.029268292682926828, 0.02553191489361702, 0.007869324619801341, 0.007882565632533338, 0.2018002086851226, 0.02608695652173913, 0.011842105263157893, 0.015334159344500005, 0.031395348837209305, 0.007981010193730417, 0.02195121951219512, 0.0, 0.00023363150786421475, 0.03484474949817896, 0.029268292682926828, 0.2574073535809972, 0.06690636271838316, 0.29958781528253675, 0.029268292682926828, 0.029268292682926828, 0.075, 0.029603452628755446, 0.030769230769230767, 0.029268292682926828, 0.029268292682926828, 0.029268292682926828, 0.029268292682926828, 0.029268292682926828, 0.0, 0.049999999999999996, 0.08084795302236233, 0.015, 0.030769230769230767, 0.1674131758230394, 0.029268292682926828, 0.029268292682926828, 0.2664938199642789, 0.007692307692307692, 0.029268292682926828, 0.062171266550170114, 0.03732863815533257, 0.029268292682926828, 0.029986765685888237, 0.05217391304347826, 0.011671758667053717, 0.008377329157259845, 0.02441860465116279, 0.007594936708860759, 0.029268292682926828, 0.030637390150243894, 0.04827586206896552, 0.04169142104541396, 0.0004990567491135724, 0.0004990567491135724, 0.029268292682926828, 0.05384615384615384, 0.007006594960252806, 0.03029992367521699, 0.08810280341865927, 0.42821810620430634, 0.030769230769230767, 0.0, 0.00017720199070206258, 0.2538781142694698, 0.007523768844122023, 0.03195127741443901, 0.00024067989794191588, 0.013953488372093023, 0.038288014863933, 0.31399201700462975, 0.0, 0.023376623376623374, 0.007711803050097511, 0.029268292682926828, 0.005645200478855432, 0.02608695652173913, 0.017241379310344827, 0.029268292682926828, 0.017045454545454544, 0.019921120675140708, 0.11061731234096923, 0.008028739281982047, 0.058841543052890584, 0.04390243902439024, 0.029268292682926828, 0.05887586802137625, 0.02526315789473684, 0.04642857142857143, 0.017045454545454544, 0.03571428571428571, 0.03571428571428571, 0.02222222222222222, 0.029268292682926828, 0.022469387178553648, 0.029268292682926828, 0.07307692307692307, 0.017618553132571824, 0.029268292682926828, 0.26873754872181765, 0.00020293050486688737, 0.000693744597268347, 0.2183474012053049, 0.01974510798901144, 0.042531827272081615, 0.008377329157259845, 0.030637390150243894, 0.02957746478873239, 0.029268292682926828, 0.029268292682926828, 0.029268292682926828, 0.01923076923076923, 0.011973807148549578, 0.021804538558328743, 0.008028739281982047, 0.02608695652173913, 0.02637994368537977, 0.024770642201834864, 0.04819465264379286, 0.02692307692307692, 0.029268292682926828, 0.028235294117647056, 0.2834945973317957, 0.02692307692307692, 0.029268292682926828, 0.042221587047334246, 0.03850336716308983, 0.20171033061176102, 0.029268292682926828, 0.029268292682926828, 0.019075451379778587, 0.03195127741443901, 0.029268292682926828, 0.030637390150243894, 0.03732863815533257, 0.17156325045420134, 0.017045454545454544, 0.029268292682926828, 0.19919413006252876, 0.022469387178553648, 0.058777531007782186, 0.0498292482520133, 0.04169142104541396, 0.02980460261290444, 0.011671758667053717, 0.032926829268292684, 0.007471866984718552, 0.029268292682926828, 0.02045454545454545, 0.0, 0.013186813186813187, 0.023032448222124503, 0.01263157894736842, 0.011200912664544791, 0.04125, 0.03599225126470834, 0.058208526748675445, 0.029268292682926828, 0.005553358291656445, 0.017502512689975186, 0.023376623376623374, 0.03591270046413775, 0.029268292682926828, 0.00023457024054606413, 0.17648180554518345, 0.02608695652173913, 0.019075451379778587, 0.0, 0.011946001655653838, 0.22073568466444485, 0.029268292682926828, 0.04105416860182914, 0.008594114001468643, 0.013793103448275862, 0.028235294117647056, 0.0, 0.029268292682926828, 0.030769230769230767, 0.07142857142857142, 0.05185185185185185, 0.04827586206896552, 0.029268292682926828, 0.2788308472076162, 0.011975612471724954, 0.06477272727272727, 0.02413793103448276, 0.022469387178553648, 0.029268292682926828, 0.029268292682926828, 0.02608695652173913, 0.014479328551254882, 0.015789473684210523, 0.015957446808510637, 0.17156325045420134, 0.029268292682926828, 0.042352941176470586, 0.01081081081081081, 0.4542305675769438, 0.29421765938334654, 0.0075, 0.015192447628498574, 0.033707865168539325, 0.029268292682926828, 0.00018802300570350695, 0.07373206285471859, 0.04814814814814814, 0.026373626373626374, 0.04468085106382978, 0.019291762059430215, 0.029268292682926828, 0.029268292682926828, 0.007142857142857142, 0.029268292682926828, 0.02195121951219512, 0.31027258667291957, 0.07927293610130469, 0.3217707491891386, 0.035526315789473684, 0.01396830998940639, 0.01081081081081081, 0.030008946881712194, 0.05398093585170844, 0.013043478260869565, 0.03143013023776703, 0.00024209037944225868, 0.01815043392077402, 0.024999999999999998, 0.014851042650805692, 0.02443097375870013, 0.18459549284031987, 0.0412064082866055, 0.02386363636363636, 0.025806451612903226, 0.024999999999999998, 0.015189873417721518, 0.029268292682926828, 0.019291762059430215, 0.006908136440122825, 0.0, 0.0007921764713150371, 0.046153846153846156, 0.029268292682926828, 0.0005928039101636814, 0.03563942149613822, 0.04, 0.005645200478855432, 0.007769194828452498, 0.015472680770206578, 0.007142857142857142, 0.0, 0.029268292682926828, 0.049999999999999996, 0.029268292682926828, 0.030637390150243894, 0.029268292682926828, 0.30826359853997354, 0.029268292682926828, 0.029268292682926828, 0.009677419354838708, 0.016237722017708924, 0.04285714285714285, 0.031395348837209305, 0.029268292682926828, 0.046153846153846156, 0.029268292682926828, 0.0, 0.0, 0.01606708236751034, 0.04390243902439024, 0.01081081081081081, 0.014634146341463414, 0.07692307692307691, 0.01396830998940639, 0.023376623376623374, 0.0, 0.029268292682926828, 0.027164403323315718, 0.014480796187406568, 0.055182469067766124, 0.07266468419525496, 0.006058822185834247, 0.029268292682926828, 0.28975825872178573, 0.02669411216520866, 0.03195127741443901, 0.029268292682926828, 0.013043478260869565, 0.015789473684210523]
Finish training and take 46m
