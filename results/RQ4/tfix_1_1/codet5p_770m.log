Namespace(log_name='./RQ5/tfix_1_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_1_1/codet5p_770m', data_dir='./data/RQ5/tfix_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'const mocks = {};   const providers = this._getProviders(klass);', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'const mocks = {};'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 5.788211293682738e+274
  global_step = 2
  train_loss = 19.7215
  ********************
Previous best ppl:inf
Achieve Best ppl:5.788211293682738e+274
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 16.15 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.15
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 5.792721062386322e+234
  global_step = 3
  train_loss = 18.6886
  ********************
Previous best ppl:5.788211293682738e+274
Achieve Best ppl:5.792721062386322e+234
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 20.13 	 Previous best codebleu 16.15
  ********************
 Achieve Best bleu:20.13
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 5.462263016920833e+226
  global_step = 4
  train_loss = 5.0004
  ********************
Previous best ppl:5.792721062386322e+234
Achieve Best ppl:5.462263016920833e+226
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 26.65 	 Previous best codebleu 20.13
  ********************
 Achieve Best bleu:26.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.9683326664665667e+227
  global_step = 5
  train_loss = 1.9496
  ********************
Previous best ppl:5.462263016920833e+226
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 30.6 	 Previous best codebleu 26.65
  ********************
 Achieve Best bleu:30.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.869124266132666e+234
  global_step = 6
  train_loss = 0.6514
  ********************
Previous best ppl:5.462263016920833e+226
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 30.79 	 Previous best codebleu 30.6
  ********************
 Achieve Best bleu:30.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 6.06797441754058e+241
  global_step = 7
  train_loss = 0.1623
  ********************
Previous best ppl:5.462263016920833e+226
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 28.33 	 Previous best codebleu 30.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 5.093308257806175e+247
  global_step = 8
  train_loss = 0.1417
  ********************
Previous best ppl:5.462263016920833e+226
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 25.15 	 Previous best codebleu 30.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 6.75449134471102e+251
  global_step = 9
  train_loss = 0.0412
  ********************
Previous best ppl:5.462263016920833e+226
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 23.16 	 Previous best codebleu 30.79
  ********************
early stopping!!!
reload model from RQ5/tfix_1_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_1/test.jsonl
  codebleu = 31.87 
  Total = 500 
  Exact Fixed = 2 
[36, 41]
  Syntax Fixed = 2 
[127, 401]
  Cleaned Fixed = 2 
[23, 92]
  ********************
  Total = 500 
  Exact Fixed = 2 
[36, 41]
  Syntax Fixed = 2 
[127, 401]
  Cleaned Fixed = 2 
[23, 92]
  codebleu = 31.87 
[0.3488152698595746, 0.2605027016919211, 0.2901345268033213, 0.0, 0.0, 0.017733069674444876, 0.0, 0.25887330845719503, 0.7924012535180505, 0.0, 0.004724409448818898, 0.19999999999999998, 0.271898412993546, 0.2964705882352941, 0.5658448779697635, 0.0, 0.22673642618601014, 0.3954789240122505, 0.5195102422011197, 0.4869881044162145, 0.4188388715436905, 0.0, 0.6126392797557267, 0.6536911179244789, 0.02519862247423641, 0.21530157847866804, 0.5677950971437925, 0.394233463342506, 0.1714285714285714, 0.7535117129535003, 0.34701272751400436, 0.3112055672079212, 0.14847049575332977, 0.6855567023566869, 0.5527818755063434, 0.6443152851788645, 0.3, 0.24554578642825906, 0.6301458438731493, 0.5888888888888889, 0.9891483218006352, 0.029580569926963382, 0.06550205464877923, 0.4592359915981675, 0.045769299527240494, 0.44417258687418615, 0.0, 0.3, 0.42998444342213826, 0.3035681546171598, 0.025630811969195744, 0.5066408688157863, 0.39360797743911796, 0.5033321444627795, 0.016368728063802206, 0.10903475775462355, 0.30364374992487564, 0.5230217245166674, 0.007086614173228346, 0.22737294543252423, 0.16214585070621973, 0.0, 0.15173613034766478, 0.16255528919393042, 0.3054225107283263, 0.03752275825165655, 0.2384408830560349, 0.31494175237249583, 0.47330171021453715, 0.40695183474637775, 0.47364268462808856, 0.21591942881784087, 0.28447402838347335, 0.5843871292056362, 0.503442769884647, 0.493447734789995, 0.2748452573534284, 0.0, 0.5986961569043951, 0.6586166130987262, 0.6132366516010306, 0.6000374307659534, 0.5895691537191319, 0.4224886975190719, 0.21878787199904354, 0.8191441569283882, 0.2690672396160939, 0.0857142857142857, 0.10638849227697998, 0.4437291481331651, 0.0005352924795257918, 0.5682185156099486, 0.29666817986616406, 0.11785714285714285, 0.42214075397696665, 0.22386585264008707, 0.22036397216525477, 0.18139372853535654, 0.048336091588109684, 0.8755729917407291, 0.2001020512560049, 0.12664326980523982, 0.027753753904838897, 0.7223747196673285, 0.6033825978341993, 0.6193949501703278, 0.5019459161591509, 0.26770498963919, 0.0, 0.009501963574956813, 0.19898277753317528, 0.016918638755598202, 0.5590480071842594, 0.5521397966708426, 0.30926808771396225, 0.6275381978884832, 0.595344810166174, 0.5706864245336225, 0.3141403292678506, 0.6023428121918077, 0.39999999999999997, 0.04918872709654805, 0.5154139662780648, 0.5557485571882866, 0.6286342463091101, 0.5522267233801039, 0.6402562811128987, 0.05129879162633799, 0.349608916974424, 0.02357226111775114, 0.0, 0.3502721636074362, 0.11534793495124901, 0.41941778183940637, 0.2825450884101482, 0.27733553577352177, 0.2820830174099041, 0.6226459484117071, 0.1908658249273341, 0.6970431590020381, 0.6996949179201964, 0.49694040509490167, 0.9201959058230955, 0.39191341059510876, 0.26534793495124903, 0.762178754714643, 0.023107425138552073, 0.48839502083117775, 0.2974610638433307, 0.21605817970406976, 0.3371770923749503, 0.025835762837694985, 0.1633892888435109, 0.0, 0.20167918995135747, 0.33835733284393976, 0.4774031904523103, 0.23754599371220822, 0.0, 0.09999999999999999, 0.5289052690315463, 0.0, 0.11360174101084311, 7.98350088716072e-05, 0.0, 0.024257700866305763, 0.0, 5.314467731901778e-05, 0.293808526265082, 0.0957354197214469, 0.30463366142805354, 0.21067733574264846, 0.01349072297901379, 0.08990626812650715, 0.024257700866305763, 0.04446766904602689, 0.4227214505158373, 0.34715272376575307, 0.12336167952230911, 0.5214020325976854, 0.20773418884052774, 0.18452984161828984, 0.5517297407464076, 0.5642010748634618, 0.2319361308746954, 0.6006394976554918, 0.30748037327168487, 0.6545676223066557, 0.4100494532758355, 0.204788220424696, 0.6339262873537368, 0.3850581958489265, 0.5484471141096646, 0.42040021690418716, 0.0, 0.0, 0.8821296454239946, 0.0, 0.08161807307473766, 0.09999999999999999, 0.025778403813039506, 0.16686743666964193, 0.7917382743298029, 0.4735560821021707, 0.017282065352619818, 0.045556751283391844, 0.3161555910733458, 0.5752955312511869, 0.8668319730064529, 0.4015503452202127, 0.5550049310474542, 0.02837012085604349, 0.8034443090225696, 0.3401984989439568, 0.6095723049113978, 0.6583332549596508, 0.753934274260637, 0.4496604481293247, 0.1978173833497796, 0.77163491554112, 0.1923370641827312, 0.5972169565554943, 0.0, 0.003222839000738419, 0.4131696708682929, 0.23111024210410877, 0.0, 0.0, 0.4666666666666667, 0.023716663214164827, 0.0, 0.05827005144385346, 0.22980787106664158, 0.3902460953544985, 0.019763940462997478, 0.5289796623092667, 0.0, 0.7147461811855021, 0.0, 0.07325382579799175, 0.7954935638657621, 0.5574837399436665, 0.5841652739512377, 0.3981648195370972, 0.7302007404183433, 0.35669695069353025, 0.1902562811128986, 0.3, 0.3583507427572924, 0.41007593934497333, 0.041527976087742405, 0.0, 0.35914511583035225, 0.2422820653526198, 0.5859699405327176, 0.015545917150946366, 0.5465085296294044, 0.052943066744013924, 0.3703988534694904, 0.38778187550634335, 0.5178989075058493, 0.0, 0.47089347798192616, 0.18308830427191106, 0.732226723380104, 0.1384912834626551, 0.2675641376147264, 0.6727386123049096, 0.5946346888883065, 0.6478359061972442, 0.2402469672601096, 0.18974785833829844, 0.3847960593523654, 0.7338660149773069, 0.579957768381151, 0.20030285796451872, 0.02011833018409796, 0.023716663214164827, 0.051021395467368925, 0.027753753904838897, 0.1431436171945805, 0.6043938999758249, 0.691993801031876, 0.1795070395114115, 0.5251696452952073, 0.4229016277251111, 0.09999999999999999, 0.2636668331849268, 0.37626655075260274, 0.5161480213553387, 0.0, 0.7519671371303185, 0.18176707981316542, 0.0169628003044457, 0.0, 0.851626549630923, 0.22442582411053988, 0.29842087209143564, 0.6182924787290234, 5.314467731901778e-05, 0.0, 0.0, 0.0, 0.6830221864537713, 0.27629879162633797, 0.638161672266765, 0.4921783165925231, 0.8178607431385985, 0.04227956743167918, 0.5738662546272181, 0.09999999999999999, 0.46459392039551944, 0.33956347168367373, 0.14098717440813605, 0.023182513656863794, 0.5977989834099386, 0.4063014373192311, 0.0, 0.5834629474113984, 0.25790629057646974, 0.7516265496309229, 0.24633165493663958, 0.024842797352915724, 0.22195561498007305, 0.573117295162589, 0.53671386479513, 0.3083835209150734, 0.0239248613969033, 0.6785117129535003, 0.4582096230882099, 0.8263615845868633, 0.13987490674543943, 0.2075875976179003, 0.4102973371873174, 0.6517765449003101, 0.21246930115380322, 0.8134046627704687, 5.314467731901778e-05, 0.3139986698995456, 0.5531738067945994, 0.0, 0.3589091280421953, 0.35088308775670685, 0.3188893215136761, 0.5199356524825135, 0.5213774277538294, 0.32653476493049677, 0.06430709861041217, 0.08905779377334148, 5.314467731901778e-05, 0.39760076840214936, 0.49040949836394265, 0.3485667747214658, 0.43779740417726637, 0.6116808931901063, 0.00016112665420783207, 0.09838520749665206, 0.2924479310929941, 0.0, 0.6454311872113995, 0.2064486562573952, 0.5960933390920177, 0.22444334999498228, 0.12282301724136441, 0.3154745710868131, 0.1621787547146431, 0.2385732387044684, 0.4156126280339347, 0.7706847188488577, 0.0, 0.18501923786697255, 0.0, 0.22145077207464214, 0.495796438827278, 0.0, 0.7816330404745347, 0.09849327773326486, 0.25768381604453205, 0.390266747951923, 0.2947551456828678, 0.6392333266349789, 0.36, 0.1781813816569817, 0.0, 0.6068622406997106, 0.5541384249646617, 0.5841664823066837, 0.24850801306021933, 0.0, 0.34650852962940437, 0.48126422583976075, 0.8041213411178856, 0.31250987006368475, 0.11534793495124901, 0.0, 0.10134410176328848, 0.09999999999999999, 0.0, 0.03871683497088112, 0.3535365987861576, 0.6305458068476276, 0.0, 0.021676967357347608, 0.37539209697730247, 0.41858237052278763, 0.0, 0.024257700866305763, 0.6130217245166675, 0.5796605122596801, 0.24272438732934998, 0.033926287353736734, 0.30965610613863465, 0.6371808959162841, 0.0, 0.3068331057660994, 0.2738967889031662, 0.05930710593392066, 0.03750087657269466, 0.1358346408073879, 0.590198049884533, 0.22381136515041503, 0.08771058450497658, 0.49582609707333175, 0.0, 0.22844308057854212, 0.3011733834899897, 0.017733069674444876, 0.3104223873642766, 0.0, 0.0, 0.567427651528984, 0.038792955708243024, 0.12, 0.0948274718875496, 0.48138849227698, 0.2946874342504709, 0.5569993949684375, 0.638875093216129, 0.4400433135071257, 0.02519862247423641, 0.0, 0.3178968529400425, 0.6693492547034694, 0.5843832253439964, 0.7025381978884832, 0.47115588878443293, 0.8207924787290235, 0.24, 0.016368728063802206, 0.3717913183345259, 0.0048530467096568495, 0.31371132702198523, 0.0, 0.5997945194103691, 0.44710627544884013, 0.34082070792517505, 0.4013106022759322, 0.5106803295472991, 0.0, 0.5876610418497374, 0.5801154532856382, 0.37082742015522346, 0.5673986075167254, 0.024821634201246195, 0.3562880317862686, 0.6001712768755635, 0.740283019158056, 0.3166508010405151, 0.04603121926729405, 0.7898131406619953, 0.07944994767545024, 0.0, 0.19773610543037484, 0.11137439980732422, 0.7451384298635613, 0.044582156603577075, 0.787763348025222, 0.25687524347702806, 0.5333333333333333, 0.3268915674386065, 0.6994812245084349, 0.0, 0.5837758819112949, 0.3302333034257401, 0.019067239616093933, 0.6601458438731493, 0.5395616509650316, 0.4964037701554345, 0.2974298380741498, 0.0, 0.05129879162633799, 0.0, 0.4015495294340845, 0.19604562088234212, 0.02616714951760708, 0.00044349831798789265, 0.5921889576017972, 0.6441441569283882, 0.0001325291001054256, 0.0]
Finish training and take 1h1m
