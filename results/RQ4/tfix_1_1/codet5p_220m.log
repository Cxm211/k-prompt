Namespace(log_name='./RQ5/tfix_1_1/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_1_1/codet5p_220m', data_dir='./data/RQ5/tfix_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'const mocks = {};   const providers = this._getProviders(klass);', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'const mocks = {};'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 6.97098560362899e+295
  global_step = 2
  train_loss = 27.2822
  ********************
Previous best ppl:inf
Achieve Best ppl:6.97098560362899e+295
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 10.51 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.51
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.2049583845482367e+262
  global_step = 3
  train_loss = 29.267
  ********************
Previous best ppl:6.97098560362899e+295
Achieve Best ppl:1.2049583845482367e+262
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 11.89 	 Previous best codebleu 10.51
  ********************
 Achieve Best bleu:11.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.3350497285505833e+246
  global_step = 4
  train_loss = 15.2164
  ********************
Previous best ppl:1.2049583845482367e+262
Achieve Best ppl:2.3350497285505833e+246
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 12.04 	 Previous best codebleu 11.89
  ********************
 Achieve Best bleu:12.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 3.044842006132787e+236
  global_step = 5
  train_loss = 5.3641
  ********************
Previous best ppl:2.3350497285505833e+246
Achieve Best ppl:3.044842006132787e+236
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 14.17 	 Previous best codebleu 12.04
  ********************
 Achieve Best bleu:14.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 9.143941924301123e+230
  global_step = 6
  train_loss = 2.8547
  ********************
Previous best ppl:3.044842006132787e+236
Achieve Best ppl:9.143941924301123e+230
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 18.21 	 Previous best codebleu 14.17
  ********************
 Achieve Best bleu:18.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.1176979632630277e+228
  global_step = 7
  train_loss = 1.1676
  ********************
Previous best ppl:9.143941924301123e+230
Achieve Best ppl:2.1176979632630277e+228
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 17.52 	 Previous best codebleu 18.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 6.584591855058039e+227
  global_step = 8
  train_loss = 1.3246
  ********************
Previous best ppl:2.1176979632630277e+228
Achieve Best ppl:6.584591855058039e+227
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 17.41 	 Previous best codebleu 18.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 2.1351571688770296e+227
  global_step = 9
  train_loss = 1.2191
  ********************
Previous best ppl:6.584591855058039e+227
Achieve Best ppl:2.1351571688770296e+227
  ********************
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 15.73 	 Previous best codebleu 18.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 4.850656546174348e+227
  global_step = 10
  train_loss = 0.6978
  ********************
Previous best ppl:2.1351571688770296e+227
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 15.58 	 Previous best codebleu 18.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 9.830062765001655e+227
  global_step = 11
  train_loss = 0.4927
  ********************
Previous best ppl:2.1351571688770296e+227
BLEU file: ./data/RQ5/tfix_1_1/validation.jsonl
  codebleu-4 = 15.69 	 Previous best codebleu 18.21
  ********************
reload model from RQ5/tfix_1_1/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_1/test.jsonl
  codebleu = 19.52 
  Total = 500 
  Exact Fixed = 2 
[36, 324]
  Syntax Fixed = 1 
[163]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 2 
[36, 324]
  Syntax Fixed = 1 
[163]
  Cleaned Fixed = 0 
[]
  codebleu = 19.52 
[0.11430808740958903, 0.06739297594453479, 0.12352941176470587, 0.08805625584493948, 0.09785407725321887, 0.19931377651936205, 0.0, 0.31360309633997624, 0.3113808918527808, 0.5474522513548441, 0.0, 0.0, 0.1296302230243985, 0.5715085296294045, 0.10296574879931644, 0.0, 0.15341955368503776, 0.3954789240122505, 0.2414832199133445, 0.1628145805350596, 0.4188388715436905, 0.0, 0.0, 0.32162734101792473, 0.15559038455279328, 0.016941393187597915, 0.017733069674444876, 0.0036717633034475455, 0.09999999999999999, 0.32087891600520335, 0.3096131664787125, 0.0, 0.36547323066942694, 0.24791466692901357, 0.09865345242030091, 0.6443152851788645, 0.32036591972890327, 0.11886792452830189, 0.6801458438731494, 0.192, 0.005581281460292225, 0.00011791962135693096, 0.06550205464877923, 0.0, 0.1359045310828676, 0.225944605278236, 0.0, 0.19999999999999998, 0.5924171984112052, 0.31592619281608747, 0.2189142593903474, 0.00021872831009389892, 0.24839811520085936, 0.21026759684117993, 0.0922560854697074, 0.35816326530612247, 0.30895924068790276, 0.0, 0.03246351414044086, 0.3316431789848117, 0.1060389311736181, 0.0, 0.15630743586547857, 0.0, 0.308323003870292, 0.1387429521865225, 0.24991781240031058, 0.0, 0.000427045060369156, 0.00010295076461636784, 0.09837708048805191, 0.0002927784551190388, 0.29635719205764605, 0.49645155084739007, 0.30957800517779893, 0.15, 0.13511514369625025, 0.0003245309366620669, 0.48, 0.2799324289075105, 0.0, 0.6000374307659534, 0.00013148204065707646, 0.14786733624963816, 0.00026273174055610113, 0.3036055823621466, 0.10709013068999415, 0.00019960223845056353, 0.020223098488117843, 0.43015794271183555, 0.0, 0.0, 0.29666817986616406, 0.12523717407239682, 0.47054033596240086, 0.178714963812298, 0.19053126677373797, 0.19758544586794674, 0.00012206189139647845, 0.2127781445710658, 0.2913638157468237, 0.007306869695976433, 0.027753753904838897, 0.07892323125569721, 0.32289382091023283, 0.00017563521132934535, 0.3341936473876544, 0.3161020443690249, 0.01276595744680851, 0.0, 0.3235300434157328, 0.016918638755598202, 0.0, 0.10216991320019282, 0.012384617417597872, 0.21640443185715405, 0.3075931090823924, 0.09040611849156499, 0.0, 0.0005467617418688703, 0.16674082803833595, 0.0, 0.12107122698694842, 0.0, 0.04715993567732543, 0.16251354965885878, 0.24478263880887718, 0.22831604141133302, 0.15168302945301543, 0.0003700298393563872, 0.00016666335568142122, 0.16967349061661957, 0.08663256098769224, 0.04537054796666057, 0.29876980484901733, 0.27733553577352177, 0.0, 0.25029591142245644, 0.1908658249273341, 0.25549392052225683, 0.30591315127434204, 0.14936708860759493, 0.3172214963880227, 0.3185027794371409, 0.26770884448420523, 0.30988639280302377, 0.16074402078221195, 0.5982385484272198, 0.14822502598907356, 0.6128250907329116, 0.0, 0.0019716740298856956, 0.1586766399417569, 0.0, 0.1714285714285714, 0.454042748727021, 0.6020516290619171, 0.22769362870893967, 0.0636584993455741, 0.09999999999999999, 0.09999999999999999, 0.0, 0.859642549895431, 0.0, 0.0, 0.0, 0.0, 0.20024785134307393, 0.0, 0.06814676498581414, 0.0, 0.0, 0.0, 0.19894736842105262, 0.4657132877459149, 0.29830794173495123, 0.004378412683792302, 0.27551916481171673, 0.5988637349358589, 0.0003123039576283516, 0.0, 0.0001541682061646785, 0.30913628016288763, 0.5642010748634618, 0.24408394676541514, 0.5957597173144876, 0.01764705882352941, 0.21719334842008858, 0.4687620664592671, 0.24789745580198796, 0.5748885515324513, 0.0, 0.20503899308767218, 0.11645035127067777, 0.0375, 0.14780058651026393, 0.3050014955423776, 0.1488307291381638, 0.08270496322526374, 0.5854666616570587, 0.03194910810933169, 0.2150213870279966, 0.7584071224059139, 0.5051071728012787, 0.0, 0.08648239049004486, 0.3161555910733458, 0.20407303891830011, 0.48, 0.5763616530170377, 0.4367092391893936, 0.5955393495773617, 0.09975607434484454, 0.30043193352111575, 0.625500606374912, 0.5894784114123974, 0.3127024805005999, 0.213305758981656, 0.0001756352113293001, 0.31179332729704934, 0.0, 0.2987277004789052, 0.0, 0.0, 0.06507936507936507, 0.0002626270208350616, 0.0, 0.0001607678629835902, 0.5189224444290103, 0.3074338797807819, 0.0, 0.29919122007474863, 0.17019964100442334, 0.5124567555628179, 0.014223048083196929, 0.24356671485091777, 0.012057068118888138, 0.19999999999999998, 0.04, 0.07105892134906477, 0.26871656271449884, 0.1369186387555982, 0.47727513834380847, 0.2929498187974932, 0.29239518646529405, 0.44388503494718445, 0.3018733389315436, 0.0, 0.021038062026745387, 0.2805203967531912, 0.04132804354908049, 0.0, 0.20150028579297374, 0.0, 0.44605041769595555, 0.15, 0.0, 0.10049846718982361, 0.0, 0.7568864709786152, 0.06795946734079411, 0.051970164092929975, 0.18383253090961824, 0.4568863270934961, 0.37904078898403104, 0.10011471414046402, 0.14687295114072998, 0.2685893103473356, 0.13983806434427754, 0.0646756164206393, 0.3740886447991165, 0.02367658423843684, 0.0004256311811669479, 0.20936959695647264, 0.2383781930763583, 0.012108979227139442, 0.2539952783614877, 0.8942213309763742, 0.0692468751175076, 0.003844588975930807, 0.17371878541303407, 0.6043938999758249, 0.31487635177880996, 0.1795070395114115, 0.12212878138062155, 0.01661942668662242, 0.15062674750040012, 0.13446823337493782, 0.0, 0.174223183119083, 0.0, 0.10628113650377972, 0.0121615667040512, 0.2204695655754227, 0.002553857010371636, 0.30679743836263906, 0.22213947261732414, 0.29696315352996716, 0.47180509653022784, 0.0031704048992569223, 0.0, 0.0001607678629835902, 0.20357142857142857, 0.3277347201895753, 0.18371490639998916, 0.0, 0.06862555578785981, 0.8178607431385985, 0.0, 0.5387085408473816, 0.09999999999999999, 0.0, 0.00023933018700023382, 0.2885201468394608, 0.2073822309724977, 0.3060471127454014, 0.11851851851851851, 0.0, 0.07061088677752592, 0.3073485116967814, 0.20783625026922903, 0.2480328289519611, 0.2773979240446927, 1.0, 0.06375317832866793, 0.008102394410850257, 0.2059023676701889, 0.1658835209150734, 0.6785117129535003, 0.29166666666666663, 0.31260290515480327, 0.16705172407741253, 0.0033017273357446157, 0.00027795618310029214, 0.2193163731011582, 0.00033069292239914925, 0.692426316104594, 0.0, 0.0, 0.01787974724018318, 0.0, 0.0, 0.08445687412729594, 0.0, 0.0, 0.29774002127036664, 0.19377418638278932, 0.0793991106335148, 0.32790670425413626, 0.12179140591536411, 0.433467983793827, 0.1012122475603591, 0.20201329249055716, 0.43779740417726637, 0.317352027954263, 0.30660500998674445, 0.1088912042100752, 0.2339850712266108, 0.0015011775220336286, 0.43260626848932227, 0.12033094852558306, 0.600695178570928, 0.26640836886978825, 0.27785389351744516, 0.08778933871680618, 0.0006297404430996138, 0.35936577542188963, 0.004847440564664679, 0.3063905709298863, 0.08688381745416643, 0.024551278020535437, 0.0036974541643011198, 0.39999999999999997, 0.00015078528776688377, 0.0778792997295584, 0.6250881277610704, 0.10331797539623118, 0.125551345095689, 0.1818046026636264, 0.26084117740843565, 0.24361720610574433, 0.23294877465869998, 0.2358598417333303, 0.0857142857142857, 0.0, 0.06, 0.5734966750506927, 0.2620452353767158, 0.0, 0.0, 0.024682322917633885, 0.6141403437512603, 0.2989937973719289, 0.29588305276971794, 0.010630925715041078, 0.10188895207707419, 0.11666666666666667, 0.0, 0.08974689287269585, 0.09921068908930947, 0.2977241835688832, 0.3069866859203241, 0.11662732289249841, 0.2052958346333183, 0.2645525320029233, 0.0, 0.12425770086630575, 0.4148432101203152, 0.30467608712290867, 0.5923156634621292, 0.30426979448949765, 0.01567246452625196, 0.3574154512273938, 0.1287057203490611, 0.30946734852416985, 0.20161146622865617, 0.15315904667269403, 0.063668594430246, 0.0, 0.24164233047008277, 0.01714285714285714, 0.0, 0.09793055710016017, 0.2063834415327773, 0.5155117664319994, 0.5665193305637899, 0.13647286842101305, 0.19593719412520205, 0.08442211055276382, 0.0001032223960568568, 0.3731856393377419, 0.0, 0.5957746478873239, 0.30963957504414014, 0.48138849227698, 0.06885245901639345, 0.139952673726547, 0.1699574388678951, 0.5005599841570081, 0.10849232684755782, 0.525, 0.12766112296108723, 0.008944389469537819, 0.12, 0.5983675102126031, 0.0, 0.8207924787290235, 0.00015430283827442333, 0.14588952186229948, 0.1418485372364422, 0.1960500801300971, 0.13156561516067397, 0.0002314237119568846, 0.15416001096334603, 0.00010326205928006011, 0.17185846911819055, 0.3983874115189844, 0.22904075797337742, 0.4706490889030359, 0.19632055817658567, 0.4642724003553169, 0.00013920763462003908, 0.5549192578530084, 0.046617042897205294, 0.3876703150020759, 0.5768364646206958, 0.48344603119593504, 0.15016346098339423, 0.0, 0.0, 0.0057445853655751205, 0.0, 0.0, 0.04537484171880182, 0.7191441569283882, 0.03703132469143918, 0.3629110344645362, 0.3, 0.37847902139030243, 0.23962976976416467, 0.8403392014409831, 0.0, 0.49796822608646785, 0.25071475545264305, 0.3, 0.2618641802606828, 0.0, 0.0934036678985382, 0.14933333333333332, 0.0, 0.0, 0.10666666666666666, 0.0048, 0.0, 0.3063595873017688, 0.3049343284341335, 0.0, 0.13353140916808148, 0.19702017956607282, 0.040836043100379485]
Finish training and take 47m
