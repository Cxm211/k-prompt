Namespace(log_name='./RQ5/sstubs_16_1/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_16_1/codet5p_220m_f', data_dir='./data/RQ5/sstubs_16_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00232
  global_step = 3
  train_loss = 0.9563
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00232
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 9.26 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.0014
  global_step = 5
  train_loss = 0.6037
  ********************
Previous best ppl:1.00232
Achieve Best ppl:1.0014
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 32.9 	 Previous best codebleu 9.26
  ********************
 Achieve Best bleu:32.9
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00101
  global_step = 7
  train_loss = 0.3206
  ********************
Previous best ppl:1.0014
Achieve Best ppl:1.00101
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 55.13 	 Previous best codebleu 32.9
  ********************
 Achieve Best bleu:55.13
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00081
  global_step = 9
  train_loss = 0.2058
  ********************
Previous best ppl:1.00101
Achieve Best ppl:1.00081
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 85.72 	 Previous best codebleu 55.13
  ********************
 Achieve Best bleu:85.72
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00074
  global_step = 11
  train_loss = 0.1802
  ********************
Previous best ppl:1.00081
Achieve Best ppl:1.00074
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 88.24 	 Previous best codebleu 85.72
  ********************
 Achieve Best bleu:88.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00069
  global_step = 13
  train_loss = 0.1158
  ********************
Previous best ppl:1.00074
Achieve Best ppl:1.00069
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 90.25 	 Previous best codebleu 88.24
  ********************
 Achieve Best bleu:90.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00066
  global_step = 15
  train_loss = 0.0811
  ********************
Previous best ppl:1.00069
Achieve Best ppl:1.00066
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 91.14 	 Previous best codebleu 90.25
  ********************
 Achieve Best bleu:91.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00064
  global_step = 17
  train_loss = 0.0843
  ********************
Previous best ppl:1.00066
Achieve Best ppl:1.00064
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 91.18 	 Previous best codebleu 91.14
  ********************
 Achieve Best bleu:91.18
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00064
  global_step = 19
  train_loss = 0.0629
  ********************
Previous best ppl:1.00064
Achieve Best ppl:1.00064
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 91.22 	 Previous best codebleu 91.18
  ********************
 Achieve Best bleu:91.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00064
  global_step = 21
  train_loss = 0.0669
  ********************
Previous best ppl:1.00064
Achieve Best ppl:1.00064
  ********************
BLEU file: ./data/RQ5/sstubs_16_1/validation.jsonl
  codebleu-4 = 90.66 	 Previous best codebleu 91.22
  ********************
reload model from RQ5/sstubs_16_1/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_16_1/test.jsonl
  codebleu = 90.88 
  Total = 500 
  Exact Fixed = 155 
[1, 3, 7, 8, 9, 10, 11, 14, 18, 20, 21, 23, 25, 32, 34, 35, 36, 37, 42, 47, 48, 49, 52, 55, 56, 58, 62, 63, 64, 65, 69, 75, 82, 87, 88, 92, 96, 101, 103, 105, 110, 112, 113, 114, 125, 126, 127, 129, 130, 131, 135, 136, 138, 139, 141, 145, 148, 149, 153, 154, 155, 160, 170, 173, 176, 183, 186, 188, 192, 196, 198, 204, 205, 209, 214, 215, 218, 221, 228, 229, 230, 231, 232, 233, 238, 240, 241, 242, 247, 248, 249, 250, 252, 259, 266, 267, 268, 270, 283, 287, 291, 295, 298, 308, 311, 316, 318, 321, 322, 323, 327, 333, 340, 342, 343, 345, 350, 359, 360, 369, 370, 379, 383, 385, 389, 394, 395, 400, 401, 411, 417, 422, 423, 427, 428, 438, 442, 448, 451, 452, 454, 455, 459, 462, 465, 467, 469, 475, 477, 480, 487, 488, 492, 494, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 155 
[1, 3, 7, 8, 9, 10, 11, 14, 18, 20, 21, 23, 25, 32, 34, 35, 36, 37, 42, 47, 48, 49, 52, 55, 56, 58, 62, 63, 64, 65, 69, 75, 82, 87, 88, 92, 96, 101, 103, 105, 110, 112, 113, 114, 125, 126, 127, 129, 130, 131, 135, 136, 138, 139, 141, 145, 148, 149, 153, 154, 155, 160, 170, 173, 176, 183, 186, 188, 192, 196, 198, 204, 205, 209, 214, 215, 218, 221, 228, 229, 230, 231, 232, 233, 238, 240, 241, 242, 247, 248, 249, 250, 252, 259, 266, 267, 268, 270, 283, 287, 291, 295, 298, 308, 311, 316, 318, 321, 322, 323, 327, 333, 340, 342, 343, 345, 350, 359, 360, 369, 370, 379, 383, 385, 389, 394, 395, 400, 401, 411, 417, 422, 423, 427, 428, 438, 442, 448, 451, 452, 454, 455, 459, 462, 465, 467, 469, 475, 477, 480, 487, 488, 492, 494, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 90.88 
[1.0, 0.9471695160234401, 1.0, 0.6239209719978497, 0.9161007909599712, 0.9636834829996033, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8748733743967635, 0.9167248063681421, 1.0, 0.936752055107628, 0.9485932253610538, 0.966410184624177, 1.0, 0.9176675865658077, 1.0, 1.0, 0.9147581010783108, 1.0, 0.9621391018662697, 1.0, 0.7010984251778435, 0.5013611784443888, 0.8892742146930692, 0.8665518607362566, 0.7255297089640285, 0.8397142137521943, 1.0, 0.7099766481408452, 1.0, 1.0, 1.0, 1.0, 0.9272945586059962, 0.966410184624177, 0.9097612564056814, 0.9106965620392573, 1.0, 0.8641939857382575, 0.85592659342388, 0.9633425185386058, 0.9471695160234401, 1.0, 1.0, 1.0, 0.8472999580048346, 0.9261420415269246, 1.0, 0.8362511196423215, 0.9435321610524647, 1.0, 1.0, 0.8489269688168015, 1.0, 0.8892742146930692, 0.37570238193981387, 0.939529666001018, 1.0, 1.0, 1.0, 1.0, 0.8897268293826727, 0.9600948805844232, 0.8961079319106557, 1.0, 0.939529666001018, 0.9544887439837764, 0.34967742302943955, 0.9600553834135903, 0.7503432335009788, 1.0, 0.9448987396548947, 0.3120073356811303, 0.9583676774082095, 0.8748733743967635, 0.9647989417148168, 0.85971075445938, 1.0, 0.9600553834135903, 0.86944691495958, 0.939529666001018, 0.9557161384210198, 1.0, 1.0, 0.9629467492522685, 0.9322236811233653, 0.8131726792272671, 1.0, 0.9633425185386058, 0.8989289901708486, 0.85592659342388, 1.0, 0.966410184624177, 0.9323598782946632, 0.8246259405315153, 0.9471695160234401, 1.0, 0.9495893815152074, 1.0, 0.8748733743967635, 1.0, 0.966410184624177, 0.8845572237610086, 0.970114879479065, 0.9474120132649901, 1.0, 0.9498733743967636, 1.0, 1.0, 1.0, 0.8688649632416632, 0.8397142137521943, 0.9364480358382332, 0.966410184624177, 0.936923058378295, 0.9474120132649901, 0.6106880800400042, 0.9432240307282556, 0.963531800540206, 0.8387796716807858, 1.0, 1.0, 1.0, 0.9474120132649901, 1.0, 1.0, 1.0, 0.9427895563342799, 0.9733211546063425, 0.9469728550743763, 1.0, 1.0, 0.9524183912964097, 1.0, 1.0, 0.9683839208477296, 1.0, 0.9326807851263892, 0.8703511904239886, 0.966410184624177, 1.0, 0.9019927469706532, 0.37069984249744314, 1.0, 1.0, 0.8611244489221688, 0.85041351003767, 0.8837843933003711, 1.0, 1.0, 1.0, 0.9026614932119538, 0.7523098682783194, 0.909624266823112, 0.8806751756510949, 1.0, 0.928815027869274, 0.8840834056802387, 0.9211131842675646, 0.8617520551076281, 0.966410184624177, 0.7255297089640285, 0.936752055107628, 0.9498733743967636, 0.9322236811233653, 1.0, 0.9587753838819759, 0.9683839208477296, 1.0, 0.9439975336408695, 0.7876421850012838, 1.0, 0.966410184624177, 0.9584627887351178, 0.9466595714446484, 0.9267516766457782, 0.38522094669307166, 0.9044642401339957, 1.0, 0.8722059305904497, 0.9530008025801151, 1.0, 0.9546221339246326, 1.0, 0.8664266028701744, 0.807689953438363, 0.3178191949219501, 1.0, 0.9581095565874824, 0.8439975336408694, 0.8665471473605286, 1.0, 0.966410184624177, 1.0, 0.7831318837364406, 0.9564084233825603, 0.9425989564545196, 0.942891989734737, 0.9485525934306429, 1.0, 1.0, 0.86944691495958, 0.9312991012532417, 0.9600948805844232, 1.0, 0.769122578011458, 0.9485525934306429, 0.26767978785751767, 0.8298603920162009, 1.0, 1.0, 0.9141720281459855, 0.8301167059600973, 1.0, 0.9583676774082095, 0.8846386294351021, 1.0, 0.9656973569421463, 0.9407613352737074, 0.2837389146931749, 0.7590096946794502, 0.936752055107628, 0.9188441984944269, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.884777214048037, 0.9725011474210121, 0.9498733743967636, 0.9683839208477296, 1.0, 0.966410184624177, 1.0, 1.0, 1.0, 0.884137748772268, 0.8222725521291268, 0.9325400085710811, 0.939529666001018, 1.0, 1.0, 1.0, 1.0, 0.9372561368734473, 1.0, 0.9600553834135903, 0.9161007909599712, 0.966410184624177, 0.9647989417148168, 0.9495565333799167, 0.9583676774082095, 1.0, 0.8531462888634065, 0.8404979526305214, 0.9683839208477296, 0.8837843933003711, 0.3556507307140955, 0.9026614932119538, 1.0, 1.0, 1.0, 0.8212636043254029, 1.0, 0.949068802064335, 0.8818900512120234, 0.6106880800400042, 0.907357971193316, 0.8989289901708486, 0.9213836287468253, 0.8821296454239946, 0.9323598782946632, 0.9608514269956967, 0.9600553834135903, 0.8721212086381336, 0.8791028584168332, 1.0, 0.8648103351245982, 0.9466595714446484, 0.9535798190943583, 1.0, 0.9565292378112795, 0.6738534846527497, 0.7016265496309229, 1.0, 0.8947836446510709, 0.8626149393966556, 0.2550666900880152, 1.0, 0.9439975336408695, 0.9543920430664012, 1.0, 0.6894411759140056, 0.8128907423396801, 0.9601466755014141, 0.9238877689380496, 0.9332624127367197, 0.9155105345510666, 0.8721507316445842, 0.9683839208477296, 0.8748733743967635, 1.0, 0.9056583090096291, 0.7026615485512439, 1.0, 0.9495893815152074, 0.7418554698250603, 0.9637364956234171, 0.9647268293826727, 1.0, 0.8784503390575601, 1.0, 0.8837843933003711, 0.9211131842675646, 1.0, 1.0, 1.0, 0.8056751756510949, 0.8989289901708486, 0.8953661348226227, 1.0, 0.9583676774082095, 0.9810075923400849, 0.9468351265733028, 0.35655917929440784, 0.9056583090096291, 1.0, 0.8215803485831588, 0.9474120132649901, 0.8952938350960071, 0.966410184624177, 0.8665471473605286, 0.889134255278525, 1.0, 0.9357237009636146, 1.0, 1.0, 0.9608514269956967, 1.0, 0.8837843933003711, 0.9161007909599712, 0.8667343878768172, 0.966664755877954, 1.0, 0.8400110594997428, 0.8748733743967635, 0.8867711748283984, 0.9698138929646263, 0.9026614932119538, 0.9493239030379825, 0.9583676774082095, 0.9114994402938219, 1.0, 1.0, 0.9370576952964131, 0.8573835536643046, 0.9443755095409017, 0.9041734668686527, 0.9370576952964131, 0.9465864294209791, 0.6665435634552244, 0.8576767641936891, 1.0, 1.0, 0.9323598782946632, 0.9472873156881443, 0.9466595714446484, 0.9287546369958737, 0.966410184624177, 0.9441518237028776, 0.19608255854493645, 0.6738534846527497, 1.0, 0.7016265496309229, 0.8910165087857704, 0.9525078218537795, 1.0, 0.8684929871970886, 1.0, 0.8806751756510949, 0.8045788816545087, 0.8648103351245982, 1.0, 0.9680975247490764, 0.7942596682976488, 0.6851513300845844, 0.3556507307140955, 1.0, 1.0, 0.8083676774082095, 0.9207265089204273, 0.940598945729278, 0.8748733743967635, 1.0, 1.0, 0.9583676774082095, 0.9321212086381336, 0.940101844638672, 0.8535922152441364, 0.8667343878768172, 0.966410184624177, 0.9470228248649237, 0.9141000841539659, 0.8821296454239946, 1.0, 0.8642470965743208, 0.9309949427736264, 0.9441518237028776, 0.9322236811233653, 0.9059642451028274, 1.0, 0.9321212086381336, 0.9523104352365284, 0.673106131305089, 0.9500330416345475, 1.0, 1.0, 0.8212347889887717, 0.9357237009636146, 0.8119957547956069, 1.0, 1.0, 0.19978072633514465, 0.7810580026281693, 0.8535087661549325, 0.7877842946802818, 0.9633425185386058, 0.8325607737852788, 0.9215834056802388, 0.9584627887351178, 0.936752055107628, 1.0, 0.957982876787582, 0.9693261143428569, 0.9354021130737269, 1.0, 0.9085309210098864, 0.9273494066519052, 0.9583676774082095, 0.8661603549963781, 0.9215834056802388, 1.0, 0.9500330416345475, 0.9497338034253828, 1.0, 1.0, 0.8655056700329364, 1.0, 1.0, 0.22365745702062095, 0.8716886379064808, 0.9565292378112795, 1.0, 0.8984375659602104, 0.8212347889887717, 1.0, 0.966410184624177, 0.8816015466501335, 1.0, 0.8837843933003711, 1.0, 0.8999789643332932, 1.0, 0.966410184624177, 0.829922431649786, 0.24392384002859482, 0.9468351265733028, 0.862347908057336, 1.0, 0.8874178021448205, 1.0, 0.9099697272423548, 0.8648103351245982, 1.0, 0.7486246705124127, 0.9141000841539659, 0.9546221339246326, 0.7942596682976488, 0.8535087661549325, 0.7345176572401336, 1.0, 1.0, 0.9207265089204273, 0.8311846501490564, 0.8871606681680837, 1.0, 0.9562237378841005, 1.0, 0.2970439349479645, 0.9636834829996033, 0.9608514269956967, 1.0, 0.8463836287468253, 0.940101844638672]
Finish training and take 51m
