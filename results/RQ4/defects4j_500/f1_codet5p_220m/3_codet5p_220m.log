Namespace(log_name='./RQ5/java_1/3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_1/3_codet5p_220m', data_dir='./data/RQ5/java_1_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'public int METHOD_1 ( java.lang.String VAR_1 ) { if ( METHOD_2 ( VAR_2 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_2 ) ; } else if ( METHOD_2 ( VAR_3 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_2 ) ; } else { return - 1 ; } }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'public int METHOD_1 ( java.lang.String VAR_1 ) { if ( METHOD_2 ( VAR_2 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_2 ) ; } else if ( METHOD_2 ( VAR_3 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_3 ) ; } else { return - 1 ; } }'}]
***** Running training *****
  Num examples = 1
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 36.2037
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 11.91 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:11.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 33.2713
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 37.76 	 Previous best codebleu 11.91
  ********************
 Achieve Best bleu:37.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 11.8845
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 57.48 	 Previous best codebleu 37.76
  ********************
 Achieve Best bleu:57.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 7.6918
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 66.59 	 Previous best codebleu 57.48
  ********************
 Achieve Best bleu:66.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 3.6217
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 69.98 	 Previous best codebleu 66.59
  ********************
 Achieve Best bleu:69.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 1.6201
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 69.16 	 Previous best codebleu 69.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 1.3109
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 68.63 	 Previous best codebleu 69.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 1.0427
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 67.71 	 Previous best codebleu 69.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.3737
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 67.84 	 Previous best codebleu 69.98
  ********************
early stopping!!!
reload model from RQ5/java_1/3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_3/test.jsonl
  codebleu = 68.2 
  Total = 500 
  Exact Fixed = 1 
[218]
  Syntax Fixed = 2 
[305, 321]
  Cleaned Fixed = 2 
[201, 468]
  ********************
  Total = 500 
  Exact Fixed = 1 
[218]
  Syntax Fixed = 2 
[305, 321]
  Cleaned Fixed = 2 
[201, 468]
  codebleu = 68.2 
[0.7016265496309229, 0.28107794704380523, 0.8340268246875062, 0.5193496366726691, 0.8267699721374209, 0.8832645658234154, 0.1865279123836416, 0.683214548174067, 0.3068923552159469, 0.7914729241068532, 0.9685944421474288, 0.7844686951789657, 0.6726874055789076, 0.340478812971107, 0.8891483936806484, 0.7401245450748448, 0.7139133850187603, 0.8994542256818958, 0.9125388104408019, 0.7620051869492858, 0.553666682084405, 0.11974978908684726, 0.5236890640788627, 0.8456347740815513, 0.43042978422632594, 0.8885258303273453, 0.8046115709504509, 0.6969720917484027, 0.614179329953796, 0.7984153870943118, 0.7670508564197556, 0.8459638589187009, 0.7307014928731188, 0.10084329462047004, 0.465715924844207, 0.31684397493217825, 0.5216314505817867, 0.8656906162781047, 0.38137827535493424, 0.7443217176118664, 0.04593423274891765, 0.3481897896395453, 0.7248873978129828, 0.8024472175773636, 0.38495597035673645, 0.1983598378199884, 0.35108908874636835, 0.9499441739165289, 0.3351197239831757, 0.7366535801418781, 0.9132838178523854, 0.29770587113560304, 0.4522466125677983, 0.5870151495012657, 0.21671443639335153, 0.5677486337058945, 0.6607340162383966, 0.8967648741334229, 0.1865279123836416, 0.7076870861137117, 0.6705684495443676, 0.7684522599138932, 0.9094559456673585, 0.6592360684959336, 0.7142162217032995, 0.9200840875608649, 0.8888025238532258, 0.8397854292144189, 0.7348314873346378, 0.02763157894736842, 0.8875485500759485, 0.226381746312505, 0.8963926483875864, 0.3481897896395453, 0.9337703133336674, 0.783597626465771, 0.7159781931627389, 0.5422875137889253, 0.8702802013900561, 0.8770064456821773, 0.7529109419180047, 0.8445543415224628, 0.7456522123731518, 0.8880530056284941, 0.8207548439274406, 0.7519894248327487, 0.7198847583339956, 0.9197682751991088, 0.8919703332756499, 0.781753788318819, 0.9460043871126994, 0.20698218604443605, 0.45097578550295725, 0.9535662529996234, 0.8797846445528494, 0.197973681752254, 0.7509224706367625, 0.5345117342866947, 0.8696882992240296, 0.8118363689467534, 0.6916772828650979, 0.3182181577804642, 0.8727349390212575, 0.884057334808475, 0.8458128892403921, 0.8360808958555175, 0.5280706734117224, 0.8656764088044264, 0.7720861332093774, 0.837841768268662, 0.33106165056840764, 0.35108908874636835, 0.931919905317208, 0.42643513314307024, 0.898122990476288, 0.48120211948077907, 0.8947233410879221, 0.9600094284589669, 0.5084641037343476, 0.17706030780697368, 0.7827682425239718, 0.9418090107609731, 0.8765208719204487, 0.8898219523517614, 0.8801022062561152, 0.7037147467795737, 0.7222101240924401, 0.8884983816805294, 0.48666825572275857, 0.3481897896395453, 0.5236890640788627, 0.9046264534870987, 0.3603598562199231, 0.8797177060998733, 0.9007752817631554, 0.7535398874159157, 0.8504956338363896, 0.5236890640788627, 0.8275239936524709, 0.6608077004724038, 0.8800080726737608, 0.7898482563516439, 0.06183873250845704, 0.023376623376623374, 0.8555613027817166, 0.7117639795515854, 0.9584247004495543, 0.6268924744954462, 0.871607281170603, 0.2304841428129761, 0.19281627107934263, 0.27541708339604765, 0.3866290257756912, 0.7583556269641405, 0.9120346801345012, 0.21671443639335153, 0.47389258290112246, 0.8790598425389664, 0.6550690221321874, 0.8721459350488947, 0.33122462995133084, 0.7119542043774734, 0.7483721189663038, 0.8561383342449158, 0.8151060177774252, 0.8437228863514427, 0.6973583055150055, 0.33949163365899815, 0.83084241467313, 0.11368277596865825, 0.9041340728227794, 0.8566728262457479, 0.840332784537337, 0.8437098534857552, 0.7922098978133367, 0.3179324540571661, 0.8024337693400394, 0.9214852005355179, 0.6106094336880674, 0.12333734485661148, 0.5236890640788627, 0.9114698188792376, 0.5033623603791932, 0.8867171706601364, 0.814665991132224, 1.0, 0.38495597035673645, 0.7673529371894313, 0.7048910897361526, 0.591651271701471, 0.22290140843325684, 0.8273870442727397, 0.798371529622017, 0.920781504026952, 0.8586435655918875, 0.6043703390638921, 0.43042978422632594, 0.06076844377839509, 0.034482758620689655, 0.7170508989263651, 0.9666641504621953, 0.9072725521291267, 0.8579007606773206, 0.7980286087413677, 0.5870054509531217, 0.9165318450152771, 0.8771455158794916, 0.8905614928708727, 0.6693212619667522, 0.2668205416905448, 0.761642782903787, 0.9734190821154947, 0.6349052428550055, 0.847448057746049, 0.19970918105869334, 0.054177168907731124, 0.6916946644043783, 1.0, 0.7756074530504335, 0.801169789534145, 0.7162570708021344, 0.5236890640788627, 0.7703813467645078, 0.8401120225715946, 0.95, 0.5236890640788627, 0.507673966802249, 0.4219310110502662, 0.7906660057247482, 0.35108908874636835, 0.9288817233176625, 0.7324599864178296, 0.8392038645056993, 0.8109195557573879, 0.7168549922570001, 0.7737581491561205, 0.5236890640788627, 0.7518298018621397, 0.3090196912573707, 0.8716059812352219, 0.31684397493217825, 0.38495597035673645, 0.8446350376513752, 0.673043248258003, 0.7786486883269359, 0.8925983506076327, 0.7624363859265285, 0.5236890640788627, 0.38621824475057503, 0.6577905250945815, 0.9006529480891606, 0.8262002123809677, 0.8426901254499688, 0.9173467324769831, 0.8061270356070682, 0.5725386401319184, 0.7235716998691815, 0.8473249685745358, 0.20154523491511608, 0.5786041801815426, 0.14264063944784805, 0.9273494742946413, 0.8998105306265591, 0.8882492511578264, 0.5721197179537898, 0.41613614524528547, 0.8125019044888795, 0.7263172003237758, 0.8646780713427795, 0.5236890640788627, 0.5859310319660064, 0.8555116843287287, 0.4219310110502662, 0.2759512266830104, 0.9131034995733858, 0.9467466074989312, 0.6413636843809638, 0.36646403983495457, 0.30793433386770624, 0.24368647767037135, 0.9005633971565483, 0.007539305846310115, 0.9310082691888477, 0.9396180092663717, 0.6244470659845913, 0.30793433386770624, 0.2080384190182858, 0.8998198978007597, 0.7976758294149948, 0.9351406123825403, 0.19970918105869334, 0.8927948046039869, 0.05173365210629404, 0.28591502592370804, 0.5460092070465667, 0.8732109158538904, 0.69250267449679, 0.6380158677472638, 0.9009470942934135, 0.813346886156975, 0.9110792360207141, 0.602746669476317, 0.0067415730337078645, 0.9427160389347342, 0.8488842319816712, 0.49303107378908023, 0.8477777451744541, 0.7181717668725522, 0.8625908243213386, 0.4788293436623317, 0.8242642610452723, 0.7752152659333222, 0.7932369845589845, 0.33379092738587857, 0.3158829552783204, 0.9142676834131117, 0.6494526326090735, 0.7459865312447075, 0.6794213988320216, 0.8624584208490358, 0.9640944874279176, 0.7944332720907786, 0.7930087587536491, 0.7678120156615096, 0.7001395411599194, 0.8026698676956595, 0.7892925066226937, 0.7986327663194268, 0.5236890640788627, 0.3126686192996407, 0.6296953325222, 0.9460006271412542, 0.824816099393142, 0.651002857872442, 0.8474115800257538, 0.465715924844207, 0.8979437414751337, 0.8411630424921515, 0.554547404131932, 0.18687365772359898, 0.5544233577589087, 0.8445769542030257, 0.7860196529322446, 0.909841641757285, 0.78107413952646, 0.941246887425869, 0.3597554292633871, 0.5236890640788627, 0.9161769176090371, 0.8103272093232525, 0.8599346112722639, 0.7699865001847461, 0.8870842725124295, 0.3726385660511553, 0.27935439804151907, 0.9388483168305821, 0.931600566509587, 0.7850292472789528, 0.9491717357274025, 0.7174184525809184, 0.9246745816794821, 0.8598175150774516, 0.3566124490189153, 0.8624523538642554, 0.4522466125677983, 0.9462525187090187, 0.863991542491233, 0.7863191026156303, 0.852734238473497, 0.8057424912814082, 0.9347001488455178, 0.04074074074074074, 0.5695758149747698, 0.7345719191061237, 0.8920926518641143, 0.3361664691432363, 0.3569483179211176, 0.7577430450542659, 0.9973062440915319, 0.9041459343852125, 0.896457542865793, 0.9120050406373192, 0.8091751557449649, 0.5120811634252873, 0.3261785836564687, 0.6953305068583024, 0.9684326167255213, 0.7964581754092617, 0.9496805564620665, 0.8256495113593998, 0.661290154869841, 0.6600795148566396, 0.8867137105763065, 0.8791292917271873, 0.8081155417588575, 0.7927590354299714, 0.8271286255071677, 0.8913427268916696, 0.6643205062170019, 0.8469230325565015, 0.7453456739763825, 0.7135449662005422, 0.6491456652079538, 0.9093270194127461, 0.8278488581694263, 0.7158102913639441, 0.7330975185316592, 0.8419378306852605, 0.9136051415192505, 0.8917836304602649, 0.1695947146866787, 0.8016167679986987, 0.6640761932093434, 0.6806597341015269, 0.22073125221934192, 0.9442100823718134, 0.6504513879970075, 0.8416214188577094, 0.857637656447326, 0.964963853850137, 0.034482758620689655, 0.8807721515676248, 0.8521745072771806, 0.9291383632772701, 0.03837209302325582, 0.8446408413227882, 0.8518391018422911, 0.5688952581332704, 0.7597272680241807, 0.7478266445785727, 0.7407928163996362, 0.481896704868353, 0.8097601341734098, 0.8870469708534579, 0.591651271701471, 0.11520227389825596, 0.86944691495958, 0.8465073862764094, 0.5361403804345419, 0.8809786623364806, 0.9311097802383854, 0.9350024698180224, 0.895540655184323, 0.38495597035673645, 0.8388030980586192, 0.7588104426782636, 0.07625371261515583, 0.7397663709114269, 0.7670599061274495, 0.8646882086291592, 0.8810554316276455, 0.6708098654023111, 0.7138844849530721, 0.7186453583876734, 0.7745800814888362, 0.2629050754742709, 0.876773525924682, 0.5236890640788627, 0.49065307510436884, 0.6252950966319775, 0.9133731397684426, 0.6991010059666837, 0.6978162276442237, 0.7522642454492299, 0.7144990029774533, 0.9272271558703311, 0.8924875728510193, 0.9411738481972705, 0.8004545807508505, 0.827378993226273, 0.9210210949858122, 0.9523560037191996, 0.6081309157897818, 0.8084791144465647, 0.29093634073934205, 0.7724155016374759, 0.8613514421762498, 0.904352113990756, 0.9117215889741954, 0.3272919869064299, 0.8519599687267062, 0.3154531188983089, 0.9083337899093722, 0.680720845064619, 0.7358928210748675, 0.862200487175184, 0.9449149097946257, 0.9069546174929648, 0.4219310110502662, 0.858117394716224, 0.38495597035673645, 0.4495970835007003, 0.8385611735106955, 0.814295430262091, 0.8514674628624775, 0.7984646361230248, 0.31649371355996003, 0.9122540568597661, 0.9196253470914761, 0.5236890640788627]
Finish training and take 49m
