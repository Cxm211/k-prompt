Namespace(log_name='./RQ5/java_10/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/1_codet5p_770m', data_dir='./data/RQ5/java_10_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
Namespace(log_name='./RQ5/java_10/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/1_codet5p_770m', data_dir='./data/RQ5/java_10_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Namespace(log_name='./RQ5/java_10/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/1_codet5p_770m', data_dir='./data/RQ5/java_10_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'private static < TYPE_1 > java.util.List < TYPE_1 > METHOD_1 ( TYPE_1 x , java.util.List < TYPE_1 > VAR_1 ) { java.util.ArrayList < TYPE_1 > VAR_2 = new java.util.ArrayList < TYPE_1 > ( ) ; VAR_2 . add ( x ) ; VAR_2 . METHOD_2 ( VAR_1 ) ; return VAR_2 ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'private static < TYPE_1 > java.util.List < TYPE_1 > append ( java.util.List < TYPE_1 > VAR_1 , TYPE_1 x ) { java.util.ArrayList < TYPE_1 > VAR_2 = new java.util.ArrayList < TYPE_1 > ( ) ; VAR_2 . METHOD_2 ( VAR_1 ) ; VAR_2 . add ( x ) ; return VAR_2 ; }'}]
***** Running training *****
  Num examples = 10
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 9396
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 4
  train_loss = 42.6219
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 32.02 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:32.02
  ********************

***** Running evaluation *****
  Num examples = 9396
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 7
  train_loss = 22.4981
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
Namespace(log_name='./RQ5/java_10/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/1_codet5p_770m', data_dir='./data/RQ5/java_10_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'private static < TYPE_1 > java.util.List < TYPE_1 > METHOD_1 ( TYPE_1 x , java.util.List < TYPE_1 > VAR_1 ) { java.util.ArrayList < TYPE_1 > VAR_2 = new java.util.ArrayList < TYPE_1 > ( ) ; VAR_2 . add ( x ) ; VAR_2 . METHOD_2 ( VAR_1 ) ; return VAR_2 ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'private static < TYPE_1 > java.util.List < TYPE_1 > append ( java.util.List < TYPE_1 > VAR_1 , TYPE_1 x ) { java.util.ArrayList < TYPE_1 > VAR_2 = new java.util.ArrayList < TYPE_1 > ( ) ; VAR_2 . METHOD_2 ( VAR_1 ) ; VAR_2 . add ( x ) ; return VAR_2 ; }'}]
***** Running training *****
  Num examples = 10
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 4
  train_loss = 42.6219
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 32.02 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:32.02
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 7
  train_loss = 22.4981
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 74.13 	 Previous best codebleu 32.02
  ********************
 Achieve Best bleu:74.13
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 10
  train_loss = 11.421
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 82.56 	 Previous best codebleu 74.13
  ********************
 Achieve Best bleu:82.56
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 13
  train_loss = 6.1912
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 83.86 	 Previous best codebleu 82.56
  ********************
 Achieve Best bleu:83.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 16
  train_loss = 4.9617
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 84.42 	 Previous best codebleu 83.86
  ********************
 Achieve Best bleu:84.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 19
  train_loss = 3.4464
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 84.68 	 Previous best codebleu 84.42
  ********************
 Achieve Best bleu:84.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 22
  train_loss = 2.8656
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 84.92 	 Previous best codebleu 84.68
  ********************
 Achieve Best bleu:84.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 25
  train_loss = 1.4057
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 84.94 	 Previous best codebleu 84.92
  ********************
 Achieve Best bleu:84.94
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 28
  train_loss = 1.3358
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 85.21 	 Previous best codebleu 84.94
  ********************
 Achieve Best bleu:85.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 31
  train_loss = 0.8742
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_1/validation.jsonl
  codebleu-4 = 85.19 	 Previous best codebleu 85.21
  ********************
reload model from RQ5/java_10/1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_10_1/test.jsonl
  codebleu = 85.67 
  Total = 500 
  Exact Fixed = 10 
[7, 54, 59, 96, 190, 265, 287, 311, 435, 497]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 10 
[7, 54, 59, 96, 190, 265, 287, 311, 435, 497]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  codebleu = 85.67 
[0.6404277202296462, 0.9100382310000767, 0.8344610480137715, 0.5830912642712589, 0.8410886457286899, 0.9000092746228069, 1.0, 0.7238925475243609, 0.7165518607362565, 0.7317094529491389, 0.9927616907569681, 0.8593411581323525, 0.6704850073712101, 0.8097008775289211, 0.9816061032688781, 0.8484683473696282, 0.7523277398705996, 0.9342846849350264, 0.9403575354525664, 0.8673567485716778, 0.8535087661549325, 0.865387748772268, 0.9438472935309958, 0.9000120435131169, 0.6113472882852959, 0.8985819684179537, 0.8702677461904487, 0.7366684698333517, 0.6151952248646311, 0.8028172817807042, 0.8961415113886637, 0.8579781671940192, 0.7321748033833095, 0.8027219981421396, 0.9289542494919141, 0.947403495661826, 0.9490051755223876, 0.8768056751919203, 0.9410056506290296, 0.7467173881179517, 0.6940078756137626, 0.9438472935309958, 0.9449436332701645, 0.8345641796204135, 0.9217146126934004, 0.8878216070603988, 0.9438472935309958, 0.976934640027229, 0.9470228248649237, 0.7366535801418781, 0.9269331830469714, 0.9217146126934004, 0.947403495661826, 1.0, 0.9150078184972721, 0.9277372103001715, 0.6993730085347135, 0.8855924534744011, 1.0, 0.8704933355439815, 0.8309401076758502, 0.7942701366539271, 0.9125104348398385, 0.683747344676549, 0.4955424550400919, 0.9471534480091441, 0.9607233175453542, 0.843537340596747, 0.8629693959820761, 0.8063559408133776, 0.8875485500759485, 0.7836690242372425, 0.8963926483875864, 0.9438472935309958, 0.9363184178206276, 0.8762476125170207, 0.7167405472833845, 0.9153385085464256, 0.9233947978166772, 0.8665291872943124, 0.7041112224982915, 0.8693647831726143, 0.9327169633466876, 0.9109429421150292, 0.9852849226532605, 0.7519894248327487, 0.8957659799478881, 0.9197682751991088, 0.9488183079463388, 0.9662763734075034, 0.961485854541051, 0.5433053301010067, 0.9492680203081187, 0.9535662529996234, 0.8629423931282016, 1.0, 0.8853701119354719, 0.9496388910002089, 0.8696882992240296, 0.8151472726544473, 0.9189173935928419, 0.9353304765953643, 0.9277295522431621, 0.9470071513080363, 0.8774842286953382, 0.8853058630953912, 0.8558905963347896, 0.8813815400934406, 0.7674080702465693, 0.8841057642921581, 0.9246060608408084, 0.9438472935309958, 0.931919905317208, 0.8043093371590795, 0.898122990476288, 0.7550361665438226, 0.8947233410879221, 0.9600094284589669, 0.7208757878471993, 0.6113932438251926, 0.9783152785012215, 0.9418090107609731, 0.8765208719204487, 0.9461408346030404, 0.9596411809238716, 0.7188935477431233, 0.7335237499509486, 0.932758510312542, 0.4008807324531617, 0.9438472935309958, 0.9438472935309958, 0.9129875032050465, 0.9498733743967636, 0.9161912443928868, 0.9310402866577385, 0.9027159738051327, 0.8684368528418116, 0.9438472935309958, 0.8275239936524709, 0.6850661340471761, 0.9140032864002167, 0.7901423077149385, 0.775729444252817, 0.9583676774082095, 0.8555613027817166, 0.6564704620622854, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.8742293276300162, 0.8641677441415241, 0.8789500164072293, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.9150078184972721, 0.8141677441415242, 0.8888435222418036, 0.657410880332295, 0.9042446421916496, 0.7702622905265097, 0.7173692510217399, 0.9209893793099855, 0.8623527611587013, 0.8234385994211576, 0.8412239301694939, 0.6456093297418615, 0.9312991012532417, 0.7448331290037746, 0.568406757907828, 0.9041340728227794, 0.8383170132670079, 0.8567354045301243, 0.933286970639994, 0.8285441083529881, 0.7452798291220228, 0.9499046047578625, 0.9214852005355179, 0.6021439638609949, 0.06115195133819237, 0.9438472935309958, 0.9171481786623457, 0.9251038625610755, 0.8867171706601364, 0.8502729075207087, 1.0, 0.9217146126934004, 0.9499861168443409, 0.7506228403443113, 1.0, 0.823727560959342, 0.8694466020257385, 0.8218670339508176, 0.9465491069053984, 0.8756429067894516, 0.7060089851548503, 0.6113472882852959, 0.5973671803692812, 0.7830011774921302, 0.9156873229543501, 0.920847271632746, 0.936752055107628, 0.8939403219941204, 0.7988848397512625, 0.9535798190943583, 0.9168635615364948, 0.9735542568395896, 0.9055256568213023, 0.6781053145008827, 0.6447758993160594, 0.798272609744273, 0.9734190821154947, 0.6268790890159027, 0.9484513298936408, 0.8640040830519085, 0.5412009653933174, 0.8516665453656083, 0.9603262193723192, 0.7945631480201435, 0.6423705553781374, 0.7170219254969812, 0.9438472935309958, 0.7703813467645078, 0.9492836546885393, 0.95, 0.9438472935309958, 0.9289542494919141, 0.947403495661826, 0.8801547132641002, 0.9438472935309958, 0.9288817233176625, 0.7636095864061366, 0.9487145883454327, 0.823130054856027, 0.7360764107540074, 0.8017624348471263, 0.9438472935309958, 0.8465816263983615, 0.9565292378112795, 0.8498388617714197, 0.947403495661826, 0.9217146126934004, 0.8651706733544999, 0.7955198695646306, 0.8519474723251221, 0.8921042672694857, 0.7730519120573912, 0.9438472935309958, 0.9310037632474744, 0.8093542785781114, 0.9676227264894173, 0.9142150440420558, 0.8761507136068414, 0.9275072085040059, 0.93092593470538, 0.5929022108454984, 0.9447883869073515, 0.8473249685745358, 0.8154702514101111, 0.5550107088205123, 0.7803845922599806, 0.9303179033192857, 0.9029228697393135, 0.8781617655249798, 1.0, 0.9446955185101911, 0.8760135800491213, 0.8344531615285882, 0.8646780713427795, 0.9438472935309958, 0.8027136845880052, 0.9052981975214478, 0.947403495661826, 0.7604933990537905, 0.9131034995733858, 0.9559045711960699, 0.9455342747735904, 0.9621391018662697, 0.9439975336408695, 0.954646559321255, 0.9005633971565483, 0.6229661941006033, 0.9694667888884985, 0.9396180092663717, 0.8910871854638854, 0.9439975336408695, 1.0, 0.9035957587334096, 0.8007377532608353, 0.9351406123825403, 0.8640040830519085, 0.9045061327824662, 0.7606222347016529, 0.8656503210366169, 0.5778624281512992, 0.9205458982768264, 0.6992043393714683, 0.647747611811963, 0.8252274644011692, 0.8178334933130291, 0.9384313483327684, 0.8591474563888408, 0.6174062603682424, 0.9361448502736844, 0.9471695160234401, 0.8467391838183034, 0.841875207469964, 0.9234334473517907, 0.8568164816944417, 0.9612141455329344, 1.0, 0.7757388605257256, 0.7932369845589845, 0.9012977329022187, 0.947403495661826, 0.957146397188152, 0.8549033669907233, 0.843728854340388, 0.8175521747778856, 0.8712005145728838, 0.9821490926793564, 0.8335028039626476, 0.9317882927808769, 0.7804434274762166, 0.7095064952051557, 0.9437628092763377, 0.6892161349104073, 0.8188865946509148, 0.9438472935309958, 0.8816015466501335, 0.6199063904498077, 0.9606383615912808, 0.7928197492452767, 0.5968332637822825, 0.8617752937444394, 0.9289542494919141, 0.9005739349163713, 0.8411630424921515, 0.9438472935309958, 0.5407513833338216, 0.8793380306954786, 0.8925235787478019, 0.8377252262881923, 0.9221557993275098, 0.6059090214855563, 0.941246887425869, 0.9583676774082095, 0.9438472935309958, 0.9288768296987449, 0.8114803769129251, 0.8641269634908533, 0.7571560504922936, 0.9214086041237446, 0.8531205107407362, 0.8754667432999914, 0.9809329721714535, 0.942663845472012, 0.785678266679733, 0.9423575001497, 0.7297862281282066, 0.8989415087276855, 0.8681954931257403, 0.9523104352365284, 0.8627830017322995, 0.947403495661826, 0.9693877551020409, 0.872223029811398, 0.8501993870186701, 0.852734238473497, 0.8748503542446726, 0.9347001488455178, 0.936752055107628, 0.5838615292604841, 0.7260096636067352, 0.8912314882918398, 0.9325400085710811, 0.9636834829996033, 0.8375472876388348, 0.9973062440915319, 0.9468414240145169, 0.896457542865793, 0.9120050406373192, 0.9452585311430763, 0.7032463562196102, 0.8734611854236554, 0.6964375357812386, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.894141073173163, 0.9598250197069826, 0.9634911520696614, 0.8712775503716599, 0.8323441696598732, 0.8958013658804396, 0.6907701206102476, 0.9642153446749153, 0.9921681817198038, 0.7135449662005422, 0.7280460155792001, 0.9122491485294033, 0.9548431641738979, 0.7523507595770178, 0.7485418846203633, 0.9177534780225702, 0.9136051415192505, 0.9060328132136453, 0.9289542494919141, 0.8505395682648067, 0.6669257319650184, 0.8960971760439522, 0.8950515255524043, 0.9442100823718134, 0.668372111419317, 0.8212324536702587, 0.8957718999086999, 0.961965530245021, 0.7830011774921302, 0.9314651191074765, 0.8985609679543393, 0.9202827191608316, 0.6410378892354339, 0.8919778286066379, 0.8403006403038296, 0.8989289901708486, 0.7915314290630997, 0.9745770345730578, 0.7419687479630056, 0.9514967635709539, 0.8839836214081072, 0.9146241894400875, 1.0, 0.9043508150104198, 0.5788268971990955, 0.8068372263029933, 0.5351130312675151, 0.8360299937101696, 0.9343267218430718, 0.9350024698180224, 0.895540655184323, 0.9217146126934004, 0.9714587722069028, 0.5775142585149569, 0.8228427124746189, 0.8179617939659143, 0.800083937746499, 0.9666327444353364, 0.8810554316276455, 0.7904665899488785, 0.7217595067833324, 0.7366505750537401, 0.9149306320417432, 0.752685839952564, 0.9150679539598123, 0.9438472935309958, 0.8475010774748575, 0.850780171352979, 0.9133731397684426, 0.8243317050386542, 0.7215961844283755, 0.896693251042838, 0.7935461504124401, 0.9272271558703311, 0.8924875728510193, 0.8362192242070374, 0.8893062778978682, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.8713421516632174, 0.9139708530670974, 0.7456159421355051, 0.772616988243183, 0.8613514421762498, 0.904352113990756, 0.9152573407816746, 0.9771473873342402, 0.9265716075901909, 0.939633869187354, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.890490591592302, 0.9449149097946257, 0.9069546174929648, 0.947403495661826, 0.9049842403132857, 0.9217146126934004, 0.9120154143254138, 0.852434776462647, 0.8254935742435682, 0.8514674628624775, 0.9250193662902535, 1.0, 0.9778527235265317, 0.9121508301280472, 0.9438472935309958]
Finish training and take 1h0m
