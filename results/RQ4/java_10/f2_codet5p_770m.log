Namespace(log_name='./RQ5/java_10/f2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/f2_codet5p_770m', data_dir='./data/RQ5/java_10_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 10 training instances 
***** Running training *****
  Num examples = 10
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 9395
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00189
  global_step = 2
  train_loss = 0.6034
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00189
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 19.14 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.14
  ********************

***** Running evaluation *****
  Num examples = 9395
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00107
  global_step = 3
  train_loss = 0.5329
  ********************
Previous best ppl:1.00189
Achieve Best ppl:1.00107
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
Namespace(log_name='./RQ5/java_10/f2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/f2_codet5p_770m', data_dir='./data/RQ5/java_10_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 10 training instances 
***** Running training *****
  Num examples = 10
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00179
  global_step = 2
  train_loss = 0.6034
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00179
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 19.36 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.36
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00101
  global_step = 3
  train_loss = 0.5329
  ********************
Previous best ppl:1.00179
Achieve Best ppl:1.00101
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 23.75 	 Previous best codebleu 19.36
  ********************
 Achieve Best bleu:23.75
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00087
  global_step = 4
  train_loss = 0.2309
  ********************
Previous best ppl:1.00101
Achieve Best ppl:1.00087
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 29.09 	 Previous best codebleu 23.75
  ********************
 Achieve Best bleu:29.09
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00082
  global_step = 5
  train_loss = 0.1664
  ********************
Previous best ppl:1.00087
Achieve Best ppl:1.00082
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 33.34 	 Previous best codebleu 29.09
  ********************
 Achieve Best bleu:33.34
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0008
  global_step = 6
  train_loss = 0.1606
  ********************
Previous best ppl:1.00082
Achieve Best ppl:1.0008
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 44.45 	 Previous best codebleu 33.34
  ********************
 Achieve Best bleu:44.45
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00077
  global_step = 7
  train_loss = 0.1049
  ********************
Previous best ppl:1.0008
Achieve Best ppl:1.00077
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 54.49 	 Previous best codebleu 44.45
  ********************
 Achieve Best bleu:54.49
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00075
  global_step = 8
  train_loss = 0.075
  ********************
Previous best ppl:1.00077
Achieve Best ppl:1.00075
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 58.3 	 Previous best codebleu 54.49
  ********************
 Achieve Best bleu:58.3
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00073
  global_step = 9
  train_loss = 0.106
  ********************
Previous best ppl:1.00075
Achieve Best ppl:1.00073
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 59.79 	 Previous best codebleu 58.3
  ********************
 Achieve Best bleu:59.79
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00073
  global_step = 10
  train_loss = 0.0703
  ********************
Previous best ppl:1.00073
Achieve Best ppl:1.00073
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 61.42 	 Previous best codebleu 59.79
  ********************
 Achieve Best bleu:61.42
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00072
  global_step = 11
  train_loss = 0.0557
  ********************
Previous best ppl:1.00073
Achieve Best ppl:1.00072
  ********************
BLEU file: ./data/RQ5/java_10_2/validation.jsonl
  codebleu-4 = 62.16 	 Previous best codebleu 61.42
  ********************
 Achieve Best bleu:62.16
  ********************
reload model from RQ5/java_10/f2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_10_2/test.jsonl
  codebleu = 60.46 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[83, 344, 419]
  codebleu = 60.46 
[0.7016265496309229, 0.30231170472951197, 0.7224662648863509, 0.4139013839089223, 0.7680215019472283, 0.27243131201165743, 0.3241218285866614, 0.6482925669271551, 0.31245065054578763, 0.7457018199401371, 0.9217179344125859, 0.747643315953749, 0.4891407487676397, 0.32659452641706954, 0.6591156456950591, 0.6481574637844272, 0.6850584915926549, 0.8601188370424977, 0.7929986720865654, 0.24042214786672658, 0.8535087661549325, 0.3008976735796797, 0.19521082178559215, 0.7195422307059687, 0.25920885988276837, 0.6145569510517432, 0.7496626570929712, 0.5283224247028109, 0.44252053351846987, 0.7736643897576958, 0.6414813941400557, 0.6873662420545538, 0.6226458883772421, 0.32863890496136616, 0.465715924844207, 0.3245986094271166, 0.6154417991606029, 0.7183096944188962, 0.36825311623087337, 0.6731590030090238, 0.3000699423287492, 0.19529994543323217, 0.33957712333735207, 0.6304453310146302, 0.24692637473546136, 0.1390285212091969, 0.19594526433790216, 0.5810641758517052, 0.9061537357063023, 0.6564919628282537, 0.7953988464436343, 0.2170209354734544, 0.20955184916799613, 0.6333941235955324, 0.840319652558106, 0.6084293225042997, 0.635598349098713, 0.8031199076159046, 0.3241218285866614, 0.6298644506190365, 0.4177361154432053, 0.6307188510165458, 0.8233775338229194, 0.506125680996846, 0.5816383294573235, 0.8040254641911254, 0.9228675103159067, 0.7228911483719709, 0.7319733117136124, 0.9207265089204273, 0.8875485500759485, 0.5892964921415242, 0.8214154016134527, 0.19529994543323217, 0.8030175580931873, 0.6697974365073438, 0.6611221464864862, 0.5960942096485482, 0.7781257324303039, 0.3345493425604319, 0.6466437069569624, 0.667134265678519, 0.8732147860718871, 0.8066470648070827, 0.6811276363446395, 0.5830837386762782, 0.751041907378877, 0.7724187456276157, 0.3603025136896165, 0.8631712921939972, 0.26377374010110777, 0.523223985220748, 0.2121898415470802, 0.828406709800946, 0.7703105574851019, 0.2147186300618687, 0.8441865090225327, 0.4513060711607555, 0.7155677885532417, 0.6638449600070209, 0.9698138929646263, 0.2035386500478053, 0.7915628636098646, 0.91794697448997, 0.8284020522463414, 0.6990143624056944, 0.3342734470462408, 0.7816803880133534, 0.6662478917299303, 0.8181224979957988, 0.7202539219484726, 0.19422229900222543, 0.8004855843549328, 0.7424250993998629, 0.7282893663057769, 0.3218424485929666, 0.7438651223470351, 0.7657503478765371, 0.5701906388101965, 0.19934494482044368, 0.9313074513783361, 0.7851806793787677, 0.6464759750524514, 0.9101474687962732, 0.42595762401835663, 0.5968390155880063, 0.6802555707558395, 0.7189247404447459, 0.363411254249774, 0.19529994543323217, 0.1981663206181987, 0.8251664954751978, 0.8950060624143692, 0.7071405522126258, 0.5496560845733569, 0.3065064752640725, 0.6406586649923417, 0.20105201456950864, 0.7094783559068917, 0.5985460132137562, 0.7523333682102028, 0.6402854773599379, 0.7664023716159446, 0.8372578493474891, 0.7193559387880331, 0.5387865762717434, 0.6550132435705477, 0.5553706305752664, 0.6285616962601648, 0.2591440297925359, 0.23042586832432663, 0.24689571568826876, 0.6543594754715916, 0.3110894892369726, 0.8279924547765871, 0.840319652558106, 0.4218869959809527, 0.7776475554643918, 0.6149021387093628, 0.699416042582541, 0.8896350992359165, 0.5516295505382781, 0.801276100910846, 0.7647181465756838, 0.7329790139314107, 0.7158397788834653, 0.8678657374052126, 0.8555641924881596, 0.6522671897094589, 0.1826554531350913, 0.7548340382627486, 0.017535503591738232, 0.7474162102897873, 0.6236313401524359, 0.7715023898991458, 0.32223943886145767, 0.6450431254038292, 0.4864317166393503, 0.3702285130210081, 0.06300363456956486, 0.19521082178559215, 0.41586409630433113, 0.6030952907520446, 0.3566989761057816, 0.05374517214185297, 0.7455887872020714, 0.20282713830471816, 0.3258054996835291, 0.611484915882025, 0.8038676920269696, 0.251735951036083, 0.7072274460188492, 0.5602368279369344, 0.3640905305292797, 0.7395648696754308, 0.44238210269404077, 0.25920885988276837, 0.208870840660423, 0.7830011774921302, 0.8669569460602955, 0.8373887697347597, 0.9072725521291267, 0.6337457277395657, 0.5809219340161808, 0.5331453996366855, 0.27604505416507286, 0.866793475888622, 0.841982201551059, 0.6125773298541886, 0.9471695160234401, 0.6151756185522186, 0.653816931789947, 0.5239015601562096, 0.6561662302917426, 0.23063258655726604, 0.5194788258730466, 0.3148391076670003, 0.8471311821354086, 0.31766712244110273, 0.6643219118729957, 0.5963432384610431, 0.19521082178559215, 0.7103676483086313, 0.9125038691229297, 0.7583609709353714, 0.19274875796844013, 0.46815082712926215, 0.20955184916799613, 0.7663246823325051, 0.19594526433790216, 0.8115270103245333, 0.6208716301688832, 0.42589554335065843, 0.7343693736927654, 0.6690874155175593, 0.722920045358004, 0.19521082178559215, 0.7703047559758772, 0.3286309979457049, 0.593033013193499, 0.3245986094271166, 0.2460788824379319, 0.6580473928528496, 0.6915165782390887, 0.6519214750525268, 0.7543087891641322, 0.6160079286053539, 0.1981663206181987, 0.20678170896280973, 0.32049297642244945, 0.8228893013961295, 0.6692144842004847, 0.7614832350017728, 0.5610698716822158, 0.9104948835879048, 0.4882931854692144, 0.6045143452033909, 0.5385006436585963, 0.1918372361119035, 0.47314446745042027, 0.13116428287411322, 0.8568065239230183, 0.5845833709496056, 0.7673682296171209, 0.21492195284617108, 0.38665324556644304, 0.6838773195839767, 0.7151792567625905, 0.358092899946694, 0.19742587948288787, 0.6488864942899576, 0.7378853667556762, 0.19978098115809917, 0.35031429848458334, 0.804363294869249, 0.8338530280205303, 0.8410756154248149, 0.3288354208277761, 0.9439975336408695, 0.8930247493714516, 0.7694496510087083, 0.5051417653851575, 0.8831843260353922, 0.7161353506408099, 0.37654458040169664, 0.9439975336408695, 0.7564701306650601, 0.6751131503343866, 0.7124674132929127, 0.738988183086073, 0.23063258655726604, 0.6199464061978432, 0.7272889013683197, 0.19567807341766788, 0.5735283309621977, 0.36196285707874193, 0.5292941457487407, 0.5755233043849655, 0.5521477045942407, 0.6956622004910507, 0.8712818103255136, 0.9207265089204273, 0.8132148025904986, 0.08771862458143367, 0.8696285272437354, 0.47308877095104906, 0.8047569965165327, 0.34803374719086067, 0.6035150895992978, 0.3493771054530853, 0.46958124932078427, 0.6101063052550567, 0.702947721035756, 0.3506470064086628, 0.20477922877833357, 0.8332862284312241, 0.6958970838985514, 0.702492349590599, 0.7168207881952758, 0.7020116534377283, 0.6877773626094266, 0.7159255999851685, 0.828473796750941, 0.7456265966096753, 0.6666366404227222, 0.8610447563885792, 0.32113977304582075, 0.7316726054894297, 0.19837138057242365, 0.6058662662330025, 0.5953975384906144, 0.7678177305737679, 0.7296149295470102, 0.5001385173166933, 0.7422313043435927, 0.465715924844207, 0.8712299947771849, 0.6417566208588255, 0.20105201456950864, 0.17180715127313195, 0.8652878446549146, 0.6780977611091767, 0.7167237596778511, 0.7557992754099112, 0.6604770886277282, 0.8004898629293901, 0.9264832677103618, 0.1981663206181987, 0.7196849690792362, 0.5905286980513793, 0.7459504764441578, 0.6370018792938192, 0.8521339127688721, 0.652557279258331, 0.27655226215799766, 0.7996978096198052, 0.8606384653875616, 0.632128147686693, 0.7805351417808857, 0.658197706493578, 0.8940560390899916, 0.7972206367497516, 0.3391437805229809, 0.8250725700487818, 0.20955184916799613, 0.7911035870286747, 0.6115761766253426, 0.8243047268067278, 0.7650493179217401, 0.7228819421809687, 0.8607242266137565, 0.8577613686356415, 0.4404265030912652, 0.5320006077943021, 0.8183652491683203, 0.3361664691432363, 0.3359834609049142, 0.7455583360178761, 0.38034522160329676, 0.8156661429117983, 0.22497841114201567, 0.28292353729794967, 0.7592684650585898, 0.673106131305089, 0.8733685981251198, 0.6617260687505855, 0.8200980608690891, 0.6741746907231199, 0.756784480940917, 0.6793460491892245, 0.5267390358451736, 0.661651055159508, 0.3454008884678714, 0.8925950035496115, 0.9336324324973175, 0.660455569230947, 0.7617421921196805, 0.27801044753877824, 0.621829145779158, 0.7437861105756236, 0.6134498911515368, 0.6102110208687133, 0.660168044753223, 0.823738207445156, 0.7375127543414581, 0.34949782345356994, 0.7338722873630522, 0.5509640960081916, 0.8268323327051484, 0.8409410104860526, 0.4849466940749762, 0.6496708353514369, 0.5770271850725854, 0.6426697606375016, 0.8950515255524043, 0.7962751212087769, 0.5384241472564193, 0.5533735948562492, 0.8622417623629541, 0.4140302024908021, 0.7830011774921302, 0.7809789214708449, 0.7319612136357371, 0.8150233339421971, 0.6410378892354339, 0.6209493885786506, 0.7205748318825633, 0.6058385748918864, 0.5420424463805805, 0.8463835931264172, 0.5509898096461916, 0.47578339854276386, 0.8042532168310335, 0.7619200542504587, 0.3320403263330028, 0.31976473100081315, 0.8821296454239946, 0.6286302467743045, 0.7130825083995453, 0.21967901377985266, 0.6174874137787116, 0.3562779737458371, 0.6835277146613887, 0.21968166962156022, 0.6949270009135877, 0.7952178398348333, 0.882842712474619, 0.736087267628174, 0.6125858123389442, 0.9297574173227554, 0.6723521041607131, 0.785323400244213, 0.6068269366055301, 0.31131559213665105, 0.8236528441863966, 0.6939975992563312, 0.8374226883180389, 0.20105201456950864, 0.8515505278871711, 0.7085190515707707, 0.8850230971469417, 0.31425206283284834, 0.5398209269393796, 0.3092847640588969, 0.7268501559820937, 0.7395655436055731, 0.554654030602497, 0.7816113575802897, 0.8016313678472227, 0.7216400995800145, 0.7875223626918182, 0.8565687425019681, 0.8713421516632174, 0.7380804525250511, 0.7084856148224523, 0.6643480585380116, 0.8698869964564975, 0.6562844541437887, 0.551337645999344, 0.85818607897772, 0.7414195967130885, 0.3251610234679879, 0.7070286623857722, 0.356042273687635, 0.657812608174269, 0.8393940032842278, 0.7928862649902179, 0.6462557587884815, 0.19978098115809917, 0.7951925101759935, 0.28127188757140104, 0.30749506830928275, 0.7297147019769785, 0.7403658679909041, 0.6889171506447884, 0.8969572961646821, 0.3233013178841037, 0.8837311438245481, 0.7978812937743411, 0.1981663206181987]
Finish training and take 5h41m
