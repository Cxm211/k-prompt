Namespace(log_name='./RQ5/java_10/3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_10/3_codet5p_220m', data_dir='./data/RQ5/java_10_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'public static interface AMQPEndpointProducerBuilder              extends                  EndpointProducerBuilder {          default AdvancedAMQPEndpointProducerBuilder advanced() {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'public interface AMQPEndpointProducerBuilder              extends                  EndpointProducerBuilder {          default AdvancedAMQPEndpointProducerBuilder advanced() {'}]
***** Running training *****
  Num examples = 10
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 49.1588
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 11.93 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:11.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 49.5438
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 35.28 	 Previous best codebleu 11.93
  ********************
 Achieve Best bleu:35.28
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 24.2326
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 61.06 	 Previous best codebleu 35.28
  ********************
 Achieve Best bleu:61.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 16.7116
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 70.48 	 Previous best codebleu 61.06
  ********************
 Achieve Best bleu:70.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 13.4486
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 72.26 	 Previous best codebleu 70.48
  ********************
 Achieve Best bleu:72.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 12.2578
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 73.15 	 Previous best codebleu 72.26
  ********************
 Achieve Best bleu:73.15
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 7.9975
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 74.25 	 Previous best codebleu 73.15
  ********************
 Achieve Best bleu:74.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 10.0309
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 74.11 	 Previous best codebleu 74.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 6.6975
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 75.36 	 Previous best codebleu 74.25
  ********************
 Achieve Best bleu:75.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 5.9217
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 75.57 	 Previous best codebleu 75.36
  ********************
 Achieve Best bleu:75.57
  ********************
reload model from RQ5/java_10/3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_10_3/test.jsonl
  codebleu = 72.54 
  Total = 500 
  Exact Fixed = 8 
[35, 45, 52, 227, 240, 336, 411, 444]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[58, 344]
  ********************
  Total = 500 
  Exact Fixed = 8 
[35, 45, 52, 227, 240, 336, 411, 444]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[58, 344]
  codebleu = 72.54 
[0.593212014052714, 0.4232240156531161, 0.8344610480137715, 0.5830912642712589, 0.8410886457286899, 0.9000092746228069, 0.3232713248704495, 0.7238925475243609, 0.3072110927368262, 0.8011782298988499, 0.9927616907569681, 0.8593411581323525, 0.6476858206343272, 0.343952930558732, 0.9816061032688781, 0.8398969187981995, 0.7523277398705996, 0.8900575648896152, 0.9254535674603117, 0.8511904850320646, 0.755766537798902, 0.2969894531809322, 0.3220396351872425, 0.9000120435131169, 0.6113472882852959, 0.8904735914871043, 0.8702677461904487, 0.7318447632403485, 0.6151952248646311, 0.774571731962115, 0.8961415113886637, 0.8579781671940192, 0.7321748033833095, 0.6094353006527755, 1.0, 0.3226065051590042, 0.9377704204852682, 0.845119220537944, 0.9410056506290296, 0.7467173881179517, 0.6305527963160726, 0.32457202897753634, 0.6654848196822212, 0.8345641796204135, 1.0, 0.24811250672041774, 0.3231806389404378, 0.976934640027229, 0.9061537357063023, 0.7366535801418781, 0.9269331830469714, 1.0, 0.32308692641682485, 0.591267892531106, 0.32395815323209987, 0.7148280384493552, 0.6993730085347135, 0.8855924534744011, 0.3232713248704495, 0.7739991864243869, 0.8309401076758502, 0.7942701366539271, 0.9125104348398385, 0.6829889948441952, 0.46704116043020866, 0.9471534480091441, 0.9607233175453542, 0.843537340596747, 0.8629693959820761, 0.49535830963747907, 0.8875485500759485, 0.8286690242372425, 0.8963926483875864, 0.32457202897753634, 0.932185453416744, 0.8762476125170207, 0.6864491243119533, 0.5422875137889253, 0.8575152802303372, 0.8516124798205915, 0.7529109419180047, 0.8693647831726143, 0.8960849572238858, 0.9109429421150292, 0.9852849226532605, 0.7249768933823052, 0.8957659799478881, 0.9197682751991088, 0.9488183079463388, 0.9662763734075034, 0.9745293328019206, 0.20681743118599583, 0.12260812915704543, 0.9535662529996234, 0.8412417736266901, 0.1950074702857457, 0.8853701119354719, 0.5354389191116804, 0.8696882992240296, 0.8151472726544473, 0.31915325944839656, 0.12274055843103836, 0.8126499005112438, 0.8936038356038187, 0.8774842286953382, 0.874950717716843, 0.8454759414654753, 0.8813815400934406, 0.7569599185049496, 0.8841057642921581, 0.18623243477012089, 0.32240081307027274, 0.931919905317208, 0.42643513314307024, 0.898122990476288, 0.3002288341950512, 0.8947233410879221, 0.9600094284589669, 0.7016104496997746, 0.21067892970316798, 0.7532954066587407, 0.9418090107609731, 0.8558052428956033, 0.9017559120400054, 0.9596411809238716, 0.7188935477431233, 0.7501904166176152, 0.932758510312542, 0.48666825572275857, 0.3231806389404378, 0.322996252295271, 0.9129875032050465, 0.29028203266451275, 0.9515492621717487, 0.9136559239327806, 0.8830601585622674, 0.8684368528418116, 0.32392604494770494, 0.8275239936524709, 0.6700676225414041, 0.9140032864002167, 0.7901423077149385, 0.5115993189580444, 0.5636925568965904, 0.8599636016279741, 0.733322243260283, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.22814410508941801, 0.2576616004082908, 0.27656757441671476, 0.8470209786548928, 0.7583556269641405, 0.95273941015886, 0.32395815323209987, 0.45780703385212695, 0.8888435222418036, 0.657410880332295, 0.8951837826828467, 0.7597193487754639, 0.7173692510217399, 0.9117574507850088, 0.8623527611587013, 0.80542312292921, 0.8412239301694939, 0.2536552281994504, 0.4302625099552771, 0.83084241467313, 0.14841983222678845, 0.9041340728227794, 0.8884670476301071, 0.8189995554735204, 0.933286970639994, 0.8285441083529881, 0.3125212228340622, 0.8995935415335088, 0.9214852005355179, 0.6121576162732347, 0.06010248026724298, 0.32425194683310976, 0.9171481786623457, 0.5047488553773856, 0.8604185749237447, 0.8120896643331899, 1.0, 0.2613455690262675, 0.8565234441018736, 0.7048910897361526, 0.5233032127829892, 0.2284899675513953, 0.8395859761019756, 0.8070296785081814, 0.9465491069053984, 0.8471890898110415, 0.5722181241559329, 0.6113472882852959, 0.10566733219015366, 0.7830011774921302, 0.8961449755814173, 0.920847271632746, 0.936752055107628, 0.8939403219941204, 0.7988848397512625, 0.81498810286409, 0.9168635615364948, 0.861665861995312, 0.8909259409954012, 0.6781053145008827, 0.9028547998643877, 0.7786436023628567, 0.9734190821154947, 0.6106048936131919, 0.9484513298936408, 0.03769230769230769, 0.2816154877677694, 0.8026084056765124, 0.9603262193723192, 0.7945631480201435, 0.801169789534145, 0.7170219254969812, 0.32425194683310976, 0.7703813467645078, 0.8543977368573089, 0.95, 0.32232518933738535, 1.0, 0.281797568163193, 0.8801547132641002, 0.3231806389404378, 0.9288817233176625, 0.7636095864061366, 0.9487145883454327, 0.8054829960324975, 0.7360764107540074, 0.7501665593336498, 0.3220396351872425, 0.8315640106866233, 0.3073883688139006, 0.9749209693367746, 0.3226065051590042, 0.26725938722560993, 0.8651706733544999, 0.7955198695646306, 0.9257462023534273, 0.8921042672694857, 0.7414102905101468, 0.322996252295271, 0.32408864514830754, 0.7907339297392293, 0.9676227264894173, 0.8759418897254363, 0.8761507136068414, 0.9159167581563037, 0.906112407876916, 0.582650387838694, 0.6078963208320649, 0.8473249685745358, 0.7635632855213623, 0.5480300945009805, 0.12304313326663358, 0.9303179033192857, 0.9029228697393135, 0.8763091456648879, 0.8674435206185069, 0.9446955185101911, 0.8760135800491213, 0.8344531615285882, 0.8646780713427795, 0.32392604494770494, 0.8027136845880052, 0.9052981975214478, 0.2730584739703617, 0.293751043131176, 0.9131034995733858, 0.9559045711960699, 0.2245878870612301, 0.3215703613444124, 0.30793433386770624, 0.24616519998436032, 0.9005633971565483, 0.08154811013133413, 0.9694667888884985, 0.9396180092663717, 0.8785395193033665, 0.30793433386770624, 0.7789657345158767, 0.9035957587334096, 0.7902051954474061, 0.9351406123825403, 0.03769230769230769, 0.9045061327824662, 0.1015726645761214, 0.14870270581134998, 0.5778624281512992, 0.9205458982768264, 0.6992043393714683, 0.647747611811963, 0.8843767323807665, 0.8178334933130291, 0.9384313483327684, 0.6043647216638229, 0.18246684586246154, 0.9790427622042994, 0.9471695160234401, 0.47814499783971315, 0.841875207469964, 0.9234334473517907, 0.948052670635467, 0.5917759010971924, 0.37543012830660444, 0.7757388605257256, 0.7932369845589845, 0.31965858324007845, 0.32331747838025604, 0.957146397188152, 0.6587171701314622, 0.843728854340388, 0.9098906194727998, 0.8712005145728838, 0.9821490926793564, 0.8335028039626476, 0.8734371768959985, 0.7687295531852028, 0.7042630548569638, 0.8886447990492503, 0.7862568624220109, 0.8188865946509148, 0.32529885977963247, 0.312246367152601, 0.638311632426873, 0.9221058191463145, 0.8971602318125442, 0.651002857872442, 0.8449548615524113, 1.0, 0.9005739349163713, 0.8411630424921515, 0.32392604494770494, 0.09985074849516491, 0.5544233577589087, 0.8925235787478019, 0.7983765155700412, 0.9221557993275098, 0.8979311034464716, 0.941246887425869, 0.3597554292633871, 0.322996252295271, 0.9288768296987449, 0.8114803769129251, 0.8641269634908533, 0.7463083244055686, 0.9214086041237446, 0.7273619991893457, 0.2802851550999643, 0.9809329721714535, 0.9131079256609971, 0.785678266679733, 0.9423575001497, 0.7297862281282066, 0.8989415087276855, 0.8528940249777082, 0.2922440874376254, 0.8627830017322995, 0.32308692641682485, 0.9693877551020409, 0.8061980128234052, 0.8098255207145801, 0.852734238473497, 0.8748503542446726, 0.9347001488455178, 0.017589291177467492, 0.5838615292604841, 0.7400913955844939, 0.8912314882918398, 0.3361664691432363, 0.12197280576887415, 0.9170972299520759, 0.9973062440915319, 0.9468414240145169, 0.896457542865793, 0.9120050406373192, 0.9452585311430763, 0.6195433038617447, 0.9396360626487428, 0.7223713204162335, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.8536649507762627, 0.9598250197069826, 0.9634911520696614, 0.8712775503716599, 0.8235421636351896, 0.8958013658804396, 0.5358641206564776, 0.9540658387729117, 0.957379211410232, 0.7135449662005422, 0.7280460155792001, 0.9122491485294033, 0.8278488581694263, 0.7543371467157234, 0.7485418846203633, 0.8450964715926044, 0.8624169307186037, 0.6825830198326693, 1.0, 0.8505395682648067, 0.6669257319650184, 0.8960971760439522, 0.2515714930596153, 0.9442100823718134, 0.6734799953637649, 0.8101792249765409, 0.8528533963690583, 0.961965530245021, 0.7830011774921302, 0.9314651191074765, 0.8985609679543393, 0.9202827191608316, 0.6410378892354339, 0.8634064000352093, 0.9487801938535607, 0.44727914936795743, 0.7915314290630997, 0.9745770345730578, 0.74212817232249, 0.5120599960520207, 0.8839836214081072, 0.9146241894400875, 0.22419768111456412, 0.6613283978673825, 0.86944691495958, 0.8068372263029933, 0.2869398783057897, 0.8756428313006783, 0.9343267218430718, 0.9350024698180224, 0.895540655184323, 1.0, 0.9714587722069028, 0.8009769001044789, 0.4413733428933175, 0.8179617939659143, 0.800083937746499, 0.9666327444353364, 0.8336643341050114, 0.48022106805051495, 0.7155276378714381, 0.7366505750537401, 0.9149306320417432, 0.2658315168407083, 0.9150679539598123, 0.32499804530507137, 0.4765881909498342, 0.850780171352979, 0.8805726252383597, 0.8092362779144251, 0.7215961844283755, 0.7522642454492299, 0.7935461504124401, 0.9272271558703311, 0.8920331055278383, 0.8362192242070374, 0.8893062778978682, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.8713421516632174, 0.9139708530670974, 0.3293914149056255, 0.772616988243183, 0.8613514421762498, 0.8802951254285889, 0.9056621799467235, 0.935546624422597, 0.9142690153913713, 0.3220404238592264, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.5744334999512706, 0.9449149097946257, 0.9069546174929648, 0.2730584739703617, 0.9049842403132857, 0.25157635585143545, 0.7743473090751825, 0.9058966001121493, 0.8254935742435682, 0.8514674628624775, 0.9290239101271212, 0.3226570559596195, 0.9778527235265317, 0.8532281457776401, 0.322996252295271]
Finish training and take 54m
