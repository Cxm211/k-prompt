Namespace(log_name='./RQ5/java_10/f3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/f3_codet5p_770m', data_dir='./data/RQ5/java_10_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 10 training instances 
***** Running training *****
  Num examples = 10
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 9395
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00189
  global_step = 2
  train_loss = 0.7335
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00189
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 19.15 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.15
  ********************

***** Running evaluation *****
  Num examples = 9395
  Batch size = 4
Namespace(log_name='./RQ5/java_10/f3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_10/f3_codet5p_770m', data_dir='./data/RQ5/java_10_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 10 training instances 
***** Running training *****
  Num examples = 10
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00179
  global_step = 2
  train_loss = 0.7335
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00179
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 19.36 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.36
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00094
  global_step = 3
  train_loss = 0.6897
  ********************
Previous best ppl:1.00179
Achieve Best ppl:1.00094
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 25.97 	 Previous best codebleu 19.36
  ********************
 Achieve Best bleu:25.97
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00082
  global_step = 4
  train_loss = 0.343
  ********************
Previous best ppl:1.00094
Achieve Best ppl:1.00082
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 30.61 	 Previous best codebleu 25.97
  ********************
 Achieve Best bleu:30.61
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00073
  global_step = 5
  train_loss = 0.2638
  ********************
Previous best ppl:1.00082
Achieve Best ppl:1.00073
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 31.91 	 Previous best codebleu 30.61
  ********************
 Achieve Best bleu:31.91
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00069
  global_step = 6
  train_loss = 0.2225
  ********************
Previous best ppl:1.00073
Achieve Best ppl:1.00069
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 36.19 	 Previous best codebleu 31.91
  ********************
 Achieve Best bleu:36.19
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00066
  global_step = 7
  train_loss = 0.1523
  ********************
Previous best ppl:1.00069
Achieve Best ppl:1.00066
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 45.43 	 Previous best codebleu 36.19
  ********************
 Achieve Best bleu:45.43
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00064
  global_step = 8
  train_loss = 0.1072
  ********************
Previous best ppl:1.00066
Achieve Best ppl:1.00064
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 54.7 	 Previous best codebleu 45.43
  ********************
 Achieve Best bleu:54.7
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00062
  global_step = 9
  train_loss = 0.1139
  ********************
Previous best ppl:1.00064
Achieve Best ppl:1.00062
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 60.18 	 Previous best codebleu 54.7
  ********************
 Achieve Best bleu:60.18
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00062
  global_step = 10
  train_loss = 0.0814
  ********************
Previous best ppl:1.00062
Achieve Best ppl:1.00062
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 62.42 	 Previous best codebleu 60.18
  ********************
 Achieve Best bleu:62.42
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00061
  global_step = 11
  train_loss = 0.0672
  ********************
Previous best ppl:1.00062
Achieve Best ppl:1.00061
  ********************
BLEU file: ./data/RQ5/java_10_3/validation.jsonl
  codebleu-4 = 64.21 	 Previous best codebleu 62.42
  ********************
 Achieve Best bleu:64.21
  ********************
reload model from RQ5/java_10/f3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_10_3/test.jsonl
  codebleu = 62.41 
  Total = 500 
  Exact Fixed = 8 
[35, 187, 227, 242, 336, 411, 444, 491]
  Syntax Fixed = 1 
[305]
  Cleaned Fixed = 3 
[58, 110, 344]
  ********************
  Total = 500 
  Exact Fixed = 8 
[35, 187, 227, 242, 336, 411, 444, 491]
  Syntax Fixed = 1 
[305]
  Cleaned Fixed = 3 
[58, 110, 344]
  codebleu = 62.41 
[0.7016265496309229, 0.2642172094997072, 0.7224662648863509, 0.15735937865400082, 0.6800705894310659, 0.8222353563559892, 0.20488243123405883, 0.6482925669271551, 0.22678466985048804, 0.7457018199401371, 0.8993314634488963, 0.7486466550489757, 0.48791803923919613, 0.3227949500287998, 0.7135122240594816, 0.657207370706549, 0.6486604777423851, 0.8601188370424977, 0.8276556341512094, 0.2737819020845597, 0.8535087661549325, 0.3008976735796797, 0.19521082178559215, 0.7195422307059687, 0.2582398306348773, 0.5804185108811815, 0.7292005467172462, 0.616891728029327, 0.5740994509755263, 0.8028172817807042, 0.6414813941400557, 0.6873662420545538, 0.6573324263927034, 0.30343171004162167, 1.0, 0.2024623597254302, 0.6017138685793391, 0.7645481686846878, 0.641523445401913, 0.6702367035311789, 0.7527494154159273, 0.19529994543323217, 0.3413971649551465, 0.7805252493107048, 0.8852005472381914, 0.32244296450081184, 0.19529994543323217, 0.7779120389404082, 0.3522930391851402, 0.6530292122346533, 0.7620566290714665, 0.7445648861187566, 0.2146341526736672, 0.39551289116851035, 0.32727673740397745, 0.6076666578198298, 0.5561205313007582, 0.8320336301888321, 0.20488243123405883, 0.29163743756905836, 0.4177361154432053, 0.6274247628794389, 0.8193792029343525, 0.4483578871708767, 0.6918812624119108, 0.8608417155100152, 0.9088348803875086, 0.7201469836925561, 0.7319733117136124, 0.9207265089204273, 0.8875485500759485, 0.756308826162818, 0.695469360292548, 0.19529994543323217, 0.7829165311013584, 0.6170387694858125, 0.6429840200915118, 0.7558434889844952, 0.7285105411366619, 0.704987145825027, 0.6466437069569624, 0.663782703619739, 0.8371846987938256, 0.8431171619150728, 0.8155647701922346, 0.5913503153779367, 0.7282328689690635, 0.769544738299242, 0.42382875325740094, 0.8695658957591088, 0.38097619820786355, 0.1012692114710404, 0.21483716369246825, 0.7583515363731967, 0.8654327987604276, 0.16852478038126129, 0.8447559468975885, 0.61206729229409, 0.7124605737954934, 0.6609657790720724, 0.3444833527762896, 0.20637444574842878, 0.7856161452051413, 0.9289939250572432, 0.8284020522463414, 0.8140509417921906, 0.7418064627164329, 0.7976525407827952, 0.6274093868228443, 0.8997211496034032, 0.7202539219484726, 0.19422229900222543, 0.8040781973155593, 0.6445362694711929, 0.7658349683429854, 0.3210308376822612, 0.7955598954540631, 0.7846118659271069, 0.630540317754772, 0.3213938827532403, 0.376204224944572, 0.7851806793787677, 0.6464759750524514, 0.9070205459407448, 0.8605129380347042, 0.6765601278342184, 0.6802555707558395, 0.590340370385788, 0.37749789680215046, 0.19529994543323217, 0.1981663206181987, 0.8357568190102038, 0.2895060036651244, 0.927480696886938, 0.622931497140175, 0.6000303556514865, 0.6893908488588405, 0.19578008869285465, 0.3176283639467019, 0.45552118050561385, 0.736567742086493, 0.686042829089292, 0.712515351758821, 0.619347995257647, 0.7570780601549043, 0.6087983979675168, 0.32890069359449015, 0.5529931907390034, 0.6269444280130805, 0.2694042357745055, 0.23042586832432663, 0.8615110032256561, 0.5785092224348418, 0.35126716536764885, 0.7888324465552805, 0.32727673740397745, 0.4001216978759047, 0.610424817477858, 0.6102712187246664, 0.7365959362123095, 0.9117966772276285, 0.21941246429786135, 0.7712505505898121, 0.8052338973331944, 0.7409021848483202, 0.2813515221140226, 0.6779076571760945, 0.8555641924881596, 0.6014210969511684, 0.15105830241239265, 0.7520549992592238, 0.7900731413015227, 0.7474162102897873, 0.6346499096435885, 0.6785807442851531, 0.32572932081548006, 0.8867842171426179, 0.3686374420833394, 0.36756080321168205, 0.060376980309335804, 0.19521082178559215, 0.8452481530863245, 0.581116792840267, 0.810985752409357, 0.6341295451511795, 0.7405995556547731, 1.0, 0.31988965413724046, 0.679881872439946, 0.2598157002644847, 0.7022633040622321, 0.8504610278440559, 0.5559494654138589, 0.3864388773691426, 0.7946326899281144, 0.4793497753583987, 0.2582398306348773, 0.20331528510486743, 0.7830011774921302, 0.8669569460602955, 0.8412601006821521, 0.9072725521291267, 0.7660828944158724, 0.644305105272672, 0.81498810286409, 0.27604505416507286, 0.8731892597275455, 0.8519509770776809, 0.6107313934319497, 0.9471695160234401, 0.6087050188741094, 0.9419870730414177, 0.5057966567326547, 0.8466628067790549, 0.17726164509350867, 0.5194788258730466, 0.30179344838414557, 0.8138209323826411, 0.5893527248866464, 0.685510074867288, 0.5930698655413258, 0.19521082178559215, 0.7103676483086313, 0.7835279332483667, 0.7583609709353714, 0.19274875796844013, 1.0, 0.20757972850692982, 0.6126728138951615, 0.19529994543323217, 0.8115270103245333, 0.6208716301688832, 0.8033279302876736, 0.7932914145722472, 0.6690874155175593, 0.7227352156130871, 0.19521082178559215, 0.7703047559758772, 0.3105840961832109, 0.7527272182560778, 0.2024623597254302, 1.0, 0.6855848302920976, 0.7177398667979686, 0.842420466165364, 0.7560928611949997, 0.5955014054836634, 0.1981663206181987, 0.1988097702436061, 0.5139161735964263, 0.7940927628104544, 0.9142150440420558, 0.7305494118606852, 0.8131280406739858, 0.9104948835879048, 0.5032931854692144, 0.6055420176461611, 0.5434819500316723, 0.2961292666049366, 0.478670209891921, 0.11236198509808452, 0.853140817342724, 0.7428177153679325, 0.7869544009609863, 0.9714639883114058, 0.38665324556644304, 0.6826603233216042, 0.7796777346475909, 0.358092899946694, 0.20105201456950864, 0.6861678203882752, 0.7566353667556762, 0.2024623597254302, 0.3996007190943573, 0.804363294869249, 0.3460496661752818, 0.29066855074621706, 0.9475995051152848, 0.350993208052654, 0.6649293942980624, 0.7694496510087083, 0.22279393033876416, 0.8156558949444299, 0.7126278279368592, 0.38706827433674507, 0.34429261927619936, 0.7688707613570311, 0.8335200904676998, 0.6745970016530839, 0.7932690379440641, 0.17726164509350867, 0.7415109501172948, 0.7606222347016529, 0.23172895327513832, 0.5735283309621977, 0.3581785529088319, 0.5122842890062982, 0.5755233043849655, 0.7991960710506856, 0.7054011928683097, 0.8712818103255136, 0.6027528384526002, 0.5550290383089966, 0.8160548763225532, 0.8169364466686262, 0.757086178473164, 0.7952451538152271, 0.7172430113814362, 0.787793641618509, 0.5978755675920615, 0.9406018863753673, 0.6075567452471453, 0.32876995016785726, 0.3506470064086628, 0.20757972850692982, 0.8382588759460022, 0.7030484913477336, 0.699493538864867, 0.7154619510915394, 0.7369455784266192, 0.6877773626094266, 0.6846204124290926, 0.8462905315721074, 0.7450148713849064, 0.6902104411450565, 0.8610447563885792, 0.6604856852777883, 0.7260210457102022, 0.19837138057242365, 0.8816015466501335, 0.5912143005982238, 0.7624940603500516, 0.8679137442108271, 0.5001385173166933, 0.8026571500521055, 1.0, 0.8651318443369054, 0.6387069674941714, 0.20105201456950864, 0.18318911859768797, 0.7923125446186285, 0.6984404852453772, 0.7216418507249673, 0.7578539418647467, 0.7696564351940739, 0.91970861258227, 0.22062381107600176, 0.1981663206181987, 0.7196849690792362, 0.24826952429711827, 0.7429825299032367, 0.641327917256018, 0.8663590716880905, 0.27230264864941944, 0.25867851261012237, 0.8114160811050608, 0.8353107396022001, 0.695910136256084, 0.7805351417808857, 0.6632208135509337, 0.8940560390899916, 0.7795903640114885, 0.37391990073871006, 0.7958782638304933, 0.2146341526736672, 0.8126762282350468, 0.7527220935890879, 0.7572010208885324, 0.7650493179217401, 0.6440441416606855, 0.8575837872544068, 0.9222725521291268, 0.14496160599823116, 0.6736400552381363, 0.8183652491683203, 0.3361664691432363, 0.3318185050930573, 0.7667922558692757, 0.8053904208102931, 0.33337302054054974, 0.824482160168883, 0.28292353729794967, 0.7952684098598067, 0.6534149691762565, 0.6401135757072111, 0.6517260687505855, 0.8200980608690891, 0.7209812531394335, 0.7697689405454309, 0.6784750506862213, 0.5267390358451736, 0.7690913005999384, 0.7774816465700545, 0.9035797722087997, 0.9336324324973175, 0.7626110076927852, 0.5353652570014981, 0.7561593602248913, 0.6457350473378869, 0.8306483758081931, 0.8520509661235343, 0.6051445268254612, 0.6509709718604075, 0.7843998668587921, 0.9548431641738979, 0.6054192159148476, 0.7246363984315111, 0.7868653301851805, 0.8040424242257508, 0.8436125596610378, 1.0, 0.7358017569266383, 0.5804463678624519, 0.6668556867136646, 0.32103383551493003, 0.3198381717742056, 0.6087825592068604, 0.6961646707977531, 0.834131552032185, 0.4051437966360041, 0.7830011774921302, 0.7788685190343172, 0.7776681442394777, 0.6601683614925793, 0.2924833162299562, 0.643630246995484, 0.7013324286817155, 0.6072445121255134, 0.5420424463805805, 0.8103645989800068, 0.6773160619578839, 0.473075011283416, 0.6610058582236744, 0.7619200542504587, 0.23791219587501583, 0.3173880774585838, 0.8821296454239946, 0.6286302467743045, 0.23243790893663335, 0.7783227803020033, 0.8749250476050281, 0.34342952107374247, 0.7211648306470044, 1.0, 0.35119854811382045, 0.4841534147222577, 0.32069013656929796, 0.736087267628174, 0.731078915841987, 0.9297574173227554, 0.677779524120442, 0.3892674552279103, 0.6110104374378629, 0.5228891578187395, 0.8216257277720131, 0.2609529886815269, 0.8374226883180389, 0.20105201456950864, 0.7882791028096856, 0.7115926446489199, 0.8850230971469417, 0.7820904381111236, 0.5391882329479211, 0.3181365289086463, 0.7268501559820937, 0.7548015882539172, 0.6515140047802226, 0.7816113575802897, 0.7522996970235227, 0.8184214759792403, 0.7306331993975117, 0.8565687425019681, 0.6091842339009592, 0.8643563915310817, 0.9077224816006332, 0.6643480585380116, 0.8355512057391163, 0.6722429701019367, 0.7910950868916502, 0.8440291883412263, 0.7842483646369758, 0.8787531944300546, 0.8001554015962489, 0.5311593590788456, 0.6476096637169121, 0.3479265792829107, 0.7928862649902179, 0.8262056150784496, 0.2024623597254302, 0.7951925101759935, 1.0, 0.3086144712943574, 0.8252272304035191, 0.6851771245672269, 0.3187303899535531, 0.9060544691396217, 0.13327470645897407, 0.8277506290284042, 0.7978812937743411, 0.1981663206181987]
Finish training and take 5h59m
