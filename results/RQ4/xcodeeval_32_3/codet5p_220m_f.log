Namespace(log_name='./RQ5/xcodeeval_32_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_32_3/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_32_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 32 training instances 
***** Running training *****
  Num examples = 32
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0005
  global_step = 5
  train_loss = 0.5186
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0005
  ********************
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 51.52 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:51.52
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00045
  global_step = 9
  train_loss = 0.4012
  ********************
Previous best ppl:1.0005
Achieve Best ppl:1.00045
  ********************
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.03 	 Previous best codebleu 51.52
  ********************
 Achieve Best bleu:72.03
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00046
  global_step = 13
  train_loss = 0.3045
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.01 	 Previous best codebleu 72.03
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00045
  global_step = 17
  train_loss = 0.2445
  ********************
Previous best ppl:1.00045
Achieve Best ppl:1.00045
  ********************
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.31 	 Previous best codebleu 72.03
  ********************
 Achieve Best bleu:72.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00046
  global_step = 21
  train_loss = 0.2139
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.29 	 Previous best codebleu 72.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00047
  global_step = 25
  train_loss = 0.1895
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.15 	 Previous best codebleu 72.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00047
  global_step = 29
  train_loss = 0.1859
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.37 	 Previous best codebleu 72.31
  ********************
 Achieve Best bleu:72.37
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00047
  global_step = 33
  train_loss = 0.1582
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.12 	 Previous best codebleu 72.37
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00048
  global_step = 37
  train_loss = 0.1368
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.52 	 Previous best codebleu 72.37
  ********************
 Achieve Best bleu:72.52
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00048
  global_step = 41
  train_loss = 0.1394
  ********************
Previous best ppl:1.00045
BLEU file: ./data/RQ5/xcodeeval_32_3/validation.jsonl
  codebleu-4 = 72.53 	 Previous best codebleu 72.52
  ********************
 Achieve Best bleu:72.53
  ********************
reload model from RQ5/xcodeeval_32_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_32_3/test.jsonl
  codebleu = 73.18 
  Total = 500 
  Exact Fixed = 3 
[10, 322, 451]
  Syntax Fixed = 1 
[177]
  Cleaned Fixed = 1 
[481]
  ********************
  Total = 500 
  Exact Fixed = 3 
[10, 322, 451]
  Syntax Fixed = 1 
[177]
  Cleaned Fixed = 1 
[481]
  codebleu = 73.18 
[0.5003909451993059, 0.9294151499698242, 0.6386890640788627, 0.4507986952593763, 0.7127243662869864, 0.7769952980182466, 0.988076477678953, 0.8394311373107994, 0.7852857892799938, 0.9612577886502744, 0.6622400912006872, 0.7772868173384335, 0.9623971198135532, 0.7864631506294787, 0.8242836546885394, 0.5927671877201272, 0.7460376977984855, 0.7571794534629802, 0.8054255423928414, 0.8928539444631554, 0.8112424789997937, 0.3446503101961176, 0.9617896364191245, 0.8925560777052948, 0.8319533008515221, 0.0, 0.9751679833306697, 0.7089270802000895, 0.38708231484420597, 0.9606328090250909, 0.952851260737474, 0.7049068237438324, 0.6660796128246393, 0.5495569575790649, 0.35043855815342795, 0.4835000669850591, 0.7336996602379647, 0.6920005108223005, 0.7411039840168427, 0.959881096323312, 0.43871215196390306, 0.8621366387420285, 0.894223011559736, 0.9104066605958898, 0.6593510546130777, 0.8616680155829046, 0.5338617744031935, 0.8602181220443288, 0.8068473065406881, 0.9714741186824503, 0.6349077239728745, 0.9888765931470758, 0.9202770890052219, 0.9168708501988758, 0.9732049512552536, 0.9307210871582703, 0.8459119185795103, 0.9339299975598521, 0.5565094531676825, 0.2932882039114366, 0.7062237935963586, 0.5415454260109173, 0.9126538866701963, 0.33460070946115567, 0.9357192313331475, 0.9752926493864778, 0.98154990280289, 0.7858041122445381, 0.7113008970718839, 0.915465568076623, 0.5129346492787287, 0.7769943668010335, 0.9268555414820475, 0.9853133667641483, 0.9191158481207378, 0.8047255281559051, 0.9189149325906292, 0.4932733020018213, 0.9835575139456028, 0.9801494737913248, 0.8979589771065096, 0.4065190849702931, 0.7597100033179813, 0.9003077208857198, 0.8729588064267548, 0.43790924621469934, 0.608990714278228, 0.9738014890069033, 0.7248773843271845, 0.9822720623046208, 0.28942496496217823, 0.9822160786396994, 0.9792525526512381, 0.8761816792179258, 0.8511121182268683, 0.3723601680799457, 0.9441722498573561, 0.9602694033492566, 0.9319272046508655, 0.8283719455395986, 0.7652053474549476, 0.7013902966965254, 0.939347427853148, 0.7402454314369824, 0.9499450445139372, 0.9036755986162253, 0.89711636095013, 0.934871427061674, 0.7219385539808008, 0.843779606891953, 0.925790468693755, 0.636874953887632, 0.44301690842111663, 0.6118154056069615, 0.8900074689819688, 0.5662939894609251, 0.8287867816868959, 0.5881328279337963, 0.5590904557449987, 0.8722391507190985, 0.3581561425025056, 0.2154366420897607, 0.6424313341672772, 0.9904426405617919, 0.5198919171128822, 0.9094363868933528, 0.9728059803922399, 0.9297985504031074, 0.802202097529589, 0.5970326258085109, 0.9450125678546649, 0.7421234302640685, 0.6005171839156628, 0.7155356267750739, 0.8345680283779763, 0.9812998208473764, 0.8355015737804767, 0.9055073576552588, 0.4727669414734672, 0.8625522579180052, 0.6333873199069301, 0.7643810428304513, 0.6237731005828068, 0.6259483208762605, 0.3053935253252723, 0.901175714210833, 0.47213667611022436, 0.15843999506651799, 0.967544928466902, 0.9630021049296447, 0.9084408246013599, 0.8055864673667326, 0.49239571749879285, 0.5459516898043106, 0.9799354517153449, 0.7665708243215428, 0.35174423017740575, 0.6711901181086953, 0.7466017902771492, 0.29082790521790003, 0.580390274877845, 0.7151755552463562, 0.2247554870980143, 0.9742694293836827, 0.8810006945894249, 0.9375410928625703, 0.9872442369212986, 0.9699049873809091, 0.9834700980058935, 0.9610721917709997, 0.7145804834442675, 0.9396221339246325, 0.5480309606435074, 0.3728341211055949, 0.5578323336329495, 0.653856905605865, 0.9521775318277603, 0.7897511138044075, 0.9936607195991256, 0.9139108143266295, 0.5174196783096494, 0.916272560211627, 0.6610216553479364, 0.5341374618194692, 0.968840592678585, 0.7729246679984216, 0.9607745973711677, 0.9753403284839008, 0.508243109335715, 0.9732316438707951, 0.6034321191256158, 0.3843652002149621, 0.2695384615384615, 0.8798091526516568, 0.9087123429574342, 0.8218539480382933, 0.5004645722281954, 0.5507825554087806, 0.3574528301245649, 0.7918180095546627, 0.35780881211287907, 0.7910965943512587, 0.6266772080766958, 0.9045356516494829, 0.6703330349752178, 0.7733039571663103, 0.591865368526167, 0.9528960563859521, 0.8533534577368376, 0.328765761435442, 0.8489792738072972, 0.35732195151786816, 0.9087427909672332, 0.9621819666971321, 0.9496735450823421, 0.5012127116833381, 0.7938304679800914, 0.9683231065091749, 0.7576176996301744, 0.8848881901831457, 0.8523029357043744, 0.8909108204830978, 0.9219807718796313, 0.5692861362349391, 0.9602235397377306, 0.7370377157385981, 0.30374499657601767, 0.9867208488747938, 0.8409624947808998, 0.6401493417553263, 0.5044112829751952, 0.8123152846477772, 0.9009882112827006, 0.7247563047282455, 0.7336298486605038, 0.8426084577017585, 0.29660247436472353, 0.9024540974639368, 0.9314156079190896, 0.9508626724864289, 0.9899172392844513, 0.20461399075573877, 0.956301548538977, 0.015303112134931612, 0.7819105691707486, 0.904590355528839, 0.9521775318277603, 0.7808145991250481, 0.9477354798086446, 0.9276347349756966, 0.9043105995752032, 0.11200213653664287, 0.8491583316973768, 0.3138642748705961, 0.10410804349888911, 0.2941969248494424, 0.791629832805936, 0.9316289255121708, 0.42689251969146835, 0.46398377548871184, 0.9082446279665759, 0.8910042410146453, 0.9852412520833183, 0.8449844779326559, 0.8920397660942785, 0.7086954249958265, 0.5296868847250604, 0.8078101910282451, 0.8188115370159288, 0.4883323987643829, 0.2942761537144616, 0.739351212728503, 0.7200309361219595, 0.8831505037407669, 0.728109152875034, 0.9179223973142112, 0.8923345869421039, 0.7601507876079147, 0.9453937256792562, 0.9312591482604915, 0.6857318503066976, 0.3329380293736354, 0.8375169762832809, 0.9166795416411146, 0.9455409660105238, 0.5741547548968033, 0.8475503490377849, 0.43808453932073843, 0.9798219468937557, 0.4119890403418556, 0.9780384047260811, 0.594007414149033, 0.41151709383031154, 0.8065424278669349, 0.8763562414963599, 0.9379197320725001, 0.5667558470674415, 0.4990931771166349, 0.9060784269672213, 0.654357949555753, 0.910835019525245, 0.5289514092792647, 0.9495780518930732, 0.9489374752009117, 0.3776630720962285, 0.7397363043654711, 0.8794100936019331, 0.8897945586059963, 0.7057072625297606, 0.8876077472866535, 0.8213798120340192, 0.3448279680258582, 0.9494075997300704, 0.9241588430999468, 0.20506553274270925, 0.5588186440853233, 0.7361044037969633, 0.8735531700555859, 0.8545467071813517, 0.7942436421266241, 0.6998183310256022, 1.0, 0.2992138853720385, 0.35015564357569384, 0.9396958600581724, 0.897868274842214, 0.9600770625994386, 0.5075736355169622, 0.6729574244178118, 0.9173075584052859, 0.3854398044418995, 0.3636944021746881, 0.41143799701648776, 0.9273759004975819, 0.29762078210150816, 0.982840810414882, 0.597398385186501, 0.8961122944240132, 0.4759198273946812, 0.4011929009233417, 0.9821809817650384, 0.7736787852368014, 0.7486759848652709, 0.6653351874795173, 0.9784487939574704, 0.6074300350442166, 0.6178552508101587, 0.8810946359492662, 0.7576613353515966, 0.974183904376235, 0.5288231788023587, 0.8301117571777112, 0.7880782435693265, 0.19242379301366364, 0.9303593500095784, 0.8787702275332199, 0.8439878665104579, 0.35599974604021273, 0.8665928958491977, 0.9168638777441369, 0.9663829641974593, 0.33756552100145387, 0.9749244080949804, 0.9837244566208403, 0.987950922923978, 0.6693406223166416, 0.7611563185052843, 0.7460110532720265, 0.7388752465147035, 0.7369406553821242, 0.9359055149846003, 0.6099316790741642, 0.9023670066624934, 0.6869534758595228, 0.6350077342091638, 0.6312231658015864, 0.9496316376212464, 0.8287940980989854, 0.921342196457583, 0.9112403027721032, 0.4155596630877777, 0.9613152634801625, 0.7684448062183817, 0.8499957788893516, 0.5506135469714823, 0.6729451811832178, 0.4130042192803641, 0.7564085960465718, 0.9409103501190933, 0.9698138929646263, 0.703409246380515, 0.7905567987043943, 0.8638402232352782, 0.2690241695881932, 0.7559987541863575, 0.9823448177520746, 0.9040449515722547, 0.40730915905680976, 0.9301710411526725, 0.5279674737073239, 0.6831371225580725, 0.9344855036532616, 0.6215157040502399, 0.9327820820761272, 0.9669138743101213, 0.8981156159858223, 0.2776969881491722, 0.41116625310173693, 0.9647434403630581, 0.420550566302297, 0.5606641237588412, 0.9189645726561522, 0.539612684703451, 0.804631742207644, 0.49935819383309804, 0.841210527867613, 0.3374521240368497, 0.663952114335559, 0.9233388471345003, 0.7756256761612133, 0.977246012599333, 0.895041284267698, 0.5425755720089517, 0.938595895727768, 0.619196145402581, 0.9057365595839495, 0.6993849093249114, 0.1459086942388002, 0.8806976111458171, 0.9385995189394607, 0.8676734624744935, 0.23187090803373867, 0.7152266858057874, 0.7648837329883056, 0.9417054785646468, 0.5220942811428554, 0.7446141056482928, 0.251863754094517, 0.6871498304934975, 0.5996229338562107, 0.8554076307279366, 0.3787964080732937, 0.967235610505617, 0.7507801222761162, 0.32932492462151003, 0.5574068797012777, 0.7718761136325316, 0.9389766686181973, 0.4211523280742633, 0.6902777777999555, 0.9870846986398101, 0.9822516624818802, 0.7412367177908079, 0.8787838168173723, 0.8021007111795594, 0.666210611458786, 0.91340386684887, 0.8885993476351091, 0.8375795025499561, 0.883875362704373, 0.11032417015561033, 0.011803874246352868, 0.9633179652022956, 0.4799605515288722, 0.7669842146074164, 0.978606091610698, 0.45592789552418267, 0.41747534088469995, 0.8627505126882897, 0.9181208637326026, 0.6169265001024554, 0.7567061343242648, 0.5735025861665138, 0.6047033545037988, 0.9487836071815641, 0.7141494776867402, 0.85404963361482, 0.9086540498917997, 0.4027266047248116, 0.5644444486027747, 0.9684863847527501, 0.590335920863919, 0.778151370761023, 0.6623464465436505, 0.9426242057849843, 0.8206082150573881, 0.9040645444308273, 0.3821136101971082, 0.8667436416243068, 0.472048350719695, 0.5687023138391892, 0.9386210590467857, 0.9096266419399884, 0.7744889932682666, 0.6511144444174561, 0.46802465703730456, 0.9545187036711082, 0.4207598960402934, 0.8042710150175236, 0.8753523839405636]
Finish training and take 50m
