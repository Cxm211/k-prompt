Namespace(log_name='./RQ5/sstubs_1_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/sstubs_1_2/codet5p_770m', data_dir='./data/RQ5/sstubs_1_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '*     * @return The closed DataStream    */  public DataStream<OUT> writeAsText(String path, int batchSize) {    return writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);   }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '*     * @return The closed DataStream    */  public DataStreamSink<OUT> writeAsText(String path, int batchSize) {    return writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);   }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.993315685488136e+287
  global_step = 2
  train_loss = 47.8615
  ********************
Previous best ppl:inf
Achieve Best ppl:1.993315685488136e+287
  ********************
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 22.05 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:22.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.933751008607021e+273
  global_step = 3
  train_loss = 44.2837
  ********************
Previous best ppl:1.993315685488136e+287
Achieve Best ppl:1.933751008607021e+273
  ********************
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 21.82 	 Previous best codebleu 22.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 7.032857574890477e+257
  global_step = 4
  train_loss = 18.8983
  ********************
Previous best ppl:1.933751008607021e+273
Achieve Best ppl:7.032857574890477e+257
  ********************
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 26.4 	 Previous best codebleu 22.05
  ********************
 Achieve Best bleu:26.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 7.104817014222443e+254
  global_step = 5
  train_loss = 6.8677
  ********************
Previous best ppl:7.032857574890477e+257
Achieve Best ppl:7.104817014222443e+254
  ********************
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 28.5 	 Previous best codebleu 26.4
  ********************
 Achieve Best bleu:28.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.8682265670791837e+257
  global_step = 6
  train_loss = 2.7702
  ********************
Previous best ppl:7.104817014222443e+254
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 29.93 	 Previous best codebleu 28.5
  ********************
 Achieve Best bleu:29.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.5386124677090268e+266
  global_step = 7
  train_loss = 2.7543
  ********************
Previous best ppl:7.104817014222443e+254
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 38.11 	 Previous best codebleu 29.93
  ********************
 Achieve Best bleu:38.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 2.2452618993030598e+269
  global_step = 8
  train_loss = 1.4088
  ********************
Previous best ppl:7.104817014222443e+254
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 38.41 	 Previous best codebleu 38.11
  ********************
 Achieve Best bleu:38.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 6.1363525862646935e+271
  global_step = 9
  train_loss = 0.5661
  ********************
Previous best ppl:7.104817014222443e+254
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 37.59 	 Previous best codebleu 38.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 3.6188259427507816e+273
  global_step = 10
  train_loss = 0.6911
  ********************
Previous best ppl:7.104817014222443e+254
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 37.56 	 Previous best codebleu 38.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.2387921231818021e+275
  global_step = 11
  train_loss = 1.4895
  ********************
Previous best ppl:7.104817014222443e+254
BLEU file: ./data/RQ5/sstubs_1_2/validation.jsonl
  codebleu-4 = 37.47 	 Previous best codebleu 38.41
  ********************
early stopping!!!
reload model from RQ5/sstubs_1_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_1_2/test.jsonl
  codebleu = 37.51 
  Total = 500 
  Exact Fixed = 5 
[111, 297, 307, 398, 489]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 5 
[111, 297, 307, 398, 489]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 37.51 
[0.3, 0.5214883874457796, 0.5953125, 0.3, 0.6111355833177097, 0.33262678552001373, 0.816531076295884, 0.5933085293673459, 0.03298094773424015, 0.3, 0.5974308929991621, 0.1544260422615648, 0.3, 0.3, 0.0, 0.200167889092908, 0.39911522493595564, 0.007100591715976331, 0.05893757940972835, 0.15138579803894353, 0.0, 0.1658636538168894, 0.21326790453587363, 0.8063115837355967, 0.8320695845443828, 0.6809950003412967, 0.5945211356531896, 0.17975937024952218, 0.200167889092908, 0.0035502958579881655, 0.006086842914020868, 0.0053254437869822485, 0.15736721873215304, 0.39999999999999997, 0.8624505828021336, 0.010418635418635419, 0.46868981791933306, 0.5953125, 0.5945211356531896, 0.5945211356531896, 0.5945211356531896, 0.3000104420466411, 0.3, 0.24897620572247178, 0.0, 0.003529411764705882, 0.2675082099836951, 0.3, 0.39911522493595564, 0.3, 0.6633601589971541, 0.0, 0.0035502958579881655, 0.3362171278633708, 0.3, 0.1544260422615648, 0.3, 0.8057663674836635, 0.22353884138521257, 0.6017266359998108, 0.3000104420466411, 0.19424005320911109, 0.7627860236541177, 0.038611177939743745, 0.30525077013260515, 0.5050656811663341, 0.5714390554372559, 0.8839569405032901, 0.9222373349754207, 0.17197257598062388, 0.10865485058424931, 0.8158321683006473, 0.319374981543331, 0.5876846078061395, 0.3, 0.39911522493595564, 0.0, 0.39911522493595564, 0.29765625, 0.24588814168483908, 0.05715447795830178, 0.7785671262795049, 0.7529597631989451, 0.44799050221649817, 0.04710071922980253, 0.15835193452380952, 0.0, 0.8908661890024412, 0.0, 0.5050656811663341, 0.8063115837355967, 0.3, 0.1671247357293869, 0.0035502958579881655, 0.0, 0.6027096277472935, 0.4771460566737586, 0.3508230628106628, 0.20016720036365293, 0.39911522493595564, 0.2836284308697359, 0.521036048215874, 0.0035502958579881655, 0.5714390554372559, 0.11654857888395853, 0.3103855324868193, 0.721918180003297, 0.6280899868253751, 0.6415412088453509, 0.39999999999999997, 1.0, 0.3017750257997936, 0.32828880387266757, 0.8762733122343855, 0.014173228346456693, 0.6115726020193117, 0.6026549082468821, 0.3, 0.3, 0.0947307896837995, 0.3, 0.36277403359595956, 0.15444309084580576, 0.5714390554372559, 0.6113948751081837, 0.15266794291681168, 0.7469241363827284, 0.001764705882352941, 0.008875739644970413, 0.8491081041733906, 0.7973312339693255, 0.5933085293673459, 0.6537079783872484, 0.9222373349754207, 0.5945355324865973, 0.7018901228027388, 0.22353884138521257, 0.6379856318243222, 0.17398477112343996, 0.7018901228027388, 0.1658636538168894, 0.3, 0.9295451446225973, 0.4247325577547993, 0.8063115837355967, 0.3, 0.5953125, 0.0, 0.5714390554372559, 0.5603140283830224, 0.17975937024952218, 0.0, 0.0, 0.014594594594594595, 0.8580430292750036, 0.6009637053528041, 0.39911522493595564, 0.13156406726635084, 0.34196402651026464, 0.31105429184678063, 0.0035502958579881655, 0.001764705882352941, 0.6017266359998108, 0.3017750257997936, 0.5645295908220735, 0.39911522493595564, 0.20016847640647667, 0.15444309084580576, 0.5603140283830224, 0.0, 0.20016748027110903, 0.3, 0.11654857888395853, 0.40205691275827504, 0.3, 0.878602979088468, 0.0, 0.0, 0.0, 0.1632495716733295, 0.3, 0.5214883874457796, 0.0922793680408852, 0.3195746877008, 0.2982456140350877, 0.24828300203160455, 0.10737628003753416, 0.3, 0.718004616527348, 0.1582769438976378, 0.8919617816507329, 0.0, 0.3448994154708214, 0.004538436450024038, 0.6584805580772018, 0.0, 0.3, 0.6111355833177097, 0.1528058302536389, 0.0, 0.22353884138521257, 0.0017751479289940828, 0.5714390554372559, 0.0, 0.09642857142857143, 0.0, 0.7513402885900375, 0.9583676774082095, 0.5945211356531896, 0.410760806494448, 0.40205691275827504, 0.08854143110967533, 0.3044711089354572, 0.4462423339355191, 0.6847225020943759, 0.7529597631989451, 0.05855117053177894, 0.3826023429967984, 0.3000699423288872, 0.15266794291681168, 0.851473618503158, 0.03400343404469311, 0.200167889092908, 0.6501974626497702, 0.15365708869382205, 0.0017751479289940828, 0.0, 0.048958126827207786, 0.23519928899661766, 0.0, 0.3563549897800658, 0.12190824467520357, 0.7933956534580795, 0.3000104420466411, 0.0035502958579881655, 0.9000116012906978, 0.0857142857142857, 0.15444309084580576, 0.40205691275827504, 0.258947632818908, 0.0, 0.5714390554372559, 0.3, 0.7529597631989451, 0.1574654480078945, 0.0, 0.7575401013437377, 0.3, 0.39911522493595564, 0.0, 0.3, 0.22353884138521257, 0.5714390554372559, 0.3000104420466411, 0.5603140283830224, 0.5714390554372559, 0.3, 0.461801790263048, 0.0, 0.5950464736659187, 0.7943484773407765, 0.3, 0.8352965648896227, 0.4216455205758487, 0.304063315744354, 0.8063115837355967, 0.6279139882560425, 0.8491081041733906, 0.5876846078061395, 0.6411835676492639, 0.031914893617021274, 0.008868095388541435, 0.2834301119102193, 0.023103363294703028, 0.12123115588504907, 0.5945211356531896, 0.9096059691496151, 0.12190824467520357, 0.0053254437869822485, 0.5945211356531896, 0.30525077013260515, 0.9491723111247035, 0.3, 0.5890950640144405, 0.3, 0.5996482799056162, 0.851473618503158, 0.5714390554372559, 0.8710877329906037, 0.0, 0.17398477112343996, 0.15265221878224974, 0.1774345886674628, 0.2982456140350877, 0.0, 0.3, 1.0, 0.0, 0.15835193452380952, 0.5714390554372559, 0.3849918505425901, 0.5836686808621074, 0.915038704727656, 0.5592165668582548, 0.49607488423322554, 0.9038115960204829, 1.0, 0.5714390554372559, 0.005294117647058823, 0.7529597631989451, 0.15266794291681168, 0.5714390554372559, 0.304063315744354, 0.0, 0.9270405384522935, 0.0, 0.565772183518662, 0.15119032972440946, 0.048855324703629605, 0.7933956534580795, 0.5720405283967344, 0.5836686808621074, 0.3, 0.39911522493595564, 0.22244545375248911, 0.9295451446225973, 0.0, 0.5876846078061395, 0.9231335375314982, 0.8801000781179811, 0.8222178322986009, 0.4203048443095777, 0.29765625, 0.31105429184678063, 0.3, 0.3, 0.8177796163617823, 0.0, 0.22884623718314856, 0.0, 0.15444309084580576, 0.3, 0.8338903424215631, 0.0, 0.22353884138521257, 0.39911522493595564, 0.001764705882352941, 0.5945211356531896, 0.29823529411764704, 0.5714390554372559, 0.0, 0.0, 0.0, 0.461801790263048, 0.520065681166334, 0.3, 0.0, 0.15249632145890663, 0.3, 0.3, 0.6950052177443995, 0.5876846078061395, 0.0, 0.1535525344488189, 0.49014082213719445, 0.5945211356531896, 0.6009637053528041, 0.9466388281978653, 0.520065681166334, 0.8919617816507329, 0.40205691275827504, 0.43705105469358285, 0.00472826495573826, 0.461801790263048, 0.7672956486967897, 0.0, 0.057476375990922965, 0.35854922732298666, 0.5890950640144405, 0.20016748027110903, 0.6303352294633915, 0.5933085293673459, 0.7545443906313413, 0.1509033613445378, 0.3000104420466411, 0.6009637053528041, 0.5890950640144405, 0.0, 0.5714390554372559, 0.1632495716733295, 0.21960390275188021, 0.39999999999999997, 0.7937792507448587, 0.8978235063872054, 0.5974308929991621, 0.4307796466718091, 0.7937792507448587, 1.0, 0.0, 0.12534849596478356, 0.0017751479289940828, 0.0, 0.9417126558697961, 0.3, 0.3, 0.322654150031617, 0.5945211356531896, 0.0, 0.39911522493595564, 0.15266794291681168, 0.6111355833177097, 0.058527851659204974, 0.39911522493595564, 0.32589830876083337, 0.48, 0.36277403359595956, 0.001764705882352941, 0.21326790453587363, 0.06769766628399457, 0.7545443906313413, 0.7237142695430476, 0.4718706224363608, 0.15277270308930718, 0.3114745785594705, 0.25855795141507687, 0.4080721780233069, 0.11502986560418761, 0.5890950640144405, 0.9490742122482716, 0.3285246983688919, 0.34863750024235235, 0.0, 0.0, 0.3488630468037407, 0.0, 0.32736490417294717, 0.3, 0.0053254437869822485, 0.9231335375314982, 0.8582125592357142, 0.3, 0.9000116012906978, 0.0, 0.8491081041733906, 0.5876846078061395, 0.3, 0.5714390554372559, 0.09942988898377537, 0.5945211356531896, 0.0, 0.5714390554372559, 0.0, 0.31105429184678063, 0.3, 0.5714390554372559, 0.5714390554372559, 0.3, 0.6009637053528041, 0.5645295908220735, 0.11654857888395853, 0.8581947585796906, 0.5603140283830224, 0.008487545593419856, 0.22353884138521257, 0.0, 0.5714390554372559, 0.6933643506684907, 0.5974308929991621, 0.3475123119913396, 0.9391101808531634, 0.5778510561745572, 0.20016748027110903, 0.0, 0.0017751479289940828, 0.40205691275827504, 0.09974489958442997, 0.005267273536587754, 0.3, 0.565772183518662, 0.8494586481735404, 0.0017751479289940828, 0.39218161804670426, 0.2836284308697359, 0.15089795494653793, 0.8491081041733906, 0.3, 0.657849870329454, 0.39911522493595564, 1.0, 0.0, 0.5645295908220735, 0.00981614539306847, 0.0, 0.3263601410058898, 0.20317941123954297, 0.30006994270871556, 0.10072114954544517, 0.519876872168143, 0.3, 0.1396959594485921]
Finish training and take 1h34m
