Namespace(log_name='./RQ5/tfix_900_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_900_2/codet5p_770m', data_dir='./data/RQ5/tfix_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "src: ['<%= appDir %>/img']       },       img: {         src: ['<%= appDir %>/fonts']", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "src: ['<%= appDir %>/img']       },       fonts: {         src: ['<%= appDir %>/fonts']"}]
***** Running training *****
  Num examples = 900
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 226
  train_loss = 17.2505
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 63.06 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:63.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 451
  train_loss = 10.1126
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 67.41 	 Previous best codebleu 63.06
  ********************
 Achieve Best bleu:67.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 676
  train_loss = 4.3693
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 65.5 	 Previous best codebleu 67.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 901
  train_loss = 2.1031
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 65.94 	 Previous best codebleu 67.41
  ********************
early stopping!!!
reload model from RQ5/tfix_900_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_900_2/test.jsonl
  codebleu = 62.51 
  Total = 500 
  Exact Fixed = 92 
[3, 17, 24, 47, 52, 54, 57, 59, 74, 81, 87, 109, 114, 118, 122, 128, 134, 140, 142, 145, 147, 153, 157, 160, 172, 176, 177, 178, 179, 182, 184, 188, 191, 192, 193, 198, 201, 202, 209, 212, 214, 225, 226, 232, 238, 252, 253, 258, 259, 263, 264, 265, 268, 289, 290, 296, 303, 305, 316, 327, 328, 333, 342, 347, 348, 350, 359, 360, 361, 367, 368, 372, 374, 377, 390, 393, 402, 415, 417, 428, 435, 439, 442, 445, 451, 458, 460, 471, 473, 474, 491, 497]
  Syntax Fixed = 5 
[58, 341, 373, 472, 482]
  Cleaned Fixed = 4 
[126, 186, 469, 475]
  ********************
  Total = 500 
  Exact Fixed = 92 
[3, 17, 24, 47, 52, 54, 57, 59, 74, 81, 87, 109, 114, 118, 122, 128, 134, 140, 142, 145, 147, 153, 157, 160, 172, 176, 177, 178, 179, 182, 184, 188, 191, 192, 193, 198, 201, 202, 209, 212, 214, 225, 226, 232, 238, 252, 253, 258, 259, 263, 264, 265, 268, 289, 290, 296, 303, 305, 316, 327, 328, 333, 342, 347, 348, 350, 359, 360, 361, 367, 368, 372, 374, 377, 390, 393, 402, 415, 417, 428, 435, 439, 442, 445, 451, 458, 460, 471, 473, 474, 491, 497]
  Syntax Fixed = 5 
[58, 341, 373, 472, 482]
  Cleaned Fixed = 4 
[126, 186, 469, 475]
  codebleu = 62.51 
[0.5463288860109367, 0.6932410335616551, 1.0, 0.31331096368700495, 0.5901760920934899, 0.5950366130605848, 0.14514545638818946, 0.5305797592584772, 0.43011545328563816, 0.12, 0.5368474083243506, 0.5638063820069366, 0.27668959821497896, 0.7569706667451968, 0.6179484048244397, 0.811117687292167, 1.0, 0.7099766481408452, 0.6885240359080308, 0.25129879162633795, 0.7145082485446812, 0.39323313167877133, 0.24915625771353556, 1.0, 0.5970182644937875, 0.00968011880999674, 0.9438645504684042, 0.32457194343511286, 0.679937506633708, 0.2627799385832944, 0.7059323615142865, 0.0, 0.5654473178899494, 0.9113854927948712, 0.6866354707252126, 0.6866388006399204, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6398412446099295, 0.5511412946838178, 0.35997270548760957, 0.10202520055807765, 0.6202064481033123, 0.7183643415906251, 0.3403395101580171, 1.0, 0.274670867776058, 0.8208732553989724, 0.5900038817627358, 0.350381564995214, 1.0, 0.6558003592103332, 1.0, 0.14111084163384835, 0.5194637253863024, 0.7135428903906851, 0.9325400085710811, 1.0, 0.6232902973738775, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.47826783263525435, 0.9429370522068694, 0.6197233100748608, 0.6877745357134093, 0.4790432550703725, 0.4199236174034782, 0.22310656320684347, 0.9488930040605423, 0.714297617187952, 0.5426744349179697, 1.0, 0.702664818575847, 0.29035107882031763, 0.12, 0.8965241752682729, 0.43317161913164237, 0.4229578192624543, 1.0, 0.3734424024864518, 0.5711359782538347, 0.7190865275431211, 0.6359585563603126, 0.5451417837590342, 0.7135428903906851, 0.9377106968911376, 0.8053935031810717, 0.09400818750619536, 0.8550899856418173, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.3056299173484432, 0.7470738524243741, 0.8349004181959196, 0.8639557211113615, 0.8097407224050479, 0.678539966221849, 0.8099513945828967, 0.9164266028701744, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.1814794487481174, 0.37449744609153746, 0.6683895153674697, 0.9348830123070253, 0.12843867304363568, 0.7919953379147748, 0.7468446370144718, 0.39272004516267667, 0.8114529051148651, 0.46400897568252475, 0.8795723049113977, 0.34650852962940437, 0.8896347633344268, 0.5374713058466929, 0.8809382266031476, 0.7302547136373172, 0.915342532525566, 0.8186184868545656, 0.12848683792609658, 0.2176694832919011, 0.8186530227858186, 0.2011770485941022, 0.8422865509093453, 0.5246680028432865, 0.4976113387647193, 0.3604625696285162, 0.7941349155411199, 0.12869111792447885, 0.9891483218006352, 0.6667528334695118, 0.45001501044136305, 0.5777099745360312, 0.5897938953870425, 0.10356766952267597, 0.7135428903906851, 0.6368551910912801, 1.0, 0.6498396339303277, 0.47180509653022784, 0.8249365300761395, 0.5459435933526045, 1.0, 0.37668959821497894, 0.5862322739141566, 0.22499999999999998, 0.4854183564853334, 0.65351688848084, 0.8114529051148651, 0.8796531437200525, 0.40825339177238446, 0.7810169160146574, 0.8933766349070695, 0.5869940359456796, 0.22499999999999998, 1.0, 0.39347209291868157, 0.42351479452714663, 0.7504004004704901, 0.7433647096337428, 0.2812652820919249, 0.6038653593066965, 0.6851384298635612, 0.3985877141559568, 0.5481181696435504, 0.40621281109105456, 0.45057251275615134, 0.7135428903906851, 0.45449078148809957, 0.8097262098770364, 0.4065032326897709, 0.8249365300761395, 1.0, 1.0, 1.0, 0.11453360085481801, 0.10281683324801448, 0.9194628641483602, 0.5164927712445991, 1.0, 0.6336876498782626, 0.8731931556322046, 0.6884739093341494, 0.7135428903906851, 0.4453557619745404, 0.7725146668513112, 1.0, 1.0, 0.8249365300761395, 0.7973077795721717, 0.43648868589003154, 0.5789908912417844, 0.007044218708112843, 1.0, 0.6441764742916929, 0.5919953379147749, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 0.41208749160752645, 0.6393098058368457, 0.4139240818757265, 0.7971354401872718, 0.8114529051148651, 0.6599223799332465, 0.4536716679605682, 1.0, 0.3954550081193335, 1.0, 0.8981777535482824, 0.174436274930823, 0.6129938455930134, 0.44619887519287305, 0.9320894171538912, 0.7586059507468779, 0.36651542412587346, 0.6740103526642764, 0.731117687292167, 0.15997084006767517, 0.8631713356602946, 1.0, 0.5811145153819444, 0.8841247388270204, 0.9414428605659417, 0.11558530246943605, 0.6307456993182354, 1.0, 0.1477105845049766, 0.7416865196891184, 0.6844125934903402, 0.7835504370427809, 0.6094129773172783, 1.0, 0.3965193255633748, 0.7875054975857689, 0.7471765364160405, 0.4268169073526533, 0.7471633983399641, 0.7263487966630597, 0.8106391824574766, 0.6127572978503959, 0.37487890958867326, 0.7225371723695806, 0.5042531454411046, 0.6350178817306947, 0.3859792791374429, 1.0, 1.0, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.2504033576162784, 0.9104757374009316, 1.0, 0.7299948711471688, 0.5017814354051087, 0.5585140118159326, 0.9250131914571023, 1.0, 1.0, 0.5095036337782055, 0.33372428676905674, 0.8249365300761395, 0.6876346520405138, 0.3714877971723338, 0.6182031708143285, 0.770797637788255, 0.6844125934903402, 0.43829433999099154, 0.6381394976554917, 0.6463729364766281, 0.40272438732935, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.5402772989489573, 0.41184583631680877, 0.5642680213004776, 0.8470997505392659, 0.14514545638818946, 0.5966289756845221, 0.8849384145327521, 1.0, 1.0, 0.7367857383161394, 0.22719082257468703, 0.6366015466501336, 0.6851384298635612, 0.7434950546708927, 0.8249365300761395, 0.7911679345786041, 0.573043959575943, 0.7212489793637704, 0.10758390671207611, 0.8374634481857526, 0.5761697895341449, 1.0, 0.2635243435799486, 0.8928704934314164, 0.6264747995737683, 0.9238877689380496, 0.8356658099532572, 0.7559607945381612, 0.8255980153481091, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 1.0, 0.714297617187952, 0.3502184077768308, 0.4737859805742357, 0.0, 0.7769082306429403, 0.5225082025963208, 0.4899659063626085, 0.6240048954556174, 0.8034645362012123, 0.9617868617067262, 1.0, 1.0, 0.4372429839446083, 0.17146180759042753, 0.7053304130816762, 0.6273488306532815, 0.8578047138519911, 0.5351981576963853, 0.656468030647374, 0.676689598214979, 0.3669033918159327, 0.6441847873258376, 0.5015372100944879, 0.7591116846422887, 0.8074255754853112, 0.9353714016159818, 0.3381394976554918, 0.7468277416227045, 0.6287796716807859, 0.44507381648898203, 1.0, 1.0, 0.513951094852316, 1.0, 0.7249586401416586, 0.63096386596976, 0.4147896756607047, 0.5652887122558813, 0.0, 0.4890714508161259, 0.5305242027189159, 0.4946481867823931, 0.9891483218006352, 1.0, 1.0, 0.5056745638342561, 0.6603944867604312, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 1.0, 1.0, 0.2696847134058581, 0.8378414230005442, 0.6311119478495857, 1.0, 0.6326079336090371, 1.0, 0.736074986663424, 0.6252766760873006, 1.0, 0.4129740706026147, 0.3053372331674803, 0.6651704051777761, 0.4621787547146431, 0.6398103351245982, 0.7394672816136985, 0.8526042515417132, 0.5200759256848564, 0.5627243873293499, 0.8233303578084277, 0.7530301180787049, 0.5418142013084455, 0.8114529051148651, 0.5654473178899494, 0.7756211334750283, 1.0, 0.19628771320267113, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.31860139828537487, 0.5566984938224081, 1.0, 0.752380452288719, 0.18585881606736232, 0.0, 0.4275835367519685, 0.6354739770278475, 0.8445506565039518, 0.6754386750930661, 0.0857142857142857, 0.0, 0.5114734401395804, 0.8655056700329364, 0.6714771286623475, 0.9020045992459838, 0.07233082870588604, 0.9891483218006352, 0.5554183564853334, 0.038139497655491794, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.5021287494162977, 0.5383089748131177, 0.5621609692548745, 0.5282467529717347, 0.5786580151554076, 1.0, 0.06808859345352361, 0.6183689771600134, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.6612315527854231, 1.0, 0.6829777303051304, 0.7304512265303131, 0.8861314475432027, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.683548763111264, 1.0, 0.636068568378893, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.6046886482005214, 1.0, 0.6124247015824554, 0.8917028689549173, 0.26174137339543524, 0.8648103351245982, 0.08484732042396881, 0.767085506912077, 1.0, 0.7387037834978107, 1.0, 0.6099380655621088, 0.859745689026256, 0.29371181369212096, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.4432027101248005, 0.859664396260277, 0.6807990952074163, 0.6809070226208283, 1.0, 0.813876410574395, 1.0, 1.0, 0.7503567123312754, 0.3, 0.6812000145029719, 0.8413708839086858, 0.8038961967467184, 0.43330399549625764, 0.527190822574687, 0.8955980328281301, 0.8115900311996962, 0.08251128145232256, 0.6785036503543816, 0.562520658360374, 0.6901940524102714, 0.46699403594567956, 0.23868348617790436, 0.5282087277319494, 1.0, 0.4619180665937579, 0.6986613814884177, 0.642418735455043, 0.2885770491944699, 0.8632148025904984, 1.0, 0.7533655306263078, 0.387966609042281, 0.6666057418239761]
Finish training and take 16m
