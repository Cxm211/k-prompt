Namespace(log_name='./RQ5/tfix_900_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_900_2/codet5p_220m_f', data_dir='./data/RQ5/tfix_900_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 900 training instances 
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00395
  global_step = 113
  train_loss = 0.8233
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00395
  ********************
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 60.29 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:60.29
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00341
  global_step = 225
  train_loss = 0.4206
  ********************
Previous best ppl:1.00395
Achieve Best ppl:1.00341
  ********************
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 62.17 	 Previous best codebleu 60.29
  ********************
 Achieve Best bleu:62.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00351
  global_step = 337
  train_loss = 0.273
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 62.96 	 Previous best codebleu 62.17
  ********************
 Achieve Best bleu:62.96
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00367
  global_step = 449
  train_loss = 0.1824
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 64.44 	 Previous best codebleu 62.96
  ********************
 Achieve Best bleu:64.44
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00387
  global_step = 561
  train_loss = 0.1303
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 66.5 	 Previous best codebleu 64.44
  ********************
 Achieve Best bleu:66.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00391
  global_step = 673
  train_loss = 0.0864
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 66.33 	 Previous best codebleu 66.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00414
  global_step = 785
  train_loss = 0.0579
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 67.07 	 Previous best codebleu 66.5
  ********************
 Achieve Best bleu:67.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00428
  global_step = 897
  train_loss = 0.043
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 65.12 	 Previous best codebleu 67.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00438
  global_step = 1009
  train_loss = 0.0334
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 66.12 	 Previous best codebleu 67.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00438
  global_step = 1121
  train_loss = 0.0301
  ********************
Previous best ppl:1.00341
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 67.33 	 Previous best codebleu 67.07
  ********************
 Achieve Best bleu:67.33
  ********************
reload model from RQ5/tfix_900_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_900_2/test.jsonl
  codebleu = 63.39 
  Total = 500 
  Exact Fixed = 99 
[3, 6, 9, 12, 17, 24, 40, 45, 54, 57, 59, 65, 74, 84, 97, 98, 109, 114, 118, 122, 128, 142, 143, 145, 147, 153, 157, 159, 170, 172, 176, 179, 182, 184, 188, 191, 192, 193, 194, 201, 202, 209, 212, 214, 215, 223, 226, 232, 238, 252, 253, 256, 263, 264, 268, 272, 289, 290, 296, 303, 305, 316, 327, 328, 330, 333, 336, 339, 340, 344, 347, 348, 350, 359, 360, 361, 367, 368, 372, 374, 390, 393, 402, 415, 417, 428, 435, 438, 439, 442, 457, 460, 471, 473, 474, 480, 481, 491, 497]
  Syntax Fixed = 4 
[58, 123, 132, 341]
  Cleaned Fixed = 8 
[35, 126, 186, 189, 198, 259, 469, 475]
  ********************
  Total = 500 
  Exact Fixed = 99 
[3, 6, 9, 12, 17, 24, 40, 45, 54, 57, 59, 65, 74, 84, 97, 98, 109, 114, 118, 122, 128, 142, 143, 145, 147, 153, 157, 159, 170, 172, 176, 179, 182, 184, 188, 191, 192, 193, 194, 201, 202, 209, 212, 214, 215, 223, 226, 232, 238, 252, 253, 256, 263, 264, 268, 272, 289, 290, 296, 303, 305, 316, 327, 328, 330, 333, 336, 339, 340, 344, 347, 348, 350, 359, 360, 361, 367, 368, 372, 374, 390, 393, 402, 415, 417, 428, 435, 438, 439, 442, 457, 460, 471, 473, 474, 480, 481, 491, 497]
  Syntax Fixed = 4 
[58, 123, 132, 341]
  Cleaned Fixed = 8 
[35, 126, 186, 189, 198, 259, 469, 475]
  codebleu = 63.39 
[0.5670445241047504, 0.6932410335616551, 1.0, 0.31331096368700495, 0.47971583246834554, 1.0, 0.6903510788203177, 0.545488887467509, 1.0, 0.12, 0.5368474083243506, 0.8182722690855053, 0.1761086396373927, 0.7569706667451968, 0.6179484048244397, 0.811117687292167, 1.0, 0.7099766481408452, 0.6885240359080308, 0.6974942116108808, 0.607332317251319, 0.39323313167877133, 0.24915625771353556, 1.0, 0.6170182644937875, 0.006758238160278511, 0.9438645504684042, 0.4868446707078401, 0.6624101654079205, 0.2627799385832944, 0.7059323615142865, 0.0, 0.5654473178899494, 0.9113854927948712, 0.8790355083665293, 0.42078646257801267, 0.0, 0.10556510016615601, 0.7155056700329363, 1.0, 0.5511412946838178, 0.35125137343938573, 0.10202520055807765, 0.6202064481033123, 1.0, 0.3403395101580171, 0.6570057594483909, 0.274670867776058, 0.8208732553989724, 0.5900038817627358, 0.350381564995214, 0.613862232528365, 0.6558003592103332, 1.0, 0.11550902229744162, 0.5194637253863024, 0.7135428903906851, 0.9325400085710811, 1.0, 0.6410676038432136, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.46839895677092, 1.0, 0.6197233100748608, 0.6877745357134093, 0.4706502402815602, 0.40941481853157663, 0.28269688894370654, 0.934857916341244, 0.714297617187952, 0.8508222909971193, 1.0, 0.702664818575847, 0.29035107882031763, 0.0, 0.8965241752682729, 0.39718089591628425, 0.41969276730357064, 0.591516286342532, 0.3734424024864518, 0.5544823112584711, 1.0, 0.6359585563603126, 0.5451417837590342, 0.48115588878443294, 0.9264825893849112, 0.8053935031810717, 0.44058280681232764, 0.8550899856418173, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.39874985593507484, 0.7470738524243741, 1.0, 1.0, 0.7962637890346522, 0.44341793336632984, 0.6227790992712059, 0.9164266028701744, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.18795337214953334, 0.4593867118293433, 0.8399114554115357, 1.0, 0.05270446328111167, 0.7919953379147748, 0.521979242240402, 0.39272004516267667, 0.8114529051148651, 0.4819989645916939, 0.8795723049113977, 0.6903510788203177, 1.0, 0.5143209096122012, 0.7017610307492962, 0.7302547136373172, 1.0, 0.9377649116482698, 0.18701137824089908, 0.2176694832919011, 0.8186530227858186, 0.25985408445050595, 1.0, 0.5246680028432865, 0.4976113387647193, 0.4488041447621831, 0.6674409515718728, 0.12869111792447885, 0.39089314466834374, 0.5818498519004407, 0.4460952454685818, 0.553492655520654, 0.3796893547104485, 0.14876752225153467, 0.4136911179244788, 0.7368551910912801, 1.0, 1.0, 0.47180509653022784, 0.8249365300761395, 0.6387218496729328, 1.0, 0.37668959821497894, 0.5862322739141566, 0.1231765493603594, 0.4854183564853334, 0.6612091961731477, 0.8114529051148651, 0.5106720338983175, 0.40825339177238446, 0.7810169160146574, 1.0, 0.5669940359456795, 0.6443152851788645, 0.6684064493975892, 0.41700150468338737, 0.5370116053397941, 0.7504004004704901, 0.7954279937030855, 0.4549898009498242, 0.6038653593066965, 0.6851384298635612, 0.49215257204284135, 0.21502357973075026, 1.0, 0.4551179673016059, 0.7135428903906851, 0.2190447235990943, 0.9500330416345475, 0.4065032326897709, 0.8249365300761395, 0.41996134394226886, 0.5745997086539989, 1.0, 0.11453360085481801, 0.36686807121046905, 1.0, 0.5164927712445991, 1.0, 0.7560741640181184, 0.8731931556322046, 0.6884739093341494, 0.7135428903906851, 0.19535576197454044, 0.7725146668513112, 1.0, 1.0, 0.8249365300761395, 1.0, 0.4892890065241897, 0.5091411104747474, 0.007044218708112843, 0.8837480609952844, 0.7955692089739603, 0.5621787547146431, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 0.41208749160752645, 0.6393098058368457, 0.4139240818757265, 0.6613122421311423, 0.8114529051148651, 0.11532493541972998, 0.6116850869767493, 1.0, 0.30462621654037875, 1.0, 1.0, 0.174436274930823, 0.6129938455930134, 0.44619887519287305, 0.9320894171538912, 0.7170168473475249, 0.4580200993563527, 0.6552622309992373, 1.0, 0.15997084006767517, 0.6315157939140275, 1.0, 0.677288551000834, 0.8554514458010952, 0.12, 0.11558530246943605, 0.6307456993182354, 1.0, 0.36501923786697255, 0.7416865196891184, 0.6844125934903402, 0.8083325080431145, 0.5981716191316424, 1.0, 0.3965193255633748, 0.7875054975857689, 0.7471765364160405, 0.5467122787808074, 0.7304967316732975, 0.7263487966630597, 0.9056583090096291, 0.6321782328024446, 0.3544440011696263, 0.7225371723695806, 0.474518199459927, 0.6350178817306947, 0.5765896565883736, 1.0, 1.0, 0.5436363636363637, 0.19999999999999998, 1.0, 0.2772943579237247, 0.836942030771336, 0.9109112023233705, 0.7299948711471688, 0.5017814354051087, 0.39450059105565455, 1.0, 1.0, 0.1737650385863868, 0.6624678525854575, 0.33372428676905674, 0.8249365300761395, 0.6876346520405138, 0.3714877971723338, 0.6515365041476618, 1.0, 0.6844125934903402, 0.43829433999099154, 0.19999999999999998, 0.6463729364766281, 0.4016534478992738, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.4785438531809256, 0.46604306419874064, 0.6877459225855624, 0.8470997505392659, 0.6903510788203177, 0.5966289756845221, 0.8849384145327521, 1.0, 1.0, 0.5516551101233702, 0.20178247382494602, 0.6366015466501336, 0.6851384298635612, 0.8576471663887342, 0.8249365300761395, 0.7911679345786041, 0.573043959575943, 0.7212489793637704, 0.10758390671207611, 0.6987789944244591, 0.5761697895341449, 1.0, 0.5170015046833875, 1.0, 0.6264747995737683, 0.9238877689380496, 0.8356658099532572, 0.769581697257735, 0.8119808925419361, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 1.0, 0.714297617187952, 0.3502184077768308, 0.366646776848975, 0.06, 0.8378414230005442, 0.5225082025963208, 0.4899659063626085, 0.6240048954556174, 0.8034645362012123, 0.9617868617067262, 1.0, 1.0, 0.17540387552681966, 1.0, 0.7359735741809917, 0.6273488306532815, 1.0, 0.5351981576963853, 0.8078356986646513, 1.0, 0.3669033918159327, 0.6857637800864577, 1.0, 1.0, 0.9547081880846386, 0.5680524575325056, 0.3381394976554918, 1.0, 0.6287796716807859, 0.5424094289706055, 1.0, 1.0, 0.513951094852316, 1.0, 0.8069007178395178, 0.9176675865658077, 0.4147896756607047, 0.593263745179413, 0.0, 0.4890714508161259, 0.5305242027189159, 0.6745228788727515, 0.9891483218006352, 1.0, 1.0, 0.6245836923955046, 0.6980083797903092, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 1.0, 1.0, 0.2696847134058581, 0.8378414230005442, 0.444006594916457, 1.0, 0.32898762131831805, 1.0, 0.7700435930643703, 0.5700879883608101, 0.34997666751308226, 0.4129740706026147, 0.3053372331674803, 0.7905056700329363, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.6642192630689461, 0.7256172063424196, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.8114529051148651, 0.5654473178899494, 0.7756211334750283, 1.0, 0.17731538118686083, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.13663064026329025, 0.5019774510104837, 0.39374707916270535, 0.5566984938224081, 1.0, 0.752380452288719, 0.18585881606736232, 0.39969884422068214, 0.6567867859128023, 0.6354739770278475, 0.8445506565039518, 0.6754386750930661, 0.0857142857142857, 0.0, 0.4423752571753023, 0.8655056700329364, 0.8, 0.9020045992459838, 0.05757638252580623, 0.9891483218006352, 0.39753490965039084, 0.038139497655491794, 0.15529892970465703, 0.2518991111280603, 0.4778533416343903, 0.7054819616225767, 0.5383089748131177, 0.5621609692548745, 0.5282467529717347, 0.6453246818220744, 1.0, 0.18702348228670979, 0.6183689771600134, 0.4376297585326638, 0.09442828917687912, 0.685303136808272, 0.6747700062911317, 1.0, 0.6829777303051304, 0.5189518586699597, 1.0, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.8845572237610086, 0.8292906179772745, 0.636068568378893, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.5518505299283467, 0.6592906179772744, 0.6124247015824554, 0.8917028689549173, 0.26174137339543524, 0.8648103351245982, 0.0828714707479615, 1.0, 0.8326781496971694, 0.7387037834978107, 1.0, 0.6326444856701964, 0.7780387576883845, 0.29371181369212096, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.32501740942395907, 0.859664396260277, 0.6807990952074163, 0.6809070226208283, 1.0, 0.7195565989965169, 1.0, 1.0, 0.8969369858475811, 0.3, 0.6812000145029719, 0.7954890670336492, 0.7894413517535362, 1.0, 0.8249365300761395, 0.8955980328281301, 0.8115900311996962, 0.08251128145232256, 0.6785036503543816, 0.6654045722268074, 0.7451384298635613, 0.5089439475762831, 0.2686834861779044, 0.5282087277319494, 1.0, 0.5986986916029925, 0.5044012054608318, 0.6580098924657095, 0.5674033738135131, 0.6385629301801026, 1.0, 0.7533655306263078, 0.2943237530591414, 0.5230501541874496]
Finish training and take 25m
Namespace(log_name='./RQ5/tfix_900_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_900_2/codet5p_220m_f', data_dir='./data/RQ5/tfix_900_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 900 training instances 
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00403
  global_step = 113
  train_loss = 0.8323
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00403
  ********************
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 61.26 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:61.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00372
  global_step = 225
  train_loss = 0.4561
  ********************
Previous best ppl:1.00403
Achieve Best ppl:1.00372
  ********************
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 63.63 	 Previous best codebleu 61.26
  ********************
 Achieve Best bleu:63.63
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00387
  global_step = 337
  train_loss = 0.3316
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 63.76 	 Previous best codebleu 63.63
  ********************
 Achieve Best bleu:63.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00394
  global_step = 449
  train_loss = 0.2244
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 64.22 	 Previous best codebleu 63.76
  ********************
 Achieve Best bleu:64.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00404
  global_step = 561
  train_loss = 0.1612
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 65.99 	 Previous best codebleu 64.22
  ********************
 Achieve Best bleu:65.99
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.0045
  global_step = 673
  train_loss = 0.122
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 66.38 	 Previous best codebleu 65.99
  ********************
 Achieve Best bleu:66.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00477
  global_step = 785
  train_loss = 0.1001
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 67.66 	 Previous best codebleu 66.38
  ********************
 Achieve Best bleu:67.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00469
  global_step = 897
  train_loss = 0.0751
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 67.01 	 Previous best codebleu 67.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00484
  global_step = 1009
  train_loss = 0.0604
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 66.35 	 Previous best codebleu 67.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00489
  global_step = 1121
  train_loss = 0.0561
  ********************
Previous best ppl:1.00372
BLEU file: ./data/RQ5/tfix_900_2/validation.jsonl
  codebleu-4 = 67.06 	 Previous best codebleu 67.66
  ********************
reload model from RQ5/tfix_900_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_900_2/test.jsonl
  codebleu = 63.41 
  Total = 500 
  Exact Fixed = 92 
[3, 12, 17, 24, 40, 45, 54, 57, 59, 84, 87, 88, 98, 118, 120, 122, 135, 143, 145, 147, 153, 154, 157, 159, 170, 172, 176, 179, 182, 184, 188, 192, 193, 194, 201, 202, 209, 212, 214, 215, 226, 232, 236, 238, 253, 255, 256, 263, 264, 268, 289, 290, 292, 296, 303, 316, 323, 327, 328, 333, 336, 340, 344, 348, 350, 360, 361, 367, 368, 372, 374, 377, 390, 391, 393, 401, 415, 417, 418, 428, 435, 438, 439, 442, 445, 460, 472, 474, 481, 483, 493, 497]
  Syntax Fixed = 6 
[123, 132, 252, 288, 341, 373]
  Cleaned Fixed = 5 
[126, 186, 198, 237, 469]
  ********************
  Total = 500 
  Exact Fixed = 92 
[3, 12, 17, 24, 40, 45, 54, 57, 59, 84, 87, 88, 98, 118, 120, 122, 135, 143, 145, 147, 153, 154, 157, 159, 170, 172, 176, 179, 182, 184, 188, 192, 193, 194, 201, 202, 209, 212, 214, 215, 226, 232, 236, 238, 253, 255, 256, 263, 264, 268, 289, 290, 292, 296, 303, 316, 323, 327, 328, 333, 336, 340, 344, 348, 350, 360, 361, 367, 368, 372, 374, 377, 390, 391, 393, 401, 415, 417, 418, 428, 435, 438, 439, 442, 445, 460, 472, 474, 481, 483, 493, 497]
  Syntax Fixed = 6 
[123, 132, 252, 288, 341, 373]
  Cleaned Fixed = 5 
[126, 186, 198, 237, 469]
  codebleu = 63.41 
[0.62189838351731, 0.6932410335616551, 1.0, 0.31331096368700495, 0.6058511595444083, 0.42061917766750423, 0.6903510788203177, 0.5483719692837501, 0.4930978393785904, 0.12, 0.5368474083243506, 0.8182722690855053, 0.27668959821497896, 0.7569706667451968, 0.6179484048244397, 0.811117687292167, 1.0, 0.7099766481408452, 0.6885240359080308, 0.6974942116108808, 0.7744985103174877, 0.39323313167877133, 0.24915625771353556, 1.0, 0.5970182644937875, 0.008533731833720836, 0.9438645504684042, 0.5714627929397353, 0.6624101654079205, 0.2627799385832944, 0.7059323615142865, 0.0, 0.5654473178899494, 0.45853203931260444, 0.596137006832348, 0.3035662793418755, 0.0, 0.10556510016615601, 0.7155056700329363, 1.0, 0.5511412946838178, 0.805671205945659, 0.10202520055807765, 0.6202064481033123, 1.0, 0.3403395101580171, 0.6570057594483909, 0.274670867776058, 0.6428181853195161, 0.5900038817627358, 0.7163708719711295, 0.6333338146326758, 0.6558003592103332, 1.0, 0.1297947365831559, 0.35143265215385816, 0.7135428903906851, 0.6395188779694462, 1.0, 0.6232902973738775, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.46839895677092, 0.8336815104017958, 0.6197233100748608, 0.6877745357134093, 0.4613914994591235, 0.41310543558529644, 0.28269688894370654, 0.934857916341244, 0.714297617187952, 0.8508222909971193, 0.706487662226795, 0.702664818575847, 0.29035107882031763, 0.0, 0.8965241752682729, 0.43317161913164237, 0.5196927673035706, 0.591516286342532, 0.3734424024864518, 0.8761578331254429, 1.0, 0.5684395130452333, 0.5451417837590342, 0.7135428903906851, 1.0, 0.8053935031810717, 0.09400818750619536, 0.8550899856418173, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.32445344676020793, 0.7470738524243741, 0.8349004181959196, 1.0, 0.8097407224050479, 0.5688724788208753, 0.6357178601838935, 0.47407940047941444, 0.7275723927836586, 0.6957883137213398, 0.7744515423179406, 0.0, 0.4593867118293433, 0.8399114554115357, 0.6862320164251722, 0.05270446328111167, 0.7919953379147748, 0.521979242240402, 0.39272004516267667, 0.6553082743436025, 0.4819989645916939, 0.8795723049113977, 0.6903510788203177, 1.0, 0.5143209096122012, 1.0, 0.7302547136373172, 1.0, 0.9377649116482698, 0.3773869844754326, 0.2176694832919011, 0.8576781496971693, 0.26608813774165146, 0.779575317067877, 0.5246680028432865, 0.4976113387647193, 0.4488041447621831, 0.6674409515718728, 0.12869111792447885, 0.39089314466834374, 1.0, 0.4460952454685818, 0.553492655520654, 0.5897938953870425, 0.14876752225153467, 0.4136911179244788, 0.6368551910912801, 0.7245741928928766, 1.0, 0.47180509653022784, 0.8249365300761395, 0.6387218496729328, 1.0, 0.37668959821497894, 0.5862322739141566, 0.26493187938965956, 0.4854183564853334, 0.65351688848084, 0.8114529051148651, 1.0, 0.40825339177238446, 0.7810169160146574, 1.0, 0.5669940359456795, 0.6443152851788645, 0.6684064493975892, 0.41700150468338737, 0.5370116053397941, 0.7504004004704901, 0.7954279937030855, 0.4236670441615923, 0.6038653593066965, 0.5908527155778469, 0.5665461297817389, 0.3500494532758355, 1.0, 0.6868474083243508, 0.7135428903906851, 0.45449078148809957, 0.914405525803786, 0.4065032326897709, 0.8249365300761395, 0.41996134394226886, 0.5745997086539989, 1.0, 0.11453360085481801, 0.36686807121046905, 1.0, 0.5164927712445991, 1.0, 0.7560741640181184, 0.8731931556322046, 0.6884739093341494, 0.7135428903906851, 0.19535576197454044, 0.7725146668513112, 0.6622821987298955, 1.0, 0.8249365300761395, 1.0, 0.43648868589003154, 0.5875015040818632, 0.007044218708112843, 0.8837480609952844, 0.7955692089739603, 0.5186834861779044, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 0.41208749160752645, 0.6271806220068676, 0.4139240818757265, 0.7971354401872718, 0.8114529051148651, 0.5433701851171201, 0.6116850869767493, 1.0, 0.32627945612776854, 1.0, 1.0, 0.12357226111775113, 0.5796605122596801, 0.44619887519287305, 0.9320894171538912, 0.7755969629986152, 0.4580200993563527, 0.6552622309992373, 0.731117687292167, 0.15997084006767517, 0.691318859382493, 1.0, 0.677288551000834, 0.8841247388270204, 0.9414428605659417, 0.11558530246943605, 0.6307456993182354, 1.0, 0.36501923786697255, 0.5992413083109663, 0.6844125934903402, 1.0, 0.6119613551150405, 1.0, 0.3965193255633748, 0.6967912641947691, 0.7471765364160405, 0.4268169073526533, 0.6676628470669137, 0.7263487966630597, 0.9056583090096291, 0.6127572978503959, 0.43111016267734115, 0.7225371723695806, 0.474518199459927, 0.6350178817306947, 0.38674741724324424, 0.9542333350628116, 1.0, 0.5436363636363637, 0.6443152851788645, 1.0, 0.2772943579237247, 0.836942030771336, 0.6368668028812028, 0.7299948711471688, 0.5017814354051087, 0.5585140118159326, 1.0, 1.0, 0.22668959821497897, 0.5588880593698827, 0.33372428676905674, 0.8249365300761395, 0.6852301695897789, 0.3714877971723338, 0.6848698374809952, 0.8041309711215883, 0.6844125934903402, 0.43829433999099154, 0.6381394976554917, 0.6463729364766281, 0.4016534478992738, 0.6174969094123448, 0.7142845202249919, 0.6, 0.5600468038501167, 0.4452105198475923, 0.46604306419874064, 0.6877459225855624, 0.8470997505392659, 0.6903510788203177, 0.5966289756845221, 0.8849384145327521, 1.0, 1.0, 0.8487963120906614, 0.8249365300761395, 0.6366015466501336, 0.6851384298635612, 0.8576471663887342, 0.8249365300761395, 0.7911679345786041, 0.573043959575943, 0.7212489793637704, 0.007841910291181075, 0.7771392226518876, 0.5761697895341449, 1.0, 0.5170015046833875, 0.7817235549062289, 0.6264747995737683, 0.9238877689380496, 0.8356658099532572, 0.769581697257735, 0.8255980153481091, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 1.0, 0.714297617187952, 0.3502184077768308, 0.3586305064272467, 0.06666666666666667, 0.7769082306429403, 0.5225082025963208, 1.0, 0.6240048954556174, 0.8034645362012123, 0.9617868617067262, 1.0, 1.0, 0.4372429839446083, 0.5686687819283739, 0.6830311640568745, 0.6273488306532815, 1.0, 0.5351981576963853, 0.656468030647374, 1.0, 0.3669033918159327, 0.5296364499994474, 0.5015372100944879, 1.0, 0.907357971193316, 0.5680524575325056, 0.3381394976554918, 1.0, 0.6287796716807859, 0.5424094289706055, 0.5398855599662549, 1.0, 0.513951094852316, 1.0, 0.8069007178395178, 0.9176675865658077, 0.45024959573777057, 0.5652887122558813, 0.0, 0.4890714508161259, 0.5305242027189159, 0.46752831296989483, 0.5964858645175034, 1.0, 1.0, 0.6245836923955046, 0.6980083797903092, 0.4614470406899055, 0.13121909816749983, 0.5932305781811854, 1.0, 1.0, 0.2696847134058581, 0.8378414230005442, 0.444006594916457, 1.0, 0.9142445273649933, 1.0, 0.7700435930643703, 0.5700879883608101, 1.0, 0.4129740706026147, 0.3053372331674803, 0.7630012724941725, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.7527386005943568, 0.6559473234898057, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.5878679124993766, 0.8114529051148651, 1.0, 0.771420955333468, 1.0, 0.17731538118686083, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.11781871369667504, 0.5019774510104837, 0.39374707916270535, 1.0, 0.6371687036544995, 0.752380452288719, 0.18585881606736232, 0.39969884422068214, 0.6567867859128023, 0.6354739770278475, 0.7192415097097951, 0.6754386750930661, 0.0857142857142857, 0.0, 0.5978989144790716, 0.8655056700329364, 0.7055680377532567, 1.0, 0.05757638252580623, 0.9891483218006352, 0.9891483218006352, 0.038139497655491794, 0.15529892970465703, 0.2815400648057302, 0.3582729220539707, 0.7054819616225767, 0.5383089748131177, 0.5621609692548745, 0.5282467529717347, 0.6786580151554076, 1.0, 0.06808859345352361, 0.6183689771600134, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.7288312733021203, 1.0, 0.6829777303051304, 0.5189518586699597, 1.0, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.8845572237610086, 1.0, 0.6, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.5518505299283467, 0.6592906179772744, 0.0, 0.8408575494851418, 0.26174137339543524, 0.8648103351245982, 0.0828714707479615, 0.767085506912077, 0.8326781496971694, 0.7387037834978107, 1.0, 0.6099380655621088, 0.7780387576883845, 0.29371181369212096, 0.4714698407062744, 0.8677459225855626, 0.6066478602423253, 0.4432027101248005, 0.859664396260277, 0.6807990952074163, 0.8170015046833874, 0.7316423857228858, 1.0, 0.6811422794173713, 1.0, 0.5564477064653401, 0.3, 0.6812000145029719, 0.7954890670336492, 0.8322984946106791, 0.6517965513447935, 0.8249365300761395, 0.8955980328281301, 1.0, 0.08251128145232256, 0.6785036503543816, 0.6654045722268074, 0.6901940524102714, 0.5089439475762831, 0.23868348617790436, 0.5282087277319494, 0.7135160729534664, 0.5986986916029925, 1.0, 0.6580098924657095, 0.4863077893818773, 0.6385629301801026, 1.0, 0.7533655306263078, 0.387966609042281, 0.7693212151712596]
Finish training and take 10m
