Namespace(log_name='./RQ5/tfix_700_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_700_3/codet5p_770m', data_dir='./data/RQ5/tfix_700_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "var filter = require('./ansi')   , crypto = require('crypto')   , _ = require('underscore')", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "var crypto = require('crypto')   , _ = require('lodash')"}]
***** Running training *****
  Num examples = 700
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 176
  train_loss = 16.7794
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_700_3/validation.jsonl
  codebleu-4 = 61.65 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:61.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 351
  train_loss = 7.9429
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_3/validation.jsonl
  codebleu-4 = 61.93 	 Previous best codebleu 61.65
  ********************
 Achieve Best bleu:61.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 526
  train_loss = 3.9106
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_3/validation.jsonl
  codebleu-4 = 62.35 	 Previous best codebleu 61.93
  ********************
 Achieve Best bleu:62.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 701
  train_loss = 1.8099
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_3/validation.jsonl
  codebleu-4 = 63.21 	 Previous best codebleu 62.35
  ********************
 Achieve Best bleu:63.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 876
  train_loss = 0.9156
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_3/validation.jsonl
  codebleu-4 = 61.72 	 Previous best codebleu 63.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1051
  train_loss = 0.5423
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_3/validation.jsonl
  codebleu-4 = 61.47 	 Previous best codebleu 63.21
  ********************
early stopping!!!
reload model from RQ5/tfix_700_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_700_3/test.jsonl
  codebleu = 62.41 
  Total = 500 
  Exact Fixed = 105 
[5, 13, 14, 19, 25, 44, 47, 55, 62, 65, 67, 69, 77, 78, 79, 85, 93, 101, 109, 111, 117, 121, 123, 126, 129, 131, 134, 136, 147, 148, 149, 151, 152, 153, 156, 160, 162, 169, 170, 176, 178, 182, 187, 188, 191, 193, 197, 198, 199, 205, 213, 217, 225, 226, 229, 233, 238, 252, 253, 261, 269, 271, 284, 291, 295, 308, 317, 336, 337, 339, 342, 344, 345, 351, 356, 357, 358, 359, 366, 377, 387, 389, 403, 406, 417, 418, 437, 439, 440, 441, 445, 446, 449, 453, 465, 475, 483, 484, 487, 488, 489, 491, 493, 495, 497]
  Syntax Fixed = 6 
[41, 263, 272, 316, 411, 425]
  Cleaned Fixed = 5 
[1, 9, 61, 120, 425]
  ********************
  Total = 500 
  Exact Fixed = 105 
[5, 13, 14, 19, 25, 44, 47, 55, 62, 65, 67, 69, 77, 78, 79, 85, 93, 101, 109, 111, 117, 121, 123, 126, 129, 131, 134, 136, 147, 148, 149, 151, 152, 153, 156, 160, 162, 169, 170, 176, 178, 182, 187, 188, 191, 193, 197, 198, 199, 205, 213, 217, 225, 226, 229, 233, 238, 252, 253, 261, 269, 271, 284, 291, 295, 308, 317, 336, 337, 339, 342, 344, 345, 351, 356, 357, 358, 359, 366, 377, 387, 389, 403, 406, 417, 418, 437, 439, 440, 441, 445, 446, 449, 453, 465, 475, 483, 484, 487, 488, 489, 491, 493, 495, 497]
  Syntax Fixed = 6 
[41, 263, 272, 316, 411, 425]
  Cleaned Fixed = 5 
[1, 9, 61, 120, 425]
  codebleu = 62.41 
[0.4719671371303185, 0.8170015046833874, 0.861672684198352, 0.3668284850162606, 0.7450395226130992, 0.8697471734123385, 0.7252232811270083, 0.11582292438020222, 0.6119613551150405, 0.6779253872262, 0.19726550033812454, 0.7067366858513033, 0.6640033451354017, 1.0, 0.39323313167877133, 0.6127572978503959, 0.4124293426116586, 0.28308268582718266, 1.0, 0.6693361062523177, 0.8214203428813993, 0.3743172882353815, 0.15, 0.5010924624736267, 1.0, 0.25196016717593894, 0.5572292603035471, 0.15747669278081078, 0.762178754714643, 0.110722640539825, 0.535133493667276, 0.6142482969348242, 0.9041157122172334, 0.4198769757196277, 0.3326555560719357, 0.8149139863647084, 0.469915144816797, 0.774882651615667, 0.6934531647858191, 0.41673766744658963, 0.8739326133501213, 0.22267396568917747, 0.5802333034257401, 0.9891483218006352, 0.5481721142074378, 0.5450835185056716, 1.0, 0.5961851509772491, 0.46877089663917104, 0.8951959058230954, 0.5524581952101042, 0.5266057619745405, 0.8223873319410013, 0.32435175818106565, 0.8249365300761395, 0.861482988138697, 0.8968939355581875, 0.767135578574561, 0.7802740295913121, 0.6148576874551961, 0.9426294298277487, 1.0, 0.5610771314034941, 0.5897692121354159, 0.8229808431453671, 0.48289849585465594, 1.0, 0.6170182644937875, 1.0, 0.7539630945946609, 0.11773099927637243, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 0.5007846217557572, 0.20613487118166404, 1.0, 1.0, 1.0, 0.6724062327149246, 0.6530747837026651, 0.6387289085944345, 0.19999999999999998, 0.6953856477238599, 1.0, 0.9179350389391119, 0.0, 0.2966597993138361, 0.0485646584311115, 0.5675986891240699, 0.1525765264139964, 0.7299948711471688, 1.0, 0.814064525684886, 0.4134404736596011, 0.4484520389345378, 0.7459627599846212, 0.016782765293716924, 0.686863907300452, 0.483668171350054, 0.8114529051148651, 0.4690750900518629, 0.8199440232953434, 0.09999999999999999, 0.07719082257468701, 0.3722124348261592, 0.48167268419835185, 0.467913762972279, 1.0, 0.5024101654079205, 1.0, 0.3970127658747812, 0.35473913473843294, 0.5379694287810409, 0.2924537101693822, 0.5566638701334943, 0.9474466761527047, 0.2610380151684039, 0.19999999999999998, 0.08953276868214524, 1.0, 0.9068864709786153, 0.9472083239018387, 0.10906101778358473, 0.8840188084091536, 1.0, 0.4118819918812901, 0.726624382466148, 1.0, 0.32616770616700436, 1.0, 0.7386991709927695, 0.4102973371873174, 1.0, 0.22619887519287302, 1.0, 0.7853313927871439, 0.5860678628763278, 0.6052067080970429, 0.6937464703296872, 0.24161578968825664, 0.5960707868709735, 0.27528728007290226, 0.5440521407974296, 0.8206722495022543, 0.9119641296947072, 1.0, 1.0, 1.0, 0.39063674796052605, 1.0, 1.0, 1.0, 0.158215633431174, 0.7728222909971192, 0.8249365300761395, 0.6241730049753154, 0.8245957586820076, 0.7057697464205994, 0.7135428903906851, 0.6119863884320217, 0.7863340021370451, 0.5968596406330632, 0.5301818996067954, 0.7424282823720908, 0.6745836923955046, 0.6639648813321528, 0.23985271067894742, 1.0, 0.7135428903906851, 0.6336131842675646, 0.49538717829403817, 0.4831458551130241, 0.5532531256091296, 0.6477517173276517, 1.0, 0.5882210413806648, 1.0, 0.9057101190955084, 0.510409269308891, 0.7269320077439674, 0.757641796129519, 0.7302007404183433, 0.8426568563575418, 0.8516700572634182, 0.03126528209192489, 1.0, 1.0, 0.4231498701419104, 0.5609260915878896, 1.0, 0.5728633222115624, 0.7989785824292399, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 1.0, 0.8114529051148651, 1.0, 0.39873667339437135, 0.15429293535427072, 0.6530550965302279, 0.74448602493095, 0.6867787885259955, 1.0, 0.225962006708376, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.31888879608349985, 0.21245371016938225, 0.349188727096548, 1.0, 0.44665344789927386, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6571186983115961, 0.3771457608737994, 0.48779329396875104, 0.59820323220824, 0.3662701788045931, 0.6473692768047277, 0.6072359854134853, 1.0, 1.0, 0.5863724797464211, 0.43907354318680836, 1.0, 0.5660528353107435, 0.39359500830217886, 0.6022970392156293, 0.9891483218006352, 0.47035576197454043, 0.7521271372862586, 0.3486710568928237, 0.6726527033303111, 1.0, 0.6056583090096291, 0.6029055732198362, 0.3698963377183132, 0.7387824622428694, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6208906511898491, 0.7450769337104594, 0.1717728294285068, 0.4435027248795795, 0.8113118214155393, 0.04101911673989318, 1.0, 1.0, 0.6179144359460748, 0.4089903936086955, 0.4223835094667513, 0.0, 0.8213422758367226, 0.7355980328281302, 0.16826111698077859, 0.8114529051148651, 0.8476757297426298, 0.969001580992227, 0.604436274930823, 0.565895296523178, 0.38953081833493347, 0.0, 0.7142857142857142, 1.0, 0.367641751923923, 1.0, 0.9090440252597953, 0.6463338290119179, 0.177190822574687, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.8020701463285549, 0.11349072297901378, 0.6533110667596613, 0.06423223587777374, 0.6260995190602316, 0.6041077305454495, 1.0, 0.48349462208179644, 0.4141117485802601, 0.29123935575300347, 0.5673438326090772, 0.6768986255548952, 0.7014788118630328, 1.0, 0.29286074313859833, 0.8398078253515011, 0.8272282238829733, 0.7826555560719357, 0.027660853979899265, 0.5550049310474542, 0.8835687037030036, 0.5843539719797703, 0.5447856242749173, 0.43701826449378744, 0.6833239592056684, 0.16077569427652932, 0.3814205333654446, 0.7191441569283882, 0.3374011779985145, 0.6107000692160132, 1.0, 0.40660822240095185, 0.816225472769031, 0.8122043420775711, 0.3433562648816456, 0.7460474584273435, 0.8162863460162433, 0.46526859680566335, 0.801169789534145, 0.9318657024016066, 0.48178507258525927, 0.6177593079856705, 0.5678911585054794, 0.5537998353931795, 0.38714411643939495, 0.6392333266349789, 0.4620851799860557, 0.37549356386576216, 0.6621875491343581, 0.8269967312248111, 0.5189589454524783, 0.5382922926202521, 0.4452247813076979, 0.6865490061448141, 0.6742738870938194, 0.5441771864234439, 0.3682936672819867, 0.4707909983708979, 0.8114529051148651, 0.9096188406411201, 0.7144948346694379, 1.0, 0.8765344915115144, 0.6352900380997268, 1.0, 0.7513521789741, 1.0, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.4322768829518761, 0.6027336075746548, 1.0, 0.6999214919756145, 0.7889213653390947, 0.629920929034602, 0.6737569575259978, 1.0, 1.0, 0.8632106722899557, 1.0, 0.5140913145175611, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.3509982897994032, 1.0, 0.4407283286877167, 0.4509771412274911, 0.34003623200632915, 0.24887760375821233, 0.6863884298635612, 0.40960005732452515, 0.4366371358026476, 0.7397718799592691, 0.22072807389765117, 0.053829931618554525, 1.0, 0.6892870539097846, 0.6361171661900396, 0.545545649326698, 0.8042470965743207, 0.5926202483423699, 0.5650407172771029, 0.05127041479686025, 0.49605468697269356, 0.6742613142686853, 1.0, 0.8623473261888206, 1.0, 0.6193453441662242, 0.8306583090096291, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 0.8572450667968947, 0.7009116410097869, 0.7308835793300246, 0.35437970233785665, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 1.0, 0.7313559408133775, 0.7017410285928236, 1.0, 0.8237476432464355, 0.4054183564853334, 0.23979057308787627, 0.5991554821598547, 0.9365276486140073, 0.691735368923108, 0.5084373166195929, 0.8696187021511392, 0.40209781994840155, 0.5386099127446873, 1.0, 1.0, 0.602487595382981, 0.24658767523161879, 0.23683637774429345, 0.6428938087244226, 0.3372632052291401, 0.6971179107073656, 0.8931507219600794, 0.16272438732935, 0.19272438732935, 0.6418894161880858, 0.5149139863647084, 0.24206654807458877, 0.21537156508959837, 0.8661547359011423, 0.15472876794829787, 0.688062756936776, 0.39999999999999997, 0.7023594991536818, 1.0, 0.8102775878383006, 1.0, 0.78750379167119, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 1.0, 1.0, 0.5167652853967755, 0.6901940524102714, 1.0, 0.6733027629890207, 0.42038175275163125, 0.8542712839031905, 1.0, 0.39206686043974615, 0.28890659929471596, 0.6578607431385983, 0.5353082743436026, 0.7826042022748454, 0.8334272760895116, 0.7153782274663947, 0.0058478456042703325, 0.8740753321011412, 0.47102574599540464, 0.5811145153819444, 1.0, 0.46628336334230014, 0.5075783901825247, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.43676944598007206, 0.8777227983675986, 0.32327815492769574, 0.7398815472813818, 0.8114529051148651, 0.643309301212212, 0.30215269230653224, 0.12, 0.8010669062882427, 0.7016265496309229, 0.766682914692771, 0.46843920200122524, 1.0, 1.0, 0.6137392159029667, 0.4333355150138679, 1.0, 1.0, 1.0, 0.6629581948739375, 1.0, 0.6186389706521688, 1.0, 0.7634373166195929, 1.0, 0.5909395885250581, 1.0, 0.6479881652559543, 0.6542743822687915, 0.7158229243802021]
Finish training and take 20m
