Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 114
  train_loss = 77.0104
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.17 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 227
  train_loss = 61.7336
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.26 	 Previous best codebleu 77.17
  ********************
 Achieve Best bleu:77.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 340
  train_loss = 50.7474
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.42 	 Previous best codebleu 77.26
  ********************
 Achieve Best bleu:77.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 453
  train_loss = 43.098
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.38 	 Previous best codebleu 77.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 566
  train_loss = 36.8521
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.14 	 Previous best codebleu 77.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 679
  train_loss = 31.8659
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.35 	 Previous best codebleu 77.42
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_900_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_900_2/test.jsonl
  codebleu = 75.12 
  Total = 500 
  Exact Fixed = 4 
[73, 122, 314, 376]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[254, 355]
  ********************
  Total = 500 
  Exact Fixed = 4 
[73, 122, 314, 376]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[254, 355]
  codebleu = 75.12 
[0.8807691318538904, 0.9895907412940455, 0.38230642541905163, 0.8285503011428544, 0.5240259477923144, 0.21679053483574018, 0.49007773220454587, 0.355723773966904, 0.5722965788545334, 0.6717364058179986, 0.9486021234366251, 0.982653611198544, 0.6917402809720604, 0.7848241483620148, 0.8808375937505108, 0.4693955864510486, 0.5313357348360503, 0.9785699270907009, 0.8061799752233711, 0.9403863087164004, 0.8757845494319172, 0.589751403979808, 0.959945180208001, 0.9194422946236107, 0.8596055360028632, 0.46678356460890147, 0.5452287811589471, 0.901175714210833, 0.9194633420202082, 0.24041604209738301, 0.7460250732072086, 0.8189939955289076, 0.8694219104751457, 0.9673280321241631, 0.5393913750037733, 0.8059663450023036, 0.6527764179246671, 0.9426787386209374, 0.9847813251545576, 0.6214028319131882, 0.38055604522674474, 0.9602327743514232, 0.9321918895830998, 0.23616145602914845, 0.806490922254441, 0.9386210590467857, 0.8574290303367051, 0.7113199842059187, 0.6194178615287717, 0.8803828210815028, 0.42728670898141874, 0.9382702025626704, 0.790108255342182, 0.6131424394366902, 0.8705702138654792, 0.9247535154657887, 0.8675700280536951, 0.8931834746717269, 0.9771748654592214, 0.6449181547930165, 0.9047326852038393, 0.3321670864049434, 0.977929685147775, 0.663952114335559, 0.8927899452472845, 0.9299687695441708, 0.7924048855268875, 0.5338617744031935, 0.8995377367121644, 0.20429331224367825, 0.9338818208296811, 0.2690529321329123, 0.9674900184215338, 0.9717119879651379, 0.871143401314145, 0.586404697829096, 0.8384056697171074, 0.9737915388152598, 0.9321212086381336, 0.3895038116127578, 0.7113008970718839, 0.9379551891165208, 0.9080151803246834, 0.8441882987189739, 0.3484876459370154, 0.42829672464868107, 0.8014139035130625, 0.8085386086430999, 0.43890738161489035, 0.39806555763771617, 0.9352829048891074, 0.8305048279578491, 0.9529223256974573, 0.5985680617000989, 0.6326608997800717, 0.9486052495001633, 0.9348200839674985, 0.3304581157501194, 0.5505450693049353, 0.8332321631831101, 0.9060561003826615, 0.948343184028229, 0.4636944731423779, 0.7763592102744433, 0.9857368328793006, 0.929608977441901, 0.8365809500852361, 0.4810010686280327, 0.9430889397206723, 0.9194883538624099, 0.6249954610889428, 0.6181106606584272, 0.8077465948099589, 0.6566376813197878, 0.6817132254112156, 0.8077897114293389, 0.6678107811875859, 0.9362801340504037, 0.3870508792782098, 0.854906905704294, 0.9027144378842089, 1.0, 0.8257815238937638, 0.8744345587385776, 0.78946048013366, 0.584684973565248, 0.676126243442005, 0.43419770508491723, 0.4657333819352129, 0.8443840073818547, 0.3560328527895647, 0.6120229206037862, 0.3110090555266167, 0.956301548538977, 0.9617303176335186, 0.3085356854689776, 0.9368026696042517, 0.553376063289682, 0.9721595555198352, 0.7920617845792342, 0.9242338290598, 0.8741647989058143, 0.9251375134812259, 0.9745780518930731, 0.928943235004591, 0.8233729810068096, 0.4982318233525185, 0.5720677290587932, 0.9521775318277603, 0.9830296722149274, 0.975255869034251, 0.8320476098374262, 0.9813483172696138, 0.9483831980682167, 0.8948670458495385, 0.774693514748012, 0.7202507328873297, 0.5488586128738006, 0.7091570039785193, 0.7242120100978349, 0.1359821622512627, 0.6777438912696802, 0.962102708701077, 0.9121866600841926, 0.9021902095080698, 0.46124203184813023, 0.6246113794477428, 0.8392067093595759, 0.968639364262265, 0.8297713512550178, 0.6258706049241418, 0.7833624454787003, 0.8975978993714945, 0.953840714887719, 0.7280873261576656, 0.8260346729220063, 0.8300820971250142, 0.824261585549334, 0.3124097564359915, 0.9410067825194739, 0.5399191018302276, 0.5474206174767057, 0.25613546330440956, 0.7364469121089812, 0.8694952470533828, 0.6867415067356185, 0.32301796222457846, 0.945208493585485, 0.8658945472295025, 0.49086107377017946, 0.59613181707828, 0.9404774184308464, 0.7167958853235616, 0.5928108277067345, 0.5946982102419093, 0.195805705470289, 0.849431210776371, 0.5430885679069021, 0.6872874943212983, 0.9642836546885394, 0.1427616762031343, 0.9508336769881993, 0.9441311059579389, 0.08464285714285713, 0.961472991307396, 0.39013478691113623, 0.6684903987689361, 0.9447313137718458, 0.5317588322123294, 0.9087123429574342, 0.9253109087939462, 0.9701007019297219, 0.8124892826539233, 0.953707845659838, 0.8090043314450341, 0.8916198680457144, 0.9520611043918519, 0.7669752623463025, 0.12695294117647057, 0.19832929234074576, 0.8055864673667326, 0.9477526452574314, 0.9324333440186039, 0.5282409716007119, 0.3330584663396393, 0.4822120322474115, 0.8044136830940034, 0.8942668643714475, 0.9232567135322138, 0.6221781999238016, 0.6234595852577036, 0.6586811579191418, 0.9488836086215009, 0.8861568600710625, 0.5085215178080368, 0.8553678887608935, 0.7803741792559828, 0.7826741503670085, 0.9634485800803125, 0.5624756082365959, 0.8695075881277006, 0.7901580927216791, 0.9861254102280717, 0.9327028990399184, 0.947573383205105, 0.9013388166447551, 0.6351531795376738, 0.9583676774082095, 0.9377858074101935, 0.9495937875206693, 0.9221927719632479, 0.9604527042523284, 0.7556814728598797, 0.943907690508595, 0.943666571833899, 0.6014294709438306, 0.7562791074610632, 0.7015341116696883, 0.2412059707864596, 0.9279866539122665, 0.19752609894716355, 0.707850059682499, 0.9284243634590652, 0.695713048670164, 0.9562005086759873, 0.9176543852669206, 0.26289253457845224, 0.9860950428335316, 0.8282194733540387, 0.6304354583069092, 0.9692036989337549, 0.6365891247781525, 0.9425986064448988, 0.9617839620555828, 0.9266465778740411, 0.9531844084589156, 0.7481412556512403, 0.9498690706862425, 0.8277424644338935, 0.7565711923511311, 0.0968369622508172, 0.8179579674310407, 0.9773514349376944, 0.8434094165290678, 0.7748901690895643, 0.7807079788494218, 0.8988279604599121, 0.8852945092796116, 0.9758763186213713, 0.5547416116865838, 0.9407613352737074, 0.7232043610871355, 0.7493577391671854, 0.5076730012609921, 0.8776015021220545, 0.8207174296242723, 0.46158825189644004, 0.9840970086239778, 0.9049996265971042, 0.9147509231273736, 0.5075905017189635, 0.849729607166194, 0.6345917614751148, 0.9732456940628658, 0.9668495847081082, 0.46192836465257714, 0.961309010760973, 0.9470279890319612, 0.8857658481998287, 0.19077672396674258, 0.7761717845434185, 0.8206082150573881, 0.940663678095734, 1.0, 0.8943767670150264, 0.6372760488740846, 0.9398939635294166, 0.26268118865076506, 0.637455335351989, 0.3454641283088835, 0.9655267507815484, 0.8397074882045157, 0.79908393484228, 0.8939721301684738, 0.6924354337443256, 0.6498389062539932, 0.9644727771943793, 0.7501255059004492, 0.8904098910010204, 0.9666856331752072, 0.9296230645706806, 0.9357982417496262, 0.9295976632675957, 0.6516235135434673, 0.9866981257943701, 0.570989795360499, 0.853963666419484, 0.7311090731099734, 0.8785117532819908, 0.9067124558663866, 0.6830631050329368, 0.802792334029212, 0.9105346103428551, 0.9156225125497286, 0.8788846428096109, 0.9094304925332901, 0.9283813677168311, 0.6216058374935409, 0.8202499593885415, 0.845605645877975, 0.7621663912158146, 0.9629364535821479, 0.30228003420613947, 0.9590506396386802, 0.9393720392848304, 0.7108578888786369, 0.29119834125570104, 0.9312349008025642, 0.9539904817368636, 0.940720679223622, 0.9498733743967636, 0.9262386824642832, 0.2535810718077191, 0.9130634810804314, 0.6272631541189376, 0.6070449868747402, 0.4328828981880556, 0.9731496374915098, 0.29399777482219386, 0.246656971808108, 0.8212828564690575, 0.3742924995543897, 0.9446842919965326, 0.42428373858678303, 0.4661937691043915, 0.9815931630248864, 0.28768815913136014, 0.8259108946799769, 0.9646055689051855, 0.9570869332853229, 0.867980815381816, 0.7552847912459166, 0.4488896705464462, 0.8961056193845135, 0.9839935011430094, 0.9200506209960793, 0.29803001771573584, 0.7243561152332358, 0.5898743279997497, 0.31764171070174957, 0.9136846145795283, 0.8036605977879999, 0.8078579828142751, 0.8562515048247652, 0.6226908114170765, 0.9544248082184021, 0.9127782544487824, 0.8699110799666838, 0.3325195752446274, 0.9465224177706513, 0.8249675351435815, 0.962229839777732, 0.9482573744384641, 0.8238518141083319, 0.41222995207253776, 0.6562955803712851, 0.5643672366251316, 0.43808453932073843, 0.5864458370254797, 0.9735465142797541, 0.321597484442518, 0.8050090290148326, 0.9624306767024622, 0.9307762742678802, 0.967150116841063, 0.990108133691983, 0.7514114718545895, 0.8485021818446042, 0.6612191334623871, 0.7832059164639751, 0.5558554508448601, 0.944817116312316, 0.945897725871365, 0.6100220174806957, 0.895887649579229, 0.7914330306940703, 0.9792319542491341, 0.9784095540181448, 0.9001704063035856, 0.9516757686175951, 0.8601706721604159, 0.7628541847804483, 0.4680849519025553, 0.9557759455949837, 0.9675589330971408, 0.7246670782300332, 0.7331328454992438, 0.5761336941152942, 0.7331472448108353, 0.6362439585829495, 0.7611677828299435, 0.8601402886946303, 0.7701904603659033, 0.390945448375335, 0.6592136870095452, 0.9794987286009202, 0.5978112872134451, 0.2256423390984703, 0.8038306806820452, 0.8836863746511332, 0.7964726901713943, 0.7601507876079147, 0.8724907736260945, 0.5412219101899178, 0.8696331987786137, 0.8118756794025181, 0.5348646474710699, 0.7562934717237396, 0.9756645028428577, 0.9489885523091097, 0.8981602981015966, 0.36665611193413983, 0.9715471473605286, 0.03831948613452212, 0.9177843916565737, 0.9031355431153669, 0.9723407670151949, 0.7262546855990275, 0.9469679666767363, 0.9439887848825383, 0.9405088388791978, 0.9823448177520746, 0.4777354426610022, 0.8992156948231914, 0.43449863768167823, 0.9262112558154699, 0.8354059606910897, 0.8648423559233974, 0.25489934536355485, 0.8564822037598797, 0.7936098293669693, 0.6810842044592471, 0.3043680407866811, 0.366718424897402, 0.9207225522863047, 0.8678976437379708, 0.44949524080022113, 0.6099943883989547, 0.8574088571104024, 0.8389497416642955, 0.8921126866250588, 1.0, 0.9292619245050848, 0.8227600561608516, 0.959095521027233, 0.4927053152538463, 0.4129771429438506, 0.8020816921073726, 0.9271328028414589, 0.8457781931186075]
Finish training and take 34m
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 114
  train_loss = 77.0104
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 114
  train_loss = 77.26
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.34 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 227
  train_loss = 61.506
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.21 	 Previous best codebleu 77.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 340
  train_loss = 50.7326
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.35 	 Previous best codebleu 77.34
  ********************
 Achieve Best bleu:77.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 453
  train_loss = 42.7142
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.36 	 Previous best codebleu 77.35
  ********************
 Achieve Best bleu:77.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 566
  train_loss = 36.3369
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.34 	 Previous best codebleu 77.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 679
  train_loss = 31.0988
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.42 	 Previous best codebleu 77.36
  ********************
 Achieve Best bleu:77.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 792
  train_loss = 27.0294
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.5 	 Previous best codebleu 77.42
  ********************
 Achieve Best bleu:77.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 905
  train_loss = 23.2963
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.42 	 Previous best codebleu 77.5
  ********************
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 114
  train_loss = 77.0104
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.14 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 227
  train_loss = 61.7336
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.23 	 Previous best codebleu 77.14
  ********************
 Achieve Best bleu:77.23
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 340
  train_loss = 50.7474
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.39 	 Previous best codebleu 77.23
  ********************
 Achieve Best bleu:77.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 453
  train_loss = 43.098
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.36 	 Previous best codebleu 77.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 566
  train_loss = 36.8521
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.12 	 Previous best codebleu 77.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 679
  train_loss = 31.8659
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.33 	 Previous best codebleu 77.39
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_900_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_900_2/test.jsonl
  codebleu = 75.13 
  Total = 500 
  Exact Fixed = 4 
[73, 122, 314, 376]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[254, 355]
  ********************
  Total = 500 
  Exact Fixed = 4 
[73, 122, 314, 376]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[254, 355]
  codebleu = 75.13 
[0.8807691318538904, 0.9895907412940455, 0.38230642541905163, 0.8285503011428544, 0.5240259477923144, 0.21679053483574018, 0.49007773220454587, 0.355723773966904, 0.5722965788545334, 0.6717364058179986, 0.9486021234366251, 0.982653611198544, 0.7050736143053937, 0.7848241483620148, 0.8808375937505108, 0.4693955864510486, 0.5313357348360503, 0.9785699270907009, 0.8061799752233711, 0.9403863087164004, 0.8757845494319172, 0.589751403979808, 0.959945180208001, 0.9194422946236107, 0.8596055360028632, 0.46678356460890147, 0.5452287811589471, 0.901175714210833, 0.9194633420202082, 0.24041604209738301, 0.7460250732072086, 0.8189939955289076, 0.8694219104751457, 0.9673280321241631, 0.5393913750037733, 0.8059663450023036, 0.6527764179246671, 0.9426787386209374, 0.9847813251545576, 0.6214028319131882, 0.38055604522674474, 0.9602327743514232, 0.9321918895830998, 0.23616145602914845, 0.806490922254441, 0.9386210590467857, 0.8574290303367051, 0.7113199842059187, 0.6194178615287717, 0.8803828210815028, 0.42728670898141874, 0.9382702025626704, 0.790108255342182, 0.6131424394366902, 0.8705702138654792, 0.9247535154657887, 0.8675700280536951, 0.8931834746717269, 0.9771748654592214, 0.6449181547930165, 0.9047326852038393, 0.3321670864049434, 0.977929685147775, 0.663952114335559, 0.8927899452472845, 0.9299687695441708, 0.7924048855268875, 0.5338617744031935, 0.8995377367121644, 0.20429331224367825, 0.9338818208296811, 0.2690529321329123, 0.9674900184215338, 0.9717119879651379, 0.871143401314145, 0.586404697829096, 0.8384056697171074, 0.9737915388152598, 0.9321212086381336, 0.3895038116127578, 0.7113008970718839, 0.9379551891165208, 0.9080151803246834, 0.8441882987189739, 0.3484876459370154, 0.42829672464868107, 0.8014139035130625, 0.8085386086430999, 0.43890738161489035, 0.39806555763771617, 0.9352829048891074, 0.8305048279578491, 0.9529223256974573, 0.5985680617000989, 0.6326608997800717, 0.9486052495001633, 0.9348200839674985, 0.3304581157501194, 0.5505450693049353, 0.8332321631831101, 0.9060561003826615, 0.948343184028229, 0.4636944731423779, 0.7763592102744433, 0.9857368328793006, 0.929608977441901, 0.8365809500852361, 0.4810010686280327, 0.9430889397206723, 0.9194883538624099, 0.6249954610889428, 0.6181106606584272, 0.8077465948099589, 0.6566376813197878, 0.6817132254112156, 0.8077897114293389, 0.6678107811875859, 0.9362801340504037, 0.4195809997601375, 0.854906905704294, 0.9027144378842089, 1.0, 0.8257815238937638, 0.8744345587385776, 0.78946048013366, 0.584684973565248, 0.676126243442005, 0.43419770508491723, 0.4657333819352129, 0.8443840073818547, 0.3560328527895647, 0.6120229206037862, 0.3110090555266167, 0.956301548538977, 0.9617303176335186, 0.3085356854689776, 0.9368026696042517, 0.553376063289682, 0.9721595555198352, 0.7920617845792342, 0.9242338290598, 0.8741647989058143, 0.9251375134812259, 0.9745780518930731, 0.928943235004591, 0.8233729810068096, 0.4982318233525185, 0.5720677290587932, 0.9521775318277603, 0.9830296722149274, 0.975255869034251, 0.8320476098374262, 0.9813483172696138, 0.9483831980682167, 0.8948670458495385, 0.774693514748012, 0.7202507328873297, 0.5488586128738006, 0.7091570039785193, 0.7242120100978349, 0.1359821622512627, 0.6777438912696802, 0.962102708701077, 0.9121866600841926, 0.9021902095080698, 0.46124203184813023, 0.6246113794477428, 0.8392067093595759, 0.968639364262265, 0.8297713512550178, 0.6258706049241418, 0.7833624454787003, 0.8975978993714945, 0.953840714887719, 0.7280873261576656, 0.8260346729220063, 0.8300820971250142, 0.824261585549334, 0.3124097564359915, 0.9410067825194739, 0.5399191018302276, 0.5474206174767057, 0.25613546330440956, 0.7364469121089812, 0.8694952470533828, 0.6867415067356185, 0.32301796222457846, 0.945208493585485, 0.8658945472295025, 0.49086107377017946, 0.59613181707828, 0.9404774184308464, 0.7167958853235616, 0.5928108277067345, 0.5946982102419093, 0.195805705470289, 0.849431210776371, 0.5430885679069021, 0.6872874943212983, 0.9642836546885394, 0.1427616762031343, 0.9508336769881993, 0.9441311059579389, 0.08464285714285713, 0.961472991307396, 0.39013478691113623, 0.6684903987689361, 0.9447313137718458, 0.5317588322123294, 0.9087123429574342, 0.9253109087939462, 0.9701007019297219, 0.8124892826539233, 0.953707845659838, 0.8090043314450341, 0.8916198680457144, 0.9520611043918519, 0.7669752623463025, 0.12695294117647057, 0.19832929234074576, 0.8055864673667326, 0.9477526452574314, 0.9324333440186039, 0.5282409716007119, 0.3330584663396393, 0.4822120322474115, 0.8044136830940034, 0.8942668643714475, 0.9232567135322138, 0.6221781999238016, 0.6234595852577036, 0.6586811579191418, 0.9488836086215009, 0.8861568600710625, 0.5085215178080368, 0.8553678887608935, 0.7803741792559828, 0.7826741503670085, 0.9634485800803125, 0.5624756082365959, 0.8695075881277006, 0.7901580927216791, 0.9861254102280717, 0.9327028990399184, 0.947573383205105, 0.9013388166447551, 0.6351531795376738, 0.9583676774082095, 0.9377858074101935, 0.9495937875206693, 0.9221927719632479, 0.9604527042523284, 0.7556814728598797, 0.943907690508595, 0.943666571833899, 0.6014294709438306, 0.7562791074610632, 0.7015341116696883, 0.2412059707864596, 0.9279866539122665, 0.19752609894716355, 0.707850059682499, 0.9284243634590652, 0.695713048670164, 0.9562005086759873, 0.9176543852669206, 0.26289253457845224, 0.9860950428335316, 0.8282194733540387, 0.6304354583069092, 0.9692036989337549, 0.6365891247781525, 0.9425986064448988, 0.9617839620555828, 0.9266465778740411, 0.9531844084589156, 0.7481412556512403, 0.9498690706862425, 0.8277424644338935, 0.7565711923511311, 0.0968369622508172, 0.8179579674310407, 0.9728059803922399, 0.8434094165290678, 0.7748901690895643, 0.7807079788494218, 0.8988279604599121, 0.8783177650935652, 0.9758763186213713, 0.5547416116865838, 0.9407613352737074, 0.7232043610871355, 0.7493577391671854, 0.5076730012609921, 0.8776015021220545, 0.8207174296242723, 0.46158825189644004, 0.9840970086239778, 0.9049996265971042, 0.9147509231273736, 0.5075905017189635, 0.849729607166194, 0.6345917614751148, 0.9732456940628658, 0.9668495847081082, 0.46192836465257714, 0.961309010760973, 0.9470279890319612, 0.8857658481998287, 0.19077672396674258, 0.7761717845434185, 0.8206082150573881, 0.940663678095734, 1.0, 0.8943767670150264, 0.6372760488740846, 0.9398939635294166, 0.26268118865076506, 0.637455335351989, 0.3454641283088835, 0.9655267507815484, 0.8397074882045157, 0.79908393484228, 0.8939721301684738, 0.6924354337443256, 0.6498389062539932, 0.9644727771943793, 0.7501255059004492, 0.8904098910010204, 0.9666856331752072, 0.9296230645706806, 0.9357982417496262, 0.9295976632675957, 0.6516235135434673, 0.9866981257943701, 0.570989795360499, 0.853963666419484, 0.7311090731099734, 0.8785117532819908, 0.9067124558663866, 0.6830631050329368, 0.802792334029212, 0.9105346103428551, 0.9156225125497286, 0.8788846428096109, 0.9094304925332901, 0.9283813677168311, 0.6216058374935409, 0.8202499593885415, 0.845605645877975, 0.7621663912158146, 0.9629364535821479, 0.30228003420613947, 0.9590506396386802, 0.9393720392848304, 0.7108578888786369, 0.29119834125570104, 0.9312349008025642, 0.9539904817368636, 0.940720679223622, 0.9498733743967636, 0.9262386824642832, 0.2535810718077191, 0.9130634810804314, 0.6272631541189376, 0.6070449868747402, 0.4328828981880556, 0.9731496374915098, 0.29399777482219386, 0.246656971808108, 0.8212828564690575, 0.3742924995543897, 0.9446842919965326, 0.42428373858678303, 0.4661937691043915, 0.9815931630248864, 0.28768815913136014, 0.8259108946799769, 0.9646055689051855, 0.9570869332853229, 0.867980815381816, 0.7552847912459166, 0.4488896705464462, 0.8990467958551017, 0.9839935011430094, 0.9200506209960793, 0.29803001771573584, 0.7243561152332358, 0.5898743279997497, 0.31764171070174957, 0.9136846145795283, 0.8036605977879999, 0.8078579828142751, 0.8562515048247652, 0.6226908114170765, 0.9544248082184021, 0.9127782544487824, 0.8699110799666838, 0.3325195752446274, 0.9465224177706513, 0.8249675351435815, 0.962229839777732, 0.9482573744384641, 0.8238518141083319, 0.41222995207253776, 0.6562955803712851, 0.5643672366251316, 0.43808453932073843, 0.5864458370254797, 0.9735465142797541, 0.321597484442518, 0.8050090290148326, 0.9624306767024622, 0.9307762742678802, 0.967150116841063, 0.990108133691983, 0.7514114718545895, 0.8485021818446042, 0.6612191334623871, 0.7832059164639751, 0.5558554508448601, 0.944817116312316, 0.945897725871365, 0.6100220174806957, 0.895887649579229, 0.7914330306940703, 0.9792319542491341, 0.9784095540181448, 0.9001704063035856, 0.9516757686175951, 0.8601706721604159, 0.7628541847804483, 0.4680849519025553, 0.9557759455949837, 0.9675589330971408, 0.7246670782300332, 0.7331328454992438, 0.5761336941152942, 0.7331472448108353, 0.6362439585829495, 0.7611677828299435, 0.8601402886946303, 0.7701904603659033, 0.390945448375335, 0.6592136870095452, 0.9794987286009202, 0.5978112872134451, 0.2256423390984703, 0.8038306806820452, 0.8836863746511332, 0.7964726901713943, 0.7601507876079147, 0.8724907736260945, 0.5412219101899178, 0.8696331987786137, 0.8118756794025181, 0.5348646474710699, 0.7562934717237396, 0.9756645028428577, 0.9489885523091097, 0.8981602981015966, 0.36665611193413983, 0.9715471473605286, 0.03831948613452212, 0.9177843916565737, 0.9031355431153669, 0.9723407670151949, 0.7262546855990275, 0.9469679666767363, 0.9439887848825383, 0.9405088388791978, 0.9823448177520746, 0.4777354426610022, 0.8992156948231914, 0.43449863768167823, 0.9262112558154699, 0.8354059606910897, 0.8648423559233974, 0.25489934536355485, 0.8564822037598797, 0.7936098293669693, 0.6810842044592471, 0.3043680407866811, 0.366718424897402, 0.9207225522863047, 0.8678976437379708, 0.44949524080022113, 0.6339943883989547, 0.8574088571104024, 0.8389497416642955, 0.8921126866250588, 1.0, 0.9292619245050848, 0.8227600561608516, 0.959095521027233, 0.4927053152538463, 0.4129771429438506, 0.8020816921073726, 0.9271328028414589, 0.8457781931186075]
Finish training and take 33m
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 114
  train_loss = 77.26
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.36 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 227
  train_loss = 61.506
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.22 	 Previous best codebleu 77.36
  ********************
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10
Namespace(log_name='./RQ5/xcodeeval_900_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_900_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_900_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h> int main() {     signed char n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x&&y&&z)         printf("NO");         else         printf("YES"); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {      int n , i ,  in ;     int x=0 , y=0 , z=0 ;     scanf("%d",&n);     for(i=0;i<n*3;i++)     {         static char j = 0 ;         scanf("%d",&in);         switch (j)         {             case 0 :              x+=in;             j++;             break ;             case 1 :              y+=in;             j++;             break;             case 2 :              z+=in;             j=0;             break;         }              }    // printf("%d %d %d",x,y,z);     if(x||y||z)         printf("NO");         else         printf("YES"); }'}]
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 114
  train_loss = 77.26
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.38 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:77.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 227
  train_loss = 61.506
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.24 	 Previous best codebleu 77.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 340
  train_loss = 50.7326
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.38 	 Previous best codebleu 77.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 453
  train_loss = 42.7142
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.39 	 Previous best codebleu 77.38
  ********************
 Achieve Best bleu:77.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 566
  train_loss = 36.3369
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.37 	 Previous best codebleu 77.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 679
  train_loss = 31.0988
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.46 	 Previous best codebleu 77.39
  ********************
 Achieve Best bleu:77.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 792
  train_loss = 27.0294
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.54 	 Previous best codebleu 77.46
  ********************
 Achieve Best bleu:77.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 905
  train_loss = 23.2963
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.46 	 Previous best codebleu 77.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 1018
  train_loss = 20.7362
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.5 	 Previous best codebleu 77.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 1131
  train_loss = 18.9539
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/xcodeeval_900_2/validation.jsonl
  codebleu-4 = 77.49 	 Previous best codebleu 77.54
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_900_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_900_2/test.jsonl
  codebleu = 75.08 
  Total = 500 
  Exact Fixed = 6 
[73, 122, 289, 314, 376, 500]
  Syntax Fixed = 2 
[20, 192]
  Cleaned Fixed = 2 
[254, 355]
  ********************
  Total = 500 
  Exact Fixed = 6 
[73, 122, 289, 314, 376, 500]
  Syntax Fixed = 2 
[20, 192]
  Cleaned Fixed = 2 
[254, 355]
  codebleu = 75.08 
[0.8815127397903371, 0.9791134139687718, 0.38230642541905163, 0.8285503011428544, 0.5240259477923144, 0.21679053483574018, 0.49007773220454587, 0.355723773966904, 0.5722965788545334, 0.676908819611102, 0.9486021234366251, 0.982653611198544, 0.5027657795043073, 0.7848241483620148, 0.8808375937505108, 0.4693955864510486, 0.5313357348360503, 0.9785699270907009, 0.8061799752233711, 0.980066765837468, 0.8757845494319172, 0.589751403979808, 0.959945180208001, 0.9194422946236107, 0.8596055360028632, 0.46678356460890147, 0.5452287811589471, 0.901175714210833, 0.9194633420202082, 0.24041604209738301, 0.7460250732072086, 0.8189939955289076, 0.8748336476451133, 0.9551838853557779, 0.5393913750037733, 0.8059663450023036, 0.6527764179246671, 0.9426787386209374, 0.9847813251545576, 0.6214028319131882, 0.38055604522674474, 0.9802578447391919, 0.5692522168307597, 0.23616145602914845, 0.806490922254441, 0.9501595205852472, 0.8574290303367051, 0.7113199842059187, 0.6194178615287717, 0.8803828210815028, 0.42728670898141874, 0.9382702025626704, 0.790108255342182, 0.6131424394366902, 0.8705702138654792, 0.9247535154657887, 0.8675700280536951, 0.8931834746717269, 0.9771748654592214, 0.6449181547930165, 0.9047326852038393, 0.3321670864049434, 0.977929685147775, 0.6494321838412691, 0.8927899452472845, 0.9299687695441708, 0.7498412208967206, 0.5338617744031935, 0.8995377367121644, 0.20429331224367825, 0.9338818208296811, 0.2690529321329123, 0.9674900184215338, 0.9717119879651379, 0.871143401314145, 0.586404697829096, 0.8384056697171074, 0.9737915388152598, 0.9321212086381336, 0.3895038116127578, 0.7113008970718839, 0.9379551891165208, 0.9080151803246834, 0.8441882987189739, 0.3484876459370154, 0.42829672464868107, 0.8014139035130625, 0.8085386086430999, 0.43890738161489035, 0.39806555763771617, 0.9352829048891074, 0.8305048279578491, 0.9282946627251291, 0.5985680617000989, 0.6326608997800717, 0.9486052495001633, 0.9348200839674985, 0.3304581157501194, 0.5505450693049353, 0.8332321631831101, 0.9060561003826615, 0.948343184028229, 0.4636944731423779, 0.7763592102744433, 0.9857368328793006, 0.929608977441901, 0.8365809500852361, 0.4810010686280327, 0.9430889397206723, 0.9194883538624099, 0.6037354851700341, 0.6288249463727129, 0.8272347501737517, 0.6566376813197878, 0.6817132254112156, 0.8077897114293389, 0.6678107811875859, 0.9362801340504037, 0.3870508792782098, 0.854906905704294, 0.9027144378842089, 1.0, 0.8257815238937638, 0.8744345587385776, 0.78946048013366, 0.5638854805512341, 0.676126243442005, 0.43419770508491723, 0.4657333819352129, 0.8443840073818547, 0.3560328527895647, 0.6120229206037862, 0.3110090555266167, 0.9225676720319869, 0.9617303176335186, 0.3085356854689776, 0.9368026696042517, 0.553376063289682, 0.9721595555198352, 0.7920617845792342, 0.9242338290598, 0.8741647989058143, 0.9251375134812259, 0.9745780518930731, 0.928943235004591, 0.8233729810068096, 0.4982318233525185, 0.5720677290587932, 0.9521775318277603, 0.9830296722149274, 0.975255869034251, 0.8320476098374262, 0.9813483172696138, 0.9483831980682167, 0.8948670458495385, 0.774693514748012, 0.7202507328873297, 0.5488586128738006, 0.7091570039785193, 0.7242120100978349, 0.1359821622512627, 0.6777438912696802, 0.962102708701077, 0.8934366600841925, 0.9021902095080698, 0.46124203184813023, 0.6246113794477428, 0.8392067093595759, 0.968639364262265, 0.8297713512550178, 0.6258706049241418, 0.7833624454787003, 0.8975978993714945, 0.9818251706356831, 0.7280873261576656, 0.8199251872608782, 0.8300820971250142, 0.824261585549334, 0.30867861529725393, 0.9410067825194739, 0.5399191018302276, 0.5661706174767057, 0.25613546330440956, 0.7364469121089812, 0.8694952470533828, 0.6867415067356185, 0.32301796222457846, 0.945208493585485, 0.8658945472295025, 0.5638272207824593, 0.59613181707828, 0.988723992351731, 0.7167958853235616, 0.5928108277067345, 0.5946982102419093, 0.195805705470289, 0.849431210776371, 0.9159958844184792, 0.6472436328644013, 0.9642836546885394, 0.1427616762031343, 0.9508336769881993, 0.9441311059579389, 0.09486426999266323, 0.961472991307396, 0.39013478691113623, 0.6684903987689361, 0.9447313137718458, 0.5317588322123294, 0.9087123429574342, 0.9253109087939462, 0.9701007019297219, 0.8124892826539233, 0.953707845659838, 0.8090043314450341, 0.8916198680457144, 0.9520611043918519, 0.7661576119244001, 0.12695294117647057, 0.19832929234074576, 0.8055864673667326, 0.9477526452574314, 0.9324333440186039, 0.597827148286705, 0.3330584663396393, 0.4822120322474115, 0.8044136830940034, 0.8942668643714475, 0.9232567135322138, 0.6221781999238016, 0.6234595852577036, 0.6559659558584913, 0.9490176561215609, 0.8861568600710625, 0.5085215178080368, 0.8553678887608935, 0.7803741792559828, 0.7826741503670085, 0.9634485800803125, 0.5624756082365959, 0.8695075881277006, 0.7901580927216791, 0.979227064439546, 0.9327028990399184, 0.947573383205105, 0.9013388166447551, 0.6351531795376738, 0.9583676774082095, 0.9377858074101935, 0.9495937875206693, 0.9221927719632479, 0.9604527042523284, 0.7919848094234092, 0.943907690508595, 0.943666571833899, 0.608292482084411, 0.7562791074610632, 0.7015341116696883, 0.2412059707864596, 0.9279866539122665, 0.19752609894716355, 0.707850059682499, 0.9284243634590652, 0.695713048670164, 0.9562005086759873, 0.8910956918182917, 0.41096569002246464, 0.9860950428335316, 0.7443694749788052, 0.6304354583069092, 0.9692036989337549, 0.6365891247781525, 0.9425986064448988, 0.9617839620555828, 0.9266465778740411, 0.9531844084589156, 0.7481412556512403, 0.9498690706862425, 0.8277424644338935, 0.7565711923511311, 0.0968369622508172, 0.8179579674310407, 0.9773514349376944, 0.8434094165290678, 0.7748901690895643, 0.7807079788494218, 0.8988279604599121, 0.8852945092796116, 1.0, 0.5952821522271243, 0.8729138028818919, 0.7232043610871355, 0.7493577391671854, 0.5076730012609921, 0.8776015021220545, 0.8207174296242723, 0.46158825189644004, 0.9840970086239778, 0.9049996265971042, 0.9147509231273736, 0.5075905017189635, 0.849729607166194, 0.6345917614751148, 0.9732456940628658, 0.9668495847081082, 0.46192836465257714, 0.961309010760973, 0.9219005411186121, 0.9437985500759485, 0.19077672396674258, 0.7840665213855238, 0.8206082150573881, 0.940663678095734, 1.0, 0.8943767670150264, 0.6372760488740846, 0.9398939635294166, 0.28567009379823677, 0.637455335351989, 0.3454641283088835, 0.9655267507815484, 0.8397074882045157, 0.79908393484228, 0.8939721301684738, 0.6924354337443256, 0.6498389062539932, 0.9644727771943793, 0.7407505059004492, 0.7698459366315475, 0.9666856331752072, 0.9296230645706806, 0.9074482656355349, 0.9295976632675957, 0.6516235135434673, 0.9866981257943701, 0.570989795360499, 0.853963666419484, 0.7311090731099734, 0.8785117532819908, 0.9067124558663866, 0.6830631050329368, 0.802792334029212, 0.9105346103428551, 0.9582488725526279, 0.8788846428096109, 0.9094304925332901, 0.9283813677168311, 0.6216058374935409, 0.8202499593885415, 0.845605645877975, 0.7514236604172718, 0.9629364535821479, 0.30228003420613947, 0.9590506396386802, 0.9393720392848304, 0.7108578888786369, 0.29119834125570104, 0.9312349008025642, 0.9539904817368636, 0.8131372578280465, 0.9498733743967636, 0.9262386824642832, 0.2535810718077191, 0.9130634810804314, 0.7074534807536211, 0.6070449868747402, 0.4328828981880556, 0.9731496374915098, 0.29399777482219386, 0.246656971808108, 0.7633414659938854, 0.3742924995543897, 0.9446842919965326, 0.42428373858678303, 0.4661937691043915, 0.9815931630248864, 0.28768815913136014, 0.8259108946799769, 0.9646055689051855, 0.9570869332853229, 0.867980815381816, 0.7552847912459166, 0.4488896705464462, 0.8990467958551017, 0.9839935011430094, 0.9200506209960793, 0.2520631239871962, 0.7243561152332358, 0.5898743279997497, 0.31764171070174957, 0.9136846145795283, 0.8036605977879999, 0.8078579828142751, 0.8562515048247652, 0.5392214644647573, 0.9544248082184021, 0.9127782544487824, 0.8057092562118815, 0.30264516996646973, 0.9465224177706513, 0.7588277670675514, 0.962229839777732, 0.9482573744384641, 0.8238518141083319, 0.41222995207253776, 0.6562955803712851, 0.5643672366251316, 0.43808453932073843, 0.5864458370254797, 0.9735465142797541, 0.321597484442518, 0.8601254321511638, 0.9624306767024622, 0.9307762742678802, 0.967150116841063, 0.990108133691983, 0.7514114718545895, 0.8485021818446042, 0.6612191334623871, 0.7832059164639751, 0.5558554508448601, 0.944817116312316, 0.945897725871365, 0.6100220174806957, 0.895887649579229, 0.7914330306940703, 0.9792319542491341, 0.9784095540181448, 0.9001704063035856, 0.9516757686175951, 0.8601706721604159, 0.7628541847804483, 0.4680849519025553, 0.9358476119371324, 0.9675589330971408, 0.7110307145936695, 0.7331328454992438, 0.5761336941152942, 0.7331472448108353, 0.6362439585829495, 0.7611677828299435, 0.8601402886946303, 0.7701904603659033, 0.390945448375335, 0.6592136870095452, 0.9794987286009202, 0.5978112872134451, 0.2256423390984703, 0.8038306806820452, 0.8836863746511332, 0.7964726901713943, 0.7601507876079147, 0.8724907736260945, 0.5412219101899178, 0.8696331987786137, 0.8118756794025181, 0.5348646474710699, 0.7562934717237396, 0.9756645028428577, 0.9489885523091097, 0.8981602981015966, 0.36665611193413983, 0.9715471473605286, 0.031930394824990466, 0.9177843916565737, 0.9031355431153669, 0.9723407670151949, 0.6283760699784661, 0.9469679666767363, 0.9439887848825383, 0.9405088388791978, 0.9823448177520746, 0.4777354426610022, 0.8702926765394572, 0.43449863768167823, 0.9262112558154699, 0.8354059606910897, 0.8721694234656858, 0.25489934536355485, 0.8564822037598797, 0.7936098293669693, 0.6810842044592471, 0.3043680407866811, 0.3760931869018366, 0.9207225522863047, 0.8678976437379708, 0.44949524080022113, 0.6339943883989547, 0.8574088571104024, 0.8389497416642955, 0.9407613352737074, 1.0, 0.9292619245050848, 0.8227600561608516, 0.959095521027233, 0.5966828462623532, 0.4129771429438506, 0.8020816921073726, 0.9271328028414589, 1.0]
Finish training and take 41m
