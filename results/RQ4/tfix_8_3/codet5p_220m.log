Namespace(log_name='./RQ5/tfix_8_3/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_8_3/codet5p_220m', data_dir='./data/RQ5/tfix_8_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "const nested_view_model = kb.viewModel(new kb.Model({ name: 'name1' }), { name: {} }, this);     const ViewModel = function () {", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "const nested_view_model = kb.viewModel(new kb.Model({ name: 'name1' }), { name: {} });     const ViewModel = function () {"}]
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 5.341953087608712e+294
  global_step = 2
  train_loss = 45.4816
  ********************
Previous best ppl:inf
Achieve Best ppl:5.341953087608712e+294
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 10.73 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.5639550445046145e+275
  global_step = 3
  train_loss = 45.7613
  ********************
Previous best ppl:5.341953087608712e+294
Achieve Best ppl:3.5639550445046145e+275
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 22.83 	 Previous best codebleu 10.73
  ********************
 Achieve Best bleu:22.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 8.005736995039927e+259
  global_step = 4
  train_loss = 23.925
  ********************
Previous best ppl:3.5639550445046145e+275
Achieve Best ppl:8.005736995039927e+259
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 25.29 	 Previous best codebleu 22.83
  ********************
 Achieve Best bleu:25.29
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.6868794552287727e+247
  global_step = 5
  train_loss = 16.9775
  ********************
Previous best ppl:8.005736995039927e+259
Achieve Best ppl:1.6868794552287727e+247
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 28.73 	 Previous best codebleu 25.29
  ********************
 Achieve Best bleu:28.73
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 4.427303009961083e+239
  global_step = 6
  train_loss = 11.6392
  ********************
Previous best ppl:1.6868794552287727e+247
Achieve Best ppl:4.427303009961083e+239
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 38.74 	 Previous best codebleu 28.73
  ********************
 Achieve Best bleu:38.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 4.804221708518547e+234
  global_step = 7
  train_loss = 8.7864
  ********************
Previous best ppl:4.427303009961083e+239
Achieve Best ppl:4.804221708518547e+234
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 42.49 	 Previous best codebleu 38.74
  ********************
 Achieve Best bleu:42.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 2.4244856463198155e+231
  global_step = 8
  train_loss = 6.7522
  ********************
Previous best ppl:4.804221708518547e+234
Achieve Best ppl:2.4244856463198155e+231
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 48.57 	 Previous best codebleu 42.49
  ********************
 Achieve Best bleu:48.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 4.742988857771976e+229
  global_step = 9
  train_loss = 5.9847
  ********************
Previous best ppl:2.4244856463198155e+231
Achieve Best ppl:4.742988857771976e+229
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.02 	 Previous best codebleu 48.57
  ********************
 Achieve Best bleu:52.02
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 2.605160789515092e+228
  global_step = 10
  train_loss = 4.8778
  ********************
Previous best ppl:4.742988857771976e+229
Achieve Best ppl:2.605160789515092e+228
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.37 	 Previous best codebleu 52.02
  ********************
 Achieve Best bleu:52.37
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 8.452090650638917e+227
  global_step = 11
  train_loss = 4.5232
  ********************
Previous best ppl:2.605160789515092e+228
Achieve Best ppl:8.452090650638917e+227
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.38 	 Previous best codebleu 52.37
  ********************
 Achieve Best bleu:52.38
  ********************
reload model from RQ5/tfix_8_3/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_3/test.jsonl
  codebleu = 52.62 
  Total = 500 
  Exact Fixed = 8 
[43, 46, 97, 156, 276, 341, 474, 489]
  Syntax Fixed = 2 
[220, 389]
  Cleaned Fixed = 9 
[53, 85, 92, 120, 199, 374, 389, 417, 445]
  ********************
  Total = 500 
  Exact Fixed = 8 
[43, 46, 97, 156, 276, 341, 474, 489]
  Syntax Fixed = 2 
[220, 389]
  Cleaned Fixed = 9 
[53, 85, 92, 120, 199, 374, 389, 417, 445]
  codebleu = 52.62 
[0.3852352687934718, 0.8170015046833874, 0.8223132623206333, 0.2580605048920401, 0.5642010748634618, 0.8697471734123385, 0.7130804239841513, 0.11582292438020222, 0.5724689055278611, 0.3078689049480329, 0.3474065849647445, 0.5569393416591119, 0.5228864516925358, 0.49718705940327845, 0.40243113967436694, 0.6127572978503959, 0.5033638986642797, 0.28944399265946075, 0.7974789818939843, 0.9207265089204273, 0.832900393434221, 0.3743172882353815, 0.1714285714285714, 0.5055188769352307, 0.4970106305726538, 0.2504033576162784, 0.8033622785493955, 0.14010483802876905, 0.24, 0.14196276172452565, 0.535133493667276, 0.6302436857061255, 0.7114217991038729, 0.3601842665079529, 0.3326555560719357, 0.47790455532955645, 0.41488045228871895, 0.8504403342456683, 0.6934531647858191, 0.3615104755889604, 0.20267175013447936, 0.22267396568917747, 1.0, 0.3514506678404373, 0.5374173071790711, 1.0, 0.3557581984899998, 0.5691472383728503, 0.46103757180445615, 0.2968222263370741, 0.5123984208296868, 0.5266057619745405, 0.8919671371303186, 0.32435175818106565, 0.0003682307805924702, 0.002373765495723797, 0.8968939355581875, 0.7882335227994882, 0.6303518920101568, 0.6247707320245329, 0.5986305906864111, 0.3972813086339288, 0.5610771314034941, 0.3465864294209792, 0.5223805997736636, 0.38618478058475125, 0.8303363603804119, 0.5970182644937875, 0.6168173810866084, 0.5448671450841394, 0.2752284068217509, 0.20369111792447886, 0.0967009264251735, 0.8917740712291482, 0.5007846217557572, 0.20613487118166404, 0.5947980679164908, 0.2446385794349512, 0.8222267233801039, 0.8033946307914341, 0.6530747837026651, 0.6387289085944345, 0.26090620551116817, 0.5977750828304829, 0.6372358081500218, 0.8382306061356919, 0.0, 0.2966597993138361, 0.0485646584311115, 0.5675986891240699, 0.40241103033310155, 0.8431082915068202, 0.582226723380104, 0.814064525684886, 0.3670159575152949, 0.4484520389345378, 1.0, 0.016782765293716924, 0.6488884298635612, 0.2993461744310722, 0.4326263272143769, 0.20799966236769143, 0.8199440232953434, 0.0, 0.07719082257468701, 0.1714285714285714, 0.48167268419835185, 0.467913762972279, 0.7764804587657133, 0.8417314059245307, 0.5799982506601298, 0.6785036503543816, 0.35473913473843294, 0.23452984161828982, 0.2924537101693822, 0.547428212010239, 0.638161672266765, 0.2635814555211407, 0.22499999999999998, 0.060872769699897536, 0.12005698946709212, 0.9068864709786153, 0.6094226924565724, 0.15160313851171453, 0.8961552234186163, 0.8150666162955742, 0.4118819918812901, 0.5551676892768422, 0.7562262989907678, 0.5737030996295007, 0.7923024832485084, 0.8433946307914342, 0.5777131912598308, 0.7509615262017801, 0.22619887519287302, 0.4544627617245256, 0.7760858398620529, 0.2572600501458519, 0.6193041117402687, 0.4389354391368106, 0.7239139638913099, 0.5960707868709735, 0.3188833170727141, 0.5440521407974296, 0.7396601242701673, 0.7530045026259591, 0.22900371828682153, 0.5097127232552463, 0.5760694567816493, 0.5063942770541421, 0.7059117582516798, 0.5470317817011201, 0.921263693253253, 0.02913215362273379, 0.7728222909971192, 0.8249365300761395, 0.734603718873536, 0.8245957586820076, 0.7318969410203645, 0.5266057619745405, 0.2832680265511309, 0.6806230074002029, 0.5461243465154161, 0.5301818996067954, 0.7750198530224517, 0.6888694066812189, 0.28359945168912193, 0.41511870560120445, 0.6957231649810731, 0.0, 0.6336131842675646, 0.4858412195420705, 0.4831458551130241, 0.4939491227305387, 0.6477517173276517, 0.7202172177276029, 0.5882210413806648, 0.6706847188488577, 0.8273136413238535, 0.3976904151485222, 0.5334863125122, 0.5417532856854141, 0.7302007404183433, 0.8426568563575418, 0.6814650334361707, 0.3812999423756144, 0.6956009081078438, 0.8638783503393002, 0.41925376624580646, 0.49716968796987465, 0.050923035847610806, 0.5568670803193168, 0.5994017185701503, 0.49393424193861524, 0.33356950877131564, 0.4718765181257531, 0.3857137856220523, 0.5236728434913505, 0.6319730195988179, 0.39873667339437135, 0.15429293535427072, 0.6530550965302279, 0.7296508600957852, 0.17775375390483888, 0.4739790984471416, 0.6459247942853364, 0.9226212781081518, 0.0731716191316424, 0.33900469517072807, 0.14499278294052598, 0.21245371016938225, 0.380038038773427, 0.7464166360104586, 0.322412788792651, 0.8249783514952764, 0.23838262056573456, 0.8837352944498882, 0.6663998078842303, 0.3039353132786876, 0.9584559291727563, 0.6234400133563041, 0.14824549023593797, 0.5960626227910577, 0.6072359854134853, 0.6387851138365109, 0.6093189247482212, 0.6271806220068676, 0.15917144253782908, 0.8541030238941549, 0.637283842723568, 0.3566743097500106, 0.7862551025473414, 0.4821379477582639, 0.3981394976554918, 0.7521271372862586, 0.2594366669003222, 0.6726527033303111, 0.7448443514503091, 0.5739135628469231, 0.6029055732198362, 0.3965630043849799, 0.7454579438162284, 0.0025868625201256232, 0.21066726805607944, 0.34062237195526035, 0.6208906511898491, 0.7080707346422974, 0.03565912784448857, 0.32221014800461983, 0.49431633594533775, 0.04101911673989318, 0.7796468115916302, 0.7780971537710544, 0.6179144359460748, 0.34857763210325887, 0.4223835094667513, 0.0, 0.4330703341169736, 0.7355980328281302, 0.09418181350952061, 0.5376172770255981, 0.5215147321007385, 0.8016000841539659, 0.604436274930823, 0.565895296523178, 0.6185886250459282, 0.0, 0.5764788021120066, 0.4496604481293247, 0.4162135382912301, 0.6568173810866085, 0.8082416075216149, 0.452161974970835, 0.14955243617952246, 0.13636363636363635, 0.7135428903906851, 0.6231893408156244, 0.8020701463285549, 0.11349072297901378, 0.8212928138736779, 0.6519830178464932, 0.6367840195066736, 0.6041077305454495, 0.45676707546873774, 0.8707376748537379, 0.6959913353679734, 0.29123935575300347, 0.6899766481408451, 0.557949955802006, 0.5516387681887083, 0.865686918650111, 0.29286074313859833, 0.7201190716494728, 0.7835249822766466, 0.4822468560843261, 0.027660853979899265, 0.5550049310474542, 0.820109647033614, 0.5843539719797703, 0.5431063628925451, 0.20189466770624753, 0.6722061306364799, 0.12906124268900346, 0.31449087216458954, 0.7839520803542337, 0.3844698211793555, 0.5766904548829572, 0.49317971385316517, 0.3979319487585916, 0.816225472769031, 0.8312424689833122, 0.21168485254147001, 0.7447332664686866, 0.8251411716151402, 0.39771561123205124, 0.7214603833869112, 0.5853377757733382, 0.16, 0.6060574827359351, 0.5405539200751719, 0.5537998353931795, 0.7442518335455897, 0.6392333266349789, 0.5110656166644442, 0.3008285762477364, 0.630608601765937, 0.5246351632397439, 0.5189589454524783, 0.5378181868018945, 0.4343002319965344, 0.6865490061448141, 0.6419975867424403, 0.4959953038804974, 0.72455988025806, 0.4707909983708979, 0.5841414787295285, 0.6362906768668978, 0.7144948346694379, 0.39571995790185704, 0.5921235082342631, 1.0, 0.7329853689231081, 0.7668758886842345, 0.5942976171879519, 0.664297617187952, 0.6209070226208282, 0.46057251275615135, 0.5909402501865353, 0.38304611372110686, 0.25505126259891203, 0.6721193723777923, 0.4704255924667339, 0.27253775924684065, 0.28613047846117523, 0.5924295517447509, 0.3426136760976885, 0.5883089748131176, 0.716463124093779, 0.7886327123579241, 0.5140913145175611, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.29715804963968223, 0.7391886750930661, 0.4842703820481581, 0.5669580702376444, 0.34003623200632915, 0.3022109370915457, 0.6863884298635612, 0.42778187550634333, 0.5152887122558812, 0.9707925219904805, 0.23322807389765116, 0.11999118775675902, 0.536560588211956, 0.6892870539097846, 0.7101527915539326, 0.49997666751308223, 0.5902838873166409, 0.6427510345668692, 0.7795599284362567, 0.15127041479686024, 0.3154745710868131, 0.34694171651642625, 0.31363978538036863, 0.8623473261888206, 0.9435222066525069, 0.6090216982984679, 0.8174230148919821, 0.5951645181513807, 0.5191184195008474, 0.31923580235667715, 0.3807256007926331, 0.8165273869902545, 0.7051340203559355, 0.7728490312188327, 0.35437970233785665, 0.4766063286360276, 0.1049818291492759, 0.5519698034749387, 0.5509782586537075, 0.6863559408133775, 0.7583127394270307, 0.7787918207406566, 0.8604352575955447, 0.15356651918683534, 0.3087195721868813, 0.8433369370338967, 0.7253890377078671, 0.691735368923108, 0.17020939396604545, 0.23548203926817277, 0.3334051771067748, 0.3374221355246658, 0.6816015466501335, 0.7976967772180291, 0.602487595382981, 0.2175320073157329, 0.0857142857142857, 0.6428938087244226, 0.3372632052291401, 0.940598945729278, 0.74621760781252, 0.43819881886061685, 0.19272438732935, 0.529289198254178, 0.4353746106262073, 0.5207350135988427, 0.23302908212357654, 0.7776218879066761, 0.09546843813282987, 0.6899719219176503, 0.39999999999999997, 0.6526642022832921, 0.779608127037918, 0.6847340670900772, 0.5665508359365867, 0.48861662650065896, 0.554739134738433, 0.7744515423179406, 0.765537139984148, 0.35630021158042324, 0.6522551430667551, 0.6375167201921692, 0.5316562756993463, 0.6901940524102714, 0.5925167201921692, 0.6733027629890207, 0.8050241598897441, 0.8821068552932578, 0.7548295193530756, 0.3691289879974663, 0.28890659929471596, 0.48848619154833894, 0.41080814539619515, 0.7322642870443095, 0.8334272760895116, 0.365865141818123, 0.003449439476672805, 0.8253119306693903, 0.5227854886870906, 0.45384892805233656, 0.4242309802201044, 0.34509049865171215, 0.5075783901825247, 0.3392286298565113, 0.4016265496309229, 0.45473913473843297, 0.0776252359266062, 0.7466086300715167, 0.13953931483110532, 1.0, 0.34650852962940437, 0.4157142776706557, 0.30215269230653224, 0.12, 0.8294519950448036, 0.7016265496309229, 0.3477554508516583, 0.6028785751316023, 0.27551007486728796, 0.756047661202274, 0.37377717010542166, 0.42880120128837773, 0.5907820443144609, 0.46975640328881985, 1.0, 0.6254581948739375, 0.5033849030627454, 0.6186389706521688, 0.7803858638186933, 0.6988219320042083, 0.7396773803089467, 0.6492633713864637, 0.7279095254199106, 0.8303164480298841, 0.20034796016239415, 0.7158229243802021]
Finish training and take 30m
