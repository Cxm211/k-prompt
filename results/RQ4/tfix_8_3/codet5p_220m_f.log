Namespace(log_name='./RQ5/tfix_8_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_8_3/codet5p_220m_f', data_dir='./data/RQ5/tfix_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01306
  global_step = 2
  train_loss = 1.5017
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01306
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 9.57 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00953
  global_step = 3
  train_loss = 1.4126
  ********************
Previous best ppl:1.01306
Achieve Best ppl:1.00953
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 14.33 	 Previous best codebleu 9.57
  ********************
 Achieve Best bleu:14.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00809
  global_step = 4
  train_loss = 0.8134
  ********************
Previous best ppl:1.00953
Achieve Best ppl:1.00809
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 23.57 	 Previous best codebleu 14.33
  ********************
 Achieve Best bleu:23.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0075
  global_step = 5
  train_loss = 0.537
  ********************
Previous best ppl:1.00809
Achieve Best ppl:1.0075
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 27.95 	 Previous best codebleu 23.57
  ********************
 Achieve Best bleu:27.95
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00729
  global_step = 6
  train_loss = 0.4485
  ********************
Previous best ppl:1.0075
Achieve Best ppl:1.00729
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 36.43 	 Previous best codebleu 27.95
  ********************
 Achieve Best bleu:36.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00724
  global_step = 7
  train_loss = 0.3652
  ********************
Previous best ppl:1.00729
Achieve Best ppl:1.00724
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 44.32 	 Previous best codebleu 36.43
  ********************
 Achieve Best bleu:44.32
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00721
  global_step = 8
  train_loss = 0.3252
  ********************
Previous best ppl:1.00724
Achieve Best ppl:1.00721
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 49.6 	 Previous best codebleu 44.32
  ********************
 Achieve Best bleu:49.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00722
  global_step = 9
  train_loss = 0.2119
  ********************
Previous best ppl:1.00721
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.56 	 Previous best codebleu 49.6
  ********************
 Achieve Best bleu:52.56
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00723
  global_step = 10
  train_loss = 0.1836
  ********************
Previous best ppl:1.00721
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.53 	 Previous best codebleu 52.56
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00724
  global_step = 11
  train_loss = 0.1624
  ********************
Previous best ppl:1.00721
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.06 	 Previous best codebleu 52.56
  ********************
reload model from RQ5/tfix_8_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_3/test.jsonl
  codebleu = 49.99 
  Total = 500 
  Exact Fixed = 6 
[80, 97, 139, 173, 389, 489]
  Syntax Fixed = 2 
[202, 371]
  Cleaned Fixed = 5 
[53, 65, 92, 199, 417]
  ********************
  Total = 500 
  Exact Fixed = 6 
[80, 97, 139, 173, 389, 489]
  Syntax Fixed = 2 
[202, 371]
  Cleaned Fixed = 5 
[53, 65, 92, 199, 417]
  codebleu = 49.99 
[0.3852352687934718, 0.8170015046833874, 0.861672684198352, 0.08582956389885285, 0.7561984520017521, 0.940598945729278, 0.7130804239841513, 0.03969884422068212, 0.5985639363699742, 0.3086432699734719, 0.3499927730653973, 0.7067366858513033, 0.3161638257739494, 0.49718705940327845, 0.3317256155367455, 0.6502572978503959, 0.527996185740333, 0.3951417837590342, 0.5930248247228662, 0.9207265089204273, 0.832900393434221, 0.3089454310335113, 0.1636363636363636, 0.6793670579146192, 0.5605123536307766, 0.23978058624034976, 0.7949680697811121, 0.14010483802876905, 0.39985271067894745, 0.0019293259065968381, 0.535133493667276, 0.6142482969348242, 0.7114217991038729, 0.3, 0.3326555560719357, 0.4153746106262073, 0.16079386919986416, 0.8504403342456683, 0.6934531647858191, 0.4296361111543423, 0.44816693827108856, 0.22267396568917747, 0.6332794459999123, 0.5229326815734496, 0.5481721142074378, 0.5952887122558812, 0.3704822042629753, 0.5961851509772491, 0.46103757180445615, 0.2968222263370741, 0.43288822323692505, 0.4668701744987186, 0.8919671371303186, 0.622877007215058, 0.0006757759533891338, 0.861482988138697, 0.8968939355581875, 0.6492270558775599, 0.4583586250389059, 0.33277016973025564, 0.6430576862511725, 0.451689598214979, 0.17687695229009986, 0.3465864294209792, 0.9025845480994932, 0.44540887763356285, 0.7820864798131745, 0.36031223083845954, 0.6168173810866084, 0.35830249685779664, 0.1621787547146431, 0.28638102196660664, 0.08978100888954238, 0.8917740712291482, 0.03606856837889304, 0.18618064635451567, 0.6056251481468706, 0.24838373704482247, 0.643291057814614, 1.0, 0.6530747837026651, 0.7265018183047776, 0.26090620551116817, 0.803950001904677, 0.5984451981160288, 0.8382306061356919, 0.0, 0.3555165294259916, 0.0485646584311115, 0.5675986891240699, 0.5927286737804728, 0.8662528514160328, 0.011593471075642264, 0.6076579066138242, 0.34446733209501945, 0.4484520389345378, 1.0, 0.016782765293716924, 0.42231753686974316, 0.4261742857516409, 0.18, 0.20613096711271314, 0.2108782246527302, 0.0, 0.07719082257468701, 0.30480616432215846, 0.21174879177673955, 0.44025895651227964, 0.5864619665221961, 0.5024101654079205, 0.14528822942938657, 0.6785036503543816, 0.24062681304816916, 0.12, 0.30657092899041877, 0.5318275776349692, 0.6994853675077588, 0.20067785202206703, 0.09200236454551065, 0.00014029954284333122, 0.10648766222679504, 0.3397190055471155, 0.7233058773873271, 0.10906101778358473, 0.8840188084091536, 0.8150666162955742, 0.196275735434565, 0.1868614827834806, 0.08105542962504894, 0.5737030996295007, 0.7977695539611169, 0.3707707065593111, 0.5777131912598308, 0.7768898072067754, 0.15, 0.4544627617245256, 0.7803897964725244, 0.24050667862673492, 1.0, 0.8432727833745755, 0.7239139638913099, 0.5960707868709735, 0.3604041695817951, 0.5440521407974296, 0.2643559854519312, 0.613240171788221, 0.19229545276671334, 0.523507649142383, 0.5760694567816493, 0.5063942770541421, 0.7059117582516798, 0.4740548855631477, 0.921263693253253, 0.40021076711635406, 0.761808836360503, 0.2979907911454897, 0.734603718873536, 0.8245957586820076, 0.9321253696376492, 0.5266057619745405, 0.5348435312891645, 0.9583822899883758, 0.5461243465154161, 0.5112409817647298, 0.6361031073231687, 0.3087227071091273, 0.44478640618236376, 0.35511870560120445, 0.6957231649810731, 0.5953125, 0.6336131842675646, 0.4858412195420705, 1.0, 0.4939491227305387, 0.6477517173276517, 0.21567721918107333, 0.5882210413806648, 0.6706847188488577, 0.8681790654632031, 0.6127276626854681, 0.6435322435220858, 0.571317339102388, 0.7302007404183433, 0.8426568563575418, 0.6814650334361707, 0.0, 0.5958414972824431, 0.8065770516693829, 0.15427761508679474, 0.49716968796987465, 0.050923035847610806, 0.5728633222115624, 0.6436176588269751, 0.49393424193861524, 0.33356950877131564, 0.4718765181257531, 0.20706447275720755, 0.5236728434913505, 0.6319730195988179, 0.39873667339437135, 0.07683567216420958, 0.756047661202274, 0.5523147455787538, 0.1898355357735218, 0.4739790984471416, 0.34734331679683483, 0.7925834740428324, 0.0731716191316424, 0.4307250470563342, 0.557918506276453, 0.21245371016938225, 0.3972934177879658, 0.9080331470954286, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 0.8956377568613374, 0.6663998078842303, 0.42493877026999405, 0.6530836285702823, 0.330161739005039, 0.11119494376553189, 0.5960626227910577, 0.6072359854134853, 0.6387851138365109, 0.6093189247482212, 0.6271806220068676, 0.15918996147007258, 0.8541030238941549, 0.43016109313909, 0.39359500830217886, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.17885198854171402, 0.6726527033303111, 0.4348799691547096, 0.6056583090096291, 0.6029055732198362, 0.36786317495162313, 0.7816396051000123, 0.005929201860802753, 0.09014732050736521, 0.5944836924880659, 0.6952637111045539, 0.8407059481807464, 0.135939341156956, 0.42072473046613645, 0.49431633594533775, 0.04030522491224135, 0.7796468115916302, 0.7533050973158391, 0.5177398274058808, 0.47535257334060044, 0.19014746693121312, 0.0, 0.21513675141780952, 0.7355980328281302, 0.16826111698077859, 0.5376172770255981, 0.5721360784673309, 0.8016000841539659, 0.5956773385643681, 0.0, 0.6875131192847503, 0.0, 0.27862618567302244, 0.4496604481293247, 0.367641751923923, 0.5978415423871026, 0.7582416075216148, 0.4566733907481716, 0.07740340464935022, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.7254239031677705, 0.11349072297901378, 0.7978758463759263, 0.6519830178464932, 0.6367840195066736, 0.6041077305454495, 0.4123305745630325, 0.8707376748537379, 0.6959913353679734, 0.29123935575300347, 0.6899766481408451, 0.6958341838531639, 0.6553451053003048, 0.2775515152085108, 0.29286074313859833, 0.7201190716494728, 0.7835249822766466, 0.5782030614679, 0.00025101354077220156, 0.5550049310474542, 0.8358445557986962, 0.5843539719797703, 0.6197856242749173, 0.15583701862023303, 0.7601650731090275, 0.15837703500732306, 0.3814205333654446, 0.756047661202274, 0.3844698211793555, 0.23340579063484157, 0.4924708350557623, 0.40660822240095185, 0.8794984978839551, 0.6556953984655758, 0.0002852064462709548, 0.8346004130918654, 0.8997846296433376, 0.5644004745782011, 0.7531697895341449, 0.24217947372216694, 0.3854194889831103, 0.6060574827359351, 0.5405539200751719, 0.5537998353931795, 0.49904817721150374, 0.6392333266349789, 0.5019100417936252, 0.37549356386576216, 0.599029654397516, 0.6835187606625395, 0.42161254940297677, 0.23979699636679497, 0.39714783887043237, 0.4030957756965064, 0.3104582650181934, 0.46801033677814774, 0.29680823054727096, 0.3605742532070972, 0.6465085296294044, 0.9195522364276514, 0.7144948346694379, 0.4709261928229648, 0.2621053625077059, 0.2304691808056094, 0.7195482768478927, 0.8138601485215005, 0.47029733718731737, 0.6738495662857232, 0.5288724788208753, 0.0003714232506159055, 0.5909402501865353, 0.40533137253115764, 0.47786341412302047, 0.2365364043248961, 0.6999214919756145, 0.6781982935333077, 0.619380623260443, 0.5924295517447509, 0.3426136760976885, 0.5883089748131176, 0.9608461673226849, 0.7886327123579241, 0.5140913145175611, 0.4147896756607047, 0.8155450160863305, 0.2525609879603835, 0.370321911798854, 0.43554564932669804, 0.7391886750930661, 0.39077528554250296, 0.6655012758056489, 0.34003623200632915, 0.3022109370915457, 0.756047661202274, 0.6171927924443892, 0.4366371358026476, 0.6616064237622327, 0.2630141483279534, 0.17332452109009233, 0.4342827422300322, 0.6892870539097846, 0.7965481359238318, 0.14754218768943683, 0.8042470965743207, 0.6427510345668692, 0.7795599284362567, 0.03890282085815591, 0.3154745710868131, 0.6400373216188153, 0.5217365165784443, 0.7678137368502398, 1.0, 0.6090216982984679, 0.8174230148919821, 0.5951645181513807, 0.586832332548443, 0.32506730878321255, 0.40233695939477276, 0.5652010091503564, 0.5414377877354021, 0.31602832354233085, 0.35437970233785665, 0.5768770516401771, 0.11841238567827725, 0.48351479452714663, 0.03526721299899491, 0.8007265089204272, 0.7017410285928236, 0.7454122664669494, 0.32724693880651623, 0.23914651925846164, 0.34575990398983625, 0.8433369370338967, 0.7253890377078671, 0.6131685378246745, 0.5084373166195929, 0.2876607462883649, 0.40209781994840155, 0.40400364537321126, 0.6816015466501335, 0.7972388343569652, 0.20242189719212172, 0.21948160223692786, 0.23683637774429345, 0.6325926350171343, 0.3562454363711762, 0.940598945729278, 0.6134230703371367, 0.43819881886061685, 0.19279230376695983, 0.38451985585546666, 0.23562237195526037, 0.7045464343812362, 0.24708942951482832, 0.8661547359011423, 0.10636551088734877, 0.6899719219176503, 0.39999999999999997, 0.2749761815969247, 0.7098872906896011, 0.6847340670900772, 0.8742687840283396, 0.6937628353106088, 0.5964393286525268, 0.8218272232334276, 0.7761486665298616, 0.3313002115804233, 0.6476309505212854, 0.5225874086647231, 0.6151628493773373, 0.6901940524102714, 0.5925167201921692, 0.6733027629890207, 0.8050241598897441, 0.8678963289774684, 0.7548295193530756, 0.12171645645715026, 0.25114069505014003, 0.6578607431385983, 0.3798418629627377, 0.7322642870443095, 0.8334272760895116, 0.6302292290312997, 0.003996009641464269, 0.8253119306693903, 0.37549572614337456, 0.35985694521906425, 0.266192508303546, 0.34509049865171215, 0.579296159037565, 0.11890548225420909, 0.21119416865411111, 0.2386459180704551, 0.19952919325470503, 0.7412242091620882, 0.24126655075260275, 0.7398815472813818, 0.4326263272143769, 0.643309301212212, 0.30215269230653224, 0.12, 0.8924270154069307, 0.5802610782719728, 0.10463395754880184, 0.6028785751316023, 0.36393172467891854, 0.756047661202274, 0.6137392159029667, 0.42880120128837773, 0.5907820443144609, 0.3939076474725782, 1.0, 0.5412922268393551, 0.24412331445752453, 0.6186389706521688, 0.7803858638186933, 0.5462862588837676, 0.7396773803089467, 0.6492633713864637, 0.6540332824421711, 0.6063962944011458, 0.073800061519366, 0.6008143422418155]
Finish training and take 28m
