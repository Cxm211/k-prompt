Namespace(log_name='./RQ5/tfix_8_3/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_8_3/codet5p_770m_f', data_dir='./data/RQ5/tfix_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01232
  global_step = 3
  train_loss = 1.9356
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01232
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 15.77 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:15.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00895
  global_step = 5
  train_loss = 1.1992
  ********************
Previous best ppl:1.01232
Achieve Best ppl:1.00895
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 23.94 	 Previous best codebleu 15.77
  ********************
 Achieve Best bleu:23.94
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00806
  global_step = 7
  train_loss = 0.6654
  ********************
Previous best ppl:1.00895
Achieve Best ppl:1.00806
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 27.54 	 Previous best codebleu 23.94
  ********************
 Achieve Best bleu:27.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00771
  global_step = 9
  train_loss = 0.5479
  ********************
Previous best ppl:1.00806
Achieve Best ppl:1.00771
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 40.18 	 Previous best codebleu 27.54
  ********************
 Achieve Best bleu:40.18
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0073
  global_step = 11
  train_loss = 0.3322
  ********************
Previous best ppl:1.00771
Achieve Best ppl:1.0073
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 48.62 	 Previous best codebleu 40.18
  ********************
 Achieve Best bleu:48.62
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00681
  global_step = 13
  train_loss = 0.3463
  ********************
Previous best ppl:1.0073
Achieve Best ppl:1.00681
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 52.09 	 Previous best codebleu 48.62
  ********************
 Achieve Best bleu:52.09
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00669
  global_step = 15
  train_loss = 0.2265
  ********************
Previous best ppl:1.00681
Achieve Best ppl:1.00669
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 54.43 	 Previous best codebleu 52.09
  ********************
 Achieve Best bleu:54.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00667
  global_step = 17
  train_loss = 0.0826
  ********************
Previous best ppl:1.00669
Achieve Best ppl:1.00667
  ********************
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 54.49 	 Previous best codebleu 54.43
  ********************
 Achieve Best bleu:54.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00667
  global_step = 19
  train_loss = 0.1262
  ********************
Previous best ppl:1.00667
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 54.09 	 Previous best codebleu 54.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00668
  global_step = 21
  train_loss = 0.103
  ********************
Previous best ppl:1.00667
BLEU file: ./data/RQ5/tfix_8_3/validation.jsonl
  codebleu-4 = 54.0 	 Previous best codebleu 54.49
  ********************
reload model from RQ5/tfix_8_3/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_3/test.jsonl
  codebleu = 49.89 
  Total = 500 
  Exact Fixed = 10 
[39, 45, 46, 65, 68, 81, 85, 97, 199, 487]
  Syntax Fixed = 6 
[43, 60, 160, 345, 378, 489]
  Cleaned Fixed = 7 
[3, 53, 139, 159, 265, 344, 345]
  ********************
  Total = 500 
  Exact Fixed = 10 
[39, 45, 46, 65, 68, 81, 85, 97, 199, 487]
  Syntax Fixed = 6 
[43, 60, 160, 345, 378, 489]
  Cleaned Fixed = 7 
[3, 53, 139, 159, 265, 344, 345]
  codebleu = 49.89 
[0.3852352687934718, 0.8170015046833874, 0.8704058946539028, 0.3389850727232054, 0.7561984520017521, 0.940598945729278, 0.7130804239841513, 0.11582292438020222, 0.17146442671557033, 0.29951710602223036, 0.3499927730653973, 0.7116586224318888, 0.731699426081697, 0.31379593128548755, 0.40243113967436694, 0.6384652034268403, 0.5333638986642796, 0.3836465025036173, 0.25248174129905016, 0.9207265089204273, 0.7468709816695149, 0.3743172882353815, 0.00474315710536183, 0.6793670579146192, 0.5893949501703277, 0.2772943579237247, 0.7949680697811121, 0.14922155894028458, 0.39985271067894745, 0.028683796115111723, 0.535133493667276, 0.5189381248702405, 0.5093023986398204, 0.38022309848811786, 0.3326555560719357, 0.460395956404703, 0.38656674784379386, 0.7501702822760026, 1.0, 0.5115292892700526, 0.7938708568112552, 0.22267396568917747, 0.9383259097532135, 0.28191806659375795, 1.0, 1.0, 0.3001390438459003, 0.5680792308405532, 0.46877089663917104, 0.0303333004926988, 0.09314111463264657, 0.5193894976554918, 0.8919671371303186, 0.622877007215058, 0.07203982316349665, 0.45306705644009193, 0.8968939355581875, 0.8072158317425806, 0.5518264054085248, 0.7298671661900396, 0.5953785687100492, 0.4479220206370105, 0.08684740832435074, 0.4803494323607689, 1.0, 0.502988841340978, 0.8222622447512038, 0.7691886750930661, 0.6168173810866084, 0.5416581477675209, 0.2151471222637567, 0.0807443297383276, 0.08978100888954238, 0.6567953537684859, 0.08591160268374923, 0.20613487118166404, 0.4887096364908238, 0.48719425841404007, 0.5341859296251839, 0.7064927606229161, 0.8249365300761395, 0.6497985900356393, 0.18, 0.803950001904677, 1.0, 0.9179350389391119, 0.0, 0.23679350152126039, 0.05999322985968293, 0.5311653720219895, 0.2650018011029693, 0.4625669535894181, 0.582226723380104, 0.6302807642535, 0.3746350747018756, 0.4484520389345378, 1.0, 0.016782765293716924, 0.6488884298635612, 0.483668171350054, 0.14948805460750852, 0.29282962113546335, 0.20420431542563985, 0.0, 0.046508529629404424, 0.2693552919690163, 0.016553925661878553, 0.467913762972279, 0.5392546309530222, 0.5024101654079205, 0.29776441867852255, 0.6785036503543816, 0.27787724896157057, 0.27986152408713605, 0.24033096084088396, 0.47715506043525124, 0.7025645357274757, 0.19638238379862666, 0.19999999999999998, 0.00020985487652544702, 0.015059285730552291, 0.31152241924524415, 0.7233058773873271, 0.01826980517838122, 0.6924169915274174, 0.8127370215679162, 0.3336736140861711, 0.5551676892768422, 0.5523640714937084, 0.5737030996295007, 0.7977695539611169, 0.8433946307914342, 0.5158719551788035, 0.6450649445208634, 0.22619887519287302, 0.1924714365500892, 0.7803897964725244, 0.8045575892555019, 0.9228963827959196, 0.5510261697983695, 0.37018721825968526, 0.5972860124827102, 0.30227625571152233, 0.5202479508123797, 0.4364272177655693, 0.7530045026259591, 0.27810543558529643, 0.40257418424359015, 0.6060694567816493, 0.5063942770541421, 0.6344351133589745, 0.5490286000303841, 0.892387748772268, 0.39420805640145656, 0.7728222909971192, 0.5632387151836264, 0.7028425048476072, 0.7204880740141585, 0.9215567956810036, 0.6536911179244789, 0.5348435312891645, 0.9583822899883758, 0.5461243465154161, 0.5445743150980631, 0.5787776870083425, 0.10530117471049148, 0.44478640618236376, 0.35511870560120445, 0.687882857735068, 0.011233551256143635, 0.6040629296941564, 0.34841446113222657, 0.5688601408273097, 0.5532531256091296, 0.771088896340493, 0.7202172177276029, 0.5882210413806648, 0.514596516758575, 0.312042096585984, 0.6190738228228467, 0.5990129017805593, 0.5853287480309621, 0.7302007404183433, 0.8160014655930372, 0.6720366568796399, 0.38766085397989924, 0.6956009081078438, 0.30319447443403724, 0.4468640641327328, 0.49716968796987465, 0.04816003385784584, 0.45308725971816477, 0.4807340436627101, 0.47915510063204303, 0.33356950877131564, 0.4718765181257531, 0.3812693452719597, 0.5236728434913505, 1.0, 0.39873667339437135, 0.08777038925241762, 0.6863884298635612, 0.4799068063140255, 0.03304940010323489, 0.4739790984471416, 0.2983875509430629, 0.8232461398584543, 0.03606856837889304, 0.4307250470563342, 0.17011833018409794, 0.18665344789927385, 0.06611802165472852, 0.8613836287468253, 0.31317519629607105, 0.8249783514952764, 0.5954522708833808, 0.8956377568613374, 0.3847145200788205, 0.42493877026999405, 0.6168853657523834, 0.5952358465506888, 0.08450329765250558, 0.5960626227910577, 0.6072359854134853, 0.5419627617245256, 0.5946296454239947, 0.6151380773742211, 0.46132529083669765, 0.8541030238941549, 0.43016109313909, 0.3678831758068888, 0.694620505777644, 0.6731716191316424, 0.47035576197454043, 0.7521271372862586, 0.3238935781913054, 0.6726527033303111, 0.699716658571939, 0.6056583090096291, 0.6029055732198362, 0.3965630043849799, 0.7387824622428694, 0.0, 0.22915440082990407, 0.5944836924880659, 0.6700451085394062, 0.597312415981885, 0.145244327318979, 0.4435027248795795, 0.40521788936613634, 0.07058823529411765, 0.6417560196852762, 0.8272756548660365, 0.6179144359460748, 0.44531416678032165, 0.5223835094667514, 0.0, 0.28969884422068215, 0.596903616069407, 0.09418181350952061, 0.44838911080860056, 0.4531115210352076, 0.7470841175017768, 0.611572567109274, 0.6255874957717769, 0.6875131192847503, 0.0, 0.19573611446176906, 0.5616798194467534, 0.367641751923923, 0.6568173810866085, 0.6214380837202983, 0.5213430758268208, 0.21922862985651131, 0.13636363636363635, 0.0, 0.7439654153934541, 0.7570701463285548, 0.11349072297901378, 0.8212928138736779, 0.10939456971258738, 0.6506923930808135, 0.6041077305454495, 0.45676707546873774, 0.8710525186025484, 0.5649770358022869, 0.29123935575300347, 0.2208339890959175, 0.6768986255548952, 0.5516387681887083, 0.07973268623716309, 0.29286074313859833, 0.6008811539123753, 0.9103556223308578, 0.48986335209949583, 0.027660853979899265, 0.35415440082990407, 0.8358445557986962, 0.44342965561329417, 0.5447856242749173, 0.20102669171465737, 0.7601650731090275, 0.07246041734311404, 0.3814205333654446, 0.756047661202274, 0.3844698211793555, 0.5494177276102299, 0.49930531945812784, 0.40660822240095185, 0.8794984978839551, 0.8312424689833122, 0.2865444396201405, 0.8274001021378707, 0.5650578918356444, 0.4354641313043082, 0.3156473166299494, 0.5399408431254569, 0.3854194889831103, 0.6060574827359351, 0.3737512965629247, 0.13598242085029544, 0.4198713891666677, 0.4589728923921914, 0.5019100417936252, 0.3008285762477364, 0.5340224714276844, 0.5183791862105838, 0.3208206201870649, 0.20786864898065394, 0.38151476945274176, 0.5760369521670946, 0.5782825466496874, 0.4959953038804974, 0.72455988025806, 0.4707909983708979, 0.5983535654700125, 0.9195522364276514, 0.689001498804813, 0.4709261928229648, 0.5921235082342631, 0.22913637536663, 0.661342657114236, 0.8591190003999107, 0.5002973371873174, 0.9228963827959196, 0.6209070226208282, 0.3577153698990085, 0.5909402501865353, 0.3356691900044979, 0.45505126259891204, 0.7135160729534664, 0.5717992106528098, 0.6265311850596921, 0.5354698543711851, 0.5924295517447509, 0.3426136760976885, 0.5883089748131176, 0.9608461673226849, 0.7282376133987647, 0.5052475236131788, 0.4147896756607047, 0.6742443243492403, 0.4724187871502895, 0.370321911798854, 0.2657972362756855, 0.31845057706178365, 0.4001882358393028, 0.6655012758056489, 0.34003623200632915, 0.3022109370915457, 0.6863884298635612, 0.6149139863647084, 0.2537102823283704, 0.6989171443189292, 0.3070604673180055, 0.08960866373350819, 0.536560588211956, 0.8648103351245982, 0.7572116150833443, 0.49997666751308223, 0.5740704509319345, 0.6464691633007411, 0.6934491360273962, 0.04448415542647708, 0.3154745710868131, 0.6400373216188153, 0.4551887210243918, 0.5579068260027604, 0.8685222066525069, 0.6090216982984679, 0.7869854510476147, 0.03752275825165655, 0.5781687723560363, 0.31964746425242696, 0.5911841946337606, 0.7450583018440083, 0.5748675606560572, 0.5770682924306744, 0.35437970233785665, 0.5763240578218234, 0.13841238567827724, 0.5519698034749387, 0.5341258702031846, 0.8007265089204272, 0.7017410285928236, 0.7454122664669494, 0.8388063820069365, 0.15356651918683534, 0.3029027611326934, 0.8004345035674447, 0.7253890377078671, 0.6131685378246745, 0.49155186073625645, 0.7551762580092702, 0.40209781994840155, 0.40400364537321126, 0.7899618578475022, 0.2826469319127096, 0.24303943067720796, 0.1851872657519698, 0.23683637774429345, 0.638072415115586, 0.3372632052291401, 0.8994160948073502, 0.8347788278270061, 0.3539261751720011, 0.1971926759757941, 0.3109192501488804, 0.14816496580927727, 0.7233004498085515, 0.41178999893307006, 0.8661547359011423, 0.15472876794829787, 0.6614160508042951, 0.18409090909090908, 0.26930080138478063, 0.6905685185170335, 0.35867706276352346, 0.8742687840283396, 0.5698605543741592, 0.5972860124827102, 0.7744515423179406, 0.8519938276459833, 0.2939281584161137, 0.6321675990462452, 0.6091258702031845, 0.6151628493773373, 0.6901940524102714, 0.5738473652507459, 0.6733027629890207, 0.8050241598897441, 0.6064788021120067, 0.7195445329739119, 0.11602631638033012, 0.27333000718930683, 0.19946762899253628, 0.08761094842012808, 0.6389827810564599, 0.8334272760895116, 0.55007181560277, 0.003028001852606003, 0.7854126882250971, 0.38678610127863244, 0.677288551000834, 0.4242309802201044, 0.34043455069677503, 0.4656235721845369, 0.3392286298565113, 0.4016265496309229, 0.0, 0.19119759183633495, 0.7352937647174818, 0.355067633384039, 0.25658620330980425, 0.2179371010579758, 0.6118807297836404, 0.30215269230653224, 0.3, 0.9082902514027809, 0.6488378866864628, 0.766682914692771, 0.0, 0.39164729797306413, 0.5952303686434826, 0.48129480963601345, 0.42880120128837773, 1.0, 0.3939076474725782, 0.762178754714643, 0.5380234635084377, 0.7494203690984217, 0.6186389706521688, 0.6039528341415055, 0.5462862588837676, 0.7396773803089467, 0.38738397192747687, 0.7040724141816117, 0.6129099427946454, 0.08019605944332792, 0.7158229243802021]
Finish training and take 1h7m
