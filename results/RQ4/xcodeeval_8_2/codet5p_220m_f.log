Namespace(log_name='./RQ5/xcodeeval_8_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_8_2/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_8_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00051
  global_step = 2
  train_loss = 0.5646
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00051
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 16.67 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00049
  global_step = 3
  train_loss = 0.5973
  ********************
Previous best ppl:1.00051
Achieve Best ppl:1.00049
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 45.78 	 Previous best codebleu 16.67
  ********************
 Achieve Best bleu:45.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00046
  global_step = 4
  train_loss = 0.3579
  ********************
Previous best ppl:1.00049
Achieve Best ppl:1.00046
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 48.67 	 Previous best codebleu 45.78
  ********************
 Achieve Best bleu:48.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00044
  global_step = 5
  train_loss = 0.2989
  ********************
Previous best ppl:1.00046
Achieve Best ppl:1.00044
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 61.41 	 Previous best codebleu 48.67
  ********************
 Achieve Best bleu:61.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00042
  global_step = 6
  train_loss = 0.2694
  ********************
Previous best ppl:1.00044
Achieve Best ppl:1.00042
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 67.42 	 Previous best codebleu 61.41
  ********************
 Achieve Best bleu:67.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00041
  global_step = 7
  train_loss = 0.2067
  ********************
Previous best ppl:1.00042
Achieve Best ppl:1.00041
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 71.9 	 Previous best codebleu 67.42
  ********************
 Achieve Best bleu:71.9
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.0004
  global_step = 8
  train_loss = 0.2208
  ********************
Previous best ppl:1.00041
Achieve Best ppl:1.0004
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 72.47 	 Previous best codebleu 71.9
  ********************
 Achieve Best bleu:72.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.0004
  global_step = 9
  train_loss = 0.2169
  ********************
Previous best ppl:1.0004
Achieve Best ppl:1.0004
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 72.53 	 Previous best codebleu 72.47
  ********************
 Achieve Best bleu:72.53
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0004
  global_step = 10
  train_loss = 0.207
  ********************
Previous best ppl:1.0004
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 72.76 	 Previous best codebleu 72.53
  ********************
 Achieve Best bleu:72.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0004
  global_step = 11
  train_loss = 0.1979
  ********************
Previous best ppl:1.0004
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 73.57 	 Previous best codebleu 72.76
  ********************
 Achieve Best bleu:73.57
  ********************
reload model from RQ5/xcodeeval_8_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_8_2/test.jsonl
  codebleu = 71.32 
  Total = 500 
  Exact Fixed = 1 
[73]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[355]
  ********************
  Total = 500 
  Exact Fixed = 1 
[73]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[355]
  codebleu = 71.32 
[0.9249386102976953, 0.9734692160381253, 0.37998384477389036, 0.7604295144571122, 0.4881583732085906, 0.16476215604972694, 0.48847434906793397, 0.2831006440428133, 0.5741146855222027, 0.676908819611102, 0.9486021234366251, 0.9568797081708029, 0.6850736143053938, 0.7660922291353187, 0.8609317515032215, 0.4360622531177153, 0.5313357348360503, 0.9785699270907009, 0.8061799752233711, 0.9403863087164004, 0.8461003923339978, 0.5551077533375529, 0.959945180208001, 0.9453698448910237, 0.8596055360028632, 0.46678356460890147, 0.46581441309546434, 0.8279156873239912, 0.9169652447222495, 0.2507911173421238, 0.7460250732072086, 0.8539726891479706, 0.8002665033594192, 0.0002725713916067496, 0.5393913750037733, 0.8059663450023036, 0.6527764179246671, 0.9426787386209374, 0.9847813251545576, 0.6214028319131882, 0.4381318755020243, 0.9602327743514232, 0.9321918895830998, 0.21659623863784408, 0.806490922254441, 0.9386210590467857, 0.8574290303367051, 0.7113199842059187, 0.6194178615287717, 0.8803828210815028, 0.4268111345134653, 0.9193292374349928, 0.7621618278400302, 0.6136977012029514, 0.8719311935523708, 0.43445510494548134, 0.8675700280536951, 0.8565079660274721, 0.7829914767019434, 0.5445425808684521, 0.31486681465038846, 0.33475542521016893, 0.908562342012443, 0.6315345262785433, 0.8927899452472845, 0.9299687695441708, 0.7924048855268875, 0.5529556678781546, 0.8995377367121644, 0.20834736629773226, 0.9285424148564965, 0.2690529321329123, 0.9674900184215338, 0.7302671755916025, 0.8332097142301187, 0.586404697829096, 0.8384056697171074, 0.8925430989125901, 0.9321212086381336, 0.3895038116127578, 0.7016234777170453, 0.9505095058348281, 0.6560548203389222, 0.833586068080588, 0.2858574379633786, 0.3855960688576284, 0.3349727863906771, 0.7609880037528558, 0.43890738161489035, 0.39806555763771617, 0.9352829048891074, 0.8097869464207725, 0.9529223256974573, 0.5985680617000989, 0.6326608997800717, 0.9486052495001633, 0.9348200839674985, 0.347723562029087, 0.5505450693049353, 0.8332321631831101, 0.9060561003826615, 0.948343184028229, 0.4636944731423779, 0.7763592102744433, 0.9857368328793006, 0.9462538104934557, 0.6783309780452145, 0.44267451189165175, 0.9430889397206723, 0.9116417741235681, 0.2911507760498403, 0.6288249463727129, 0.8077465948099589, 0.11811023728587368, 0.672952360812484, 0.8001844844308488, 0.6678107811875859, 0.9362801340504037, 0.4181199689363335, 0.35965216591218374, 0.8837009843050735, 0.7970123315164127, 0.8257815238937638, 0.8744345587385776, 0.78946048013366, 0.584684973565248, 0.6471668664134249, 0.43419770508491723, 0.4657333819352129, 0.8443840073818547, 0.3560328527895647, 0.30824164538025367, 0.3110090555266167, 0.878269311458618, 0.9617303176335186, 0.30811043441925673, 0.9545958335617004, 0.539118640682064, 0.9644719624146827, 0.7916891137717808, 0.6224661600213497, 0.8741647989058143, 0.9251375134812259, 0.9745780518930731, 0.8963192882992566, 0.8274771270568544, 0.4982318233525185, 0.5720677290587932, 0.38946510876683693, 0.9830296722149274, 0.975255869034251, 0.7980774742607, 0.9624508596135883, 0.9762098677705813, 0.8948670458495385, 0.39167523377685554, 0.3821815788899228, 0.5431347028253669, 0.7091570039785193, 0.7242120100978349, 0.46605105364909455, 0.6777438912696802, 0.962102708701077, 0.9121866600841926, 0.8363748448061394, 0.46124203184813023, 0.6297073935688398, 0.6689393267418842, 0.735836657212377, 0.8297713512550178, 0.6258706049241418, 0.7561297844703577, 0.8975978993714945, 0.9406355270516908, 0.7280873261576656, 0.8330297501813293, 0.8300820971250142, 0.8159253036594389, 0.30882718893256894, 0.9410067825194739, 0.5178532923835986, 0.5168705072336932, 0.25613546330440956, 0.7479396806454728, 0.8694952470533828, 0.6894606167498317, 0.2618430628808806, 0.8987929462632915, 0.8658945472295025, 0.49086107377017946, 0.59613181707828, 0.717589780548425, 0.6960042765420407, 0.5928108277067345, 0.5946982102419093, 0.18957885456144186, 0.8125429145708349, 0.5430885679069021, 0.6865008694013481, 0.9642836546885394, 0.18441572572586246, 0.4723657343437477, 0.9065301843307121, 0.06701038259138103, 0.961472991307396, 0.39013478691113623, 0.6684903987689361, 0.8899011055495665, 0.5317588322123294, 0.9087123429574342, 0.3217656583398487, 0.9701007019297219, 0.8124892826539233, 0.7919582878369461, 0.42726049975017477, 0.8916198680457144, 0.9520611043918519, 0.7600775679259587, 0.12730179028132993, 0.19832929234074576, 0.776318880268258, 0.9477526452574314, 0.9324333440186039, 0.6019356408305215, 0.3291548118545895, 0.2956153404088565, 0.8044136830940034, 0.887237437251347, 0.9232567135322138, 0.6221781999238016, 0.6425755439259011, 0.6586811579191418, 0.9742419889117975, 0.8861568600710625, 0.49258923240954355, 0.8524124557860939, 0.8027866174267142, 0.7701381826184588, 0.9592869582651908, 0.5624756082365959, 0.8094408922949592, 0.7901580927216791, 0.841011258648742, 0.8321827026016073, 0.914369714112974, 0.9013388166447551, 0.6351531795376738, 0.9583676774082095, 0.7446861580179203, 0.9495937875206693, 0.9221927719632479, 0.9604527042523284, 0.5715422205021564, 0.924728546329088, 0.8660383634676538, 0.6014294709438306, 0.48343067185580935, 0.7015341116696883, 0.24118599383070502, 0.9279866539122665, 0.19752609894716355, 0.707850059682499, 0.889711696500326, 0.6980968398510645, 0.755835248897915, 0.9176543852669206, 0.3933186311989352, 0.9295619143964093, 0.7443694749788052, 0.6304354583069092, 0.9692036989337549, 0.6365891247781525, 0.9339970633570227, 0.9617839620555828, 0.9086301441477829, 0.9474658767322486, 0.7481412556512403, 0.878781781520761, 0.8277424644338935, 0.8245371606540082, 0.11058008813899588, 0.6818221484038003, 0.9728059803922399, 0.9751679833306697, 0.7416781597721337, 0.7807079788494218, 0.8604009000221486, 0.7665745283586765, 0.9284866069974095, 0.5547416116865838, 0.9407613352737074, 0.7153611643096792, 0.7493577391671854, 0.4735327448106446, 0.8986557290877399, 0.8207174296242723, 0.46158825189644004, 0.9928116843198462, 0.8909894183337242, 0.9147509231273736, 0.5075905017189635, 0.8454999470655291, 0.6345917614751148, 0.958583134523183, 0.9668495847081082, 0.46192836465257714, 0.961309010760973, 0.9601642855670185, 0.8857658481998287, 0.18940439457026145, 0.7250391880729705, 0.7756464567009647, 0.940663678095734, 0.8085567471320465, 0.8943767670150264, 0.6187583373727502, 0.8993275205684159, 0.25592663683341055, 0.637455335351989, 0.3454641283088835, 0.9655267507815484, 0.8285539409510343, 0.79908393484228, 0.8664012628813093, 0.6924354337443256, 0.6548454072481232, 0.9644727771943793, 0.7501255059004492, 0.7698459366315475, 0.44536012363799415, 0.8963201384879521, 0.5109224308026599, 0.9295976632675957, 0.6598038918531729, 0.9866981257943701, 0.570989795360499, 0.8489768594294123, 0.7311090731099734, 0.8785117532819908, 0.9067124558663866, 0.6830631050329368, 0.802792334029212, 0.737021262667544, 0.9582488725526279, 0.8788846428096109, 0.8390300825138557, 0.928413279757291, 0.36337540286288694, 0.8202499593885415, 0.845605645877975, 0.7556198582080526, 0.9495240547623498, 0.28614549112660004, 0.792138728165867, 0.9393720392848304, 0.7108578888786369, 0.29119834125570104, 0.9312349008025642, 0.925062822998491, 0.940720679223622, 0.9498733743967636, 0.9262386824642832, 0.2535810718077191, 0.8871298765176816, 0.6272631541189376, 0.5784467989760066, 0.4328828981880556, 0.9731496374915098, 0.2941255367029886, 0.246656971808108, 0.8489426523492336, 0.3742924995543897, 0.9446842919965326, 0.42428373858678303, 0.3221318287268715, 0.9484006050970011, 0.28768815913136014, 0.8259108946799769, 0.7663035507898899, 0.9088996902488689, 0.8210746549389312, 0.3625313796933851, 0.13579710600674177, 0.8961056193845135, 0.9851962960842999, 0.9200506209960793, 0.2954719472250509, 0.9167809264001574, 0.5898743279997497, 0.31764171070174957, 0.9136846145795283, 0.7806363178563275, 0.7987629709683144, 0.8562515048247652, 0.7307773569489759, 0.9544248082184021, 0.9127782544487824, 0.8623693520002298, 0.3024846314454541, 0.9309406234616495, 0.8128239022460542, 0.962229839777732, 0.8892973988656888, 0.7734693211963917, 0.41750169513643015, 0.6562955803712851, 0.5643672366251316, 0.43808453932073843, 0.5864458370254797, 0.9816670394082339, 0.3197203202276078, 0.8050090290148326, 0.9624306767024622, 0.9307762742678802, 0.8623434308448796, 0.990108133691983, 0.746018706541076, 0.47746255007547755, 0.6612191334623871, 0.783001228290453, 0.5558554508448601, 0.944817116312316, 0.945897725871365, 0.6113887462384089, 0.895887649579229, 0.6644087112368149, 0.31443578868236866, 0.8852612684130018, 0.8536073839505716, 0.9483790653208918, 0.8601706721604159, 0.7628541847804483, 0.4680849519025553, 0.9352483368056331, 0.9616369527664776, 0.7246670782300332, 0.6024557974505752, 0.5602651539064495, 0.34786478305971896, 0.5993520809023306, 0.852471803082816, 0.8601402886946303, 0.7042320811598818, 0.37194891290568377, 0.6592136870095452, 0.8846090040719556, 0.5978112872134451, 0.8193391153301905, 0.8038306806820452, 0.8836863746511332, 0.7964726901713943, 0.421379641673294, 0.8140304698935792, 0.45573025670379774, 0.8696331987786137, 0.8118756794025181, 0.8953891378002148, 0.7699457110604023, 0.8727478146605352, 0.7871196482843112, 0.8981602981015966, 0.36591379411238956, 0.9491812042948471, 0.03831948613452212, 0.779609604768952, 0.8511522312537751, 0.9723407670151949, 0.5998046414070375, 0.9469679666767363, 0.8806439440993223, 0.9232458815785805, 0.8719960485738172, 0.44179105344216824, 0.8948517700902153, 0.3078926774411471, 0.9262112558154699, 0.8354059606910897, 0.8388432666137864, 0.25489934536355485, 0.8272807566379535, 0.5077494637816576, 0.6810842044592471, 0.2986154834187281, 0.0464782524093152, 0.9051416254393914, 0.5723592168720059, 0.44949524080022113, 0.6099943883989547, 0.9290885216076363, 0.8389497416642955, 0.9407613352737074, 1.0, 0.9292619245050848, 0.7954649546256424, 0.959095521027233, 0.4927053152538463, 0.4129771429438506, 0.65645915206365, 0.9271328028414589, 0.8457781931186075]
Finish training and take 1h8m
