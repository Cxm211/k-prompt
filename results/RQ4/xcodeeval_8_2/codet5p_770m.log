Namespace(log_name='./RQ5/xcodeeval_8_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_8_2/codet5p_770m', data_dir='./data/RQ5/xcodeeval_8_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> #include <string.h>  #define MAXSIZE 1000005  int main() {  char s[MAXSIZE];  int weight[MAXSIZE];  long i, lever, length;  long long sum;   scanf("%s", s);   length = strlen(s);  for (i = 0; i < strlen(s); i++){   weight[i] = 0;   if (s[i] == \'^\')    lever = i;   if (s[i] >= \'1\' && s[i] <= \'9\')    weight[i] = s[i] - \'0\';  }   sum = 0;   for (i = 0; i < strlen(s); i++){   sum += weight[i] * (lever - i);  }   if (sum > 0)   printf("left\\n");  else if (sum < 0)   printf("right\\n");  else   printf("balance\\n");   return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> #include <string.h>  #define MAXSIZE 1000005  int main() {  char s[MAXSIZE];  int weight[MAXSIZE];  long long sum, i, lever, length;;   scanf("%s", s);   length = strlen(s);  for (i = 0; i < length; i++){   weight[i] = 0;   if (s[i] == \'^\')    lever = i;   if (s[i] >= \'1\' && s[i] <= \'9\')    weight[i] = s[i] - \'0\';  }   sum = 0;   for (i = 0; i < length; i++){   sum += weight[i] * (lever - i);  }   if (sum > 0)   printf("left\\n");  else if (sum < 0)   printf("right\\n");  else   printf("balance\\n");   return 0; }'}]
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.5204819704312e+225
  global_step = 3
  train_loss = 73.5514
  ********************
Previous best ppl:inf
Achieve Best ppl:1.5204819704312e+225
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 61.43 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:61.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.243870681774101e+266
  global_step = 5
  train_loss = 45.1734
  ********************
Previous best ppl:1.5204819704312e+225
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 76.61 	 Previous best codebleu 61.43
  ********************
 Achieve Best bleu:76.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 8.45815336485306e+283
  global_step = 7
  train_loss = 25.6916
  ********************
Previous best ppl:1.5204819704312e+225
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 75.64 	 Previous best codebleu 76.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 7.3749492265392434e+289
  global_step = 9
  train_loss = 17.0125
  ********************
Previous best ppl:1.5204819704312e+225
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 76.18 	 Previous best codebleu 76.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 6.451472158428583e+283
  global_step = 11
  train_loss = 12.0761
  ********************
Previous best ppl:1.5204819704312e+225
BLEU file: ./data/RQ5/xcodeeval_8_2/validation.jsonl
  codebleu-4 = 75.49 	 Previous best codebleu 76.61
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_8_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_8_2/test.jsonl
  codebleu = 72.67 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 1 
[122]
  Cleaned Fixed = 2 
[254, 355]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 1 
[122]
  Cleaned Fixed = 2 
[254, 355]
  codebleu = 72.67 
[0.8807691318538904, 0.942724331075167, 0.3660686015839301, 0.8021139828346996, 0.5240259477923144, 0.21679053483574018, 0.49007773220454587, 0.28583200344068094, 0.5722965788545334, 0.676908819611102, 0.9486021234366251, 0.982653611198544, 0.7050736143053937, 0.1750992279298951, 0.8808375937505108, 0.4693955864510486, 0.5313357348360503, 0.9785699270907009, 0.7996636861006762, 0.9428409553072321, 0.7356680426568907, 0.5190852212337861, 0.959945180208001, 0.3146654568062952, 0.8596055360028632, 0.46678356460890147, 0.5452287811589471, 0.901175714210833, 0.895848422530404, 0.24041604209738301, 0.7460250732072086, 0.7907898942785752, 0.8748336476451133, 0.9469342789655903, 0.5323905799367682, 0.8059663450023036, 0.6527764179246671, 0.9426787386209374, 0.9847813251545576, 0.6214028319131882, 0.38055604522674474, 0.9802578447391919, 0.9321918895830998, 0.23616145602914845, 0.6881727989782682, 0.9386210590467857, 0.8574290303367051, 0.7113199842059187, 0.6194178615287717, 0.8803828210815028, 0.3364781645618809, 0.9193292374349928, 0.7698287087336295, 0.1900990099009901, 0.8705702138654792, 0.9247535154657887, 0.8675700280536951, 0.8565079660274721, 0.9771748654592214, 0.6449181547930165, 0.8638031623976636, 0.3321670864049434, 0.8079020131911673, 0.6338566422070385, 0.8927899452472845, 0.9299687695441708, 0.7924048855268875, 0.5338617744031935, 0.8995377367121644, 0.20429331224367825, 0.9338818208296811, 0.2690529321329123, 0.9339606849033809, 0.9717119879651379, 0.8019962306794002, 0.586404697829096, 0.8384056697171074, 0.9737915388152598, 0.9321212086381336, 0.3895038116127578, 0.6866407148315193, 0.9379551891165208, 0.9080151803246834, 0.6135730784759318, 0.2817672257345978, 0.39017622745904734, 0.8014139035130625, 0.8085386086430999, 0.43890738161489035, 0.39806555763771617, 0.9352829048891074, 0.8305048279578491, 0.9489326686943023, 0.599341442822406, 0.6326608997800717, 0.8344535541787368, 0.9348200839674985, 0.36720723099234137, 0.5505450693049353, 0.8332321631831101, 0.9060561003826615, 0.948343184028229, 0.450837330285235, 0.802635943713546, 0.9312551790965369, 0.9230872383114661, 0.8142245074783462, 0.4810010686280327, 0.9389273179055506, 0.9194883538624099, 0.6249954610889428, 0.6288249463727129, 0.6341370749042499, 0.6566376813197878, 0.6575538363200332, 0.7878254197043979, 0.6672661099494505, 0.9339515361598569, 0.4172202753046456, 0.854906905704294, 0.9027144378842089, 0.7971886413768894, 0.7906625766400034, 0.8744345587385776, 0.78946048013366, 0.6359084461047401, 0.646635559366449, 0.4184082314007067, 0.4657333819352129, 0.8443840073818547, 0.3560328527895647, 0.6096590342443714, 0.3110090555266167, 0.9547478616285707, 0.9617303176335186, 0.27233438676767885, 0.9368026696042517, 0.553376063289682, 0.965511575622183, 0.7920617845792342, 0.8854933044567805, 0.8741647989058143, 0.8846725968442206, 0.9745780518930731, 0.928943235004591, 0.8274771270568544, 0.4982318233525185, 0.5720677290587932, 0.8154336071307788, 0.965853695227304, 0.9540531761318283, 0.8320476098374262, 0.9813483172696138, 0.9295373056699359, 0.8948670458495385, 0.774693514748012, 0.7202507328873297, 0.5488586128738006, 0.7091570039785193, 0.7242120100978349, 0.14416315509056196, 0.6777438912696802, 0.7768336977766237, 0.9121866600841926, 0.7800770417334086, 0.4653069689629789, 0.6246113794477428, 0.8392067093595759, 0.968639364262265, 0.8297713512550178, 0.6258706049241418, 0.7833624454787003, 0.8975978993714945, 0.9200067227928574, 0.7280873261576656, 0.675300701026039, 0.8105874531543336, 0.8167667864596717, 0.30799772846225093, 0.8942007940880037, 0.5399191018302276, 0.5661706174767057, 0.25613546330440956, 0.7426039882832349, 0.7883304875197246, 0.6739155532953526, 0.32301796222457846, 0.9191335221980989, 0.8658945472295025, 0.49086107377017946, 0.40721584272039063, 0.8668658919336307, 0.7110329080825644, 0.5866640357968006, 0.5946982102419093, 0.19467732417880795, 0.849431210776371, 0.9159958844184792, 0.7109236301536477, 0.9642836546885394, 0.1427616762031343, 0.9508336769881993, 0.9441311059579389, 0.584820978173092, 0.9654058796749723, 0.39013478691113623, 0.6684903987689361, 0.9447313137718458, 0.5317588322123294, 0.9087123429574342, 0.9182307855266456, 0.9701007019297219, 0.8124892826539233, 0.953707845659838, 0.8019342019883358, 0.2684670634754587, 0.9520611043918519, 0.7486299869750686, 0.12695294117647057, 0.19832929234074576, 0.8055864673667326, 0.9477526452574314, 0.9025076180904277, 0.5282409716007119, 0.33127353249068686, 0.4822120322474115, 0.9280096433880707, 0.8942668643714475, 0.9232567135322138, 0.6221781999238016, 0.6234595852577036, 0.6559659558584913, 0.33584239236221536, 0.8861568600710625, 0.5085215178080368, 0.05103147935620976, 0.7803741792559828, 0.7826741503670085, 0.9592869582651908, 0.5640123867140044, 0.8695075881277006, 0.7901580927216791, 0.9861254102280717, 0.24979512120906117, 0.947573383205105, 0.9013388166447551, 0.6351531795376738, 0.9583676774082095, 0.9377858074101935, 0.9495937875206693, 0.8613585021668202, 0.9604527042523284, 0.7486459496235565, 0.943907690508595, 0.943666571833899, 0.5978059067232355, 0.7562791074610632, 0.7079968421142651, 0.2358263748547042, 0.9279866539122665, 0.19752609894716355, 0.707850059682499, 0.9284243634590652, 0.695713048670164, 0.9562005086759873, 0.4900557939243055, 0.26289253457845224, 0.9539813610821002, 0.7610628900445153, 0.6304354583069092, 0.9692036989337549, 0.6365891247781525, 0.911281561187657, 0.9617839620555828, 0.903750017912635, 0.9531844084589156, 0.7481412556512403, 0.8880374239386721, 0.769947946958748, 0.7565711923511311, 0.0968369622508172, 0.7688601947151529, 0.9773514349376944, 0.8434094165290678, 0.7416781597721337, 0.7807079788494218, 0.8988279604599121, 0.8852945092796116, 0.8689045664422734, 0.31750633481262835, 0.9407613352737074, 0.7232043610871355, 0.7493577391671854, 0.49136995734919425, 0.8912891763677813, 0.8098546763224763, 0.4594080745351665, 0.9840970086239778, 0.9049996265971042, 0.9147509231273736, 0.5075905017189635, 0.849729607166194, 0.6447036900806008, 0.9003387307587589, 0.9668495847081082, 0.4498517371274297, 0.8769132226404848, 0.9470279890319612, 0.8857658481998287, 0.17353695695308027, 0.7840665213855238, 0.6085935634827532, 0.940663678095734, 0.8113688567236648, 0.8943767670150264, 0.6372760488740846, 0.821848545144533, 0.2539556067822629, 0.637455335351989, 0.3454641283088835, 0.9655267507815484, 0.2224586558281518, 0.7519596419367407, 0.8745032870690486, 0.6924354337443256, 0.6548454072481232, 0.9823448177520746, 0.7501255059004492, 0.8904098910010204, 0.9124030351144818, 0.8466131344590825, 0.9357982417496262, 0.8928994109473654, 0.6516235135434673, 0.9866981257943701, 0.570989795360499, 0.8489768594294123, 0.7311090731099734, 0.8785117532819908, 0.8771966176842465, 0.6830631050329368, 0.802792334029212, 0.9105346103428551, 0.9253278672264125, 0.8788846428096109, 0.8977472372885162, 0.9283813677168311, 0.6216058374935409, 0.8202499593885415, 0.845605645877975, 0.6970468778868895, 0.8860716719613275, 0.30228003420613947, 0.9590506396386802, 0.9393720392848304, 0.7108578888786369, 0.29119834125570104, 0.9312349008025642, 0.9467492038330694, 0.932980155042173, 0.9498733743967636, 0.9262386824642832, 0.2473215885810292, 0.890101454002618, 0.6272631541189376, 0.6070449868747402, 0.4249666682537381, 0.9731496374915098, 0.2941255367029886, 0.246656971808108, 0.8212828564690575, 0.33404489753893724, 0.9446842919965326, 0.3974118326461889, 0.4661937691043915, 0.9537248736721973, 0.28685396953518916, 0.8259108946799769, 0.9646055689051855, 0.9570869332853229, 0.867980815381816, 0.7552847912459166, 0.4488896705464462, 0.8961056193845135, 0.9839935011430094, 0.9276272361835951, 0.2954719472250509, 0.7243561152332358, 0.5898743279997497, 0.31764171070174957, 0.9136846145795283, 0.8036605977879999, 0.42639023693189876, 0.8512192192927005, 0.6226908114170765, 0.9544248082184021, 0.9127782544487824, 0.8699110799666838, 0.3024846314454541, 0.6569462667996653, 0.8128239022460542, 0.962229839777732, 0.8703995968827191, 0.8252442861515586, 0.41222995207253776, 0.6960064951057922, 0.5643672366251316, 0.43808453932073843, 0.5864458370254797, 0.9735465142797541, 0.321597484442518, 0.8050090290148326, 0.9624306767024622, 0.9307762742678802, 0.967150116841063, 0.990108133691983, 0.7514114718545895, 0.8485021818446042, 0.6650714465361753, 0.7832059164639751, 0.5558554508448601, 0.93940004072672, 0.945897725871365, 0.6100220174806957, 0.895887649579229, 0.7914330306940703, 0.3114248549694978, 0.9035566827643757, 0.9232926862932371, 0.9167136747882797, 0.8601706721604159, 0.7628541847804483, 0.4680849519025553, 0.908233805652093, 0.9571685208867404, 0.7110307145936695, 0.7331328454992438, 0.5566636615552689, 0.7031519230668104, 0.6228228990682426, 0.7482990160434674, 0.8601402886946303, 0.7814066763867201, 0.38930242467943893, 0.6592136870095452, 0.9701592623036333, 0.5978112872134451, 0.2056057819660625, 0.8038306806820452, 0.734041394905642, 0.7964726901713943, 0.7618961489677459, 0.8724907736260945, 0.4945502466508575, 0.8696331987786137, 0.8118756794025181, 0.5348646474710699, 0.7562934717237396, 0.9756645028428577, 0.9489885523091097, 0.8981602981015966, 0.36833929575279795, 0.9491812042948471, 0.03917201637609433, 0.8735599062602175, 0.9040848481420551, 0.9723407670151949, 0.7262546855990275, 0.9413594067714341, 0.9233632180476237, 0.9003677727832611, 0.9259174527776866, 0.4777354426610022, 0.8992156948231914, 0.4320611735352054, 0.9262112558154699, 0.8354059606910897, 0.8541640707395943, 0.2545517987397291, 0.8564822037598797, 0.7797983602547747, 0.6810842044592471, 0.3080187559387044, 0.3702360177240729, 0.9207225522863047, 0.8678976437379708, 0.44949524080022113, 0.6099943883989547, 0.7377778419375021, 0.6059949127882636, 0.7675476163917795, 1.0, 0.9000200119172996, 0.8227600561608516, 0.9447655341908561, 0.4927053152538463, 0.4129771429438506, 0.8020816921073726, 0.9271328028414589, 0.8457781931186075]
Finish training and take 55m
