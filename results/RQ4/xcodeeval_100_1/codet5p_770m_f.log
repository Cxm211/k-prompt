Namespace(log_name='./RQ5/xcodeeval_100_1/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_100_1/codet5p_770m_f', data_dir='./data/RQ5/xcodeeval_100_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 100 training instances 
***** Running training *****
  Num examples = 100
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00026
  global_step = 26
  train_loss = 0.3904
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00026
  ********************
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 74.25 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:74.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00027
  global_step = 51
  train_loss = 0.2504
  ********************
Previous best ppl:1.00026
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 73.89 	 Previous best codebleu 74.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00029
  global_step = 76
  train_loss = 0.1589
  ********************
Previous best ppl:1.00026
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 74.42 	 Previous best codebleu 74.25
  ********************
 Achieve Best bleu:74.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0003
  global_step = 101
  train_loss = 0.1152
  ********************
Previous best ppl:1.00026
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 74.68 	 Previous best codebleu 74.42
  ********************
 Achieve Best bleu:74.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00033
  global_step = 126
  train_loss = 0.0774
  ********************
Previous best ppl:1.00026
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 74.02 	 Previous best codebleu 74.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00035
  global_step = 151
  train_loss = 0.0477
  ********************
Previous best ppl:1.00026
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 74.34 	 Previous best codebleu 74.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00039
  global_step = 176
  train_loss = 0.0357
  ********************
Previous best ppl:1.00026
BLEU file: ./data/RQ5/xcodeeval_100_1/validation.jsonl
  codebleu-4 = 74.62 	 Previous best codebleu 74.68
  ********************
reload model from RQ5/xcodeeval_100_1/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_100_1/test.jsonl
  codebleu = 73.08 
  Total = 500 
  Exact Fixed = 8 
[18, 59, 188, 320, 322, 359, 407, 455]
  Syntax Fixed = 1 
[45]
  Cleaned Fixed = 2 
[54, 276]
  ********************
  Total = 500 
  Exact Fixed = 8 
[18, 59, 188, 320, 322, 359, 407, 455]
  Syntax Fixed = 1 
[45]
  Cleaned Fixed = 2 
[54, 276]
  codebleu = 73.08 
[0.6906751815123662, 0.42430243947049623, 0.8702165828687908, 0.9211411224988879, 0.8764336026680739, 0.9544741722900356, 0.8989273882874643, 0.2930837534053943, 0.28590084848815933, 0.30025487675475604, 0.33757627523685907, 0.8239908684888106, 0.8840280108223306, 0.9357255678321519, 0.9320028922042589, 0.586505294091998, 0.8694464017012884, 1.0, 0.8944958165491306, 0.8685842432291047, 0.8193236358223526, 0.896734987325442, 0.9888412502412365, 0.936852705708489, 0.4011929009233417, 0.9724578322391857, 0.9799477722427206, 0.7042466967023711, 0.9864610512549261, 0.36688277713419926, 0.9690270665107408, 0.9540138870746515, 0.49381641927806796, 0.7138182723577209, 0.8016482889000909, 0.7480983802318328, 0.7810019348324313, 0.8172796747159843, 0.9678131775953225, 0.03914807619997354, 0.7370716036244591, 0.6797481699178505, 0.7646504374656518, 0.42140940250689873, 0.9744751689257993, 0.9171509212435536, 0.9103068259298337, 0.9727151010874617, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6681147742216362, 0.4096453519493349, 0.9520401723147278, 0.035053425263124334, 0.26967898121145945, 0.8267820318104704, 0.8817623929694763, 0.9753396087264441, 0.8481153341699572, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.347162812695482, 0.9821809817650384, 0.4982318233525185, 0.9829433066762787, 0.934387097307501, 0.9220017344612644, 0.4954735182274035, 0.9023385068099146, 0.3294036734712466, 0.6463782423660047, 0.7854009805543415, 0.4286735863723244, 0.4072595760539546, 0.6888365462671723, 0.9251783727522638, 0.3885946899772229, 0.8569508387019281, 0.9564084233825603, 0.9738014890069033, 0.751057511765648, 0.9809329721714535, 0.9356085333661817, 0.9295279223019721, 0.8859407157558806, 0.5480309606435074, 0.4299202460730849, 0.7075727841380755, 0.9168021557818813, 0.03281115469695418, 0.9106033686427741, 0.3532450248136815, 0.8375795025499561, 0.49643876062226794, 0.4420093605524046, 0.5019736400698931, 0.3860564185444154, 0.48795243922324283, 0.8370974599619434, 0.7794825125038006, 0.8203495978509039, 0.8848337229243108, 0.8947330957271415, 0.5961510984329409, 0.4999369426424631, 0.782190826746432, 0.9168539881373812, 0.5529814252415316, 0.8324183912964096, 0.14992380558663324, 0.7914278759686615, 0.9617458190493637, 0.9092903522106406, 0.9268555414820475, 0.28288736800459546, 0.9921128069031226, 0.41093966321126485, 0.7480407185735214, 0.9374523007440652, 0.9566283907946067, 0.4542071293267559, 0.9717916167891165, 0.7684232762284017, 0.7614331493899362, 0.8763084913143724, 0.976435199722677, 0.9656770287836146, 0.33150754715452724, 0.0774898769343977, 0.8165996742814894, 0.9135214269666101, 0.9631318837364407, 0.9678539444631555, 0.9798800402828953, 0.9798219468937557, 0.3058015211465632, 0.9560322423864296, 0.5739579759923696, 0.9732129467715989, 0.9236644367804976, 0.7366033348508686, 0.4764868334336053, 0.3185794229928485, 0.7846138466894739, 0.27086338909565627, 0.463923845442921, 0.6877917301640358, 0.9299552251878043, 0.5010923309399078, 0.8220218494315206, 0.5031028066634594, 0.8624100358037953, 0.9435407234863082, 0.47942318833040354, 0.7223951844558967, 0.7749060747283243, 0.9602403186628621, 0.9790854580792194, 0.7567133045799818, 0.6567874018519466, 0.7628424889916378, 0.3931548138311015, 0.3776630720962285, 0.408600022348932, 0.9654461577493623, 0.9598135790579945, 0.7942417816647632, 0.782794014430714, 0.8815079864399178, 0.8349159404499051, 0.6052970083217083, 0.9384697992732594, 0.9683466343264948, 0.3706598417143405, 0.8792228737352459, 0.9902911708211317, 0.3723443145512656, 0.7065722131859199, 0.8042710150175236, 0.7852571791995187, 0.5370318012864018, 0.9835764714570019, 0.9931785684115373, 0.9709756774276397, 0.8829948461632615, 0.946689158675359, 0.956301548538977, 0.9324043404373232, 0.9659102393274985, 0.8856566772820293, 0.6163950321676452, 0.8879620656093707, 0.9744119964056819, 0.8241047036645743, 0.8386875022509077, 0.41395796030346144, 0.3475222634058708, 0.9725315176192211, 0.9679884172683169, 0.4841195614277229, 0.8396431642664623, 0.9221265623517527, 0.9757745973711676, 0.825040224285887, 0.9166512013909699, 0.6528376982721772, 0.41474785475115006, 0.8712547010751163, 0.43501932676709715, 0.940598945729278, 0.5041574727616096, 0.6895051363145968, 0.7945502142295664, 0.273577547436182, 0.9589077111122837, 0.7507483205316109, 0.9187091832462004, 0.12371942079219195, 0.9699392701042773, 0.9840867555998587, 0.6619343972487068, 0.3541587417472071, 0.9425018741615083, 0.5313357348360503, 0.6297029892537214, 0.8605129711660844, 0.5784302566819072, 0.18419061660526362, 0.7836353492903962, 0.6542307012408493, 0.42497916943931774, 0.9345121855559886, 0.6121051294344302, 0.5859400643182706, 0.8207174296242723, 0.5831163274893049, 0.6916629272878606, 0.9533257646563149, 0.774674995957515, 0.6871897384788783, 0.27516721651477627, 0.28759777369338146, 0.5362395483988209, 0.9059801416992133, 0.508305988851434, 0.807627751423805, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.970099658268671, 0.4234097157305317, 0.8654416336422587, 0.6922592432779943, 0.9001946345327689, 0.8896805109136188, 0.713357448673426, 0.5177405757219062, 0.958489957733323, 0.3564744015106223, 0.41574277333701404, 0.6577727687203423, 0.8703232021938421, 0.7029526332566041, 0.9798514722492089, 0.38035740917095934, 0.7652198209183457, 0.9179853420852879, 0.9617720754477719, 0.8395423873461488, 0.9088084733438031, 0.9522217025290924, 0.23899729367746342, 0.27949142984930087, 0.9259714761461866, 0.927607533719276, 0.9286279274366336, 0.4155596630877777, 0.8533534577368376, 0.9581930467118089, 0.48228443707897417, 0.9352829048891074, 0.9749407822636968, 0.7631695126965249, 0.9322263155732342, 0.7630430656058934, 0.08933095416816453, 0.9594008959382734, 0.7965380113808502, 0.8984188648676468, 0.9238932796791619, 0.8928853312494447, 0.9747232059340507, 0.9203677694182635, 0.8855808153415308, 0.16032097457139385, 0.9341110513082564, 0.5157443236412498, 0.5712483923766941, 0.7738062382871427, 0.5790261988125887, 0.8634444765352047, 0.849729607166194, 0.8149571768480716, 0.857366514241093, 0.9121176482601447, 0.6283052858442292, 0.8302530908052741, 0.7557968074254318, 0.716166106713046, 0.8086811749495997, 0.9789372431935826, 0.7086211932640385, 0.9820653195195825, 0.9616102999368312, 0.4927694436439236, 0.05187936299146971, 0.9747269061338388, 1.0, 0.8009263268259385, 1.0, 0.8063665964216313, 0.39034769598369434, 0.41243081325512787, 0.8678205596422623, 0.8106937513235901, 0.7015252862450747, 0.3003350039751592, 0.9582005259168291, 0.9100561171054498, 0.9713651945608406, 0.939821716744068, 0.9785132817927387, 0.8171220069513339, 0.8542311710040063, 0.8186162245393346, 0.19832929234074576, 0.9406018863753673, 0.9773977581221669, 0.7786642529241632, 0.3776268460097124, 0.9432193576582235, 0.3273472980821144, 0.5074834355521683, 0.9872106081152878, 0.7772868173384335, 0.9517626327839217, 0.6047285652338439, 0.9784080121481948, 0.6495267219526154, 0.9068495810967309, 0.8164468849403668, 0.44241812450524454, 0.7852033214625844, 0.9629452959874893, 0.9454632184934535, 0.27459370351787593, 0.9701742405810041, 0.704137667465099, 0.6136084613959742, 0.4882403759621151, 0.4071384349543077, 0.4532830736687707, 0.8423297245928909, 0.05694945736130954, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.9728742867739708, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.5918447909537363, 0.7146371215272531, 0.3017311101814578, 0.9576736878812553, 0.9052457799184037, 0.6806042168843407, 0.6567079107412255, 0.8936598047708784, 0.38361540357357476, 0.9731496374915098, 0.8113179485743385, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9908744134634584, 0.9016204944152904, 0.3512649870044113, 0.7606600167190423, 0.40463236212571363, 0.9323453252450715, 0.7213287859429754, 0.461329065338275, 0.0864718984572822, 0.8463708584021533, 0.6817721564098593, 0.5135793600096521, 0.39222880900005397, 0.9686198599811506, 0.2718871669900238, 0.2649928249554196, 0.6279718857632882, 0.7327353004332166, 0.907558018935852, 1.0, 0.9778801105259323, 0.5588208186817714, 0.9157728873149173, 0.5388098930387301, 0.30387397503062613, 0.44384152820607475, 0.9108900697675792, 0.9027309916196755, 0.27546886242465657, 0.9231424851959731, 0.9190922894214792, 0.9732456940628658, 0.2381264929825428, 0.7882456293247788, 0.9086540498917997, 0.9239244320178677, 0.37607692088303035, 0.9668495847081082, 0.8019711204854889, 0.7673290885284549, 0.41399244311108085, 0.6875660275588513, 0.5864458370254797, 0.687542465552887, 0.9469672199403614, 0.5587912679655723, 0.05987019451564787, 0.9630540715975315, 0.8807613352737074, 0.9590732370227437, 0.6912905988814966, 0.797289532644271, 0.8216258903091462, 0.9685698290675941, 0.3103661364296084, 0.9427008679312978, 0.9708907201479928, 0.6903096337780377, 0.37426527756628813, 0.10191441441441441, 0.8055864673667326, 0.5557717705357138, 0.8905074545882428, 0.37333812510379816, 0.9117475844136459, 0.9635847916815603, 0.96804234407929, 0.9409121125706139, 0.36168412495357327, 0.9123161971538115, 0.9296835011666059, 0.578752861349264, 0.9632825394277797, 0.9205724657465093, 0.8122136254293291, 0.9498733743967636, 0.9714834981320903, 0.773611277512299, 0.8728375908477501, 0.2973434798199742, 0.9526474078906453, 0.786338666612526, 0.796166197021682, 0.38230642541905163, 0.5953316922726575, 0.8804232565396799, 0.8896531297074333, 0.8960729821237088, 0.6575319963393351, 0.9086210618726622, 0.8449901270019782, 0.8664855589488811, 0.8947729239116204, 0.8550885359377574, 0.7752855690103572, 0.3824686492796324, 0.576816427634421, 0.5588425607842956, 0.5763413299251462, 0.7966748977261056, 0.9154854838904751, 0.9390632563354309, 0.9652162738384751, 0.9545774975085777, 0.9166795416411146, 0.5999149165321056, 0.9064913890104815, 0.9788598843598264, 0.8739286595309739, 0.5852842064212694, 0.9403437700960667, 0.7956531382382709, 0.35058711079082805]
Finish training and take 1h7m
