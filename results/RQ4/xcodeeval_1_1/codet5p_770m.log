Namespace(log_name='./RQ5/xcodeeval_1_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_1_1/codet5p_770m', data_dir='./data/RQ5/xcodeeval_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h>  int main() {  int n;  scanf("%d",&n);  int a[n], min = 10001,j = 0;  for(int i = 0; i < n; i++)  {   scanf("%d",&a[i]);   if(a[i] < min)   {    min = a[i];    j = i+1;   }  }  if(n == 1)  {   printf("-1");  }  else if(n == 2)  {   if(a[0] == a[1])   {    printf("-1");   }  }  else  {   printf("1\\n");   printf("%d",j);  }  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h>  int main() {  int n;  scanf("%d",&n);  int a[n], min = 10001,j = 0;  for(int i = 0; i < n; i++)  {   scanf("%d",&a[i]);   if(a[i] < min)   {    min = a[i];    j = i+1;   }  }  if(n == 1)  {   printf("-1");  }  else if(n == 2 && (a[0] == a[1]))  {    printf("-1");     }  else  {   printf("1\\n");   printf("%d",j);  }  }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 2.3538449854811213e+243
  global_step = 2
  train_loss = 60.4179
  ********************
Previous best ppl:inf
Achieve Best ppl:2.3538449854811213e+243
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 35.02 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:35.02
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 6.889456564820694e+224
  global_step = 3
  train_loss = 51.107
  ********************
Previous best ppl:2.3538449854811213e+243
Achieve Best ppl:6.889456564820694e+224
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 59.83 	 Previous best codebleu 35.02
  ********************
 Achieve Best bleu:59.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.4767069082418932e+229
  global_step = 4
  train_loss = 30.7805
  ********************
Previous best ppl:6.889456564820694e+224
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 64.35 	 Previous best codebleu 59.83
  ********************
 Achieve Best bleu:64.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.199074340253969e+219
  global_step = 5
  train_loss = 17.9413
  ********************
Previous best ppl:6.889456564820694e+224
Achieve Best ppl:1.199074340253969e+219
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 62.34 	 Previous best codebleu 64.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 7.43822521504246e+222
  global_step = 6
  train_loss = 12.5739
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 67.05 	 Previous best codebleu 64.35
  ********************
 Achieve Best bleu:67.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.5899085339375616e+226
  global_step = 7
  train_loss = 8.1119
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 69.75 	 Previous best codebleu 67.05
  ********************
 Achieve Best bleu:69.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.4528238306692261e+230
  global_step = 8
  train_loss = 5.7856
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.55 	 Previous best codebleu 69.75
  ********************
 Achieve Best bleu:73.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 3.7487983039280867e+235
  global_step = 9
  train_loss = 5.3692
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.29 	 Previous best codebleu 73.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 2.493544922550896e+239
  global_step = 10
  train_loss = 2.7728
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.27 	 Previous best codebleu 73.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 2.1500930335132696e+241
  global_step = 11
  train_loss = 3.3023
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.24 	 Previous best codebleu 73.55
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_1_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_1/test.jsonl
  codebleu = 71.54 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[54, 268, 455]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[54, 268, 455]
  codebleu = 71.54 
[0.6906751815123662, 0.41202823660523735, 0.8702165828687908, 0.9620326637261998, 0.9323764597381581, 0.9544741722900356, 0.9334580759528754, 0.2931339866854209, 0.28590084848815933, 0.5896551724137931, 0.33757627523685907, 0.7850852988046231, 0.8840280108223306, 0.9357255678321519, 0.9320028922042589, 0.5687091742654554, 0.8694464017012884, 0.968864963241663, 0.4327080873739337, 0.8960614087063516, 0.6744421181633318, 0.896734987325442, 0.9694820913897946, 0.8838266581951679, 0.36449531656015954, 0.9592811635353202, 0.34393669628153556, 0.7042466967023711, 0.9409902191432074, 0.5800323580497375, 0.9609938513658753, 0.9039048770966918, 0.4893024570340459, 0.7138182723577209, 0.8016482889000909, 0.6958065433271075, 0.7772997668666193, 0.8546126291418532, 0.9513060711945898, 0.03914807619997354, 0.8036605977879999, 0.6439737215147783, 0.7575749613252776, 0.4262386448606621, 0.9053367119444256, 0.8917646611290957, 0.9722452161669002, 0.7547166973241834, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6932485940047782, 0.4096453519493349, 0.9548248224093864, 0.033115099806957524, 0.26630536517678816, 0.8267820318104704, 0.8817623929694763, 0.9730627447571871, 0.8827108485449091, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.347162812695482, 0.9821809817650384, 0.464477517956422, 0.31109151847105065, 0.934387097307501, 0.874087902232034, 0.4954735182274035, 0.8885313418278076, 0.3294036734712466, 0.6463782423660047, 0.7784759480546787, 0.3575315957840437, 0.4126494475904573, 0.6625461135203673, 0.9157056493543516, 0.407827558395895, 0.8691934016309899, 0.7816195669364977, 0.9738014890069033, 0.1351038802052721, 0.9809329721714535, 0.8619074734926304, 0.895555975855334, 0.8859407157558806, 0.5559256974856127, 0.4443730302862996, 0.7124434118278082, 0.9168021557818813, 0.03281115469695418, 0.9106033686427741, 0.3532450248136815, 0.8209169743155011, 0.49643876062226794, 0.38818990730052355, 0.5382166585227783, 0.36163873606499564, 0.48795243922324283, 0.8668858102395753, 0.8478250861445829, 0.8203495978509039, 0.9193864599685562, 0.8947330957271415, 0.5961510984329409, 0.5003909451993059, 0.7833636144245735, 0.9168539881373812, 0.558272408075298, 0.8324183912964096, 0.15292383803232715, 0.7914278759686615, 0.9654058796749723, 0.4809908521120931, 0.7422251981910283, 0.2849664187950911, 0.9711509754837166, 0.41093966321126485, 0.7480407185735214, 0.9374523007440652, 0.9566283907946067, 0.4542071293267559, 0.9717916167891165, 0.7684232762284017, 0.7614331493899362, 0.9368495847081082, 0.965289401838378, 0.9656770287836146, 0.33945728624618854, 0.9416744734511042, 0.8165996742814894, 0.9739901642593853, 0.9631318837364407, 0.960953000485429, 0.9798800402828953, 0.9798219468937557, 0.2984600211190037, 0.9062345043647053, 0.5739579759923696, 0.8142392195641672, 0.9150833522873736, 0.7332328509698608, 0.4351923796721864, 0.3157691625069721, 0.7846138466894739, 0.26515420087085695, 0.4568639140683286, 0.6235358458582476, 0.9299552251878043, 0.5010923309399078, 0.7868281386651695, 0.15802281624118797, 0.8624100358037953, 0.400378041129073, 0.4775369858583672, 0.7223951844558967, 0.927982876787582, 0.9602403186628621, 0.9793040838341944, 0.7246670782300332, 0.6567874018519466, 0.7078027943363154, 0.3931548138311015, 0.3821796477166763, 0.33784866007801845, 0.9136359424820766, 0.9598135790579945, 0.7942417816647632, 0.782794014430714, 0.8574233779834963, 0.7764433185439494, 0.5761814586370668, 0.7281657492847414, 0.962864448804861, 0.3706598417143405, 0.8792228737352459, 0.9453963170140316, 0.3723443145512656, 0.7065722131859199, 0.8042710150175236, 0.74820232594881, 0.5370318012864018, 0.8992562005799716, 0.9931785684115373, 0.9199858184584705, 0.8829948461632615, 0.7252635829264688, 0.9547478616285707, 0.9047535173079659, 0.9662947787898215, 0.48256040006922624, 0.6163950321676452, 0.8637970376685664, 0.8916234603244133, 0.8908110243776604, 0.5492010360849566, 0.41517848920468226, 0.34815969326415935, 0.9346720862236988, 0.9679884172683169, 0.4841195614277229, 0.8396431642664623, 0.9130555191813399, 0.925069437180823, 0.825040224285887, 0.9080441504666455, 0.7143619594283981, 0.41474785475115006, 0.8712547010751163, 0.425395527226478, 0.940598945729278, 0.5315782939047562, 0.6895051363145968, 0.7709540621971285, 0.2632327198499751, 0.9589077111122837, 0.7507483205316109, 0.6869732397519104, 0.6386890640788627, 0.8982347642004256, 0.9840867555998587, 0.6959236523849142, 0.33155458508060087, 0.7393418730414729, 0.5313357348360503, 0.6297029892537214, 0.8898733743967635, 0.5693956708982961, 0.19484130941461325, 0.7836353492903962, 0.5439420078074064, 0.42497916943931774, 0.9130672209743587, 0.6121051294344302, 0.578502292273178, 0.8207174296242723, 0.5831163274893049, 0.32088532674605136, 0.9533257646563149, 0.774674995957515, 0.6744081368630407, 0.2558381069058622, 0.28333753139041007, 0.5326441504389217, 0.9059801416992133, 0.4920928783231675, 0.9802031318403173, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.7567210214955552, 0.4234097157305317, 0.9069324708128441, 0.6295969047270834, 0.7723685068565823, 0.9089815080638761, 0.737357448673426, 0.5177405757219062, 0.939230675483474, 0.37665359276468713, 0.41574277333701404, 0.6486630160206606, 0.8932802454708535, 0.7029526332566041, 0.9798514722492089, 0.3795209460200526, 0.9428491633077438, 0.9008796482913477, 0.9617720754477719, 0.8344780346578161, 0.9173718504389621, 0.9522217025290924, 0.2368846176211254, 0.27949142984930087, 0.889706710410247, 0.927607533719276, 0.7561339915787862, 0.4155596630877777, 0.8533534577368376, 0.9479424149532027, 0.48228443707897417, 0.9352829048891074, 0.9596628528083495, 0.6917816871810398, 0.9322263155732342, 0.6541844570263293, 0.09129598358938275, 0.9770479547618027, 0.8069218073878743, 0.8347992588887381, 0.9138964940864869, 0.8743296682472348, 0.9747232059340507, 0.9470722443623478, 0.8342493345745878, 0.1645276846716329, 0.9341110513082564, 0.5157443236412498, 0.5750582617526012, 0.7712340923081431, 0.5740781578772014, 0.8672370204588049, 0.8326340635999792, 0.8121978155567737, 0.857366514241093, 0.8849241766235095, 0.6240195715585148, 0.8302530908052741, 0.7485066002648197, 0.7588553443518992, 0.8086811749495997, 0.9881263545755623, 0.7637496742313488, 0.9706186597280273, 0.9616102999368312, 0.5012143983741278, 0.051466049375048936, 0.9647219325099632, 0.8270160908514356, 0.8071763268259384, 0.8335967671595595, 0.9478278042336812, 0.3719962215610561, 0.41243081325512787, 0.8861971446522757, 0.8154187738366516, 0.6165817530090041, 0.5909090909090909, 0.9707135589044302, 0.8262221510665422, 0.9077960809169878, 0.939821716744068, 0.9785132817927387, 0.8171220069513339, 0.8077517860405217, 0.9141988313032683, 0.19832929234074576, 0.9706304665534506, 0.9359870875822385, 0.8559263155115343, 0.3776268460097124, 0.9829927321997591, 0.3273472980821144, 0.49292032875605185, 0.9160045179281715, 0.7772868173384335, 0.9169359195167013, 0.6688983661442429, 0.9672403188654792, 0.6492575875669616, 0.8673337463694788, 0.8164468849403668, 0.4874181245052445, 0.7697481588334356, 0.971798943338843, 0.9454632184934535, 0.27459370351787593, 0.9001973868591611, 0.704137667465099, 0.6139964929335162, 0.47541291345050607, 0.3917722655682701, 0.4532830736687707, 0.8224263032037706, 0.05652271482930384, 0.5682945197985401, 0.8415064514686508, 0.7725612258856176, 0.9728742867739708, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.6359084461047401, 0.7146371215272531, 0.29785696017873564, 0.9576736878812553, 0.933792626439619, 0.5507463552897676, 0.6567079107412255, 0.8936598047708784, 0.3723588344016272, 0.9731496374915098, 0.8250877060561935, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9747348871931607, 0.9044469849626766, 0.32268206968241614, 0.802634473449721, 0.40463236212571363, 0.9323453252450715, 0.8719563478420309, 0.461329065338275, 0.0864718984572822, 0.9176516704321582, 0.5415956283196598, 0.5195959848942016, 0.3902680246863285, 0.8994259796362998, 0.2547023325417334, 0.48035988410166097, 0.6042453659870897, 0.7351648102743649, 0.8282693990751074, 0.8642224065245785, 0.9663344674837377, 0.5423511622121149, 0.9157728873149173, 0.4856942048435414, 0.30623724611551506, 0.4317393796631991, 0.8961704370267427, 0.8497251305586286, 0.2506930541154952, 0.9714552656085744, 0.9190922894214792, 0.9732456940628658, 0.2579343193335908, 0.9228407408956874, 0.8813734515601019, 0.9128071327141931, 0.3451344686024709, 0.9668495847081082, 0.8019711204854889, 0.713091127061847, 0.3509442655676449, 0.7055479829922993, 0.5696800204384717, 0.687542465552887, 0.9469672199403614, 0.5622864055659367, 0.059192199400695825, 0.9299430225011394, 0.8807613352737074, 0.9590732370227437, 0.6912905988814966, 0.8726023412012331, 0.8009289509082698, 0.9717033980223364, 0.3478242508876328, 0.8154992577284073, 0.9476061609388879, 0.7141494776867402, 0.3752162382299256, 0.30428571428571427, 0.8055864673667326, 0.5475329309937669, 0.42872903619764563, 0.37333812510379816, 0.7253944142180402, 0.9358319531159482, 0.96804234407929, 0.9306848398433412, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5831646260551463, 0.9399257675249202, 0.9151731735749239, 0.8122136254293291, 0.9498733743967636, 0.9587024989820909, 0.7197504206194683, 0.8716916779371313, 0.30153211154794746, 0.8956579345798319, 0.7499726263990811, 0.7882153547000694, 0.38230642541905163, 0.5833753791475811, 0.8804232565396799, 0.8661607175346979, 0.8920643294874622, 0.6260431714360146, 0.9086210618726622, 0.9078336364448021, 0.8664855589488811, 0.892350207681204, 0.8757845494319172, 0.7752855690103572, 0.3652748120602648, 0.5734004854523596, 0.5608973553048435, 0.5763413299251462, 0.8698366814058456, 0.9086920534597274, 0.9860013241305441, 0.9819992034986089, 0.9583856491079104, 0.9002385163284392, 0.5964698941302258, 0.8764422999406316, 0.9788598843598264, 0.8270021204349962, 0.5441029026123381, 0.9288393936670948, 0.8190595233656908, 0.3559429663587866]
Finish training and take 1h27m
Namespace(log_name='./RQ5/xcodeeval_1_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_1_1/codet5p_770m', data_dir='./data/RQ5/xcodeeval_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h>  int main() {  int n;  scanf("%d",&n);  int a[n], min = 10001,j = 0;  for(int i = 0; i < n; i++)  {   scanf("%d",&a[i]);   if(a[i] < min)   {    min = a[i];    j = i+1;   }  }  if(n == 1)  {   printf("-1");  }  else if(n == 2)  {   if(a[0] == a[1])   {    printf("-1");   }  }  else  {   printf("1\\n");   printf("%d",j);  }  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h>  int main() {  int n;  scanf("%d",&n);  int a[n], min = 10001,j = 0;  for(int i = 0; i < n; i++)  {   scanf("%d",&a[i]);   if(a[i] < min)   {    min = a[i];    j = i+1;   }  }  if(n == 1)  {   printf("-1");  }  else if(n == 2 && (a[0] == a[1]))  {    printf("-1");     }  else  {   printf("1\\n");   printf("%d",j);  }  }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 2.3538449854811213e+243
  global_step = 2
  train_loss = 60.4179
  ********************
Previous best ppl:inf
Achieve Best ppl:2.3538449854811213e+243
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 35.01 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:35.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 6.889456564820694e+224
  global_step = 3
  train_loss = 51.107
  ********************
Previous best ppl:2.3538449854811213e+243
Achieve Best ppl:6.889456564820694e+224
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 59.81 	 Previous best codebleu 35.01
  ********************
 Achieve Best bleu:59.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.4767069082418932e+229
  global_step = 4
  train_loss = 30.7805
  ********************
Previous best ppl:6.889456564820694e+224
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 64.36 	 Previous best codebleu 59.81
  ********************
 Achieve Best bleu:64.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.199074340253969e+219
  global_step = 5
  train_loss = 17.9413
  ********************
Previous best ppl:6.889456564820694e+224
Achieve Best ppl:1.199074340253969e+219
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 62.33 	 Previous best codebleu 64.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 7.43822521504246e+222
  global_step = 6
  train_loss = 12.5739
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 67.04 	 Previous best codebleu 64.36
  ********************
 Achieve Best bleu:67.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.5899085339375616e+226
  global_step = 7
  train_loss = 8.1119
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 69.74 	 Previous best codebleu 67.04
  ********************
 Achieve Best bleu:69.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.4528238306692261e+230
  global_step = 8
  train_loss = 5.7856
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.55 	 Previous best codebleu 69.74
  ********************
 Achieve Best bleu:73.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 3.7487983039280867e+235
  global_step = 9
  train_loss = 5.3692
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.29 	 Previous best codebleu 73.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 2.493544922550896e+239
  global_step = 10
  train_loss = 2.7728
  ********************
Previous best ppl:1.199074340253969e+219
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 73.27 	 Previous best codebleu 73.55
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_1_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_1/test.jsonl
  codebleu = 71.54 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[54, 268, 455]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[54, 268, 455]
  codebleu = 71.54 
[0.6906751815123662, 0.41202823660523735, 0.8702165828687908, 0.9620326637261998, 0.9323764597381581, 0.9544741722900356, 0.9334580759528754, 0.2931339866854209, 0.28590084848815933, 0.5896551724137931, 0.33757627523685907, 0.7850852988046231, 0.8840280108223306, 0.9357255678321519, 0.9320028922042589, 0.5687091742654554, 0.8694464017012884, 0.968864963241663, 0.4327080873739337, 0.8960614087063516, 0.6744421181633318, 0.896734987325442, 0.9694820913897946, 0.8838266581951679, 0.36449531656015954, 0.9592811635353202, 0.34393669628153556, 0.7042466967023711, 0.9409902191432074, 0.5800323580497375, 0.9609938513658753, 0.9039048770966918, 0.4893024570340459, 0.7138182723577209, 0.8016482889000909, 0.6958065433271075, 0.7772997668666193, 0.8546126291418532, 0.9513060711945898, 0.03914807619997354, 0.8036605977879999, 0.6439737215147783, 0.7575749613252776, 0.4262386448606621, 0.9053367119444256, 0.8917646611290957, 0.9722452161669002, 0.7547166973241834, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6932485940047782, 0.4096453519493349, 0.9548248224093864, 0.033115099806957524, 0.26630536517678816, 0.8267820318104704, 0.8817623929694763, 0.9730627447571871, 0.8827108485449091, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.347162812695482, 0.9821809817650384, 0.464477517956422, 0.31109151847105065, 0.934387097307501, 0.874087902232034, 0.4954735182274035, 0.8885313418278076, 0.3294036734712466, 0.6463782423660047, 0.7784759480546787, 0.3575315957840437, 0.4126494475904573, 0.6625461135203673, 0.9157056493543516, 0.407827558395895, 0.8691934016309899, 0.7816195669364977, 0.9738014890069033, 0.1351038802052721, 0.9809329721714535, 0.8619074734926304, 0.895555975855334, 0.8859407157558806, 0.563820434327718, 0.4443730302862996, 0.7124434118278082, 0.9168021557818813, 0.03281115469695418, 0.9106033686427741, 0.3532450248136815, 0.8209169743155011, 0.49643876062226794, 0.38818990730052355, 0.5382166585227783, 0.36163873606499564, 0.48795243922324283, 0.8668858102395753, 0.8478250861445829, 0.8203495978509039, 0.9193864599685562, 0.8947330957271415, 0.5961510984329409, 0.5003909451993059, 0.7833636144245735, 0.9168539881373812, 0.558272408075298, 0.8324183912964096, 0.15292383803232715, 0.7914278759686615, 0.9654058796749723, 0.4809908521120931, 0.7422251981910283, 0.2849664187950911, 0.9711509754837166, 0.41093966321126485, 0.7480407185735214, 0.9374523007440652, 0.9566283907946067, 0.4542071293267559, 0.9717916167891165, 0.7684232762284017, 0.7614331493899362, 0.9368495847081082, 0.965289401838378, 0.9656770287836146, 0.33945728624618854, 0.9416744734511042, 0.8165996742814894, 0.9739901642593853, 0.9631318837364407, 0.960953000485429, 0.9798800402828953, 0.9798219468937557, 0.2984600211190037, 0.9062345043647053, 0.5739579759923696, 0.8142392195641672, 0.9150833522873736, 0.7332328509698608, 0.4351923796721864, 0.3157691625069721, 0.7846138466894739, 0.26515420087085695, 0.4568639140683286, 0.6235358458582476, 0.9299552251878043, 0.5010923309399078, 0.7868281386651695, 0.15802281624118797, 0.8624100358037953, 0.400378041129073, 0.4775369858583672, 0.7223951844558967, 0.927982876787582, 0.9602403186628621, 0.9793040838341944, 0.7246670782300332, 0.6567874018519466, 0.7078027943363154, 0.3931548138311015, 0.3821796477166763, 0.33784866007801845, 0.9136359424820766, 0.9598135790579945, 0.7942417816647632, 0.782794014430714, 0.8574233779834963, 0.7764433185439494, 0.5761814586370668, 0.7281657492847414, 0.962864448804861, 0.3706598417143405, 0.8792228737352459, 0.9453963170140316, 0.3723443145512656, 0.7065722131859199, 0.8042710150175236, 0.74820232594881, 0.5370318012864018, 0.8992562005799716, 0.9931785684115373, 0.9199858184584705, 0.8829948461632615, 0.7252635829264688, 0.9547478616285707, 0.9047535173079659, 0.9662947787898215, 0.48256040006922624, 0.6163950321676452, 0.8637970376685664, 0.8916234603244133, 0.8908110243776604, 0.5492010360849566, 0.41517848920468226, 0.34815969326415935, 0.9346720862236988, 0.9679884172683169, 0.4841195614277229, 0.8396431642664623, 0.9130555191813399, 0.925069437180823, 0.825040224285887, 0.9080441504666455, 0.7143619594283981, 0.41474785475115006, 0.8712547010751163, 0.425395527226478, 0.940598945729278, 0.5315782939047562, 0.6895051363145968, 0.7709540621971285, 0.2632327198499751, 0.9589077111122837, 0.7507483205316109, 0.6869732397519104, 0.6386890640788627, 0.8982347642004256, 0.9840867555998587, 0.6959236523849142, 0.33155458508060087, 0.7393418730414729, 0.5313357348360503, 0.6297029892537214, 0.8898733743967635, 0.5693956708982961, 0.19484130941461325, 0.7836353492903962, 0.5439420078074064, 0.42497916943931774, 0.9130672209743587, 0.6121051294344302, 0.578502292273178, 0.8207174296242723, 0.5831163274893049, 0.32088532674605136, 0.9533257646563149, 0.774674995957515, 0.6744081368630407, 0.2558381069058622, 0.28333753139041007, 0.5326441504389217, 0.9059801416992133, 0.4920928783231675, 0.9802031318403173, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.7567210214955552, 0.4234097157305317, 0.9069324708128441, 0.6295969047270834, 0.7723685068565823, 0.9089815080638761, 0.713357448673426, 0.5177405757219062, 0.939230675483474, 0.37665359276468713, 0.41574277333701404, 0.6486630160206606, 0.8932802454708535, 0.7029526332566041, 0.9798514722492089, 0.3795209460200526, 0.9428491633077438, 0.9008796482913477, 0.9617720754477719, 0.8344780346578161, 0.9173718504389621, 0.9522217025290924, 0.2368846176211254, 0.27949142984930087, 0.8947067104102469, 0.927607533719276, 0.7561339915787862, 0.4155596630877777, 0.8533534577368376, 0.9479424149532027, 0.48228443707897417, 0.9352829048891074, 0.9596628528083495, 0.6917816871810398, 0.9322263155732342, 0.6541844570263293, 0.09129598358938275, 0.9770479547618027, 0.8069218073878743, 0.8347992588887381, 0.9138964940864869, 0.8743296682472348, 0.9747232059340507, 0.9470722443623478, 0.8342493345745878, 0.1682776846716329, 0.9341110513082564, 0.5157443236412498, 0.5750582617526012, 0.7712340923081431, 0.5740781578772014, 0.8672370204588049, 0.8326340635999792, 0.8121978155567737, 0.857366514241093, 0.8849241766235095, 0.6240195715585148, 0.8302530908052741, 0.7485066002648197, 0.7588553443518992, 0.8086811749495997, 0.9881263545755623, 0.7637496742313488, 0.9706186597280273, 0.9616102999368312, 0.5012143983741278, 0.051466049375048936, 0.9647219325099632, 0.8270160908514356, 0.8196763268259384, 0.8335967671595595, 0.9478278042336812, 0.3963205458853804, 0.41243081325512787, 0.8861971446522757, 0.8154187738366516, 0.6165817530090041, 0.5909090909090909, 0.9707135589044302, 0.8262221510665422, 0.9077960809169878, 0.939821716744068, 0.9785132817927387, 0.8171220069513339, 0.8077517860405217, 0.9141988313032683, 0.19832929234074576, 0.9706304665534506, 0.9359870875822385, 0.8559263155115343, 0.3776268460097124, 0.9829927321997591, 0.3273472980821144, 0.49292032875605185, 0.9160045179281715, 0.7772868173384335, 0.9169359195167013, 0.6688983661442429, 0.9672403188654792, 0.6492575875669616, 0.8673337463694788, 0.8164468849403668, 0.45741812450524455, 0.7697481588334356, 0.971798943338843, 0.9454632184934535, 0.27459370351787593, 0.9001973868591611, 0.704137667465099, 0.6139964929335162, 0.47541291345050607, 0.3917722655682701, 0.4532830736687707, 0.8224263032037706, 0.05652271482930384, 0.5682945197985401, 0.8415064514686508, 0.7725612258856176, 0.9728742867739708, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.6359084461047401, 0.7146371215272531, 0.29785696017873564, 0.9576736878812553, 0.933792626439619, 0.5507463552897676, 0.6567079107412255, 0.8936598047708784, 0.3723588344016272, 0.9731496374915098, 0.8250877060561935, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9747348871931607, 0.9044469849626766, 0.32268206968241614, 0.802634473449721, 0.40463236212571363, 0.9323453252450715, 0.8719563478420309, 0.461329065338275, 0.0864718984572822, 0.9176516704321582, 0.5415956283196598, 0.5195959848942016, 0.39222880900005397, 0.8994259796362998, 0.24994042777982867, 0.48035988410166097, 0.6042453659870897, 0.7351648102743649, 0.8282693990751074, 0.8642224065245785, 0.9663344674837377, 0.5423511622121149, 0.9157728873149173, 0.4856942048435414, 0.30623724611551506, 0.4317393796631991, 0.8961704370267427, 0.8497251305586286, 0.2506930541154952, 0.9714552656085744, 0.9190922894214792, 0.9732456940628658, 0.2579343193335908, 0.9228407408956874, 0.8813734515601019, 0.9128071327141931, 0.3451344686024709, 0.9668495847081082, 0.8019711204854889, 0.713091127061847, 0.3509442655676449, 0.7055479829922993, 0.5696800204384717, 0.687542465552887, 0.9469672199403614, 0.5622864055659367, 0.059192199400695825, 0.9299430225011394, 0.8807613352737074, 0.9590732370227437, 0.6912905988814966, 0.8640309126298046, 0.8009289509082698, 0.9717033980223364, 0.3478242508876328, 0.8154992577284073, 0.9476061609388879, 0.7141494776867402, 0.3752162382299256, 0.30428571428571427, 0.8055864673667326, 0.5475329309937669, 0.42872903619764563, 0.37333812510379816, 0.7253944142180402, 0.9358319531159482, 0.96804234407929, 0.9306848398433412, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5649828078733281, 0.9399257675249202, 0.9151731735749239, 0.8122136254293291, 0.9498733743967636, 0.9587024989820909, 0.7197504206194683, 0.8716916779371313, 0.30153211154794746, 0.8956579345798319, 0.7499726263990811, 0.7882153547000694, 0.38230642541905163, 0.5833753791475811, 0.8804232565396799, 0.8661607175346979, 0.8920643294874622, 0.6260431714360146, 0.9086210618726622, 0.9078336364448021, 0.8664855589488811, 0.892350207681204, 0.8757845494319172, 0.7752855690103572, 0.3652748120602648, 0.5734004854523596, 0.5608973553048435, 0.5763413299251462, 0.8698366814058456, 0.9086920534597274, 0.9860013241305441, 0.9819992034986089, 0.9583856491079104, 0.9002385163284392, 0.5964698941302258, 0.8764422999406316, 0.9788598843598264, 0.8270021204349962, 0.5441029026123381, 0.9288393936670948, 0.8190595233656908, 0.3559429663587866]
Finish training and take 1h17m
