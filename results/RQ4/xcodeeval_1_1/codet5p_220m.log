Namespace(log_name='./RQ5/xcodeeval_1_1/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_1_1/codet5p_220m', data_dir='./data/RQ5/xcodeeval_1_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include<stdio.h>  int main() {  int n;  scanf("%d",&n);  int a[n], min = 10001,j = 0;  for(int i = 0; i < n; i++)  {   scanf("%d",&a[i]);   if(a[i] < min)   {    min = a[i];    j = i+1;   }  }  if(n == 1)  {   printf("-1");  }  else if(n == 2)  {   if(a[0] == a[1])   {    printf("-1");   }  }  else  {   printf("1\\n");   printf("%d",j);  }  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h>  int main() {  int n;  scanf("%d",&n);  int a[n], min = 10001,j = 0;  for(int i = 0; i < n; i++)  {   scanf("%d",&a[i]);   if(a[i] < min)   {    min = a[i];    j = i+1;   }  }  if(n == 1)  {   printf("-1");  }  else if(n == 2 && (a[0] == a[1]))  {    printf("-1");     }  else  {   printf("1\\n");   printf("%d",j);  }  }'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.936635966233584e+299
  global_step = 2
  train_loss = 75.2085
  ********************
Previous best ppl:inf
Achieve Best ppl:3.936635966233584e+299
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 25.67 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:25.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 8.994843862878248e+270
  global_step = 3
  train_loss = 65.0867
  ********************
Previous best ppl:3.936635966233584e+299
Achieve Best ppl:8.994843862878248e+270
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 58.65 	 Previous best codebleu 25.67
  ********************
 Achieve Best bleu:58.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 7.706147824606184e+254
  global_step = 4
  train_loss = 30.8187
  ********************
Previous best ppl:8.994843862878248e+270
Achieve Best ppl:7.706147824606184e+254
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 56.72 	 Previous best codebleu 58.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 3.757897326289022e+250
  global_step = 5
  train_loss = 27.7523
  ********************
Previous best ppl:7.706147824606184e+254
Achieve Best ppl:3.757897326289022e+250
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 61.99 	 Previous best codebleu 58.65
  ********************
 Achieve Best bleu:61.99
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 3.1498106769589497e+258
  global_step = 6
  train_loss = 19.1768
  ********************
Previous best ppl:3.757897326289022e+250
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 64.92 	 Previous best codebleu 61.99
  ********************
 Achieve Best bleu:64.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.9809509656187282e+267
  global_step = 7
  train_loss = 8.9257
  ********************
Previous best ppl:3.757897326289022e+250
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 66.37 	 Previous best codebleu 64.92
  ********************
 Achieve Best bleu:66.37
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.68977358924222e+276
  global_step = 8
  train_loss = 12.1414
  ********************
Previous best ppl:3.757897326289022e+250
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 69.1 	 Previous best codebleu 66.37
  ********************
 Achieve Best bleu:69.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.6110893744330411e+283
  global_step = 9
  train_loss = 9.7044
  ********************
Previous best ppl:3.757897326289022e+250
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 69.97 	 Previous best codebleu 69.1
  ********************
 Achieve Best bleu:69.97
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.382794425060197e+289
  global_step = 10
  train_loss = 6.5841
  ********************
Previous best ppl:3.757897326289022e+250
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 70.33 	 Previous best codebleu 69.97
  ********************
 Achieve Best bleu:70.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.326099095165161e+292
  global_step = 11
  train_loss = 4.4007
  ********************
Previous best ppl:3.757897326289022e+250
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 70.36 	 Previous best codebleu 70.33
  ********************
 Achieve Best bleu:70.36
  ********************
reload model from RQ5/xcodeeval_1_1/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_1/test.jsonl
  codebleu = 69.77 
  Total = 500 
  Exact Fixed = 3 
[59, 359, 455]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[54, 268, 276]
  ********************
  Total = 500 
  Exact Fixed = 3 
[59, 359, 455]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[54, 268, 276]
  codebleu = 69.77 
[0.6906751815123662, 0.4658094583130275, 0.8702165828687908, 0.8836690331142116, 0.9370958145914035, 0.9544741722900356, 0.9334580759528754, 0.2930837534053943, 0.28590084848815933, 0.5818879630922669, 0.29173186712042565, 0.7781235710525856, 0.8840280108223306, 0.9357255678321519, 0.536781407534114, 0.9904619066549505, 0.8694464017012884, 0.8665682979518612, 0.8944958165491306, 0.8960614087063516, 0.6583778856715901, 0.896734987325442, 0.9746647949358924, 0.9625525637745691, 0.4011929009233417, 0.8570800930218292, 0.34393669628153556, 0.6927527844201661, 0.887856987272224, 0.5800323580497375, 0.9609938513658753, 0.7183843672766401, 0.4534839532463868, 0.7138182723577209, 0.7314756947744787, 0.7480983802318328, 0.7810019348324313, 0.7958032019228529, 0.9513060711945898, 0.030071118311844978, 0.5675867253145536, 0.5706003890001163, 0.7646504374656518, 0.4262386448606621, 0.9028978279925957, 0.8819335183657793, 0.9722452161669002, 0.8330958457225617, 0.7693582389764291, 0.8458233745010357, 0.7255124534801245, 0.6751922228302627, 0.4096453519493349, 0.9836521549336295, 0.024920912243110736, 0.2652100539123435, 0.8267820318104704, 0.8421938483881394, 0.9753396087264441, 0.8827108485449091, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.347162812695482, 0.7428394482308103, 0.4982318233525185, 0.31109151847105065, 0.934387097307501, 0.9081388482822838, 0.4954735182274035, 0.7084415798247257, 0.3294036734712466, 0.6463782423660047, 0.7676880486891622, 0.4300770341785877, 0.4080703027889234, 0.6666962650203826, 0.7372243183793246, 0.33625905037721543, 0.8691934016309899, 0.9564084233825603, 0.798150762501139, 0.1172546004049857, 0.9809329721714535, 0.6682534349488988, 0.927737917623999, 0.8859407157558806, 0.5476730264054692, 0.4443730302862996, 0.6971205806342553, 0.7974909662077054, 0.03281115469695418, 0.8298976682212666, 0.3532450248136815, 0.7087717917341088, 0.45687240969649345, 0.3813691517432485, 0.48494947352844575, 0.36163873606499564, 0.4870120477262502, 0.8668858102395753, 0.8042481005631815, 0.8203495978509039, 0.8742617258994843, 0.8947330957271415, 0.5961510984329409, 0.4999369426424631, 0.6149849603928601, 0.8913542339847487, 0.5398037776736023, 0.8324183912964096, 0.1508814432155347, 0.7914278759686615, 0.7106185584845027, 0.9017791891177018, 0.8611654450940318, 0.27639487612997105, 0.9296362830963647, 0.41093966321126485, 0.7359219968551923, 0.9374523007440652, 0.9566283907946067, 0.45098132287514303, 0.93072467044504, 0.7684232762284017, 0.7553728596543776, 0.9368495847081082, 0.9620319994472966, 0.9656770287836146, 0.33945728624618854, 0.9367312474154765, 0.771255515282774, 0.9739901642593853, 0.9631318837364407, 0.9678539444631555, 0.9798800402828953, 0.9798219468937557, 0.2488438180849786, 0.9560322423864296, 0.5739579759923696, 0.9732129467715989, 0.9194941061046169, 0.7332328509698608, 0.4756458442334832, 0.3157691625069721, 0.7846138466894739, 0.27086338909565627, 0.46622047742046324, 0.6877917301640358, 0.9299552251878043, 0.5010923309399078, 0.8267162447332495, 0.5031028066634594, 0.8624100358037953, 0.9435407234863082, 0.45590791769765143, 0.7223951844558967, 0.927982876787582, 0.9602403186628621, 0.9793040838341944, 0.7246670782300332, 0.5917980820482778, 0.8419080936404689, 0.3931548138311015, 0.3624564370116114, 0.3381602874105267, 0.9654461577493623, 0.9598135790579945, 0.7634946917542313, 0.7668179831078072, 0.8815079864399178, 0.8695346475752126, 0.4059995459808893, 0.9384697992732594, 0.8134888819102581, 0.3706598417143405, 0.8094744697641767, 0.6920319654588054, 0.3723443145512656, 0.6765949840634901, 0.8042710150175236, 0.74820232594881, 0.48082565379666875, 0.8800733716143734, 0.8298447405046755, 0.6702464051513078, 0.8829948461632615, 0.3966824095200942, 0.956301548538977, 0.9103506290794949, 0.8496939615161634, 0.8856566772820293, 0.6163950321676452, 0.6571990808756469, 0.9400782927797837, 0.8908110243776604, 0.8052152383828418, 0.2937670163530297, 0.28241040741736145, 0.9346720862236988, 0.9305976379807441, 0.4841195614277229, 0.8396431642664623, 0.9130555191813399, 0.74954447054365, 0.8107555086673924, 0.894879158840495, 0.719697134106106, 0.41474785475115006, 0.7755494685893669, 0.43419770508491723, 0.940598945729278, 0.4699908060949429, 0.6895051363145968, 0.7945502142295664, 0.2632327198499751, 0.9589077111122837, 0.7276139660588652, 0.6739000919726313, 0.2126917512171749, 0.9699392701042773, 0.6841382498181219, 0.8198387442649027, 0.30629628128160413, 0.9425018741615083, 0.5313357348360503, 0.6297029892537214, 0.8365574420549129, 0.5784302566819072, 0.16399264570983196, 0.6832538865775302, 0.05342039503139552, 0.3578325338830722, 0.9223899942569893, 0.6121051294344302, 0.578502292273178, 0.7817372619277106, 0.54387399094573, 0.6916629272878606, 0.8157060304271155, 0.7413465397225829, 0.6284610421473568, 0.2521899422077732, 0.24020759514388898, 0.438324903429722, 0.8593732188694371, 0.47629884445300513, 0.7775207613892368, 0.8175867052313311, 0.915465568076623, 0.8157296117327372, 0.9789628153220145, 0.4054665988510343, 0.5098369354440941, 0.6463849453391025, 0.9251375134812259, 0.8987307445084196, 0.713357448673426, 0.487298453344198, 0.7810695375918225, 0.38163008179290037, 0.41574277333701404, 0.5971884890111211, 0.8620825442984739, 0.6898800585173648, 0.9798514722492089, 0.3802882391069806, 0.9428491633077438, 0.9179853420852879, 0.8832119168550208, 0.8344780346578161, 0.7337268760980411, 0.9522217025290924, 0.2321964067275699, 0.24263082907911449, 0.9259714761461866, 0.927607533719276, 0.7579311391745474, 0.4155596630877777, 0.8533534577368376, 0.9289929678685975, 0.48228443707897417, 0.9352829048891074, 0.9596628528083495, 0.7856805095233113, 0.9322263155732342, 0.5206413410011861, 0.09240783929382662, 0.9462525423248562, 0.8588461682692663, 0.8554577619034105, 0.6770015308554281, 0.9473491006263874, 0.7587864817594181, 0.9470722443623478, 0.8855808153415308, 0.16294781979179696, 0.9341110513082564, 0.4727472450197888, 0.5651223643167038, 0.7752206558917019, 0.5597295293014373, 0.775767676250582, 0.8391279570353534, 0.8121978155567737, 0.857366514241093, 0.8523565826373221, 0.6673980702602468, 0.8056718171154067, 0.7297653465705738, 0.6984541761230825, 0.5992678325990608, 0.7968008392869605, 0.7698315590133648, 0.9820653195195825, 0.9616102999368312, 0.5012143983741278, 0.051466049375048936, 0.9664437619662871, 0.9147509231273736, 0.7704055135572375, 0.7557101078366268, 0.7510060226700728, 0.39034769598369434, 0.40068476427484834, 0.7671078725965988, 0.5208582218640574, 0.7015252862450747, 0.0, 0.8432011949517255, 0.8923107242330248, 0.9713651945608406, 0.5457430023971002, 0.9305282227891472, 0.7773204582946743, 0.8542311710040063, 0.9562656795376752, 0.19832929234074576, 0.3619347436847535, 0.9887423993227424, 0.6644855259225466, 0.3776268460097124, 0.969421572238713, 0.3273472980821144, 0.4711485012441934, 0.9831861441061518, 0.7772868173384335, 0.9332689017753089, 0.6688983661442429, 0.8039504910727902, 0.6480560353015524, 0.6408477367403859, 0.8164468849403668, 0.45415199547298646, 0.7852033214625844, 0.9634211947250284, 0.9260760377722852, 0.27459370351787593, 0.9701742405810041, 0.704137667465099, 0.6136084613959742, 0.47541291345050607, 0.8617675603039494, 0.4532830736687707, 0.8674701285162645, 0.05652271482930384, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.8013786471371253, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.5795301464266706, 0.6155307344583338, 0.3185268894665423, 0.9486901642449315, 0.933792626439619, 0.4665444472011316, 0.6567079107412255, 0.8936598047708784, 0.3723588344016272, 0.8943187389012723, 0.8250877060561935, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9747348871931607, 0.7844173254460529, 0.3571976195424252, 0.8712420862261012, 0.382894024314283, 0.9323453252450715, 0.6607721969523331, 0.461329065338275, 0.0864718984572822, 0.7451801516964521, 0.7900451252092993, 0.5135793600096521, 0.3901605146623929, 0.9686198599811506, 0.27294561275494955, 0.8814858027886263, 0.6549057600812135, 0.7706648767181559, 0.907558018935852, 0.8642224065245785, 0.9564790243552823, 0.5588208186817714, 0.9157728873149173, 0.4660818033371182, 0.2913462909813452, 0.4319220231775237, 0.8838117005498956, 0.9027309916196755, 0.24631945218824464, 0.9714552656085744, 0.9190922894214792, 0.942709379790015, 0.2579343193335908, 0.9654855521209342, 0.7566083676772686, 0.8533433708218274, 0.3451344686024709, 0.9668495847081082, 0.7703214834126948, 0.7348529689836785, 0.3532301871931892, 0.7153099526991196, 0.5864458370254797, 0.6671272905210213, 0.9469672199403614, 0.5587912679655723, 0.05987019451564787, 0.9630540715975315, 0.8807613352737074, 0.7840820363718872, 0.6912905988814966, 0.775307403968114, 0.6285849086129202, 0.6649668321513595, 0.3487737235833769, 0.5750737908156447, 0.9582017829875438, 0.7141494776867402, 0.37341326717137424, 0.36, 0.7658409863562893, 0.4587960761603226, 0.8299413157852011, 0.37333812510379816, 0.9117475844136459, 0.677291456386624, 0.9352208870016105, 0.9409121125706139, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5740737169642371, 0.9399257675249202, 0.9205724657465093, 0.8122136254293291, 0.9498733743967636, 0.9714834981320903, 0.7488923610303595, 0.8728375908477501, 0.2937766395637181, 0.9526474078906453, 0.7657538252330831, 0.74325514009868, 0.38230642541905163, 0.29070593598613614, 0.6778627945668195, 0.8896531297074333, 0.8436173548118322, 0.6065593560074345, 0.8734548627096907, 0.8829472051724168, 0.8664855589488811, 0.9417139803454657, 0.8756203907451867, 0.7440404960918107, 0.38167413960632135, 0.576816427634421, 0.556642496296224, 0.5763413299251462, 0.8491953677983177, 0.7376268380265145, 0.9860013241305441, 0.9595217323953522, 0.9758294505908078, 0.9166795416411146, 0.6008587815258186, 0.8596555865506181, 0.9788598843598264, 0.8270021204349962, 0.5259210844305199, 0.9060684994481739, 0.6771421775082016, 0.3162465070806947]
Finish training and take 48m
