Namespace(log_name='./RQ5/xcodeeval_1_1/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_1_1/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_1_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00043
  global_step = 1
  train_loss = 0.3936
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00043
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 16.09 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.09
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00043
  global_step = 1
  train_loss = 0.3536
  ********************
Previous best ppl:1.00043
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 16.09 	 Previous best codebleu 16.09
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00043
  global_step = 1
  train_loss = 0.4106
  ********************
Previous best ppl:1.00043
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 16.09 	 Previous best codebleu 16.09
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00043
  global_step = 1
  train_loss = 0.4382
  ********************
Previous best ppl:1.00043
BLEU file: ./data/RQ5/xcodeeval_1_1/validation.jsonl
  codebleu-4 = 16.09 	 Previous best codebleu 16.09
  ********************
reload model from RQ5/xcodeeval_1_1/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_1/test.jsonl
  codebleu = 14.76 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 14.76 
[0.12339561269632277, 0.3006576433301687, 0.19112415780753622, 0.05371885321338549, 0.30034565956691206, 0.09398850952479251, 0.214835447806239, 0.20485445696493682, 0.15179242094453488, 0.0, 0.2302745110140213, 0.13343249320936698, 0.0006658682506698937, 0.09113924050632911, 0.18076923076923077, 0.0005439412840465215, 0.30073641068042306, 0.191223680673204, 0.09291580199305571, 0.05346397108630603, 0.02727272727272727, 0.3005272201146775, 0.24635307203337092, 0.20745894727477693, 0.13695652173913042, 0.04690557407922499, 0.0, 0.1651806333609855, 0.001311121605504365, 0.08423591718986621, 0.12391304347826086, 0.3007302622870799, 0.06951219512195123, 0.2507481575431645, 0.24596753975512148, 0.1310344827586207, 0.3006061379631626, 0.18368915634740643, 0.0006586738428245976, 0.0005989551197653954, 0.11923076923076922, 0.2397571942655842, 0.07912087912087912, 0.12103719968738388, 0.3003598590969609, 0.16212174972425403, 0.13520282662150757, 0.0006577945385547044, 0.2427959116199351, 0.3080164452623599, 0.11538461538461539, 0.18190809749713183, 0.21959367495087262, 0.11396412266515114, 0.001656609846640073, 0.1523910054198696, 0.2309890363365364, 0.1346153846153846, 0.1818025121152746, 0.1940144340780579, 0.0, 0.0004305662616522954, 0.1809974228839919, 0.3081225234624241, 0.15384615384615383, 0.3004848678594026, 0.0007992941677387044, 0.12307692307692307, 0.15161290322580645, 0.23986094134477104, 0.07368881929832032, 0.16249893914813723, 0.24543372597093874, 0.14866551393929375, 0.3008788576874634, 0.11141521021380009, 0.13525058361718992, 0.0004525068221899741, 0.30073937375781595, 0.1837406686344254, 0.18630649686841114, 0.0010400630399593825, 0.0005834649686159152, 0.0, 0.14116735690765, 0.000689459695440633, 0.23140806339252754, 0.2003926115190677, 0.2075, 0.13105339267882588, 0.016822135766178595, 0.001194421312815831, 0.12295151005782066, 0.0005617993974912951, 0.3011581872762641, 0.11832533980406779, 0.19680092066229654, 0.0857142857142857, 0.0, 0.30084372257537917, 0.08414634146341464, 0.300634424928052, 0.16170373193508628, 0.30076474852379204, 0.1107089789962786, 0.3081291229960921, 0.15578083511118881, 0.0, 0.1581789549461922, 0.16708860759493668, 0.057692307692307696, 0.14177370652795301, 0.10384615384615384, 0.1692307692307692, 0.10384615384615384, 0.09999999999999999, 0.11894751107077489, 0.0006818888646945834, 0.14254975304524525, 0.20099266270119504, 0.0019642725569085054, 0.21400555920401312, 0.11538461538461539, 0.09615384615384616, 0.000559113826014396, 0.3007110600852846, 0.21046599326673887, 0.28554281595797776, 0.20960763923131365, 0.120838551261182, 0.03343098021028169, 0.0005740089089290477, 0.12151898734177215, 0.19116655501474575, 0.3006576433301687, 0.18965694608350717, 0.22001839197315554, 0.0975, 0.21094805311979462, 0.2313508879897393, 0.0, 0.24640722211305027, 0.301193083193514, 0.19009796946461743, 0.16018521138485214, 0.235247300086541, 0.09615384615384616, 0.21916282619499863, 0.19073065588332785, 0.21487817507122675, 0.18262152355892683, 0.15446605847113154, 0.3005272201146775, 0.19784686189792433, 0.18118591096538236, 0.17692307692307693, 0.017241379310344827, 0.16164817866019104, 0.10315243681545519, 0.001160456881867111, 0.1423020473227284, 0.23597616604663757, 0.18076923076923077, 0.21063033450014215, 0.18693335808812928, 0.2428698873534896, 0.11080185446087071, 0.25407425991804733, 0.13104789798734082, 0.1713310831811148, 0.26441714238206615, 0.20070097660577532, 0.14294243043726704, 0.20921778101280852, 0.14273573230023168, 0.09999999999999999, 0.15184710305141716, 0.0, 0.12287217182770968, 0.19127549579046474, 0.25053846462504903, 0.21163218043300264, 0.13267396850079843, 0.2585263007604544, 0.06521725092468364, 0.2760392079202244, 0.15212822341547982, 0.041176470588235294, 0.0006596065384037266, 0.2007671771995598, 0.3010007478175342, 0.11923076923076922, 0.13265821773604589, 0.14868904947475192, 0.1193548387096774, 0.08791919029957393, 0.12335274760846109, 0.2193893537005103, 0.12207838278516699, 0.11688311688311688, 0.09399479510355797, 0.09113924050632911, 0.0, 0.00092633605956765, 0.18716258846318012, 0.19094147122780364, 0.20157332036150227, 0.09554615112647694, 0.06455696202531645, 0.17759656657088083, 0.15238751579756024, 0.12349775157300524, 0.24260199364473561, 0.3005979278767458, 0.300586162306168, 0.09411187618219516, 0.23922489774297273, 0.3011357884957696, 0.1348314606741573, 0.02278481012658228, 0.22028375449772894, 0.09999999999999999, 0.12610460519430314, 0.11326656705208259, 0.30075661490012534, 0.1626168053043628, 0.23538058292547712, 0.0379746835443038, 0.18350491660753132, 0.0013666136723514645, 0.20097981363568945, 0.0007724108144499865, 0.06923076923076923, 0.08052592624991987, 0.12149684079093802, 0.264571980844976, 0.30033208443546644, 0.07307692307692307, 0.19397081961788304, 0.00091873659911734, 0.12369253135072089, 0.196729246599894, 0.0, 0.12350720526851915, 0.07036615378214493, 0.1268346626454565, 0.04294797311704262, 0.21081567331724488, 0.23547935961257455, 0.09857686483892272, 0.16708860759493668, 0.1292243840510244, 0.17441551412309306, 0.04969842245843129, 0.14842484044143042, 0.3007481575431645, 0.21496248745144672, 0.19717345897199418, 0.09993172593261178, 0.0005095188994506759, 0.14235820593161339, 0.3007280136077542, 0.3011129654446983, 0.0008070305853795555, 0.30055300991885436, 0.0004724150994421264, 0.19801602422705294, 0.09264260847219341, 0.07692307692307691, 0.3080983403930235, 0.16046795248930143, 0.12374999999999999, 0.11012658227848102, 0.11841900258050926, 0.2512758541657686, 0.3006104793190763, 0.12151898734177215, 0.17692307692307693, 0.14197669384559553, 0.08354430379746834, 0.11225777384038324, 0.06451612903225806, 0.3006905806627892, 0.0005897004274545714, 0.21984271708785033, 0.16204410450684914, 0.3008070327738027, 0.0005474227091244403, 0.14470588235294118, 0.22969538148286073, 0.060759493670886074, 0.1658639937774244, 0.10632911392405063, 0.0013387738870382488, 0.0007975676674501981, 0.13504017006174576, 0.12025486073783706, 0.13354261570914147, 0.06923076923076923, 0.12307692307692307, 0.0007566149001253621, 0.16393375354930093, 0.0, 0.0015207194972400345, 0.1311735786493179, 0.15251829087501434, 0.20092765428559092, 0.18597344009278316, 0.2769497639431232, 0.0006127794995965823, 0.22499999999999998, 0.26533823460374795, 0.0, 0.18784032838804904, 0.010660129336520425, 0.09999999999999999, 0.14222020092544801, 0.0011936590464110214, 0.00148119965398436, 0.1422988251165987, 0.20834070217571043, 0.1291139240506329, 0.0005848547258742403, 0.1941809053790831, 0.2691026388743249, 0.12366719121257429, 0.13232099667115854, 0.21986298858600506, 0.5835616438356164, 0.13148121099470042, 0.14304861604041608, 0.3012339655898982, 0.17134826773235715, 0.18720447948620744, 0.3010609249716599, 0.21427220695813776, 0.09615384615384616, 0.09873417721518986, 0.3007642531336035, 0.007692307692307692, 0.1425, 0.30030260422463084, 0.11923076923076922, 0.24606575248626258, 0.3008130517478245, 0.0008981144570472241, 0.21064373615352297, 0.0004102750044801223, 0.11321985993119688, 0.3008479999853572, 0.14810126582278482, 0.3012309762179009, 0.20051644526235987, 0.18267293955053984, 0.22478422919376895, 0.20896842969793786, 0.09529411764705882, 0.24894612044147402, 0.24266289854506534, 0.10384615384615384, 0.3008364496769054, 0.17180381915610518, 0.001431566622830476, 0.3004625558010146, 0.06878446145070227, 0.0006370945721708669, 0.09146341463414634, 0.08505477217823745, 0.05737124524774023, 0.3010707379690917, 0.1918340426254004, 0.060759493670886074, 0.15233603088193445, 0.08076923076923076, 0.30082124343767225, 0.04893617021276596, 0.0379746835443038, 0.1291139240506329, 0.05384615384615384, 0.20984413531548987, 0.13267725234590758, 0.17249177472448673, 0.02967032967032967, 0.2010069465662993, 0.30057400890892905, 0.07782320250209188, 0.18184937694035136, 0.1005686315124855, 0.08505833607304432, 0.1119405720007521, 0.15184710480262767, 0.051801133222182126, 0.10438838948276022, 0.14615384615384613, 0.13902439024390245, 0.0, 0.2541891101445064, 0.09422357603916347, 0.12296951303203428, 0.3083437225753792, 0.17307692307692304, 0.10690264547885393, 0.0006156082737978275, 0.10337042570276533, 0.15193844529217232, 0.17163225031937437, 0.10685654459201528, 0.08354430379746834, 0.17158114006997222, 0.3006104793190763, 0.12307692307692307, 0.1088764705711198, 0.23154460989143663, 0.06283070337713591, 0.08354430379746834, 0.09400401475646503, 0.18177094842331687, 0.09999999999999999, 0.17244160398404051, 0.0015323178234320175, 0.08354430379746834, 0.2287662994468863, 0.25127820752600083, 0.6479458575506856, 0.11923076923076922, 0.22002249763755144, 0.21981820035710836, 0.1969756905980063, 0.30062912299609207, 0.2687759841280663, 0.1521137098866406, 0.15330576800679097, 0.14160777843193126, 0.0007533206161533025, 0.09999999999999999, 0.1519511997587716, 0.300872275585378, 0.11324039153719859, 0.09230769230769231, 0.11392405063291139, 0.3007110600852846, 0.1622432377259486, 0.18740759325138065, 0.0006829132583344347, 0.2245390954334438, 0.09282700383283111, 0.0, 0.032926829268292684, 0.13738549673584372, 0.0, 0.3006578345546219, 0.09615384615384616, 0.0005890778161935928, 0.0, 0.12564981342851803, 0.30048802790116746, 0.30056195263796476, 0.11094508439558312, 0.2003090628344566, 0.0574468085106383, 0.1646129436442539, 0.3011804134867767, 0.03846153846153846, 0.01276595744680851, 0.10417951157846932, 0.26416053128371586, 0.20038394764485126, 0.08776152894254265, 0.19829061806808868, 0.14615384615384613, 0.12307692307692307, 0.13085106382978723, 0.0, 0.30807400890892905, 0.1423681401865356, 0.0846153846153846, 0.07419354838709677, 0.30792570273406544, 0.14810126582278482, 0.1211802725396186, 0.15673971396260308, 0.08076923076923076, 0.23046611395681443, 0.0064516129032258064, 0.17307692307692304, 0.10898679284579912, 0.2287496484254587, 0.2566036688055148, 0.0, 0.04042553191489361, 0.00048351803342822997, 0.09615384615384616, 0.13119897797069324, 0.20448910718080224, 0.001169322061265125, 0.0423076923076923, 0.17134191303879417, 0.16708860759493668, 0.016040888254270254, 0.1727838218175686]
Finish training and take 46m
