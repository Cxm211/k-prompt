Namespace(log_name='./RQ5/xcodeeval_16_1/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_16_1/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_16_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00039
  global_step = 3
  train_loss = 0.5747
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00039
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 43.41 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:43.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00037
  global_step = 5
  train_loss = 0.4021
  ********************
Previous best ppl:1.00039
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 62.74 	 Previous best codebleu 43.41
  ********************
 Achieve Best bleu:62.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00034
  global_step = 7
  train_loss = 0.3372
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00034
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 71.68 	 Previous best codebleu 62.74
  ********************
 Achieve Best bleu:71.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00034
  global_step = 9
  train_loss = 0.2614
  ********************
Previous best ppl:1.00034
Achieve Best ppl:1.00034
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 73.33 	 Previous best codebleu 71.68
  ********************
 Achieve Best bleu:73.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00034
  global_step = 11
  train_loss = 0.2492
  ********************
Previous best ppl:1.00034
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 73.54 	 Previous best codebleu 73.33
  ********************
 Achieve Best bleu:73.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00035
  global_step = 13
  train_loss = 0.1886
  ********************
Previous best ppl:1.00034
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 73.8 	 Previous best codebleu 73.54
  ********************
 Achieve Best bleu:73.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00036
  global_step = 15
  train_loss = 0.1698
  ********************
Previous best ppl:1.00034
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 74.05 	 Previous best codebleu 73.8
  ********************
 Achieve Best bleu:74.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00036
  global_step = 17
  train_loss = 0.1726
  ********************
Previous best ppl:1.00034
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 74.06 	 Previous best codebleu 74.05
  ********************
 Achieve Best bleu:74.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00037
  global_step = 19
  train_loss = 0.1444
  ********************
Previous best ppl:1.00034
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 74.07 	 Previous best codebleu 74.06
  ********************
 Achieve Best bleu:74.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00037
  global_step = 21
  train_loss = 0.1391
  ********************
Previous best ppl:1.00034
BLEU file: ./data/RQ5/xcodeeval_16_1/validation.jsonl
  codebleu-4 = 74.08 	 Previous best codebleu 74.07
  ********************
 Achieve Best bleu:74.08
  ********************
reload model from RQ5/xcodeeval_16_1/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_16_1/test.jsonl
  codebleu = 74.09 
  Total = 500 
  Exact Fixed = 3 
[54, 59, 359]
  Syntax Fixed = 2 
[228, 440]
  Cleaned Fixed = 5 
[188, 268, 276, 424, 455]
  ********************
  Total = 500 
  Exact Fixed = 3 
[54, 59, 359]
  Syntax Fixed = 2 
[228, 440]
  Cleaned Fixed = 5 
[188, 268, 276, 424, 455]
  codebleu = 74.09 
[0.6906751815123662, 0.4658094583130275, 0.8702165828687908, 0.9223640947520029, 0.94502845638256, 0.9544741722900356, 0.9334580759528754, 0.2930837534053943, 0.28590084848815933, 0.5884615384615384, 0.33757627523685907, 0.7781235710525856, 0.8840280108223306, 0.9126153116284059, 0.9320028922042589, 0.9904619066549505, 0.8694464017012884, 0.904425341889916, 0.8944958165491306, 0.8685842432291047, 0.8193236358223526, 0.896734987325442, 0.9888412502412365, 0.9714741186824503, 0.35117993983532114, 0.9693017305513745, 0.9900076373246287, 0.6895886374885628, 0.9864610512549261, 0.5800323580497375, 0.9690270665107408, 0.9540138870746515, 0.4669941875462229, 0.7138182723577209, 0.9735441531161189, 0.7480983802318328, 0.7810019348324313, 0.8209475165854486, 0.9678131775953225, 0.03914807619997354, 0.7806363178563275, 0.6797481699178505, 0.7646504374656518, 0.4262386448606621, 0.9226227924173891, 0.9673001684656128, 0.9103068259298337, 0.8770292945010878, 0.7693582389764291, 0.8458233745010357, 0.7860054491592668, 0.6751922228302627, 0.4096453519493349, 1.0, 0.035231219957085363, 0.2652100539123435, 0.8267820318104704, 0.8817623929694763, 0.9753396087264441, 0.8827108485449091, 0.9837171835242919, 0.9447089104540864, 0.8113688567236648, 0.34259249782254486, 0.9821809817650384, 0.4982318233525185, 0.9829433066762787, 0.934387097307501, 0.8944539975641081, 0.4936529526137271, 0.8797233953429866, 0.3294036734712466, 0.6463782423660047, 0.7678455615777474, 0.4268318935823558, 0.4072595760539546, 0.6888365462671723, 0.9421953806476688, 0.3885946899772229, 0.8691934016309899, 0.9564084233825603, 0.9738014890069033, 0.7365455054053898, 0.9809329721714535, 0.9424663933583042, 0.9295279223019721, 0.8859407157558806, 0.557519215318141, 0.4443730302862996, 0.7075727841380755, 0.9168021557818813, 0.03638680962604285, 0.9106033686427741, 0.33751388171710217, 0.8023915790313436, 0.49519022709932214, 0.4538081682483768, 0.5007722042578182, 0.9901350861027634, 0.48795243922324283, 0.8668858102395753, 0.8884437011415969, 0.8203495978509039, 0.9193864599685562, 0.8947330957271415, 0.5961510984329409, 0.5003909451993059, 0.7833636144245735, 0.8941059510400393, 0.5556037693500977, 0.8324183912964096, 0.14992380558663324, 0.7914278759686615, 0.9617458190493637, 0.9092903522106406, 0.9114957939152456, 0.28288736800459546, 0.9773958820584223, 0.41093966321126485, 0.7480407185735214, 0.9374523007440652, 0.9018695019229972, 0.4542071293267559, 0.9717916167891165, 0.7684232762284017, 0.7614331493899362, 0.8877804056779384, 0.9673349576458696, 0.9656770287836146, 0.33150754715452724, 0.9416744734511042, 0.8165996742814894, 0.9739901642593853, 0.9631318837364407, 0.9678539444631555, 0.9798800402828953, 0.9798219468937557, 0.2984600211190037, 0.9560322423864296, 0.5739579759923696, 0.9732129467715989, 0.9236644367804976, 0.7366033348508686, 0.47428344360309693, 0.3121392853809564, 0.7846138466894739, 0.27086338909565627, 0.4501887204811095, 0.6877917301640358, 0.9299552251878043, 0.5010923309399078, 0.7970615800807633, 0.4997698113105318, 0.872087455158634, 0.9435407234863082, 0.4775369858583672, 0.7223951844558967, 0.927982876787582, 0.9602403186628621, 0.9707263241417066, 0.7110307145936695, 0.6567874018519466, 0.7787797669196548, 0.3931548138311015, 0.37547178079713645, 0.408600022348932, 0.9654461577493623, 0.9402347939477611, 0.7942417816647632, 0.7631321567029603, 0.8815079864399178, 0.8649251515455869, 0.6052970083217083, 0.9384697992732594, 0.9842551012740206, 0.3706598417143405, 0.8792228737352459, 0.9822804615297587, 0.3723443145512656, 0.7065722131859199, 0.8042710150175236, 0.7852571791995187, 0.48082565379666875, 0.9835764714570019, 0.9931785684115373, 0.9709756774276397, 0.8579005033763996, 0.9090374997030346, 0.956301548538977, 0.9324043404373232, 0.8564128174402423, 0.8856566772820293, 0.6163950321676452, 0.6912144204261992, 0.9744119964056819, 0.8908110243776604, 0.8351548935552557, 0.41395796030346144, 0.3475222634058708, 0.9725315176192211, 0.9231207548844165, 0.4841195614277229, 0.8396431642664623, 0.9221265623517527, 0.9757745973711676, 0.825040224285887, 0.9166512013909699, 0.7218124370745092, 0.4104804294310931, 0.8421624962234442, 0.43419770508491723, 0.940598945729278, 0.5343454846801664, 0.6895051363145968, 0.6571504344495466, 0.2642512874892324, 0.9589077111122837, 0.7507483205316109, 0.8628813404256144, 0.6386890640788627, 0.9699392701042773, 0.8099419487239318, 0.8198387442649027, 0.41212045710578005, 0.9425018741615083, 0.5219616682903572, 0.5786626159445815, 0.9120837307732141, 0.5784302566819072, 0.20906455911542446, 0.7836353492903962, 0.543348733581992, 0.42497916943931774, 0.9345121855559886, 0.6121051294344302, 0.5859400643182706, 0.8207174296242723, 0.5831163274893049, 0.6916629272878606, 0.9293300527612651, 0.774674995957515, 0.6871897384788783, 0.8980438351581705, 0.28759777369338146, 0.5108393281757484, 0.9059801416992133, 0.508305988851434, 0.8909434578028737, 0.938899487965138, 0.915465568076623, 0.9772597588062424, 0.9789628153220145, 0.4234097157305317, 0.9163819539750226, 0.6766685735617091, 0.8638825156565595, 0.9089815080638761, 0.737357448673426, 0.5177405757219062, 0.958489957733323, 0.3755076328133085, 0.41574277333701404, 0.6577727687203423, 0.8703232021938421, 0.6928192511783815, 0.9798514722492089, 0.38035740917095934, 0.9428491633077438, 0.8768757863294787, 0.8832119168550208, 0.8395423873461488, 0.9239858991360816, 0.9522217025290924, 0.23899729367746342, 0.27949142984930087, 0.9259714761461866, 0.914831901042849, 0.9286279274366336, 0.4155596630877777, 0.8533534577368376, 0.9462502736355272, 0.48228443707897417, 0.9352829048891074, 0.9749407822636968, 0.7631695126965249, 0.9322263155732342, 0.7371919209173916, 0.089996975657093, 0.9462525423248562, 0.8588461682692663, 0.8984188648676468, 0.9238932796791619, 0.9251019161564213, 0.9747232059340507, 0.9162855444275764, 0.8855808153415308, 0.12003270903996337, 0.9341110513082564, 0.5157443236412498, 0.5750582617526012, 0.7738062382871427, 0.5790261988125887, 0.8652351213546239, 0.849729607166194, 0.8121978155567737, 0.857366514241093, 0.9121176482601447, 0.6214669377460703, 0.8302530908052741, 0.7557968074254318, 0.7624146378614128, 0.8086811749495997, 0.9881263545755623, 0.7698315590133648, 0.9820653195195825, 0.9616102999368312, 0.5012143983741278, 0.050838348640303524, 0.9747269061338388, 0.9147509231273736, 0.8196763268259384, 0.923256750082746, 0.7510060226700728, 0.4140319065100101, 0.41243081325512787, 0.8861971446522757, 0.8154187738366516, 0.7015252862450747, 0.5884615384615384, 0.9611973390985742, 0.8767145827907143, 0.9447805402354847, 0.939821716744068, 0.9607099804953305, 0.8171220069513339, 0.8542311710040063, 0.97633392937918, 0.19832929234074576, 0.9706304665534506, 0.9887423993227424, 0.7711795825442882, 0.3776268460097124, 0.9432193576582235, 0.3273472980821144, 0.5074834355521683, 0.9743114564383237, 0.7772868173384335, 0.9517626327839217, 0.6688983661442429, 0.9643581037039888, 0.6598038918531729, 0.8889705535239042, 0.7876233555286021, 0.44241812450524454, 0.7852033214625844, 0.20467571141443458, 0.9454632184934535, 0.27459370351787593, 0.9701742405810041, 0.704137667465099, 0.6136084613959742, 0.47541291345050607, 0.8521117818071369, 0.4532830736687707, 0.8674701285162645, 0.05652271482930384, 0.56926112019934, 0.9239412941551246, 0.7852960010452446, 0.9443378875292432, 0.8242816812765809, 0.4966154218075377, 0.8437436803649183, 0.6299262763303686, 0.7146371215272531, 0.3017311101814578, 0.9174017247496151, 0.933792626439619, 0.5507463552897676, 0.6567079107412255, 0.8769931381042118, 0.38361540357357476, 0.9731496374915098, 0.8250877060561935, 0.33295797590328613, 0.7259849312989834, 0.8467013377193615, 0.9908744134634584, 0.8858102874808143, 0.35423431936551, 0.8712420862261012, 0.4198136333318135, 0.9323453252450715, 0.9006848455688443, 0.461329065338275, 0.0864718984572822, 0.884708305956111, 0.7900451252092993, 0.5272157236460158, 0.3901605146623929, 0.9443169641246619, 0.267125262228119, 0.7945180536839083, 0.6619659569130805, 0.7599505910038701, 0.907558018935852, 0.8642224065245785, 0.9445023909811394, 0.5588208186817714, 0.9157728873149173, 0.5452287811589471, 0.29739831379152576, 0.46486558977596226, 0.9108900697675792, 0.8497251305586286, 0.25160207241540583, 0.9714552656085744, 0.9190922894214792, 0.9732456940628658, 0.2579343193335908, 0.9654855521209342, 0.9086540498917997, 0.9397626412560047, 0.9541895950679766, 0.9668495847081082, 0.8019711204854889, 0.7348529689836785, 0.3532301871931892, 0.6933604779837957, 0.5864458370254797, 0.687542465552887, 0.9145624244186732, 0.5587912679655723, 0.059192199400695825, 0.9630540715975315, 0.8807613352737074, 0.978350857747293, 0.6912905988814966, 0.8144323897871283, 0.9100428683627355, 0.9685698290675941, 0.379441623929603, 0.9427008679312978, 0.9833885445588753, 0.7141494776867402, 0.3737662111795117, 0.3, 0.8055864673667326, 0.554920595677622, 0.8905074545882428, 0.37333812510379816, 0.9117475844136459, 0.9635847916815603, 0.96804234407929, 0.9306848398433412, 0.36168412495357327, 0.7270998991360063, 0.9296835011666059, 0.5649828078733281, 0.9632825394277797, 0.9151731735749239, 0.8122136254293291, 0.9498733743967636, 0.9676811135173389, 0.773611277512299, 0.8728375908477501, 0.2973434798199742, 0.9526474078906453, 0.7657538252330831, 0.8025217812965468, 0.4628959302474374, 0.6061429236354531, 0.8049944433164105, 0.8896531297074333, 0.8920643294874622, 0.5993827904741307, 0.9086210618726622, 0.9078336364448021, 0.8664855589488811, 0.9417139803454657, 0.8687207744628378, 0.7752855690103572, 0.38167413960632135, 0.576816427634421, 0.5480001979839206, 0.5565506578618167, 0.8698366814058456, 0.9508336769881993, 0.9588892317481068, 0.9819992034986089, 0.9758294505908078, 0.9166795416411146, 0.5083541179840113, 0.9064913890104815, 0.9788598843598264, 0.8739286595309739, 0.5259210844305199, 0.9403437700960667, 0.8190595233656908, 0.3568783357390725]
Finish training and take 55m
