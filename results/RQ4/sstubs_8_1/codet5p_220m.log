Namespace(log_name='./RQ5/sstubs_8_1/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_8_1/codet5p_220m', data_dir='./data/RQ5/sstubs_8_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'final PrintWriter output = mock(PrintWriter.class);          when(request.getMethod()).thenReturn("POST");         when(request.getRequestURI()).thenReturn("/tasks/gc");          when(request.getParameterNames()).thenReturn(Collections.enumeration(ImmutableList.of("runs")));          when(request.getParameterValues("runs")).thenReturn(new String[]{"1"});          when(response.getWriter()).thenReturn(output);', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'final PrintWriter output = mock(PrintWriter.class);          when(request.getMethod()).thenReturn("POST");         when(request.getServletPath()).thenReturn("/tasks/gc");          when(request.getParameterNames()).thenReturn(Collections.enumeration(ImmutableList.of("runs")));          when(request.getParameterValues("runs")).thenReturn(new String[]{"1"});          when(response.getWriter()).thenReturn(output);'}]
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 7.622326975141684e+299
  global_step = 2
  train_loss = 46.5811
  ********************
Previous best ppl:inf
Achieve Best ppl:7.622326975141684e+299
  ********************
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 18.74 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:18.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.500174926912195e+291
  global_step = 3
  train_loss = 47.4716
  ********************
Previous best ppl:7.622326975141684e+299
Achieve Best ppl:1.500174926912195e+291
  ********************
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 24.53 	 Previous best codebleu 18.74
  ********************
 Achieve Best bleu:24.53
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.0137927604420238e+285
  global_step = 4
  train_loss = 22.6895
  ********************
Previous best ppl:1.500174926912195e+291
Achieve Best ppl:2.0137927604420238e+285
  ********************
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 28.97 	 Previous best codebleu 24.53
  ********************
 Achieve Best bleu:28.97
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 7.08033316840065e+277
  global_step = 5
  train_loss = 16.3694
  ********************
Previous best ppl:2.0137927604420238e+285
Achieve Best ppl:7.08033316840065e+277
  ********************
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 30.83 	 Previous best codebleu 28.97
  ********************
 Achieve Best bleu:30.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 9.194909157438823e+272
  global_step = 6
  train_loss = 10.92
  ********************
Previous best ppl:7.08033316840065e+277
Achieve Best ppl:9.194909157438823e+272
  ********************
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 34.69 	 Previous best codebleu 30.83
  ********************
 Achieve Best bleu:34.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.429145304706034e+271
  global_step = 7
  train_loss = 7.2014
  ********************
Previous best ppl:9.194909157438823e+272
Achieve Best ppl:2.429145304706034e+271
  ********************
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 46.38 	 Previous best codebleu 34.69
  ********************
 Achieve Best bleu:46.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 4.389579107463088e+271
  global_step = 8
  train_loss = 4.8646
  ********************
Previous best ppl:2.429145304706034e+271
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 62.76 	 Previous best codebleu 46.38
  ********************
 Achieve Best bleu:62.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.3819387382704834e+272
  global_step = 9
  train_loss = 4.3937
  ********************
Previous best ppl:2.429145304706034e+271
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 72.0 	 Previous best codebleu 62.76
  ********************
 Achieve Best bleu:72.0
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.6663132573934646e+272
  global_step = 10
  train_loss = 3.1196
  ********************
Previous best ppl:2.429145304706034e+271
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 80.16 	 Previous best codebleu 72.0
  ********************
 Achieve Best bleu:80.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 2.0148129287075962e+272
  global_step = 11
  train_loss = 2.5571
  ********************
Previous best ppl:2.429145304706034e+271
BLEU file: ./data/RQ5/sstubs_8_1/validation.jsonl
  codebleu-4 = 81.2 	 Previous best codebleu 80.16
  ********************
 Achieve Best bleu:81.2
  ********************
reload model from RQ5/sstubs_8_1/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_8_1/test.jsonl
  codebleu = 82.24 
  Total = 500 
  Exact Fixed = 89 
[3, 14, 18, 20, 32, 34, 36, 37, 47, 48, 52, 56, 58, 65, 69, 75, 82, 87, 88, 96, 101, 102, 109, 110, 112, 114, 120, 125, 127, 128, 138, 148, 160, 176, 183, 186, 196, 203, 204, 209, 211, 214, 215, 228, 232, 233, 238, 240, 241, 248, 249, 252, 268, 283, 287, 291, 298, 308, 311, 312, 321, 322, 323, 333, 335, 342, 345, 350, 360, 370, 383, 389, 394, 400, 401, 422, 423, 427, 448, 454, 464, 465, 467, 469, 475, 477, 488, 494, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[61, 70, 85, 246, 341, 425]
  ********************
  Total = 500 
  Exact Fixed = 89 
[3, 14, 18, 20, 32, 34, 36, 37, 47, 48, 52, 56, 58, 65, 69, 75, 82, 87, 88, 96, 101, 102, 109, 110, 112, 114, 120, 125, 127, 128, 138, 148, 160, 176, 183, 186, 196, 203, 204, 209, 211, 214, 215, 228, 232, 233, 238, 240, 241, 248, 249, 252, 268, 283, 287, 291, 298, 308, 311, 312, 321, 322, 323, 333, 335, 342, 345, 350, 360, 370, 383, 389, 394, 400, 401, 422, 423, 427, 448, 454, 464, 465, 467, 469, 475, 477, 488, 494, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[61, 70, 85, 246, 341, 425]
  codebleu = 82.24 
[0.9747753852831109, 0.9471695160234401, 1.0, 0.8484837259585243, 0.8176625406504057, 0.8412123173599384, 0.783801630906283, 0.792956903878446, 0.783801630906283, 0.792956903878446, 0.792956903878446, 0.8597135835101596, 0.878794610762374, 1.0, 0.058558007592186156, 0.9257686479202227, 0.9024486727710714, 1.0, 0.9176675865658077, 1.0, 0.8301963970996288, 0.8336339962320415, 0.2676492757734398, 0.853964085660508, 0.9558104142545454, 0.6647490526414709, 0.343235363097799, 0.2591440297925359, 0.5984221724028904, 0.8042353937453603, 0.8240939437638961, 1.0, 0.6589498857394984, 1.0, 0.792956903878446, 1.0, 1.0, 0.907745404768207, 0.9024486727710714, 0.7675287557224962, 0.7718478546332077, 0.9736596070124768, 0.23664213077914492, 0.7806930145174624, 0.8509958739837391, 0.9471695160234401, 1.0, 1.0, 0.792956903878446, 0.8472999580048346, 0.8971127902247926, 1.0, 0.8362511196423215, 0.9435321610524647, 0.2873758936130652, 1.0, 0.8565117855372517, 1.0, 0.2591440297925359, 0.7049533902559844, 0.987026696724848, 0.2620704705727732, 0.783801630906283, 0.792956903878446, 1.0, 0.8596302028883616, 0.9600948805844232, 0.8961079319106557, 1.0, 0.987026696724848, 0.9259066255937691, 0.3503293733703647, 0.0961359817982625, 0.7499929854363869, 1.0, 0.9307233460793374, 0.8348805422215138, 0.9451794431973461, 0.8597135835101596, 0.8482227062370575, 0.8892385063159653, 1.0, 0.0961359817982625, 0.32155072082891467, 0.987026696724848, 0.9557161384210198, 1.0, 1.0, 0.7863014393153781, 0.9275923820999126, 0.48720590733172464, 0.792956903878446, 0.8509958739837391, 0.7932555322546102, 0.7806930145174624, 1.0, 0.9024486727710714, 0.9028856926673474, 0.9339261075829457, 0.9471695160234401, 1.0, 1.0, 0.792956903878446, 0.5975606673191927, 0.2548273831290111, 0.9024486727710714, 0.8142735303489506, 0.8542545001206854, 1.0, 1.0, 0.8933342731653321, 1.0, 0.792956903878446, 1.0, 0.8870743699434802, 0.8240939437638961, 0.9027245358790694, 0.9024486727710714, 0.8984627547856614, 1.0, 0.9088645430449838, 0.897525227825418, 0.926961550993225, 0.8387796716807858, 1.0, 0.474151604647724, 1.0, 1.0, 0.9537546369958738, 0.792956903878446, 0.783801630906283, 0.9427895563342799, 0.864213127281625, 0.7969728550743762, 0.9540286538025624, 0.9514967635709539, 0.9160192719604185, 1.0, 0.25848807629549897, 0.9083279484782856, 0.792956903878446, 0.8809131878905576, 0.8376886366846652, 0.9024486727710714, 0.9493239030379825, 0.9019927469706532, 0.9148954992547789, 1.0, 0.792956903878446, 0.8691302219929937, 0.8740066381203766, 0.778217378209213, 0.9610055919835154, 0.792956903878446, 0.792956903878446, 0.7475650368522859, 0.7691044962015358, 0.8185803147258994, 0.6789257970590783, 1.0, 0.9544591590118743, 0.826991033561655, 0.7790851968105689, 0.8224094477109077, 0.9024486727710714, 0.8221857173709346, 0.05837121430469294, 0.9116366604332367, 0.9275923820999126, 0.792956903878446, 0.9587753838819759, 0.9083279484782856, 0.474151604647724, 0.16408360228673477, 0.7695271789607849, 1.0, 0.9024486727710714, 0.9239302319973048, 0.9466595714446484, 0.9143879239397519, 0.3579182195302808, 0.8686665661064985, 1.0, 0.6307175118470745, 0.9441518237028776, 1.0, 0.9106048798235331, 0.47412578491242163, 0.781652830747834, 0.8792664763784894, 0.7671242177118058, 0.4698001585145247, 0.9253401143941313, 0.8274959094636727, 0.8568598136202887, 1.0, 0.9024486727710714, 0.4985178383291766, 0.7396196795552973, 0.9250223010992678, 0.29591594330204485, 0.942891989734737, 1.0, 1.0, 0.792956903878446, 0.86944691495958, 0.8617487370385535, 0.9600948805844232, 1.0, 0.26040138630043863, 1.0, 0.3109819890479551, 0.8876525998084088, 1.0, 1.0, 0.8925890411755255, 0.8799038013550624, 0.792956903878446, 0.9221025201204232, 0.83613017589157, 0.2873758936130652, 0.9222862123570568, 0.9407613352737074, 0.8108004498085515, 0.7590096946794502, 0.9188380191394792, 0.8765182158436144, 1.0, 0.783801630906283, 0.8936804719246167, 0.783801630906283, 1.0, 1.0, 0.884777214048037, 0.31843309606089665, 0.9116366604332367, 0.9083279484782856, 1.0, 0.9024486727710714, 1.0, 1.0, 0.792956903878446, 0.8547792927646232, 0.8222725521291268, 0.8272363907340718, 0.987026696724848, 0.792956903878446, 1.0, 1.0, 0.783801630906283, 0.3103165022518894, 1.0, 0.9314952029144408, 0.8176625406504057, 0.9024486727710714, 0.8482227062370575, 0.9341247388270204, 0.8803070181138672, 0.9493239030379825, 0.8531462888634065, 0.8404979526305214, 0.9083279484782856, 0.778217378209213, 0.8755319098520675, 0.7475650368522859, 0.9558104142545454, 0.9558104142545454, 1.0, 0.8212636043254029, 0.2548273831290111, 0.949068802064335, 0.9259881201117277, 0.9088645430449838, 0.907357971193316, 0.7932555322546102, 0.8180333928054078, 0.3018208994219872, 0.9070475294213802, 0.8571486858327837, 0.0961359817982625, 0.8202798291220228, 0.8772054210786566, 1.0, 0.8648103351245982, 0.9466595714446484, 0.9399989903319628, 1.0, 0.32941434082172544, 0.796139897410997, 0.6721258702031845, 1.0, 0.6688542022748454, 0.857146189011335, 0.8919048935552556, 0.9610055919835154, 0.9439975336408695, 0.8860545954971506, 1.0, 0.8698869964564975, 0.7767937457754306, 0.9274571491777241, 0.7984150789562205, 0.3087408193601098, 0.9155105345510666, 0.8721507316445842, 0.9083279484782856, 0.5975606673191927, 1.0, 0.324509555869859, 0.6939809337479977, 1.0, 1.0, 0.7497502066671656, 0.9565292378112795, 0.9647268293826727, 0.2676492757734398, 0.8673504699934959, 0.9493239030379825, 0.778217378209213, 0.8895342368991435, 1.0, 1.0, 1.0, 0.5704770660663978, 0.7932555322546102, 0.8517982611930965, 0.9610055919835154, 0.330645113392379, 0.8240413374811482, 0.8469363699187857, 0.9360388950736327, 0.9056583090096291, 1.0, 0.8215803485831588, 1.0, 0.8164806644750696, 0.9024486727710714, 0.8568598136202887, 0.7638153524994504, 0.783801630906283, 0.98612097182042, 1.0, 0.9704328316376207, 0.8571486858327837, 1.0, 0.778217378209213, 0.8176625406504057, 0.859328962153648, 0.966664755877954, 1.0, 0.9214967236107354, 0.8597135835101596, 0.8227408112273605, 0.9596338254485273, 0.7475650368522859, 0.9493239030379825, 0.8803070181138672, 0.33413278216116743, 0.962474977395186, 1.0, 0.9370576952964131, 0.6997765110261632, 0.9443755095409017, 0.9170965437917298, 0.9136222331075061, 0.911599432360769, 0.8573095522130979, 0.8576767641936891, 0.792956903878446, 1.0, 0.9070475294213802, 0.9030744125576402, 0.9466595714446484, 0.8854205726440119, 0.9024486727710714, 0.6110678960423811, 0.17697147539746677, 0.796139897410997, 0.9704328316376207, 0.6868474083243508, 0.8865082025945992, 0.8634186520798544, 1.0, 0.8422237749567263, 0.9736596070124768, 0.6789257970590783, 0.6391108701117993, 0.8648103351245982, 1.0, 0.8856206996558257, 0.8059121255820779, 0.6851513300845844, 0.8755319098520675, 1.0, 0.783801630906283, 0.8083676774082095, 0.776692365689096, 0.6426875593664854, 0.5975606673191927, 1.0, 1.0, 0.9221025201204232, 0.7593826847525079, 0.7525419308523374, 0.8215638795108318, 0.859328962153648, 0.9024486727710714, 0.9470228248649237, 0.9141000841539659, 0.3235702458779912, 0.792956903878446, 0.7983252880753817, 0.8509993641674662, 0.9101345653417019, 0.9275923820999126, 0.8709708325672811, 0.783801630906283, 0.32979761719271283, 0.9154723200083481, 0.6859790330861256, 0.9500330416345475, 1.0, 1.0, 0.7661589996382119, 0.98612097182042, 0.9093560883576932, 1.0, 0.792956903878446, 0.5076359565984532, 0.7730177152011628, 0.8409278889332943, 0.7877842946802818, 0.8509958739837391, 0.6769523479516004, 0.9215834056802388, 0.9239302319973048, 0.05604922291447177, 0.27933746467856935, 0.957982876787582, 0.9444533409865377, 0.9005419686157212, 0.792956903878446, 0.8583303578084276, 0.8902552294738826, 0.9221025201204232, 0.8234778751522029, 0.8752020069230333, 1.0, 0.9500330416345475, 0.9072054025967227, 0.792956903878446, 0.9493239030379825, 0.8655056700329364, 1.0, 0.9493239030379825, 0.06380679277418971, 0.821969278867606, 0.32941434082172544, 0.2620704705727732, 0.19592761543910267, 0.7661589996382119, 0.8301963970996288, 0.9024486727710714, 1.0, 1.0, 0.778217378209213, 1.0, 0.9509004453320677, 1.0, 0.9024486727710714, 0.829922431649786, 0.8064501924857368, 0.8802197756418187, 0.29810593804945296, 1.0, 0.8182339253320035, 1.0, 0.42019932548544126, 0.30280761743613904, 0.9537546369958738, 0.7486246705124127, 0.9141000841539659, 0.6936882487796171, 0.8059121255820779, 0.8409278889332943, 0.7345176572401336, 0.792956903878446, 1.0, 0.8235955083368927, 0.7963795010367266, 0.8871606681680837, 0.783801630906283, 0.9562237378841005, 1.0, 0.870917365833942, 0.8412123173599384, 0.8571486858327837, 1.0, 0.8193695391087877, 0.7525419308523374]
Finish training and take 43m
