Namespace(log_name='./RQ5/tfix_16_1/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_16_1/codet5p_770m_f', data_dir='./data/RQ5/tfix_16_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0098
  global_step = 5
  train_loss = 1.8653
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0098
  ********************
BLEU file: ./data/RQ5/tfix_16_1/validation.jsonl
  codebleu-4 = 24.57 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:24.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00704
  global_step = 9
  train_loss = 0.9302
  ********************
Previous best ppl:1.0098
Achieve Best ppl:1.00704
  ********************
BLEU file: ./data/RQ5/tfix_16_1/validation.jsonl
  codebleu-4 = 40.01 	 Previous best codebleu 24.57
  ********************
 Achieve Best bleu:40.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00696
  global_step = 13
  train_loss = 0.3001
  ********************
Previous best ppl:1.00704
Achieve Best ppl:1.00696
  ********************
BLEU file: ./data/RQ5/tfix_16_1/validation.jsonl
  codebleu-4 = 55.84 	 Previous best codebleu 40.01
  ********************
 Achieve Best bleu:55.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00677
  global_step = 17
  train_loss = 0.1673
  ********************
Previous best ppl:1.00696
Achieve Best ppl:1.00677
  ********************
BLEU file: ./data/RQ5/tfix_16_1/validation.jsonl
  codebleu-4 = 51.03 	 Previous best codebleu 55.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00689
  global_step = 21
  train_loss = 0.0971
  ********************
Previous best ppl:1.00677
BLEU file: ./data/RQ5/tfix_16_1/validation.jsonl
  codebleu-4 = 47.22 	 Previous best codebleu 55.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.007
  global_step = 25
  train_loss = 0.0462
  ********************
Previous best ppl:1.00677
BLEU file: ./data/RQ5/tfix_16_1/validation.jsonl
  codebleu-4 = 48.48 	 Previous best codebleu 55.84
  ********************
reload model from RQ5/tfix_16_1/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_16_1/test.jsonl
  codebleu = 55.03 
  Total = 500 
  Exact Fixed = 7 
[14, 41, 78, 184, 253, 371, 419]
  Syntax Fixed = 1 
[87]
  Cleaned Fixed = 6 
[119, 300, 372, 411, 432, 482]
  ********************
  Total = 500 
  Exact Fixed = 7 
[14, 41, 78, 184, 253, 371, 419]
  Syntax Fixed = 1 
[87]
  Cleaned Fixed = 6 
[119, 300, 372, 411, 432, 482]
  codebleu = 55.03 
[0.36593621655465947, 0.8805090515828415, 0.8415580218210685, 0.5149008676453644, 0.3519980391978243, 0.8216460780407606, 0.20749713980746443, 0.7752358656451599, 0.7489935400021297, 0.5714922760466468, 0.824425175651095, 0.2617283541645111, 0.271898412993546, 0.8114529051148651, 0.49987619040560277, 0.6993092824417293, 0.6399139863647083, 0.5404095184045611, 0.5701122864094044, 0.522583151455642, 0.901308373762346, 0.04051096962423911, 0.5607942439151187, 0.5058144125160369, 0.3540134104327906, 0.7906322192982298, 0.5428297584838295, 0.6287206814660993, 0.3333333333333333, 0.7348892033125801, 0.7953425325255659, 0.6670892174866461, 0.7461273419647574, 0.621862347508016, 0.5471580496396822, 0.45040497426175063, 0.7349975480234779, 0.26493187938965956, 0.8439376810693919, 0.5181394976554918, 0.9891483218006352, 0.27666535735640196, 0.7122853668557851, 0.4592359915981675, 0.8356274376774735, 0.9431571237072573, 0.29572971310756263, 0.49462220891418046, 0.6000167201921691, 0.5205584248799071, 0.3235580320620687, 0.4266408688157862, 0.8715580647670746, 0.4610011597025807, 0.8081779991158766, 0.3376323968592499, 0.6863884298635612, 0.4866609735509537, 0.33085726070295773, 0.6273520828335467, 0.4905880158658711, 0.5854978414909923, 0.8052607518236046, 0.16255528919393042, 0.7517070403538422, 0.8148057063751251, 0.2376473938080234, 0.7605120385823865, 0.5050720368693571, 0.5073450172258992, 0.5129834344293507, 0.6914188211062342, 0.28890659929471596, 0.5843871292056362, 0.503442769884647, 0.4996163148470709, 0.351292879515946, 1.0, 0.7158229243802021, 0.6884734483714399, 0.6132366516010306, 0.6953602200025533, 0.6901940524102714, 0.5962916380054436, 0.6952135917851546, 0.5443098058368457, 0.6295443998237225, 0.5844322860768962, 0.5295971223679855, 0.6377811122305421, 0.05356651918683534, 0.6870857528793353, 0.43706909478476685, 0.7735128412847356, 0.44375943570639337, 0.6745410470195684, 0.7887437726770301, 0.39873667339437135, 0.6266865967254232, 0.9266829146927709, 0.5229326815734496, 0.6416532852733938, 0.7275723927836586, 0.7524099059094923, 0.7136575680257891, 0.6265958401383632, 0.7903356967388899, 0.7450583018440083, 0.04535576197454044, 0.784257328861597, 0.8974634481857526, 0.7347169240849791, 0.5295331624338563, 0.6245487261605229, 0.6316585996401112, 0.6357741351064675, 0.8255980153481091, 0.6221584174428315, 0.35530827434360257, 0.7506656240001803, 0.48049144966615476, 0.0731716191316424, 0.619380623260443, 0.6967165256510812, 0.8990265449613299, 0.506187385602821, 0.4531855447309179, 0.556487662226795, 0.5523753896742678, 0.02357226111775114, 0.4965085296294044, 0.47665753264258404, 0.7320054046730671, 0.884411967378405, 0.3395702836109532, 0.2298355357735218, 0.6201281281538, 0.504138988439773, 0.4106098317062211, 0.8591190003999107, 0.5342366061507683, 0.49694040509490167, 0.6076390099325508, 0.5287803464322112, 0.5548295193530757, 0.762178754714643, 0.16003375286688942, 0.36482184137761786, 0.5157990952074163, 0.5365573165566148, 0.24177600732710475, 0.10209781994840159, 0.6002569359025927, 0.6381394976554917, 0.20167918995135747, 0.8330693404648544, 0.737896877118873, 0.26067970730217216, 0.0, 0.2979644538256022, 0.542720104512177, 0.11367377306961232, 0.8807251792644784, 0.4471047149903593, 0.47029733718731737, 0.003157006751173676, 0.39999999999999997, 0.6307456993182354, 0.5264886858900315, 0.6625508357540224, 0.378235966508134, 0.4086538539823914, 0.027660853979899265, 0.28168179022383655, 0.3391886844282236, 0.7216544978142745, 0.2967214894577404, 0.2690474829925025, 0.5920081417477667, 0.5852685371188103, 0.21410743157431567, 0.29021319125983075, 0.571700750841684, 1.0, 0.7154170894739689, 0.6006394976554918, 0.6450192378669726, 0.6653650919390386, 0.3549155125612238, 0.580680399042054, 0.711138773958444, 0.3850581958489265, 0.3597013850433538, 0.5624218274488504, 0.0, 0.11523607763797766, 0.8821296454239946, 0.49531525511831237, 0.5332104052978875, 0.4736728434913504, 0.45676707546873774, 0.33660944616115085, 0.7634310458508899, 0.9130169160146575, 0.8724634481857527, 0.7417094915222344, 0.7082375734166584, 0.7838676187144158, 0.7623676872921671, 0.26213469909221376, 0.5550049310474542, 0.4793608360838398, 0.8034443090225696, 0.37875429622126355, 0.625500606374912, 0.6583332549596508, 0.753934274260637, 0.5616798194467534, 0.22719082257468703, 0.8291884689534064, 0.21100892280323053, 0.5924424048742483, 0.0, 0.17159349779704655, 0.45066219168465615, 0.6097854672917307, 0.7301961448188755, 0.0, 0.6092728765398896, 0.5418455338416813, 0.0, 0.468132700259547, 0.8232148025904986, 0.4836349689891838, 0.679122578011458, 0.5987433254597354, 0.44053159703911837, 0.7145087250676192, 0.5227886222566653, 0.09138854364080462, 0.7954935638657621, 0.5566133762594777, 0.6047568636054375, 0.4103589624715474, 0.6905145919895326, 0.3166102403736392, 0.3449411645032098, 0.786074986663424, 0.3583507427572924, 0.47146716741543304, 0.08152573352530099, 0.7170369693946632, 1.0, 0.43096007508570255, 0.43389678890316624, 0.6391740914713618, 0.4065085296294044, 0.6451645648968785, 0.3703988534694904, 0.5625034521772871, 0.5178989075058493, 0.8630009487173869, 0.7686975521303816, 0.7752251888155339, 0.6049888916030399, 0.25456463735979495, 0.26502184049155375, 0.503404575411008, 0.8513726633741598, 0.6661120443406857, 0.38516451815138075, 0.8134557045901138, 0.3847960593523654, 0.7509473128584951, 0.8736081095465102, 0.8052186899151903, 0.7107744001757458, 0.6105281160161475, 0.07153421598018944, 0.2923083608934159, 0.5180587939014458, 0.6043938999758249, 0.9033331043272477, 0.5847385307544747, 0.49531525511831237, 0.3961983248070703, 0.4536465025036174, 0.2636668331849268, 0.38690016126589566, 0.6366159932299253, 0.5567052722262924, 0.8289342742606369, 0.3412308151402773, 0.39969884422068214, 0.30394890303951755, 0.45314931726633356, 0.6292379751052773, 0.8291884689534064, 0.3780411766511379, 0.7816265496309229, 0.4567932655066944, 0.0, 0.6563299449135231, 0.7358502882945588, 0.6324794353697702, 0.7044680825384305, 0.6174303177367817, 0.8178607431385985, 0.620688256491589, 0.5861742727509048, 0.39069441076454825, 0.6484951352350596, 0.4855602537204482, 0.47105473967173056, 0.4213203854285221, 0.5092272948800609, 0.5645629956441349, 0.5765188209577117, 0.36919120510737674, 0.8642470965743208, 0.26347186640984777, 0.8038961967467184, 0.4714698407062744, 0.4964902913435728, 0.8475948805844231, 0.7431916688758325, 0.3788887960834998, 0.43732658824904136, 0.5365573165566148, 0.4582096230882099, 0.8434613529595205, 0.6892870539097846, 0.5879792831098578, 0.714297617187952, 0.6519291431830856, 0.6532749656962724, 0.692426316104594, 0.13810543558529642, 0.33556025372044823, 0.4926834019121302, 0.38315024898948147, 0.38107485454531753, 0.4742320746295033, 0.5551317306014439, 0.596274005405593, 0.6431227591816079, 0.8017397827309225, 0.06318453573472939, 0.8035159553501994, 0.12179140591536411, 0.6066855853913183, 0.7699838488343556, 0.3882203984945388, 0.49716968796987465, 0.6519266365055134, 0.7470182644937875, 0.1942247917643758, 0.5073016279385246, 0.05454545454545454, 0.6793788456922385, 0.3231065632068434, 0.6174077791195637, 0.9237479530474457, 0.885801886423617, 0.3154745710868131, 0.15421129818718932, 0.3949005844802781, 0.7731012556269232, 0.6863884298635612, 0.7970182644937875, 1.0, 0.6176873053131037, 0.5774963429007336, 0.6208332549596509, 0.5519698034749387, 0.745669573986067, 0.646595498068946, 0.7883092393403118, 0.6471395484126266, 0.3610202619925232, 0.6392333266349789, 0.5402233947178415, 0.605238198310239, 0.075, 0.5271052192777594, 0.40660822240095185, 0.6685733388407632, 0.47180509653022784, 0.9039342742606371, 0.6903510788203177, 0.6540674839146357, 0.8357967156935899, 0.6289313661614635, 0.5845720003763292, 0.676275735434565, 0.3714764154580026, 0.09999999999999999, 0.2465085296294044, 0.6731093971662766, 0.7591116846422887, 0.6320615067866382, 0.367641751923923, 0.4637437083625371, 0.6054181834983692, 0.7161975446395948, 0.0, 0.1534943042196143, 0.5705557495072049, 0.5796605122596801, 0.64272438732935, 0.8587480609952844, 0.6999197794523513, 0.5927200451626766, 0.15, 0.6280140656881399, 0.3930289716892833, 0.628162097767208, 0.4328478746496224, 0.6922382617563554, 0.6114524395050357, 0.49935498108571197, 0.13116306037298567, 0.6297374013089642, 0.7144948346694379, 0.7543841330490832, 0.7941625761303395, 0.6607530176165711, 0.4463008362331218, 0.0, 0.026198875192873054, 0.5745852221368928, 0.7919953379147748, 0.38183354569032213, 0.8166433426243935, 0.8661547359011423, 0.28129969793939424, 0.551107169103835, 0.6923850277218164, 0.6804859797458157, 0.9320894171538912, 0.5798418629627378, 0.7722344586267278, 0.6801359448960247, 0.6088652057858469, 0.7346810550313403, 0.48535576197454045, 0.7730817392743747, 0.1714285714285714, 0.6983761581889011, 0.7502567313635018, 0.8102736261199102, 0.8386800272334847, 0.27860584199380367, 0.6838528189756714, 0.5470783037679993, 0.5353746106262074, 0.566344928301069, 0.859655200961138, 0.5408401405239117, 0.6780323751412478, 0.5801154532856382, 0.388645918070455, 0.6281448885690524, 0.41790403060506587, 0.6545980623470591, 0.7076923076923076, 0.6545687334437704, 0.3166508010405151, 0.0446797846622413, 0.9019145140739386, 0.12444994767545024, 0.33433367971754413, 0.2141529587579214, 0.7569706667451968, 0.7451384298635613, 0.13337173484667555, 0.7681224012788497, 0.29262229392490574, 0.8857441887419302, 0.679367292167537, 0.8164562066675629, 0.9189641994569014, 0.658009131829517, 0.5252887122558813, 0.5040427487270209, 0.6391676509242572, 0.34556562059599494, 0.8586654659334061, 0.5943893444913173, 0.6, 0.10648766222679504, 0.571308593033083, 0.2475891977778692, 0.39696520560566795, 0.6290960494103124, 0.8150666162955742, 0.47828170295739425, 0.6190128819697975, 0.2171971667841459, 0.6607637451794129]
Finish training and take 43m
