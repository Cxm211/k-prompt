Namespace(log_name='./RQ5/tfix_700_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_700_2/codet5p_770m', data_dir='./data/RQ5/tfix_700_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'el.classList.add(fromClass);              el.offsetWidth = el.offsetWidth;       el.classList.add("animating");', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'el.classList.add(fromClass);              el.getBoundingClientRect();       el.classList.add("animating");'}]
***** Running training *****
  Num examples = 700
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 176
  train_loss = 16.5478
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 64.02 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:64.02
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 351
  train_loss = 8.0182
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 64.51 	 Previous best codebleu 64.02
  ********************
 Achieve Best bleu:64.51
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 526
  train_loss = 3.7688
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 65.82 	 Previous best codebleu 64.51
  ********************
 Achieve Best bleu:65.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 701
  train_loss = 1.8861
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 65.06 	 Previous best codebleu 65.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 876
  train_loss = 0.9663
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 65.7 	 Previous best codebleu 65.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 1051
  train_loss = 0.5499
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 66.39 	 Previous best codebleu 65.82
  ********************
 Achieve Best bleu:66.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 1226
  train_loss = 0.3273
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 65.41 	 Previous best codebleu 66.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 1401
  train_loss = 0.1724
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 64.74 	 Previous best codebleu 66.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 1576
  train_loss = 0.1259
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 65.63 	 Previous best codebleu 66.39
  ********************
early stopping!!!
reload model from RQ5/tfix_700_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_700_2/test.jsonl
  codebleu = 62.83 
  Total = 500 
  Exact Fixed = 103 
[9, 12, 17, 24, 34, 40, 45, 49, 54, 57, 59, 65, 84, 88, 91, 94, 98, 109, 113, 118, 122, 128, 140, 145, 147, 153, 157, 166, 172, 176, 179, 184, 188, 189, 191, 193, 196, 198, 201, 202, 205, 209, 212, 214, 226, 232, 236, 238, 252, 253, 254, 256, 259, 260, 263, 264, 268, 272, 289, 290, 296, 303, 305, 316, 318, 319, 325, 327, 328, 329, 333, 339, 340, 347, 348, 350, 354, 360, 367, 368, 372, 374, 375, 390, 391, 393, 415, 417, 428, 435, 438, 439, 442, 446, 457, 460, 466, 473, 474, 480, 481, 491, 497]
  Syntax Fixed = 5 
[258, 314, 341, 351, 380]
  Cleaned Fixed = 6 
[35, 126, 186, 283, 469, 475]
  ********************
  Total = 500 
  Exact Fixed = 103 
[9, 12, 17, 24, 34, 40, 45, 49, 54, 57, 59, 65, 84, 88, 91, 94, 98, 109, 113, 118, 122, 128, 140, 145, 147, 153, 157, 166, 172, 176, 179, 184, 188, 189, 191, 193, 196, 198, 201, 202, 205, 209, 212, 214, 226, 232, 236, 238, 252, 253, 254, 256, 259, 260, 263, 264, 268, 272, 289, 290, 296, 303, 305, 316, 318, 319, 325, 327, 328, 329, 333, 339, 340, 347, 348, 350, 354, 360, 367, 368, 372, 374, 375, 390, 391, 393, 415, 417, 428, 435, 438, 439, 442, 446, 457, 460, 466, 473, 474, 480, 481, 491, 497]
  Syntax Fixed = 5 
[258, 314, 341, 351, 380]
  Cleaned Fixed = 6 
[35, 126, 186, 283, 469, 475]
  codebleu = 62.83 
[0.62189838351731, 0.6932410335616551, 0.8143710208532546, 0.31331096368700495, 0.6058511595444083, 0.6057547866760294, 0.0, 0.5483719692837501, 0.49440116757135255, 0.12, 0.5368474083243506, 0.9250131914571023, 0.40537805984662106, 0.7569706667451968, 0.6179484048244397, 0.5519860083188967, 1.0, 0.7099766481408452, 0.6885240359080308, 0.25129879162633795, 0.7145082485446812, 0.39323313167877133, 0.24915625771353556, 1.0, 0.6170182644937875, 0.008533731833720836, 0.8555847879042959, 0.30110699874952696, 0.6624101654079205, 0.2627799385832944, 0.7059323615142865, 0.0, 0.5654473178899494, 1.0, 0.8790355083665293, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 1.0, 0.5511412946838178, 0.34268959247415387, 0.10202520055807765, 0.6202064481033123, 1.0, 0.3403395101580171, 0.6570057594483909, 0.274670867776058, 1.0, 0.5900038817627358, 0.350381564995214, 0.6433348104458009, 0.6558003592103332, 1.0, 0.11554591715094636, 0.5194637253863024, 0.7135428903906851, 0.6395188779694462, 1.0, 0.6232902973738775, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.46839895677092, 1.0, 0.28465238167598567, 0.7583139646397308, 0.4790432550703725, 0.4199748401004021, 0.28269688894370654, 0.934857916341244, 0.7063659179388997, 0.5426744349179697, 0.5077265232765378, 0.702664818575847, 0.29035107882031763, 0.465443375698108, 0.8965241752682729, 0.43317161913164237, 0.4229578192624543, 0.3924624192436025, 0.3734424024864518, 0.48011341482542935, 1.0, 0.19999999999999998, 0.5451417837590342, 0.48115588878443294, 1.0, 0.8053935031810717, 0.35583982285741034, 1.0, 0.7402739997668998, 0.7524101654079205, 0.8114529051148651, 0.3144534467602079, 0.7470738524243741, 0.8240488513938333, 1.0, 0.7962637890346522, 0.4059432050969217, 0.48253678796150645, 0.9164266028701744, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.0, 0.4593867118293433, 0.6683895153674697, 0.9348830123070253, 0.05270446328111167, 0.7919953379147748, 0.5876935279546878, 1.0, 0.3079510025390523, 0.4770970038073802, 0.8795723049113977, 0.6903510788203177, 0.8896347633344268, 0.5374713058466929, 0.7231896021778675, 0.7302547136373172, 0.915342532525566, 0.763761398744189, 0.12, 0.2176694832919011, 0.8186530227858186, 0.2011770485941022, 0.8422865509093453, 0.5246680028432865, 0.4976113387647193, 0.2282902740979413, 0.5383507427572923, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.4460952454685818, 0.5777099745360312, 0.753207079624809, 0.10356766952267597, 0.7135428903906851, 0.6368551910912801, 0.7245741928928766, 0.601625348216042, 0.3780411766511379, 0.8249365300761395, 0.5459435933526045, 1.0, 0.676689598214979, 0.5862322739141566, 0.22499999999999998, 0.4854183564853334, 0.65351688848084, 0.8114529051148651, 0.8796531437200525, 0.40825339177238446, 0.7810169160146574, 0.8933766349070695, 0.5869940359456796, 0.22499999999999998, 0.6684064493975892, 0.39347209291868157, 0.3543708710892336, 0.7504004004704901, 0.7433647096337428, 0.45443627493082295, 1.0, 0.5908527155778469, 0.339320402762534, 0.316689598214979, 0.4289051187833622, 0.3177904555329556, 0.7135428903906851, 0.37619479224422014, 0.8123789640962358, 0.4065032326897709, 0.8249365300761395, 0.41996134394226886, 0.5745997086539989, 1.0, 0.11453360085481801, 0.21414079848319634, 0.6304241997444744, 0.5164927712445991, 1.0, 0.7833647096337428, 0.8731931556322046, 0.6884739093341494, 0.7135428903906851, 0.7135428903906851, 0.672350796520816, 1.0, 0.8724178638423756, 0.8249365300761395, 0.7973077795721717, 0.43648868589003154, 1.0, 0.007044218708112843, 1.0, 0.6441764742916929, 0.5621787547146431, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 1.0, 0.5733425270986982, 0.4139240818757265, 0.7971354401872718, 0.8114529051148651, 0.7904210990316385, 0.7814647339984158, 1.0, 0.3954550081193335, 1.0, 0.9548374691056617, 0.174436274930823, 0.5796605122596801, 0.44619887519287305, 0.9320894171538912, 0.7139829574267369, 0.36651542412587346, 0.6740103526642764, 0.731117687292167, 0.15997084006767517, 0.6324885471323345, 1.0, 0.5811145153819444, 0.8841247388270204, 0.7718592407603793, 0.11558530246943605, 0.48074569931823535, 1.0, 0.0, 0.5992413083109663, 0.6844125934903402, 1.0, 0.5981716191316424, 1.0, 0.3965193255633748, 0.7812973541764453, 0.7471765364160405, 0.4268169073526533, 0.7304967316732975, 0.7263487966630597, 0.8106391824574766, 0.6127572978503959, 0.353591681194957, 0.6230908208928772, 0.5042531454411046, 0.6350178817306947, 0.5073028085492076, 1.0, 1.0, 1.0, 0.22499999999999998, 1.0, 0.2504033576162784, 0.7693044144252814, 1.0, 1.0, 0.5017814354051087, 0.5585140118159326, 0.9250131914571023, 1.0, 0.22668959821497897, 0.5588880593698827, 0.33372428676905674, 0.8249365300761395, 0.7209679853738471, 0.3714877971723338, 0.6848698374809952, 1.0, 0.6844125934903402, 0.43829433999099154, 0.6381394976554917, 0.6736456637493553, 0.4016534478992738, 0.8222267233801039, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.4561694688162282, 0.47129759934695603, 0.6877459225855624, 0.8470997505392659, 0.0, 0.5701527915539326, 0.8685438890992636, 1.0, 1.0, 0.7367857383161394, 0.22719082257468703, 0.6366015466501336, 0.3424787987384051, 0.8232059780715621, 0.8249365300761395, 0.7911679345786041, 0.573043959575943, 0.7212489793637704, 0.10758390671207611, 0.8374634481857526, 0.5761697895341449, 1.0, 0.31070499877743873, 0.8928704934314164, 0.44006298409283984, 0.9238877689380496, 0.8356658099532572, 0.7559607945381612, 0.7589313486814424, 0.4029951513565681, 0.1344057918034266, 0.22368441215365098, 0.6539809400446861, 0.5680054404199835, 1.0, 0.714297617187952, 0.6443152851788645, 0.9220338824725971, 0.06666666666666667, 0.6414461089618364, 0.5225082025963208, 0.5702518310230941, 0.6240048954556174, 1.0, 0.9617868617067262, 1.0, 1.0, 0.7135428903906851, 0.4636920210413561, 0.7053304130816762, 0.6273488306532815, 0.8578047138519911, 0.7897721788095522, 0.7204473402052558, 0.676689598214979, 0.3669033918159327, 0.5098094920903372, 1.0, 1.0, 0.8074255754853112, 0.5225961549173092, 0.3381394976554918, 0.6968952972483801, 0.6287796716807859, 0.5424094289706055, 1.0, 1.0, 0.5193052677964398, 1.0, 0.8052134503704533, 0.9176675865658077, 0.414241931213002, 1.0, 0.0, 0.4890714508161259, 0.5305242027189159, 0.6745228788727515, 0.5964858645175034, 1.0, 0.7191441569283882, 0.6245836923955046, 0.8472772140413081, 0.4614470406899055, 0.6527054768195429, 0.5932305781811854, 1.0, 1.0, 0.2696847134058581, 0.8378414230005442, 0.6311119478495857, 1.0, 0.32898762131831805, 1.0, 1.0, 0.5745059758349538, 0.11895673365479492, 0.4129740706026147, 0.3053372331674803, 0.8655056700329364, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.7811990674748787, 0.8059411673492949, 0.5627243873293499, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.8114529051148651, 1.0, 0.7514792897658744, 1.0, 0.17731538118686083, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.31860139828537487, 0.7815666749225483, 0.7108049881272258, 0.6662395775862777, 0.2160900738057458, 0.39969884422068214, 0.6567867859128023, 0.6354739770278475, 0.8600057888291379, 0.6754386750930661, 0.0857142857142857, 0.0, 0.5481846901799162, 0.8655056700329364, 0.6714771286623475, 1.0, 0.13968894088891465, 0.9891483218006352, 0.5554183564853334, 0.05369111792447885, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.5021287494162977, 0.5383089748131177, 0.5621609692548745, 0.5282467529717347, 0.5087792992033554, 1.0, 0.14288357388370543, 0.5938591732384446, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.6612315527854231, 1.0, 0.6829777303051304, 0.6447369408160274, 1.0, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.8845572237610086, 0.8292906179772745, 0.6398527106789474, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.5518505299283467, 0.6592906179772744, 0.8379283989138028, 0.8917028689549173, 0.19999999999999998, 0.8648103351245982, 0.08484732042396881, 1.0, 0.8326781496971694, 0.7925655163069002, 1.0, 0.6099380655621088, 0.7780387576883845, 0.5901857846721861, 0.3657177504818794, 0.8677459225855626, 1.0, 0.4432027101248005, 0.8052225438785547, 0.6807990952074163, 0.6809070226208283, 0.7316423857228858, 0.6914103256997808, 1.0, 1.0, 0.7224929352414922, 0.3, 0.6812000145029719, 0.7220543125543928, 0.7894413517535362, 1.0, 0.8249365300761395, 0.7586172059015853, 0.6931155273806628, 0.08251128145232256, 0.6785036503543816, 0.562520658360374, 0.6901940524102714, 0.46699403594567956, 0.23868348617790436, 0.5282087277319494, 1.0, 0.46690632964164597, 0.6986613814884177, 0.5101642022832921, 0.5674033738135131, 0.6385629301801026, 1.0, 0.7533655306263078, 0.2943237530591414, 0.6666057418239761]
Finish training and take 27m
