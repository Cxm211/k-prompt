Namespace(log_name='./RQ5/tfix_700_2/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_700_2/codet5p_770m_f', data_dir='./data/RQ5/tfix_700_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 700 training instances 
***** Running training *****
  Num examples = 700
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00352
  global_step = 176
  train_loss = 0.7883
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00352
  ********************
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 63.77 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:63.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.0034
  global_step = 351
  train_loss = 0.3484
  ********************
Previous best ppl:1.00352
Achieve Best ppl:1.0034
  ********************
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 64.75 	 Previous best codebleu 63.77
  ********************
 Achieve Best bleu:64.75
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00374
  global_step = 526
  train_loss = 0.1664
  ********************
Previous best ppl:1.0034
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 66.22 	 Previous best codebleu 64.75
  ********************
 Achieve Best bleu:66.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00388
  global_step = 701
  train_loss = 0.0898
  ********************
Previous best ppl:1.0034
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 65.03 	 Previous best codebleu 66.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00418
  global_step = 876
  train_loss = 0.1064
  ********************
Previous best ppl:1.0034
BLEU file: ./data/RQ5/tfix_700_2/validation.jsonl
  codebleu-4 = 66.18 	 Previous best codebleu 66.22
  ********************
reload model from RQ5/tfix_700_2/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_700_2/test.jsonl
  codebleu = 62.45 
  Total = 500 
  Exact Fixed = 90 
[9, 17, 24, 34, 54, 57, 59, 61, 65, 84, 88, 94, 98, 109, 118, 122, 140, 142, 147, 153, 157, 159, 166, 172, 176, 177, 184, 189, 191, 192, 193, 196, 198, 202, 205, 212, 214, 215, 228, 232, 252, 253, 254, 255, 256, 259, 260, 264, 268, 272, 275, 280, 290, 291, 292, 296, 297, 303, 310, 325, 327, 328, 329, 333, 339, 340, 347, 348, 350, 360, 361, 368, 372, 374, 390, 393, 409, 417, 428, 438, 439, 442, 457, 460, 466, 473, 480, 481, 493, 497]
  Syntax Fixed = 2 
[238, 330]
  Cleaned Fixed = 4 
[186, 237, 469, 475]
  ********************
  Total = 500 
  Exact Fixed = 90 
[9, 17, 24, 34, 54, 57, 59, 61, 65, 84, 88, 94, 98, 109, 118, 122, 140, 142, 147, 153, 157, 159, 166, 172, 176, 177, 184, 189, 191, 192, 193, 196, 198, 202, 205, 212, 214, 215, 228, 232, 252, 253, 254, 255, 256, 259, 260, 264, 268, 272, 275, 280, 290, 291, 292, 296, 297, 303, 310, 325, 327, 328, 329, 333, 339, 340, 347, 348, 350, 360, 361, 368, 372, 374, 390, 393, 409, 417, 428, 438, 439, 442, 457, 460, 466, 473, 480, 481, 493, 497]
  Syntax Fixed = 2 
[238, 330]
  Cleaned Fixed = 4 
[186, 237, 469, 475]
  codebleu = 62.45 
[0.5177180281314928, 0.6932410335616551, 0.8143710208532546, 0.31331096368700495, 0.6058511595444083, 0.33778263774904715, 0.0, 0.5417785626903434, 1.0, 0.12, 0.518137810617965, 0.543709349483924, 0.40537805984662106, 0.7569706667451968, 0.5534984161111883, 0.811117687292167, 1.0, 0.7099766481408452, 0.5149193791727077, 0.6974942116108808, 0.6716511056875383, 0.39323313167877133, 0.24915625771353556, 1.0, 0.6170182644937875, 0.005650021814713472, 0.9438645504684042, 0.31253261676497857, 0.6234088995518119, 0.2627799385832944, 0.7059323615142865, 0.0, 0.5654473178899494, 1.0, 0.7750712504516231, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6398412446099295, 0.5511412946838178, 0.34268959247415387, 0.10202520055807765, 0.6202064481033123, 0.7954279937030855, 0.3403395101580171, 0.6570057594483909, 0.274670867776058, 0.7642967117435533, 0.5900038817627358, 0.350381564995214, 0.6500004812993425, 0.6558003592103332, 1.0, 0.1297947365831559, 0.5194637253863024, 0.7135428903906851, 0.6149722466957285, 1.0, 0.6232902973738775, 1.0, 0.714297617187952, 0.5348435312891645, 0.46839895677092, 1.0, 0.28465238167598567, 0.5027003744837852, 0.4706502402815602, 0.4199748401004021, 0.28269688894370654, 0.9488930040605423, 0.714297617187952, 0.8508222909971193, 0.5077265232765378, 0.702664818575847, 0.29035107882031763, 0.15, 0.8413953187748859, 0.39718089591628425, 0.6587382250761771, 0.4198081329071017, 0.354297617187952, 0.657437884673292, 1.0, 0.5433514594401595, 0.5451417837590342, 0.48115588878443294, 1.0, 0.8053935031810717, 0.10789049912002, 0.7971052197289237, 0.5729941496289213, 0.7524101654079205, 0.8114529051148651, 0.32445344676020793, 0.7470738524243741, 0.8349004181959196, 1.0, 0.7962637890346522, 0.5688724788208753, 0.6527790992712059, 0.6591216027794504, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.0, 0.4593867118293433, 0.8399114554115357, 1.0, 0.05270446328111167, 0.6478852119582799, 0.5876935279546878, 0.39272004516267667, 0.3079510025390523, 0.4770970038073802, 0.8795723049113977, 0.6903510788203177, 1.0, 0.5866765036460571, 0.7231896021778675, 0.7302547136373172, 1.0, 0.763761398744189, 0.24, 0.26375606856926587, 0.7017406828338617, 0.20389443989845002, 0.5600382105380115, 0.5246680028432865, 0.4976113387647193, 0.48213747809551644, 0.6485918913581522, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.4460952454685818, 0.553492655520654, 0.753207079624809, 0.14876752225153467, 0.7135428903906851, 0.7368551910912801, 1.0, 0.601625348216042, 0.47180509653022784, 0.6558003592103332, 0.6387218496729328, 1.0, 0.37668959821497894, 0.5862322739141566, 0.26493187938965956, 0.4854183564853334, 0.4051798613404909, 0.8114529051148651, 0.6144756127507046, 0.40825339177238446, 0.7810169160146574, 1.0, 0.5869940359456796, 0.6443152851788645, 0.6684064493975892, 0.39347209291868157, 0.42351479452714663, 0.7504004004704901, 0.7954279937030855, 0.2542503865607112, 1.0, 0.5908527155778469, 0.49215257204284135, 0.316689598214979, 0.3414349512294102, 0.6868474083243508, 0.7135428903906851, 0.45449078148809957, 0.8774367507486662, 0.4065032326897709, 0.8249365300761395, 1.0, 0.5745997086539989, 0.6278910313966588, 0.11453360085481801, 0.36686807121046905, 0.6780323751412478, 0.2600001494491083, 1.0, 0.7365936445375989, 0.8731931556322046, 0.6884739093341494, 0.5203577845911456, 0.7135428903906851, 0.672350796520816, 1.0, 1.0, 0.8249365300761395, 0.7973077795721717, 0.43648868589003154, 1.0, 0.007044218708112843, 1.0, 0.7955692089739603, 0.5621787547146431, 0.6540244039185052, 1.0, 0.40485075092622447, 0.7825255142177938, 1.0, 0.7749468307518907, 0.4061369097043084, 0.7971354401872718, 0.6903510788203177, 0.6599223799332465, 0.7814647339984158, 1.0, 0.3954550081193335, 1.0, 1.0, 0.174436274930823, 0.5796605122596801, 0.42619887519287303, 0.9320894171538912, 0.7755969629986152, 0.4580200993563527, 0.6552622309992373, 0.5515950349550134, 0.16145715622801626, 0.691318859382493, 0.9542830183287001, 0.8034443090225696, 1.0, 0.9414428605659417, 0.0, 0.48074569931823535, 0.920857913365112, 0.0, 0.5992413083109663, 0.6844125934903402, 0.8083325080431145, 0.6119613551150405, 0.9145980101874507, 0.3965193255633748, 0.7812973541764453, 0.7471765364160405, 0.4268169073526533, 0.713241033561655, 0.7263487966630597, 0.9056583090096291, 0.6127572978503959, 0.3544440011696263, 0.7225371723695806, 0.474518199459927, 0.6350178817306947, 0.5073028085492076, 1.0, 1.0, 1.0, 0.6443152851788645, 1.0, 0.2772943579237247, 0.836942030771336, 1.0, 1.0, 0.5017814354051087, 0.5585140118159326, 0.8501022096540376, 1.0, 0.07292202063701048, 0.6624678525854575, 0.3943303473751174, 0.8249365300761395, 0.7209679853738471, 0.3714877971723338, 0.5432894325725358, 1.0, 0.6844125934903402, 0.43829433999099154, 0.7135428903906851, 0.6463729364766281, 0.4016534478992738, 0.594544670797994, 0.7142845202249919, 0.6443152851788645, 0.5600468038501167, 0.5402772989489573, 0.46604306419874064, 0.6877459225855624, 0.8470997505392659, 0.0, 0.5966289756845221, 0.8420812716756092, 0.4562834271976408, 1.0, 1.0, 0.8249365300761395, 0.5648128669920147, 0.3424787987384051, 0.8232059780715621, 0.8249365300761395, 1.0, 0.573043959575943, 0.7212489793637704, 0.36389874222714214, 0.8374634481857526, 0.5761697895341449, 1.0, 0.204229426016456, 0.7672014441385868, 0.5446783687082245, 0.9238877689380496, 0.8356658099532572, 0.7654636689687846, 0.8255980153481091, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5113616550848404, 0.6276136760976885, 0.714297617187952, 0.3502184077768308, 0.6516274875052652, 0.06666666666666667, 0.8164329076215759, 0.5174693084765555, 0.3457533886637675, 0.5777058255819119, 1.0, 0.9617868617067262, 1.0, 1.0, 0.7135428903906851, 0.9093279612505663, 0.6652043389295401, 0.6273488306532815, 1.0, 0.6766220660403222, 0.7204473402052558, 0.676689598214979, 0.3669033918159327, 0.5332516758833834, 1.0, 1.0, 0.8559236663562839, 0.5347191241991722, 0.3381394976554918, 0.6968952972483801, 0.6287796716807859, 0.5424094289706055, 1.0, 1.0, 0.6319683101653729, 1.0, 0.8069007178395178, 0.9176675865658077, 0.4147896756607047, 0.5652887122558813, 0.0, 0.37739378590222805, 0.4607209252762136, 0.6745228788727515, 0.5964858645175034, 1.0, 1.0, 0.6499999999999999, 0.9265757127424608, 0.1709442817534614, 0.5027054768195429, 0.5932305781811854, 0.712020325565049, 1.0, 0.2549516775057695, 0.8378414230005442, 0.444006594916457, 1.0, 0.31826402883081917, 1.0, 0.7700435930643703, 0.5296396769016352, 0.9068864709786153, 0.4129740706026147, 0.3053372331674803, 0.7965413257707367, 0.4621787547146431, 0.6398103351245982, 0.4797319820105432, 0.7916479842275448, 0.5578421405564804, 0.5758081453961951, 0.8242749040329442, 0.7530301180787049, 0.7648042806727606, 0.8114529051148651, 0.5654473178899494, 0.7756211334750283, 1.0, 0.15558690305737846, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.11806185848818948, 0.5019774510104837, 0.39374707916270535, 0.5976435605147894, 0.7108049881272258, 0.6662395775862777, 0.2160900738057458, 0.39969884422068214, 0.5410464370034509, 0.6354739770278475, 0.5775720906730404, 1.0, 0.0857142857142857, 0.0, 0.645753961971828, 0.8655056700329364, 0.8, 0.6595289813282088, 0.09713682208624577, 0.9891483218006352, 0.5554183564853334, 0.038139497655491794, 0.00783625026922906, 0.16065884347701442, 0.4778533416343903, 0.7054819616225767, 0.46459392039551944, 0.5621609692548745, 0.5282467529717347, 0.5786580151554076, 1.0, 0.06808859345352361, 0.6087198543529957, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.7288312733021203, 0.8863585661014859, 0.6829777303051304, 0.6447369408160274, 1.0, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.8845572237610086, 0.8292906179772745, 0.636068568378893, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.6046886482005214, 0.4755831472572245, 0.8379283989138028, 0.8917028689549173, 0.2670447530210518, 0.8648103351245982, 0.0828714707479615, 1.0, 0.8326781496971694, 0.7387037834978107, 1.0, 0.6099380655621088, 0.7500189997069144, 0.5901857846721861, 0.4064057684774388, 0.8677459225855626, 1.0, 0.4432027101248005, 0.859664396260277, 0.6807990952074163, 0.6809070226208283, 0.7316423857228858, 0.7044095068017608, 1.0, 0.6099306985155997, 0.9248007629373642, 0.14702442372871036, 0.6812000145029719, 0.6893720612567699, 0.8322984946106791, 1.0, 0.8249365300761395, 0.8622646994947969, 0.7845525506958333, 0.11582616733482173, 0.6785036503543816, 0.562520658360374, 0.6901940524102714, 0.5089439475762831, 0.23868348617790436, 0.5282087277319494, 0.7135160729534664, 0.4695633649758072, 1.0, 0.5687363510007433, 0.5674033738135131, 0.8632148025904984, 1.0, 0.7533655306263078, 0.387966609042281, 0.6666057418239761]
Finish training and take 25m
