Namespace(log_name='./RQ5/tfix_300_2/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_300_2/codet5p_770m_f', data_dir='./data/RQ5/tfix_300_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 300 training instances 
***** Running training *****
  Num examples = 300
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00386
  global_step = 76
  train_loss = 0.9644
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00386
  ********************
BLEU file: ./data/RQ5/tfix_300_2/validation.jsonl
  codebleu-4 = 61.79 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:61.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00368
  global_step = 151
  train_loss = 0.3292
  ********************
Previous best ppl:1.00386
Achieve Best ppl:1.00368
  ********************
BLEU file: ./data/RQ5/tfix_300_2/validation.jsonl
  codebleu-4 = 62.25 	 Previous best codebleu 61.79
  ********************
 Achieve Best bleu:62.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00393
  global_step = 226
  train_loss = 0.1674
  ********************
Previous best ppl:1.00368
BLEU file: ./data/RQ5/tfix_300_2/validation.jsonl
  codebleu-4 = 64.08 	 Previous best codebleu 62.25
  ********************
 Achieve Best bleu:64.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00453
  global_step = 301
  train_loss = 0.0759
  ********************
Previous best ppl:1.00368
BLEU file: ./data/RQ5/tfix_300_2/validation.jsonl
  codebleu-4 = 63.11 	 Previous best codebleu 64.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0048
  global_step = 376
  train_loss = 0.0346
  ********************
Previous best ppl:1.00368
BLEU file: ./data/RQ5/tfix_300_2/validation.jsonl
  codebleu-4 = 63.29 	 Previous best codebleu 64.08
  ********************
reload model from RQ5/tfix_300_2/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_300_2/test.jsonl
  codebleu = 62.45 
  Total = 500 
  Exact Fixed = 66 
[3, 17, 24, 49, 57, 59, 84, 107, 115, 122, 132, 140, 147, 157, 166, 170, 174, 176, 184, 186, 189, 194, 205, 209, 214, 215, 226, 232, 252, 255, 258, 263, 264, 290, 292, 296, 303, 305, 327, 328, 330, 336, 347, 348, 349, 350, 359, 360, 361, 368, 372, 374, 391, 393, 415, 418, 426, 428, 439, 457, 460, 462, 473, 477, 492, 497]
  Syntax Fixed = 3 
[77, 472, 499]
  Cleaned Fixed = 7 
[35, 54, 154, 200, 237, 390, 469]
  ********************
  Total = 500 
  Exact Fixed = 66 
[3, 17, 24, 49, 57, 59, 84, 107, 115, 122, 132, 140, 147, 157, 166, 170, 174, 176, 184, 186, 189, 194, 205, 209, 214, 215, 226, 232, 252, 255, 258, 263, 264, 290, 292, 296, 303, 305, 327, 328, 330, 336, 347, 348, 349, 350, 359, 360, 361, 368, 372, 374, 391, 393, 415, 418, 426, 428, 439, 457, 460, 462, 473, 477, 492, 497]
  Syntax Fixed = 3 
[77, 472, 499]
  Cleaned Fixed = 7 
[35, 54, 154, 200, 237, 390, 469]
  codebleu = 62.45 
[0.5670445241047504, 0.6932410335616551, 1.0, 0.2122047676692162, 0.9433946642877236, 0.8126742078107725, 0.0, 0.6058016612544186, 0.4930978393785904, 0.09999999999999999, 0.5368474083243506, 0.5638063820069366, 0.27668959821497896, 0.7569706667451968, 0.5534984161111883, 0.811117687292167, 1.0, 0.7099766481408452, 0.6885240359080308, 0.5989463698013806, 0.7145082485446812, 0.39323313167877133, 0.24915625771353556, 1.0, 0.5970182644937875, 0.006758238160278511, 0.9438645504684042, 0.26164945162585174, 0.6624101654079205, 0.2627799385832944, 0.7059323615142865, 0.0, 0.5654473178899494, 0.927299329695247, 0.8790355083665293, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6398412446099295, 0.5511412946838178, 0.805671205945659, 0.04202520055807766, 0.6574171333734883, 0.551438679121576, 0.3403395101580171, 0.572116493710585, 0.274670867776058, 1.0, 0.5900038817627358, 0.350381564995214, 0.6500004812993425, 0.6558003592103332, 0.9198338539201028, 0.1297947365831559, 0.5194637253863024, 0.7135428903906851, 0.6395188779694462, 1.0, 0.6232902973738775, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.7618233428889547, 0.7476033018912212, 0.6197233100748608, 0.6877745357134093, 0.3580136920499279, 0.4199748401004021, 0.28269688894370654, 0.9488930040605423, 0.714297617187952, 0.8508222909971193, 0.5077265232765378, 0.702664818575847, 0.29035107882031763, 0.8, 0.8413953187748859, 0.43317161913164237, 0.6587382250761771, 0.591516286342532, 0.3734424024864518, 0.6895579051414615, 1.0, 0.5684395130452333, 0.5451417837590342, 0.48115588878443294, 0.726751676645778, 0.8053935031810717, 0.3591365261541136, 0.8550899856418173, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.39874985593507484, 0.7470738524243741, 0.8349004181959196, 0.8642415207541971, 0.8097407224050479, 0.5688724788208753, 0.6989456359513437, 0.6073924341116668, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.0, 1.0, 0.4024028696079703, 0.8107791184560279, 0.05270446328111167, 0.2878852119582799, 0.7468446370144718, 0.39272004516267667, 0.3079510025390523, 1.0, 0.8795723049113977, 0.6903510788203177, 0.8802687759518133, 0.5143209096122012, 0.7017610307492962, 0.7302547136373172, 1.0, 0.7421110988498729, 0.18962441329574325, 0.14375606856926587, 0.6791118740117017, 0.2011770485941022, 0.5600382105380115, 0.5246680028432865, 0.4976113387647193, 0.41602639711957595, 1.0, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.43859524546858186, 0.553492655520654, 0.753207079624809, 0.14876752225153467, 0.7135428903906851, 0.6368551910912801, 0.770152703330311, 0.601625348216042, 0.47180509653022784, 0.3992286298565113, 0.6387218496729328, 1.0, 0.3561219555899987, 0.5862322739141566, 0.26493187938965956, 0.4854183564853334, 0.6612091961731477, 0.500808145396195, 0.784336399011221, 0.40825339177238446, 0.7588783494196636, 1.0, 0.5869940359456796, 0.22499999999999998, 0.5814461089618363, 0.39347209291868157, 0.42351479452714663, 0.7504004004704901, 0.7954279937030855, 0.45443627493082295, 1.0, 0.5908527155778469, 0.49215257204284135, 0.316689598214979, 1.0, 0.6868474083243508, 0.4822625464959074, 0.45449078148809957, 1.0, 0.40693291435805307, 0.8249365300761395, 0.41996134394226886, 0.5745997086539989, 0.7899219905188113, 0.053829931618554525, 0.36686807121046905, 0.6780323751412478, 0.5164927712445991, 1.0, 0.7651650731090275, 1.0, 0.6884739093341494, 0.5203577845911456, 0.7135428903906851, 0.7725146668513112, 0.5563519009807991, 0.9044606924705487, 0.3992286298565113, 1.0, 0.43648868589003154, 0.5758089748131177, 0.00906124268900347, 0.7960658705890922, 0.7955692089739603, 0.6907889899003139, 0.6540244039185052, 0.8346354401872718, 0.40485075092622447, 0.516529645794386, 1.0, 0.6271806220068676, 0.7122124794272888, 0.6613122421311423, 0.8114529051148651, 0.6599223799332465, 0.6116850869767493, 0.9145840400226954, 0.14071453490147373, 1.0, 1.0, 0.174436274930823, 0.6827564874918781, 0.44619887519287305, 0.9320894171538912, 0.7755969629986152, 0.4580200993563527, 0.6552622309992373, 0.731117687292167, 0.16145715622801626, 0.691318859382493, 1.0, 0.6452078183025891, 0.8841247388270204, 0.9414428605659417, 0.11558530246943605, 0.48074569931823535, 1.0, 0.0, 0.5992413083109663, 0.6844125934903402, 0.8011559766709215, 0.6119613551150405, 0.8561634413516603, 0.3965193255633748, 0.7812973541764453, 0.5976126300502733, 0.4268169073526533, 0.6834523207511243, 0.7263487966630597, 0.9056583090096291, 0.6321782328024446, 0.3544440011696263, 0.7225371723695806, 0.474518199459927, 0.6350178817306947, 0.5073028085492076, 1.0, 0.6088783494196637, 0.6636363636363636, 0.6443152851788645, 0.6680779662080889, 0.2772943579237247, 1.0, 0.5814345843863841, 0.7299948711471688, 0.5017814354051087, 0.5585140118159326, 1.0, 1.0, 0.39078898990031385, 0.6624678525854575, 0.33372428676905674, 0.4501947080651152, 0.6876346520405138, 0.5078514335359701, 0.6848698374809952, 0.8831685250103409, 0.6844125934903402, 0.43829433999099154, 0.6381394976554917, 0.6463729364766281, 0.4016534478992738, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.4452105198475923, 0.4615074577124803, 0.6877459225855624, 0.8470997505392659, 0.0, 0.5966289756845221, 0.8849384145327521, 0.44452452291536887, 1.0, 0.8487963120906614, 0.8249365300761395, 0.6366015466501336, 0.6851384298635612, 0.8232059780715621, 0.8249365300761395, 0.6842965709830897, 0.573043959575943, 0.7212489793637704, 0.2358114916555782, 0.8374634481857526, 0.5761697895341449, 1.0, 0.43129097285515594, 1.0, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.769581697257735, 0.7589313486814424, 0.3127695874467937, 0.1344057918034266, 0.34272438732934996, 0.6482375056804599, 0.5680054404199835, 0.6276136760976885, 0.714297617187952, 0.3502184077768308, 0.6370596293853276, 0.075, 0.8378414230005442, 0.5174693084765555, 0.5702518310230941, 0.6240048954556174, 0.8034645362012123, 0.9211619479436641, 1.0, 1.0, 0.4372429839446083, 1.0, 0.5989472240840852, 0.6273488306532815, 0.6870606775436042, 0.7189894209825303, 0.7204473402052558, 1.0, 0.8050241598897441, 0.47093759385342976, 0.5015372100944879, 0.8033946307914341, 0.8387251821778285, 0.624178976103728, 0.3381394976554918, 0.6968952972483801, 0.6287796716807859, 0.5424094289706055, 1.0, 1.0, 1.0, 1.0, 0.6253451984838653, 0.9176675865658077, 0.4147896756607047, 0.42775375390483883, 0.0, 0.4890714508161259, 0.5305242027189159, 0.6745228788727515, 0.9891483218006352, 1.0, 1.0, 0.3057265995447144, 0.6980083797903092, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 0.712020325565049, 1.0, 0.2670805526242285, 0.8378414230005442, 0.444006594916457, 1.0, 0.32898762131831805, 1.0, 0.8853031368082722, 0.8250515340301889, 0.9068864709786153, 0.4129740706026147, 0.3053372331674803, 0.7905056700329363, 0.4621787547146431, 0.6848103351245982, 0.8184276974159628, 0.7916479842275448, 0.5884119940010644, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.6983535654700125, 1.0, 0.6421033608242943, 1.0, 0.17731538118686083, 0.4592493207167594, 0.6363229742058536, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.31860139828537487, 0.7821377151388884, 0.7108049881272258, 0.752380452288719, 0.2160900738057458, 0.39969884422068214, 0.4737783678784365, 0.6354739770278475, 0.5775720906730404, 0.6754386750930661, 0.0857142857142857, 0.0, 0.8701798515467687, 0.8655056700329364, 0.739532400808212, 1.0, 0.09713682208624577, 0.5833105633098615, 0.9891483218006352, 0.038139497655491794, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.4446724306281599, 0.5383089748131177, 0.5621609692548745, 0.8249365300761395, 0.6100746123790788, 1.0, 0.21056545775751523, 0.6183689771600134, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.7288312733021203, 0.8863585661014859, 0.6829777303051304, 0.7511655122445988, 0.5616686574107796, 1.0, 0.09999999999999999, 0.40739727760582384, 0.5036911179244787, 0.7260484492137269, 0.8845572237610086, 0.8292906179772745, 0.636068568378893, 0.6883329804487951, 0.8953732211389798, 0.21012013947128094, 0.5518505299283467, 0.6592906179772744, 0.8379283989138028, 0.8917028689549173, 0.33594700943102185, 0.8648103351245982, 0.0828714707479615, 1.0, 0.8326781496971694, 0.7925655163069002, 1.0, 0.6099380655621088, 1.0, 0.46468337790838865, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.3583507427572924, 0.859664396260277, 0.6807990952074163, 0.7738783494196637, 0.7316423857228858, 0.813876410574395, 1.0, 0.7710658705890923, 0.7574218493475542, 0.3, 1.0, 0.6893720612567699, 0.8322984946106791, 0.5134441848216565, 0.6992286298565114, 0.8622646994947969, 0.6931155273806628, 0.056223753085077204, 0.6785036503543816, 0.562520658360374, 0.6901940524102714, 0.5089439475762831, 0.23868348617790436, 0.5282087277319494, 0.7502495415001358, 0.9891483218006352, 0.6986613814884177, 0.6266252770810942, 0.5491454654050718, 0.6385629301801026, 1.0, 0.7533655306263078, 0.6371808959162841, 0.6101458438731493]
Finish training and take 12m
