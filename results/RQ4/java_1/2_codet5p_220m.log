Namespace(log_name='./RQ5/java_1/2_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_1/2_codet5p_220m', data_dir='./data/RQ5/java_1_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'public void METHOD_1 ( TYPE_1 VAR_1 , TYPE_2 VAR_2 ) throws java.io.IOException { if ( ( VAR_3 ) != null ) { VAR_2 . METHOD_2 ( ( ( VAR_1 . METHOD_3 ( ) ) + STRING_1 ) ) ; return ; } getData ( ) ; TYPE_3 . METHOD_4 ( VAR_1 , VAR_2 , TYPE_4 . METHOD_5 ( this . METHOD_6 ( ) . METHOD_7 ( ) , this . METHOD_8 ( ) ) , INT_1 , INT_2 ) ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'public void METHOD_1 ( TYPE_1 VAR_1 , TYPE_2 VAR_2 ) throws java.io.IOException { if ( ( VAR_3 ) != null ) { VAR_2 . METHOD_2 ( ( ( VAR_1 . METHOD_3 ( ) ) + STRING_1 ) ) ; return ; } TYPE_3 . METHOD_4 ( VAR_1 , VAR_2 , TYPE_4 . METHOD_5 ( this . METHOD_6 ( ) . METHOD_7 ( ) , this . METHOD_8 ( ) ) , INT_1 , INT_2 ) ; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 39.5931
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 14.1 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:14.1
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 36.1011
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 30.32 	 Previous best codebleu 14.1
  ********************
 Achieve Best bleu:30.32
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 17.9134
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 49.78 	 Previous best codebleu 30.32
  ********************
 Achieve Best bleu:49.78
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 13.6746
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 59.1 	 Previous best codebleu 49.78
  ********************
 Achieve Best bleu:59.1
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 9.576
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 67.15 	 Previous best codebleu 59.1
  ********************
 Achieve Best bleu:67.15
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 7.5485
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 69.01 	 Previous best codebleu 67.15
  ********************
 Achieve Best bleu:69.01
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 1.7865
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 68.83 	 Previous best codebleu 69.01
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 4.7792
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 68.89 	 Previous best codebleu 69.01
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 11.5704
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 69.51 	 Previous best codebleu 69.01
  ********************
 Achieve Best bleu:69.51
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 3.6691
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 69.74 	 Previous best codebleu 69.51
  ********************
 Achieve Best bleu:69.74
  ********************
reload model from RQ5/java_1/2_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_2/test.jsonl
  codebleu = 71.13 
  Total = 500 
  Exact Fixed = 2 
[304, 401]
  Syntax Fixed = 1 
[305]
  Cleaned Fixed = 2 
[58, 419]
  ********************
  Total = 500 
  Exact Fixed = 2 
[304, 401]
  Syntax Fixed = 1 
[305]
  Cleaned Fixed = 2 
[58, 419]
  codebleu = 71.13 
[0.014433119644478333, 0.29014999301819394, 0.8340268246875062, 0.44434963667266913, 0.8410886457286899, 0.9000092746228069, 0.18715746984834256, 0.683214548174067, 0.3068923552159469, 0.7914729241068532, 0.9685944421474288, 0.823693193306005, 0.624456233158069, 0.3262907325159499, 0.8891483936806484, 0.831325490226771, 0.7523277398705996, 0.9342846849350264, 0.8640832471274409, 0.8673567485716778, 0.755766537798902, 0.1272230381667122, 0.31642949430617967, 0.9000120435131169, 0.43042978422632594, 0.8956609927523811, 0.8197132433292174, 0.7375419314015366, 0.6151952248646311, 0.774571731962115, 0.8168986346497626, 0.8459638589187009, 0.7321748033833095, 0.3122331057392717, 0.46815082712926215, 0.5744144825826514, 0.7662351040986912, 0.8768056751919203, 0.9410056506290296, 0.7467173881179517, 0.12005836410960098, 0.5518277150635162, 0.7836446397032701, 0.8345641796204135, 0.4338121251386362, 0.23305498404148167, 0.5518277150635162, 0.976934640027229, 0.9061537357063023, 0.7366535801418781, 0.9269331830469714, 0.4020839132815655, 0.5771443977924295, 0.591267892531106, 0.7478138851365419, 0.7574036506399683, 0.6824219142733001, 0.8855924534744011, 0.18715746984834256, 0.7113205021880318, 0.4460835470686716, 0.7942701366539271, 0.9125104348398385, 0.6592360684959336, 0.5138320137630494, 0.9471534480091441, 0.9607233175453542, 0.8330625895958035, 0.8134138867268921, 0.31150077951039323, 0.8875485500759485, 0.7484556246477858, 0.8713162795518272, 0.5518277150635162, 0.9442131546627328, 0.8762476125170207, 0.736591300985149, 0.5422875137889253, 0.8657498877948182, 0.8291179288334182, 0.7529109419180047, 0.8693647831726143, 0.8075596067167303, 0.9192762754483625, 0.9778918624590627, 0.755084165214517, 0.8402219670486633, 0.9197682751991088, 0.9422650011672864, 0.7512326245782468, 0.961485854541051, 0.29944922659064166, 0.5778932566244337, 0.9535662529996234, 0.8597631970767317, 0.8434464829239963, 0.8853701119354719, 0.5354389191116804, 0.8696882992240296, 0.8064544260431346, 0.6916772828650979, 0.1922460048432551, 0.8854899209545841, 0.8694837670796829, 0.8774842286953382, 0.6529646953025697, 0.8591401093529241, 0.8813815400934406, 0.7720861332093774, 0.8328826274635627, 0.3270754300560513, 0.18770544295058486, 0.931919905317208, 0.680424467207345, 0.7033804044648055, 0.21202795456555992, 0.8947233410879221, 0.9600094284589669, 0.7221216537281969, 0.48097952528287635, 0.36688032730714837, 0.9418090107609731, 0.8221154033234108, 0.8948136334077694, 0.8810145991156664, 0.7037147467795737, 0.7184862435619404, 0.932758510312542, 0.48666825572275857, 0.554547404131932, 0.554547404131932, 0.9046264534870987, 0.1541784653762989, 0.9515492621717487, 0.781379096553734, 0.8613534179878808, 0.8684368528418116, 0.5518277150635162, 0.8275239936524709, 0.6850661340471761, 0.9072172678934285, 0.7901423077149385, 0.24803825858570258, 0.5496457765472276, 0.8936973938003048, 0.7053890303340073, 0.9584247004495543, 0.6438544353710891, 0.871607281170603, 0.26042871833101394, 0.2540549363904691, 0.2730308984520964, 0.9298833643858577, 0.7323260615249401, 0.95273941015886, 0.7478138851365419, 0.4577875000912996, 0.8797777769539077, 0.6639326194627299, 0.8721459350488947, 0.6693371325929338, 0.7173692510217399, 0.9209893793099855, 0.8692714817305559, 0.8234385994211576, 0.7057865120521449, 0.28269705337603623, 0.1925915895002325, 0.83084241467313, 0.06503524893327237, 0.8792165800089584, 0.8564158030224163, 0.8189995554735204, 0.8705354665443821, 0.7922098978133367, 0.316047177024382, 0.8964315204701094, 0.9359282995607665, 0.6121576162732347, 0.06585256308868531, 0.5204525623408196, 0.9171481786623457, 0.5033623603791932, 0.8867171706601364, 0.6857004907776587, 1.0, 0.4419034289589019, 0.7540961153100033, 0.583756774652523, 0.5269387155748682, 0.17246626612134738, 0.8560885502124413, 0.8218670339508176, 0.9344178676633157, 0.841294042438234, 0.5913923016540148, 0.43042978422632594, 0.029801806202234958, 0.510261257068833, 0.9156873229543501, 0.8571088360379246, 0.7343514158064606, 0.8939403219941204, 0.7988848397512625, 0.4241253179337926, 0.9168635615364948, 0.923789967265608, 0.9055256568213023, 0.6708388745519409, 0.07650002609661181, 0.761642782903787, 0.9534190821154946, 0.6268790890159027, 0.912004975833203, 0.3726201284133503, 0.39009775489211856, 0.8516665453656083, 0.9603262193723192, 0.7830685373049465, 0.801169789534145, 0.7170219254969812, 0.5204525623408196, 0.7703813467645078, 0.8741762120847619, 0.95, 0.3246559666184787, 0.507673966802249, 0.5771443977924295, 0.8801547132641002, 0.5518277150635162, 0.9288817233176625, 0.7052846166174914, 0.9487145883454327, 0.811601302152221, 0.7360764107540074, 0.8017624348471263, 0.5518277150635162, 0.8465816263983615, 0.5370386408651547, 0.708021211702571, 0.5744144825826514, 0.4419034289589019, 0.8651706733544999, 0.7931351046504085, 0.9257462023534273, 0.8921042672694857, 0.8347288631031309, 0.554547404131932, 0.18543677131679467, 0.8093542785781114, 0.9586325789386465, 0.8759418897254363, 0.8706458223016678, 0.8423353042818088, 0.7975223482831406, 0.5843616264174137, 0.15711583258453737, 0.8473249685745358, 0.15536660122815021, 0.5535209724526217, 0.13475116084855343, 0.9303179033192857, 0.9029228697393135, 0.8769394563844016, 0.5721197179537898, 0.9438191843973569, 0.8307437388721057, 0.7528022474984176, 0.8398619708402335, 0.5518277150635162, 0.7967226445240709, 0.9052981975214478, 0.5744144825826514, 0.5213628132698136, 0.8675066797977258, 0.9559045711960699, 0.4710005705593421, 0.20557992375625006, 0.30793433386770624, 0.24557877385038965, 0.9005633971565483, 0.008333333333333333, 0.9623590512009765, 0.8605569379186453, 0.5643756153678857, 0.2963719869729004, 0.04255638812607713, 0.9035957587334096, 0.8060819624269768, 0.8770730075752209, 0.3726201284133503, 0.8362457929142617, 0.07133577232937884, 0.1301705413035893, 0.5778624281512992, 0.742735238966309, 0.5506575627795242, 0.6384487608342887, 0.8847957654616538, 0.7784068738024617, 0.9312180270651347, 0.602746669476317, 0.003947368421052631, 0.9855701074802354, 0.9224026999966255, 0.47814499783971315, 0.8384022446788228, 0.9168036200630483, 0.8230617674989805, 0.45795847152350877, 0.8278702046937575, 0.7757388605257256, 0.7932369845589845, 0.776166402403458, 0.31860308309460966, 0.8922083576528005, 0.4807480783278252, 0.8445742508516902, 0.9587008191521351, 0.8712005145728838, 0.9821490926793564, 0.8273537247829404, 0.8322717488058633, 0.7753542141251331, 0.7357948547742252, 0.8045536528703048, 0.8019309772658032, 0.7968175330905409, 0.5204525623408196, 0.26809290665643454, 0.6684568643706137, 0.9606383615912808, 0.8153893678094166, 0.6039912357161369, 0.8926703816314219, 0.46815082712926215, 0.8880218220668674, 0.8384301990510942, 0.6479476402971456, 0.15954760596207063, 0.7650782870421127, 0.8925235787478019, 0.7879222294127737, 0.8115304642443637, 0.8979311034464716, 0.941246887425869, 0.3597554292633871, 0.554547404131932, 0.9161769176090371, 0.8178083071683226, 0.8111432291363376, 0.7571560504922936, 0.9133014669327173, 0.8696537204746508, 0.27935439804151907, 0.9809329721714535, 0.8942988805742025, 0.785678266679733, 0.8409105832325275, 0.5895449142652194, 0.8894866935279488, 0.805572702962198, 0.15558346484882482, 0.8627830017322995, 0.5771443977924295, 0.9693877551020409, 0.8492232955507442, 0.8046302983128589, 0.8218150979094205, 0.8859614653557839, 0.9422001488455178, 0.4381945350786666, 0.5695758149747698, 0.732829501424611, 0.8406526362427964, 0.3361664691432363, 0.22668033582087496, 0.877704790303369, 0.9973062440915319, 0.9468414240145169, 0.8715520398106922, 0.9120050406373192, 0.9064448569214978, 0.6831381239952166, 0.885289279145866, 0.7223713204162335, 0.9684326167255213, 0.7622826604743864, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.7959131487019773, 0.8591254859048258, 0.9598250197069826, 0.9634911520696614, 0.8712775503716599, 0.6384524255147421, 0.7168520397592135, 0.66337495947072, 0.9102264849813417, 1.0, 0.7135449662005422, 0.7203156880267696, 0.8959113683826374, 0.8278488581694263, 0.7643743559822425, 0.756234192312671, 0.821054400101145, 0.9136051415192505, 0.9054056984566428, 0.44634077476229317, 0.8505395682648067, 0.6640761932093434, 0.8868262725136591, 0.22073125221934192, 0.9442100823718134, 0.6593180595213053, 0.8106770910754153, 0.8957718999086999, 0.964963853850137, 0.510261257068833, 0.9314651191074765, 0.8876750318246271, 0.7500812413688931, 0.6410378892354339, 0.8100620346485852, 0.9936419998424288, 0.6993476920760513, 0.7865392437305967, 0.9745770345730578, 0.74212817232249, 0.44795252631484794, 0.7630758171918134, 0.9146241894400875, 0.5269387155748682, 0.37965976077181546, 0.86944691495958, 0.8120273304829477, 0.11741223753617559, 0.8022554780331761, 0.9343267218430718, 0.9350024698180224, 0.895540655184323, 0.4535640797757121, 0.8800985187243549, 0.8825445333927069, 0.2553569063859492, 0.8103265047147887, 0.800083937746499, 0.8891662607781625, 0.8810554316276455, 0.3660870680963601, 0.7165234804991123, 0.6947890673248284, 0.839141576433869, 0.5962116304155733, 0.8511236782448381, 0.554547404131932, 0.47783923348528035, 0.6416187976181017, 0.9133731397684426, 0.8304146913339561, 0.7065961844283755, 0.7522642454492299, 0.7846187729184473, 0.9272271558703311, 0.9016850690943105, 0.8362192242070374, 0.8385916102626618, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.8624563127165505, 0.9054182822734687, 0.4957452663826483, 0.7724155016374759, 0.808963947023041, 0.8802951254285889, 0.9152573407816746, 0.8661071090470243, 0.8893823781682744, 0.5527279801997969, 0.9206598027093102, 0.7077743623236582, 0.7432816303642229, 0.02, 0.9399365804904811, 0.8966347717272292, 0.5744144825826514, 0.8697827914428056, 0.4955221217337541, 0.3279154300022623, 0.8965966751963506, 0.814295430262091, 0.8514674628624775, 0.9250193662902535, 0.15911191953617884, 0.8987479724682896, 0.7407857570111461, 0.554547404131932]
Finish training and take 3h50m
