Namespace(log_name='./RQ5/java_1/f2_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_1/f2_codet5p_220m', data_dir='./data/RQ5/java_1_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 0
  eval_ppl = 1.00068
  global_step = 1
  train_loss = 0.4442
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00068
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 10.48 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 1
  eval_ppl = 1.00068
  global_step = 1
  train_loss = 0.3737
  ********************
Previous best ppl:1.00068
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 10.48 	 Previous best codebleu 10.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 2
  eval_ppl = 1.00068
  global_step = 1
  train_loss = 0.4375
  ********************
Previous best ppl:1.00068
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 10.48 	 Previous best codebleu 10.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 3
  eval_ppl = 1.00068
  global_step = 1
  train_loss = 0.4349
  ********************
Previous best ppl:1.00068
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 10.48 	 Previous best codebleu 10.48
  ********************
reload model from RQ5/java_1/f2_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_2/test.jsonl
  codebleu = 7.9 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 7.9 
[0.017241379310344827, 0.029032258064516127, 0.03797817725172102, 0.03846153846153846, 0.05045762541297066, 0.3209786258216798, 0.029268292682926828, 0.03846153846153846, 0.08157630998376403, 0.052941176470588235, 0.04431818181818182, 0.02571217668919666, 0.029347826086956522, 0.04305032927922931, 0.046153846153846156, 0.06585365853658537, 0.3253582974167686, 0.046605672315752196, 0.3911495971232658, 0.042674400572361215, 0.01396830998940639, 0.022518933248275478, 0.029268292682926828, 0.05384615384615384, 0.00020276794409418948, 0.04431818181818182, 0.02920353982300885, 0.02967032967032967, 0.03461538461538462, 0.05709119621674511, 0.030769230769230767, 0.03409090909090909, 0.03885301713788768, 0.007758620689655172, 0.017451604930786388, 0.029268292682926828, 0.02219624587345279, 0.08780487804878048, 0.305597793240271, 0.07364711535893097, 0.013636363636363636, 0.029268292682926828, 0.051219512195121955, 0.046387274813432605, 0.0688964080282791, 0.42821810620430634, 0.029268292682926828, 0.05042003523732404, 0.06, 0.03896103896103895, 0.33885638189558726, 0.097107824485589, 0.024999999999999998, 0.04288535013424039, 0.3285321824598165, 0.0, 0.04685665743044287, 0.038910857998737614, 0.029268292682926828, 0.0375, 0.4194623683340445, 0.06415175295666074, 0.04815611845668949, 0.04390243902439024, 0.046797706948592206, 0.33198924152850245, 0.03846153846153846, 0.02727272727272727, 0.04285714285714285, 0.02692307692307692, 0.05384615384615384, 0.03461538461538462, 0.013636363636363636, 0.029268292682926828, 0.3565262630917669, 0.29709100754734336, 0.05427634766755757, 0.5960942096485482, 0.041379310344827586, 0.03333333333333333, 0.023153945646603055, 0.04891304347826087, 0.36798669066036105, 0.04577558681951053, 0.0375, 0.04772727272727272, 0.055349957797826026, 0.05454545454545454, 0.29552481528773145, 0.04675324675324675, 0.05526315789473684, 0.009375, 0.029268292682926828, 0.032926829268292684, 0.0423076923076923, 0.014233724572109151, 0.3395826039290692, 0.0007094956948356311, 0.03896103896103895, 0.02386363636363636, 0.0498292482520133, 0.029268292682926828, 0.054311853608977397, 0.046153846153846156, 0.0423076923076923, 0.027586206896551724, 0.2938613137040842, 0.03614457831325301, 0.03461538461538462, 0.06538461538461539, 0.0151086613635237, 0.029268292682926828, 0.015384615384615384, 0.007317073170731707, 0.0002533239021058805, 0.015021332742429453, 0.08780487804878048, 0.03846153846153846, 0.012032241809090039, 0.051219512195121955, 0.05030872025688913, 0.049999999999999996, 0.016216216216216217, 0.03506493506493506, 0.306561375364589, 0.03846153846153846, 0.046153846153846156, 0.2466439071233324, 0.0, 0.029268292682926828, 0.029268292682926828, 0.057692307692307696, 0.030769230769230767, 0.03896103896103895, 0.3223958720872033, 0.3223304869139209, 0.06233766233766234, 0.029268292682926828, 0.03793103448275862, 0.08076923076923076, 0.07362338553914922, 0.044399588214132, 0.03534998720007273, 0.023076923076923078, 0.05103271143275472, 0.05454545454545454, 0.03334799512835367, 0.029268292682926828, 0.06818181818181818, 0.007669587519459089, 0.008377329157259845, 0.018898970703367386, 0.05384615384615384, 0.3110894892369726, 0.027027027027027025, 0.3285321824598165, 0.0005928039101636814, 0.046153846153846156, 0.02727272727272727, 0.35565111491590046, 0.0475609756097561, 0.02608695652173913, 0.3548177109547931, 0.05826863939735614, 0.302100766148357, 0.04321396126836524, 0.027027027027027025, 0.0, 0.05454545454545454, 0.013793103448275862, 0.046153846153846156, 0.05384615384615384, 0.04475499807826606, 0.075, 0.06923076923076923, 0.013953488372093023, 0.057142857142857134, 0.34045773196878537, 0.0, 0.029268292682926828, 0.029268292682926828, 0.3906199547329486, 0.0004990567491135724, 0.33036174092440546, 0.02727272727272727, 0.3029978452275344, 0.27906677472626434, 0.012499999999999999, 0.34747659714657564, 0.022469387178553648, 0.18106635154850986, 0.028571428571428567, 0.30150020702791364, 0.23119980271817642, 0.04675324675324675, 0.0, 0.00020276794409418948, 0.019573206619418686, 0.051219512195121955, 0.061538461538461535, 0.032926829268292684, 0.013793103448275862, 0.05398616077767758, 0.0423076923076923, 0.007711803050097511, 0.057692307692307696, 0.04264598215201174, 0.060448653730324674, 0.03571428571428571, 0.027114533038062785, 0.032432432432432434, 0.03846153846153846, 0.05384615384615384, 0.023076923076923078, 0.19919413006252876, 0.030303621826395327, 0.013636363636363636, 0.04675324675324675, 0.19379895809749564, 0.015584415584415584, 0.24741767658038208, 0.029268292682926828, 0.2299674525084578, 0.06296296296296296, 0.3812355616367847, 0.030769230769230767, 0.0, 0.029268292682926828, 0.06174363238456214, 0.029268292682926828, 0.046153846153846156, 0.04675324675324675, 0.1294026700695121, 0.04225934998632716, 0.05966794001981932, 0.05454545454545454, 0.029268292682926828, 0.046575012013907145, 0.02413793103448276, 0.023333333333333334, 0.029268292682926828, 0.29958781528253675, 0.046153846153846156, 0.02727272727272727, 0.023076923076923078, 0.0475609756097561, 0.04772727272727272, 0.029268292682926828, 0.029268292682926828, 0.293799948715573, 0.05581395348837209, 0.011976984065018521, 0.04163717244855782, 0.01923076923076923, 0.056957887014682365, 0.017241379310344827, 0.0, 0.0, 0.05570124976067377, 0.0344663017301881, 0.026152174094734366, 0.07353359432426505, 0.03846153846153846, 0.05555555555555555, 0.03814965652757861, 0.3457038805765606, 0.0237907477850631, 0.04272574327919244, 0.35043426110262776, 0.029268292682926828, 0.03461538461538462, 0.30015057790115174, 0.029268292682926828, 0.036108428434359775, 0.05454545454545454, 0.058536585365853655, 0.02386062103392467, 0.027872973289042428, 0.00023457024054606413, 0.008020359258136605, 0.02413793103448276, 0.0, 0.3901006372521852, 0.046153846153846156, 0.053901483039974586, 0.00023457024054606413, 0.029268292682926828, 0.04675324675324675, 0.0345030975399587, 0.023076923076923078, 0.19919413006252876, 0.049999999999999996, 0.023333333333333334, 0.008358318297727464, 0.046153846153846156, 0.33197995099722055, 0.31255938918956483, 0.011842105263157893, 0.014285714285714284, 0.026373626373626374, 0.050948042467755986, 0.0, 0.006666666666666667, 0.05921052631578947, 0.03488372093023256, 0.0008241107787393142, 0.04273192101558757, 0.3112320682968898, 0.05844155844155844, 0.0008283313594340638, 0.07643358776673119, 0.0423076923076923, 0.03461538461538462, 0.3383030653301121, 0.029268292682926828, 0.40685339556145106, 0.0, 0.2735117125234816, 0.018918918918918916, 0.07692307692307691, 0.049999999999999996, 0.0423076923076923, 0.049999999999999996, 0.021428571428571425, 0.03515375991475846, 0.044666378993541964, 0.350044387751385, 0.042719363146595406, 0.029268292682926828, 0.0067415730337078645, 0.027616657393505623, 0.05075490783725169, 0.2772948038769057, 0.015384615384615384, 0.061538461538461535, 0.0, 0.057692307692307696, 0.2564086913149391, 0.029268292682926828, 0.1596077792735865, 0.02386363636363636, 0.06623376623376623, 0.049999999999999996, 0.05454545454545454, 0.012, 0.03461538461538462, 0.000241420243847319, 0.029268292682926828, 0.027586206896551724, 0.03252630952956215, 0.01923076923076923, 0.0423076923076923, 0.05709822357297709, 0.345436682945624, 0.018816373738748265, 0.04186046511627907, 0.06970464435108044, 0.035408865185713415, 0.057692307692307696, 0.04285714285714285, 0.04024390243902439, 0.0409090909090909, 0.029268292682926828, 0.05113636363636363, 0.024999999999999998, 0.046153846153846156, 0.032432432432432434, 0.06136363636363636, 0.03461538461538462, 0.036585365853658534, 0.03793103448275862, 0.0423076923076923, 0.043373493975903614, 0.25869516145682986, 0.02045454545454545, 0.015, 0.02669411216520866, 0.01923076923076923, 0.051219512195121955, 0.038918209708880436, 0.22050065329163568, 0.03421052631578947, 0.03975903614457831, 0.04468085106382978, 0.04666666666666667, 0.2748018755694242, 0.04801280177166214, 0.03956043956043956, 0.07692307692307691, 0.06428571428571428, 0.0379746835443038, 0.03896103896103895, 0.04653800576279379, 0.03499632643237445, 0.057692307692307696, 0.0375, 0.05131578947368421, 0.21822173417863763, 0.12904510473028033, 0.03107744866587895, 0.19186983271648572, 0.06923076923076923, 0.2879705706605108, 0.06664544315579368, 0.04285714285714285, 0.00029508698089641716, 0.054118896291522486, 0.02413793103448276, 0.05844155844155844, 0.05454545454545454, 0.0, 0.3130453107116026, 0.007317073170731707, 0.04431818181818182, 0.005645200478855432, 0.030769230769230767, 0.02692307692307692, 0.013636363636363636, 0.03409090909090909, 0.36181548588184476, 0.051219512195121955, 0.057692307692307696, 0.07453720893023823, 0.04431818181818182, 0.03626373626373626, 0.22811657329860277, 0.3307636304003667, 0.0, 0.02692307692307692, 0.01923076923076923, 0.023180080314442623, 0.0005642876553915427, 0.04675324675324675, 0.29685217323442853, 0.022469387178553648, 0.005263157894736842, 0.4542305675769438, 0.036585365853658534, 0.020689655172413793, 0.007979859326069169, 0.061841527761619636, 0.273056552998024, 0.023076923076923078, 0.07063146046854707, 0.05823816612284794, 0.03332858222495342, 0.032926829268292684, 0.04482758620689655, 0.036585365853658534, 0.04283268071850897, 0.07894736842105263, 0.07453420305160591, 0.2082116271412683, 0.3347235080082949, 0.036585365853658534, 0.277331680522859, 0.05172413793103448, 0.029268292682926828, 0.0007921764713150371, 0.05895862003988199, 0.02692307692307692, 0.306059234271029, 0.04247787610619469, 0.3045160141340286, 0.05902757774373584, 0.060714285714285714, 0.08414634146341464, 0.023076923076923078, 0.03108684333324418, 0.04991441542231299, 0.049999999999999996, 0.02727272727272727, 0.00019051019251358075, 0.05427339219597774, 0.046788970266111195, 0.023076923076923078, 0.05887586802137625, 0.007692307692307692, 0.046153846153846156, 0.04276651792556869, 0.07834770896078436, 0.029268292682926828, 0.06580884409251066, 0.1988053810030721, 0.04675324675324675, 0.031395348837209305, 0.03846153846153846, 0.03506493506493506, 0.029268292682926828, 0.05384615384615384, 0.07097901618899076, 0.3193855228668339, 0.03846153846153846, 0.03488372093023256, 0.03461538461538462, 0.04329900063583424, 0.029268292682926828, 0.05159417363300629, 0.028235294117647056, 0.029268292682926828]
Finish training and take 22m
