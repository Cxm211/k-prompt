Namespace(log_name='./RQ5/java_1/1_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_1/1_codet5p_220m', data_dir='./data/RQ5/java_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'private java.util.List < TYPE_1 > METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , int VAR_3 , TYPE_4 VAR_4 ) { java.util.ArrayList < TYPE_1 > params = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_5 VAR_5 : VAR_1 . METHOD_2 ( ) ) { final java.util.ArrayList < TYPE_6 > VAR_6 = TYPE_7 . METHOD_3 ( VAR_5 , new TYPE_4 [ ] { VAR_4 } ) ; params . add ( new TYPE_1 ( VAR_5 , VAR_2 , VAR_3 , VAR_6 ) ) ; } return params ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'private java.util.List < TYPE_1 > METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , int VAR_3 , TYPE_4 VAR_4 ) { java.util.ArrayList < TYPE_1 > params = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_5 VAR_5 : VAR_1 . METHOD_2 ( ) ) { final java.util.ArrayList < TYPE_6 > VAR_6 = TYPE_7 . METHOD_3 ( VAR_5 , VAR_4 ) ; params . add ( new TYPE_1 ( VAR_5 , VAR_2 , VAR_3 , VAR_6 ) ) ; } return params ; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 27.9908
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 14.1 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:14.1
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 31.5027
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 19.47 	 Previous best codebleu 14.1
  ********************
 Achieve Best bleu:19.47
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 11.448
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 48.17 	 Previous best codebleu 19.47
  ********************
 Achieve Best bleu:48.17
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 5.7856
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 63.39 	 Previous best codebleu 48.17
  ********************
 Achieve Best bleu:63.39
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 2.8714
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 68.04 	 Previous best codebleu 63.39
  ********************
 Achieve Best bleu:68.04
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 11.8676
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 71.27 	 Previous best codebleu 68.04
  ********************
 Achieve Best bleu:71.27
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 12.8173
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 72.41 	 Previous best codebleu 71.27
  ********************
 Achieve Best bleu:72.41
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 6.7649
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 73.39 	 Previous best codebleu 72.41
  ********************
 Achieve Best bleu:73.39
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 6.2264
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 74.45 	 Previous best codebleu 73.39
  ********************
 Achieve Best bleu:74.45
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 1.7352
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 74.78 	 Previous best codebleu 74.45
  ********************
 Achieve Best bleu:74.78
  ********************
reload model from RQ5/java_1/1_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_1/test.jsonl
  codebleu = 74.25 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  codebleu = 74.25 
[0.19923963657412946, 0.29106973642619605, 0.8422919234660288, 0.5393370720549331, 0.8320330314544029, 0.9000092746228069, 0.6856528250650227, 0.7282175787408902, 0.3068923552159469, 0.8011782298988499, 0.9927616907569681, 0.8295959232537946, 0.6071846621495387, 0.34255612468317126, 0.9003430139277828, 0.8227540616553424, 0.7523277398705996, 0.9342846849350264, 0.9621816403645371, 0.8673567485716778, 0.32970659483700576, 0.2969894531809322, 0.6912594140452217, 0.9000120435131169, 0.43042978422632594, 0.8956609927523811, 0.7944328442208259, 0.733654116601005, 0.6151952248646311, 0.774571731962115, 0.875136182542483, 0.8434248757045004, 0.7321748033833095, 0.18596069379061556, 0.45705106461692346, 0.7071939573916466, 0.9377704204852682, 0.854210129628853, 0.9410056506290296, 0.7467173881179517, 0.6811404084875223, 0.18882301749132502, 0.5672061083260166, 0.8345641796204135, 0.4535640797757121, 0.4163660023820415, 0.6912594140452217, 0.9687688396653635, 0.9061537357063023, 0.7366535801418781, 0.9269331830469714, 0.4535640797757121, 0.7071939573916466, 0.6011679702270646, 0.7478138851365419, 0.7334465130253064, 0.6910453374015129, 0.8855924534744011, 0.6856528250650227, 0.7223144183446568, 0.6647147910077822, 0.7942701366539271, 0.9125104348398385, 0.6779860684959337, 0.5312731871562226, 0.9471534480091441, 0.9607233175453542, 0.843537340596747, 0.8215894539265881, 0.6589309130890797, 0.8310533713486344, 0.8286690242372425, 0.8963926483875864, 0.18882301749132502, 0.9308275480698895, 0.8762476125170207, 0.7237172914694312, 0.5422875137889253, 0.890482671815521, 0.8428952821871947, 0.7621674550453241, 0.8693647831726143, 0.9327169633466876, 0.9044908370314977, 0.9852849226532605, 0.7519894248327487, 0.8855796544773514, 0.9111712209366578, 0.9488183079463388, 0.7671727276894938, 0.9745293328019206, 0.19382300762980428, 0.7125225032796623, 0.9535662529996234, 0.8629423931282016, 0.24523252496546288, 0.8853701119354719, 0.6074096428850606, 0.8696882992240296, 0.8151472726544473, 0.6678120308610653, 0.7168078560285123, 0.8676450786318817, 0.9077471737888583, 0.8774842286953382, 0.8754545414581918, 0.8447228975598562, 0.8813815400934406, 0.7720861332093774, 0.9009840842158907, 0.15726580236954762, 0.6912594140452217, 0.931919905317208, 0.877987617801798, 0.898122990476288, 0.8651491856883211, 0.8850765897260173, 0.9600094284589669, 0.7103265974772611, 0.4045466553609169, 0.15175512142141084, 0.9418090107609731, 0.8765208719204487, 0.945690679454285, 0.9596411809238716, 0.7037147467795737, 0.7501904166176152, 0.932758510312542, 0.4860164667735114, 0.6912594140452217, 0.6912594140452217, 0.9105568726695912, 0.8987135835101596, 0.9515492621717487, 0.9019546896652846, 0.9027159738051327, 0.8486719420369903, 0.6912594140452217, 0.8275239936524709, 0.6850661340471761, 0.9140032864002167, 0.7901423077149385, 0.5960872715467919, 0.5722639854680189, 0.8936973938003048, 0.7399046460615835, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.263014985961873, 0.2518132624074213, 0.27656757441671476, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.7478138851365419, 0.4784356778160315, 0.8888435222418036, 0.6639326194627299, 0.8951837826828467, 0.7313317070419026, 0.7076271835927661, 0.9209893793099855, 0.8748360029466259, 0.8234385994211576, 0.7385229872041738, 0.8678657374052126, 0.4015460426408952, 0.83084241467313, 0.07208934123797124, 0.9041340728227794, 0.8884670476301071, 0.8189995554735204, 0.933286970639994, 0.7478951620294005, 0.3211072601678462, 0.8988412996180373, 0.9214852005355179, 0.6121576162732347, 0.2236670861891925, 0.1810925508313547, 0.9171481786623457, 0.599035177837552, 0.8867171706601364, 0.8502729075207087, 1.0, 0.4348075119429158, 0.21855959991194607, 0.7061680458268889, 0.5269387155748682, 0.17200768251153378, 0.8452307311766434, 0.7856672391579589, 0.9465491069053984, 0.8756429067894516, 0.41725550880556994, 0.43042978422632594, 0.24821074133880205, 0.5758168566943813, 0.9156873229543501, 0.920847271632746, 0.12260103046719396, 0.8989694373603894, 0.7988848397512625, 0.4117110158971776, 0.9168635615364948, 0.9735542568395896, 0.9055256568213023, 0.6781053145008827, 0.09077747067802244, 0.761642782903787, 0.965363836952819, 0.6268790890159027, 0.9407207501028625, 0.09838591466829727, 0.44780586128190936, 0.8080288438271355, 0.9603262193723192, 0.7945631480201435, 0.801169789534145, 0.7170219254969812, 0.1810925508313547, 0.7703813467645078, 0.8387293288825669, 0.95, 0.6912594140452217, 0.465715924844207, 0.7071939573916466, 0.8801547132641002, 0.6912594140452217, 0.9288817233176625, 0.7636095864061366, 0.9487145883454327, 0.7783366598475142, 0.7360764107540074, 0.8017624348471263, 0.6912594140452217, 0.8465816263983615, 0.8811816046232601, 0.7949734042076875, 0.7071939573916466, 0.4348075119429158, 0.829834772754479, 0.7931351046504085, 0.9257462023534273, 0.8921042672694857, 0.8027646328709265, 0.6912594140452217, 0.32392736799457017, 0.723568950051922, 0.9676227264894173, 0.9142150440420558, 0.8706458223016678, 0.9084588579004271, 0.815154955775445, 0.5929022108454984, 0.7235716998691815, 0.8473249685745358, 0.29623953679935455, 0.5587417415347213, 0.1504442348323325, 0.9303179033192857, 0.9029228697393135, 0.8893504469902027, 0.8619179987828729, 0.9509455185101909, 0.8760135800491213, 0.7614959164657727, 0.8646780713427795, 0.6912594140452217, 0.7967226445240709, 0.9052981975214478, 0.7071939573916466, 0.4434777020517846, 0.9131034995733858, 0.9559045711960699, 0.7074795401488326, 0.6331524617665174, 0.27648299671986126, 0.6903773291344846, 0.9005633971565483, 0.08586324702678241, 0.9694667888884985, 0.9396180092663717, 0.8563172970811442, 0.27648299671986126, 0.08170611788221309, 0.9035957587334096, 0.8060819624269768, 0.9351406123825403, 0.09838591466829727, 0.9045061327824662, 0.16867645903410125, 0.17886331717528745, 0.5537022517306248, 0.9205458982768264, 0.6992043393714683, 0.647747611811963, 0.940888866830786, 0.8136155047906588, 0.9384313483327684, 0.5669494717393724, 0.14309533333393243, 0.9361448502736844, 0.9471695160234401, 0.49882561390893554, 0.841875207469964, 0.9234334473517907, 0.8499919362599789, 0.48148796469756233, 0.8425859310747911, 0.7526619374488026, 0.7932369845589845, 0.7705282361327324, 0.6452430999754634, 0.957146397188152, 0.6587171701314622, 0.8478896852982588, 0.9199861380074088, 0.8712005145728838, 0.9821490926793564, 0.8335028039626476, 0.879441572217744, 0.7753542141251331, 0.7225963853375947, 0.9437628092763377, 0.8019309772658032, 0.8053063378718949, 0.6912594140452217, 0.06691709792617241, 0.8066642318142871, 0.9352654750819369, 0.9204440155861249, 0.651002857872442, 0.8926703816314219, 0.465715924844207, 0.9005739349163713, 0.8411630424921515, 0.6912594140452217, 0.16079129017911303, 0.8861708022314685, 0.8925235787478019, 0.8377252262881923, 0.9221557993275098, 0.8979311034464716, 0.933336972426126, 0.3371101009417278, 0.6912594140452217, 0.9288768296987449, 0.8178083071683226, 0.8641269634908533, 0.748194197514225, 0.9214086041237446, 0.8696537204746508, 0.27746815000223046, 0.9809329721714535, 0.9490468241954162, 0.785678266679733, 0.9423575001497, 0.729714905094921, 0.8989415087276855, 0.8630436585308239, 0.8863289621536481, 0.8627830017322995, 0.7071939573916466, 0.9693877551020409, 0.8550806666339565, 0.8435391888743566, 0.8610675718068304, 0.8859614653557839, 0.9422001488455178, 0.10379633866221923, 0.5838615292604841, 0.7400913955844939, 0.8339002983623484, 0.3443778463938027, 0.3237789200892571, 0.8870110715942725, 0.9973062440915319, 0.9258134433447371, 0.896457542865793, 0.9120050406373192, 0.9452585311430763, 0.6831381239952166, 0.885289279145866, 0.6723713204162336, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.7959131487019773, 0.8591254859048258, 0.9598250197069826, 0.9138755716257492, 0.8712775503716599, 0.8323441696598732, 0.8796588813528277, 0.6600459725096746, 0.9642153446749153, 0.9921681817198038, 0.7037475411572638, 0.7280460155792001, 0.9122491485294033, 0.8762638509930809, 0.7523507595770178, 0.756234192312671, 0.906575193438814, 0.9136051415192505, 0.9060328132136453, 0.465715924844207, 0.8505395682648067, 0.6718613566981233, 0.8960971760439522, 0.32893764199003567, 0.9442100823718134, 0.668372111419317, 0.8212324536702587, 0.8957718999086999, 0.961965530245021, 0.5758168566943813, 0.9314651191074765, 0.8985609679543393, 0.9202827191608316, 0.6410378892354339, 0.8919778286066379, 0.8518391018422911, 0.7446841036831817, 0.7915314290630997, 0.9745770345730578, 0.6891891576116927, 0.4480676913038102, 0.8809377467555202, 0.9146241894400875, 0.2270489703592313, 0.12559162040673894, 0.86944691495958, 0.8287194092123789, 0.12719926684717986, 0.9237914376874523, 0.9343267218430718, 0.9350024698180224, 0.895540655184323, 0.4348075119429158, 0.8730499508629959, 0.883136773482629, 0.27031589248499055, 0.8179617939659143, 0.800083937746499, 0.921719438628056, 0.8810554316276455, 0.721264406492447, 0.7229151142178896, 0.7366505750537401, 0.9149306320417432, 0.6195517635451487, 0.9150679539598123, 0.6912594140452217, 0.47783923348528035, 0.6362574204742485, 0.9133731397684426, 0.8243317050386542, 0.7215961844283755, 0.7522642454492299, 0.7935461504124401, 0.9272271558703311, 0.8924875728510193, 0.8362192242070374, 0.8385916102626618, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.8354872098637184, 0.9139708530670974, 0.17976137384295546, 0.7579350824711015, 0.858888441131888, 0.8682672799659632, 0.9152573407816746, 0.9231484983791971, 0.8893823781682744, 0.3205875875415151, 0.9206598027093102, 0.7155660357121529, 0.737061236792362, 0.37882426089982146, 0.9449149097946257, 0.8979197503590246, 0.7071939573916466, 0.9049842403132857, 0.4348075119429158, 0.3507040563922398, 0.8965966751963506, 0.8254935742435682, 0.8514674628624775, 0.9250193662902535, 0.604711911454841, 0.9244093879104769, 0.9121508301280472, 0.6912594140452217]
Finish training and take 3h34m
