Namespace(log_name='./RQ5/java_1/3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_1/3_codet5p_770m', data_dir='./data/RQ5/java_1_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'public int METHOD_1 ( java.lang.String VAR_1 ) { if ( METHOD_2 ( VAR_2 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_2 ) ; } else if ( METHOD_2 ( VAR_3 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_2 ) ; } else { return - 1 ; } }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'public int METHOD_1 ( java.lang.String VAR_1 ) { if ( METHOD_2 ( VAR_2 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_2 ) ; } else if ( METHOD_2 ( VAR_3 , VAR_1 ) ) { return METHOD_3 ( VAR_1 , VAR_3 ) ; } else { return - 1 ; } }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 31.7558
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 28.58 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:28.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 34.7317
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 29.61 	 Previous best codebleu 28.58
  ********************
 Achieve Best bleu:29.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 24.1911
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 36.99 	 Previous best codebleu 29.61
  ********************
 Achieve Best bleu:36.99
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 12.8223
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 60.63 	 Previous best codebleu 36.99
  ********************
 Achieve Best bleu:60.63
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 3.6824
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 67.7 	 Previous best codebleu 60.63
  ********************
 Achieve Best bleu:67.7
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 1.5082
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 71.08 	 Previous best codebleu 67.7
  ********************
 Achieve Best bleu:71.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 0.7038
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 73.93 	 Previous best codebleu 71.08
  ********************
 Achieve Best bleu:73.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 0.5336
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 73.82 	 Previous best codebleu 73.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.2198
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 75.13 	 Previous best codebleu 73.93
  ********************
 Achieve Best bleu:75.13
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 0.1771
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_3/validation.jsonl
  codebleu-4 = 75.17 	 Previous best codebleu 75.13
  ********************
 Achieve Best bleu:75.17
  ********************
reload model from RQ5/java_1/3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_3/test.jsonl
  codebleu = 73.13 
  Total = 500 
  Exact Fixed = 3 
[178, 201, 361]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 3 
[178, 201, 361]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  codebleu = 73.13 
[0.7016265496309229, 0.264358131970707, 0.8419296277117998, 0.5830912642712589, 0.8410886457286899, 0.8476256236323098, 0.49958921843119786, 0.7282175787408902, 0.3068923552159469, 0.8011782298988499, 0.9927616907569681, 0.8648805028408657, 0.6561992930854957, 0.3236926755738183, 0.9003430139277828, 0.8398969187981995, 0.7300563165032914, 0.9342846849350264, 0.9403575354525664, 0.7447335016630307, 0.3473758104729789, 0.2989304139435289, 0.6589232685364614, 0.9000120435131169, 0.43042978422632594, 0.7788482211824655, 0.8197843289967022, 0.7257696188763945, 0.6151952248646311, 0.7984153870943118, 0.8961415113886637, 0.8579781671940192, 0.6661813774368406, 0.605056732171306, 0.46815082712926215, 0.6718197399549267, 0.5824270659839991, 0.8679780508037692, 0.93478710908663, 0.7467173881179517, 0.17666707553209318, 0.6589232685364614, 0.5840230519605066, 0.8603189321064189, 0.45554737673469164, 0.4291760718740731, 0.6589232685364614, 0.976934640027229, 0.7643523007524766, 0.7366535801418781, 0.9269331830469714, 0.45554737673469164, 0.6752430999754634, 0.591267892531106, 0.7478138851365419, 0.6878384332486639, 0.6908192217850397, 0.8855924534744011, 0.49958921843119786, 0.8032801252738178, 0.058079910119824496, 0.7942701366539271, 0.9094559456673585, 0.6779860684959337, 0.6820638883923582, 0.9471534480091441, 0.9607233175453542, 0.7143821550319622, 0.6600924143764231, 0.8063559408133776, 0.8875485500759485, 0.819254979508772, 0.8613619975979245, 0.6589232685364614, 0.9323697483083602, 0.8762476125170207, 0.7033531927872048, 0.7365245039265085, 0.5807763648679797, 0.8180283736032669, 0.7291112224982915, 0.6413597675301194, 0.9327169633466876, 0.9025087901816693, 0.9852849226532605, 0.7519894248327487, 0.7728567160381576, 0.9197682751991088, 0.9488183079463388, 0.7901234236354204, 0.961485854541051, 0.20440990034897852, 0.6811269575222985, 0.9535662529996234, 0.7892870066005413, 0.7743869049892766, 0.8853701119354719, 0.8234153841991039, 0.8696882992240296, 0.8151472726544473, 0.31915325944839656, 0.1775357775008561, 0.8710283872232445, 0.8919104529646031, 0.8639542373717854, 0.7101301147943028, 0.8645816088827822, 0.8686662727523751, 0.7720861332093774, 0.8841057642921581, 0.4468040807457751, 0.6589232685364614, 0.7628408820012711, 0.6648892787085345, 0.898122990476288, 0.1982695067408569, 0.3711392019095685, 0.9600094284589669, 0.7221216537281969, 0.4128088425811616, 0.7480258751276629, 0.9418090107609731, 0.8765208719204487, 0.9461408346030404, 0.8747158648433697, 0.7037147467795737, 0.7428277899789907, 0.8716672886038406, 0.3940118865759852, 0.6589232685364614, 0.6589232685364614, 0.9129875032050465, 0.29028203266451275, 0.9515492621717487, 0.9019546896652846, 0.7346913652989063, 0.8684368528418116, 0.6589232685364614, 0.7593252671434187, 0.6721764843799538, 0.9072172678934285, 0.7898482563516439, 0.7306857787099679, 0.9492523028091309, 0.8076318071499946, 0.6369857512290368, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.26195044036081133, 0.695792309819726, 0.7521995774305402, 0.9298833643858577, 0.7583556269641405, 0.95273941015886, 0.7478138851365419, 0.695792309819726, 0.8888435222418036, 0.6172730003034526, 0.8721459350488947, 0.746196404474651, 0.7173692510217399, 0.9209893793099855, 0.8620700454998175, 0.6382356727541193, 0.7032288695571149, 0.20342668146208326, 0.7549863434493944, 0.7408329358437402, 0.08016249497939767, 0.9041340728227794, 0.8459305832608148, 0.8567354045301243, 0.933286970639994, 0.8285441083529881, 0.32572932081548006, 0.8602332753995905, 1.0, 0.5917005656876095, 0.06007263075352842, 0.655390839220539, 0.9102746416813228, 0.7534461455968529, 0.8085890031173357, 0.6760573542006177, 0.6973656820563914, 0.13741920461433096, 0.8351385434047116, 0.7234415406833986, 0.8038676920269696, 0.22401454014920658, 0.784308188403971, 0.798371529622017, 0.9465491069053984, 0.8756429067894516, 0.59133905787831, 0.43042978422632594, 0.012873225260547932, 0.5687511804094965, 0.7259441387616234, 1.0, 0.9072725521291267, 0.8989694373603894, 0.7988848397512625, 0.4241253179337926, 0.9168635615364948, 0.8112673231226284, 0.9055256568213023, 0.6781053145008827, 0.7248370082925963, 0.7783579555734584, 0.9534190821154946, 0.6210624389742789, 0.9484513298936408, 0.15584881836222492, 0.4116271079550242, 0.8051395267912629, 0.9603262193723192, 0.7228414670507243, 0.801169789534145, 0.7170219254969812, 0.655390839220539, 0.7703813467645078, 0.8643535398863182, 0.823275999618051, 0.6589232685364614, 0.46815082712926215, 0.6718197399549267, 0.8801547132641002, 0.6589232685364614, 0.9288817233176625, 0.7479319152334224, 0.9487145883454327, 0.823130054856027, 0.7360764107540074, 0.7086984307195587, 0.6589232685364614, 0.8465816263983615, 0.3073883688139006, 0.8299980832506018, 0.6718197399549267, 0.45554737673469164, 0.7456820555287096, 0.7955198695646306, 0.9257462023534273, 0.8991089322525483, 0.7249477612399358, 0.6589232685364614, 0.1721904889220881, 0.720459986846073, 0.9676227264894173, 0.9142150440420558, 0.8614770441441622, 0.9159167581563037, 0.8061270356070682, 0.5929022108454984, 0.1994714658450149, 0.8473249685745358, 0.28204077417830653, 0.5839583723687973, 0.10265585995086694, 0.9303179033192857, 0.9029228697393135, 0.7943886128274569, 0.5685449024586822, 0.9375691843973568, 0.6226436831532681, 0.8344531615285882, 0.8646780713427795, 0.5204525623408196, 0.8027136845880052, 0.9052981975214478, 0.6718197399549267, 0.2595427401933015, 0.7748498009032916, 0.9559045711960699, 0.7536137064407555, 0.47908970164538794, 0.30793433386770624, 0.7036991679288631, 0.9005633971565483, 0.6229661941006033, 0.9623590512009765, 0.9396180092663717, 0.24028299936751535, 0.30793433386770624, 0.46093331819064065, 0.9035957587334096, 0.8060819624269768, 0.7737680000005919, 0.15584881836222492, 0.8927948046039869, 0.01420864944618995, 0.21340677605629713, 0.5375737413299527, 0.9205458982768264, 0.6992043393714683, 0.6449272831051709, 0.7249317964735473, 0.8178334933130291, 0.7577238477193765, 0.602746669476317, 0.4011793157284955, 0.9073766759987465, 0.7375619410974997, 0.709649314154076, 0.815476962940123, 0.7355854764535221, 0.8568164816944417, 0.6781752601195755, 0.8342699907959322, 0.7757388605257256, 0.7932369845589845, 0.3624880605395551, 0.6718197399549267, 0.80592278028376, 0.49406650680425845, 0.7664409036424705, 0.91382009996624, 0.8712005145728838, 0.9821490926793564, 0.664386025646081, 0.9199806098801184, 0.7804434274762166, 0.7170100747544315, 0.8045536528703048, 0.7634966594892589, 0.6879281549088984, 0.6589232685364614, 0.8501853697811779, 0.6492943956735663, 0.9097138510617249, 0.6866562038049453, 0.6801949092553191, 0.9375025822296563, 0.46815082712926215, 0.9083369898419058, 0.8411630424921515, 0.6589232685364614, 0.31963971674592223, 0.8878744440414785, 0.8925235787478019, 0.5617174633060669, 0.9221557993275098, 0.8979311034464716, 0.8712611209917995, 0.3597554292633871, 0.6589232685364614, 0.9288768296987449, 0.8178083071683226, 0.8641269634908533, 0.7571560504922936, 0.9214086041237446, 0.8531205107407362, 0.7298792311101, 0.971066645885067, 0.8504194852611375, 0.785678266679733, 0.858336008551728, 0.7233456259389972, 0.9768637002894194, 0.8649812818570359, 0.8863289621536481, 0.8162478916758759, 0.6752430999754634, 0.9693877551020409, 0.7972938462463264, 0.8116970239519281, 0.7321937526513821, 0.8748503542446726, 0.9347001488455178, 0.03867117022575254, 0.5695758149747698, 0.7400913955844939, 0.883410950263467, 0.34459299269328636, 0.4758651661724217, 0.7473318772569635, 0.9586344859265934, 0.9468414240145169, 0.8560433526860873, 0.9120050406373192, 0.8091751557449649, 0.6831381239952166, 0.8734611854236554, 0.7153305068583025, 0.9243498225105617, 0.6622602217780105, 0.8664288754760663, 0.8256495113593998, 0.6682225093889983, 0.7844745133109293, 0.8591254859048258, 0.9598250197069826, 0.9138755716257492, 0.8712775503716599, 0.8267840889288627, 0.8711249838905324, 0.6892443760734666, 0.834578104115165, 0.9921681817198038, 0.7135449662005422, 0.7203156880267696, 0.9122491485294033, 0.8278488581694263, 0.7355596791683106, 0.7421241713737772, 0.7829987393414672, 0.9136051415192505, 0.8992084458999257, 0.46815082712926215, 0.7938150126338898, 0.6640761932093434, 0.23808969904369254, 0.32893764199003567, 0.9239078673473535, 0.7258618476080282, 0.8212324536702587, 0.8957718999086999, 0.964963853850137, 0.5687511804094965, 0.851616422059306, 0.85067316104014, 0.7902750941120615, 0.6410378892354339, 0.8173681140500609, 0.8518391018422911, 0.026863983253709423, 0.7915314290630997, 0.9745770345730578, 0.74212817232249, 0.711791739177658, 0.8839836214081072, 0.9146241894400875, 0.8038676920269696, 0.6613283978673825, 0.8821296454239946, 0.8068372263029933, 0.2265012632745266, 0.8020772238770139, 0.9343267218430718, 0.8024170396206473, 0.7567658001169395, 0.45554737673469164, 0.8800985187243549, 0.8675455448057359, 0.882842712474619, 0.8152995576745317, 0.800083937746499, 0.9001195259803629, 0.74355950364166, 0.4137208983021114, 0.694701412578363, 0.7366505750537401, 0.7360284217811189, 0.6195517635451487, 0.9150679539598123, 0.6589232685364614, 0.6367763829863196, 0.7683001203920434, 0.8679711118078801, 0.7866652583812512, 0.6978162276442237, 0.7522642454492299, 0.7846187729184473, 0.9060342188985211, 0.8924875728510193, 0.8362192242070374, 0.8577456725163606, 0.8629938783941542, 0.7703492851430023, 0.5941403536214046, 0.8713421516632174, 0.905027244174293, 0.4870542725085807, 0.7668955375108737, 0.858888441131888, 0.904352113990756, 0.8178741273904495, 0.8217171956879716, 0.8893823781682744, 0.5206850242130919, 0.9206598027093102, 0.7122334758917799, 0.7080156400459767, 0.6887272735746125, 0.9253835101466485, 0.9069546174929648, 0.6718197399549267, 0.8697827914428056, 0.45554737673469164, 0.2574286830697897, 0.8706923717203505, 0.8254935742435682, 0.8218516000663557, 0.9250193662902535, 0.6530642995290828, 0.9244093879104769, 0.8382732495901645, 0.6589232685364614]
Finish training and take 1h41m
