Namespace(log_name='./RQ5/java_1/2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_1/2_codet5p_770m', data_dir='./data/RQ5/java_1_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'public void METHOD_1 ( TYPE_1 VAR_1 , TYPE_2 VAR_2 ) throws java.io.IOException { if ( ( VAR_3 ) != null ) { VAR_2 . METHOD_2 ( ( ( VAR_1 . METHOD_3 ( ) ) + STRING_1 ) ) ; return ; } getData ( ) ; TYPE_3 . METHOD_4 ( VAR_1 , VAR_2 , TYPE_4 . METHOD_5 ( this . METHOD_6 ( ) . METHOD_7 ( ) , this . METHOD_8 ( ) ) , INT_1 , INT_2 ) ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'public void METHOD_1 ( TYPE_1 VAR_1 , TYPE_2 VAR_2 ) throws java.io.IOException { if ( ( VAR_3 ) != null ) { VAR_2 . METHOD_2 ( ( ( VAR_1 . METHOD_3 ( ) ) + STRING_1 ) ) ; return ; } TYPE_3 . METHOD_4 ( VAR_1 , VAR_2 , TYPE_4 . METHOD_5 ( this . METHOD_6 ( ) . METHOD_7 ( ) , this . METHOD_8 ( ) ) , INT_1 , INT_2 ) ; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 45.9023
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 28.61 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:28.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 38.6217
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 30.02 	 Previous best codebleu 28.61
  ********************
 Achieve Best bleu:30.02
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 21.4245
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 35.27 	 Previous best codebleu 30.02
  ********************
 Achieve Best bleu:35.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 17.958
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 47.47 	 Previous best codebleu 35.27
  ********************
 Achieve Best bleu:47.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 5.997
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 60.11 	 Previous best codebleu 47.47
  ********************
 Achieve Best bleu:60.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 8.1687
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 66.95 	 Previous best codebleu 60.11
  ********************
 Achieve Best bleu:66.95
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 9.5726
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 70.89 	 Previous best codebleu 66.95
  ********************
 Achieve Best bleu:70.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 8.4078
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 71.39 	 Previous best codebleu 70.89
  ********************
 Achieve Best bleu:71.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 7.8376
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 71.34 	 Previous best codebleu 71.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 2.0719
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_2/validation.jsonl
  codebleu-4 = 71.79 	 Previous best codebleu 71.39
  ********************
 Achieve Best bleu:71.79
  ********************
reload model from RQ5/java_1/2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_2/test.jsonl
  codebleu = 67.44 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[83, 110, 344]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[83, 110, 344]
  codebleu = 67.44 
[0.644257524437746, 0.264358131970707, 0.8340268246875062, 0.5830912642712589, 0.8250021179098808, 0.9000092746228069, 0.1651493787765671, 0.73265228411089, 0.3110266504434719, 0.7499094659851777, 0.7224515559475264, 0.8593411581323525, 0.6561992930854957, 0.3281150311082436, 0.6664683696590401, 0.8484683473696282, 0.7316683798683135, 0.9342846849350264, 0.9403575354525664, 0.7163852093351412, 0.4996427791823768, 0.2819488209149206, 0.1871196606988669, 0.9000120435131169, 0.4595964508929925, 0.8923308425391698, 0.7913441158136512, 0.6485670555155822, 0.6060035616850651, 0.7984153870943118, 0.8126245857037546, 0.8579781671940192, 0.7321748033833095, 0.0829436206101795, 0.20542563382459356, 0.6752430999754634, 0.51814847265315, 0.8631042920307476, 0.9410056506290296, 0.7222614550061308, 0.647685804031753, 0.17671147922514893, 0.22385811140021197, 0.7794706509817552, 0.38738891993947816, 0.22523977117075955, 0.6589232685364614, 0.976934640027229, 0.9061537357063023, 0.7366535801418781, 0.9269331830469714, 0.21282239513038326, 0.6752430999754634, 0.34863491652120915, 0.32529047235771064, 0.6895553850072222, 0.6247376768453758, 0.8418822402101853, 0.1651493787765671, 0.8038838089371854, 0.7191646915993712, 0.7942701366539271, 0.18306474787916538, 0.702497344676549, 0.5178161749453096, 0.9471534480091441, 0.8881097127791044, 0.8278676378156142, 0.8629693959820761, 0.8063559408133776, 0.1797373256036649, 0.8264079322343163, 0.8744499895592948, 0.17671147922514893, 0.9337703133336674, 0.8466828299858558, 0.711979924212595, 0.9153385085464256, 0.8674680309570795, 0.8590221290282165, 0.7621674550453241, 0.8693647831726143, 0.9271170184103228, 0.9027337676249234, 0.9852849226532605, 0.7519894248327487, 0.8957659799478881, 0.9187370665691934, 0.9488183079463388, 0.9374655148636681, 0.9745293328019206, 0.005263157894736842, 0.6811269575222985, 0.9535662529996234, 0.8683873290232058, 0.6305247490329959, 0.8853701119354719, 0.8234153841991039, 0.8517236587578707, 0.8037773577392966, 0.6916772828650979, 0.16302323336591107, 0.8528660451405246, 0.6858747671638807, 0.8774842286953382, 0.8853058630953912, 0.8417659021209546, 0.8813815400934406, 0.6855898884283875, 0.9751282885504369, 0.380002616704105, 0.16900413759635244, 0.8568279581548277, 0.2960152630226721, 0.898122990476288, 0.3227825592454476, 0.8947233410879221, 0.9600094284589669, 0.720355886954887, 0.4414039152402614, 0.33513483491413093, 0.9418090107609731, 0.8765208719204487, 0.8804819441021998, 0.951441232094494, 0.7188935477431233, 0.7501904166176152, 0.932758510312542, 0.4008807324531617, 0.17822379424369983, 0.1898698576957259, 0.9105568726695912, 0.8987135835101596, 0.9268830810984099, 0.8510144532694651, 0.9027159738051327, 0.8684368528418116, 0.18793150544575055, 0.8130548071219024, 0.6850661340471761, 0.9140032864002167, 0.7898482563516439, 0.1051223496748106, 0.013138686131386862, 0.8936973938003048, 0.30895503175172684, 0.9584247004495543, 0.6708135801152272, 0.8781840915091574, 0.19325386376106943, 0.021712434136845, 0.2771704281179515, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.32529047235771064, 0.46712678553649734, 0.7729267524512633, 0.6824863253567593, 0.7755781732319521, 0.7282016585353115, 0.7173692510217399, 0.938879555515129, 0.8620700454998175, 0.80542312292921, 0.7800096635595182, 0.2513162061904691, 0.37018219991727486, 0.83084241467313, 0.06188278229446135, 0.8529575180278832, 0.7785101513432741, 0.8567354045301243, 0.933286970639994, 0.8033849935373052, 0.3021181099236727, 0.6398726944711456, 0.9214852005355179, 0.6121576162732347, 0.07813691649029733, 0.16665643886050607, 0.9171481786623457, 0.5905633122685983, 0.8065831427074652, 0.7433119115877092, 1.0, 0.1137669508557052, 0.7287644577202811, 0.2013376189114725, 0.34410911990326887, 0.11961942894005637, 0.2807312921132264, 0.8218670339508176, 0.9465491069053984, 0.8500973277910668, 0.5603036123944464, 0.4595964508929925, 0.012013490744920842, 0.6172616722941056, 0.9156873229543501, 0.920847271632746, 0.3823391311929186, 0.8939403219941204, 0.7988848397512625, 0.1054091048563662, 0.9168635615364948, 0.7366093301097806, 0.9055256568213023, 0.6781053145008827, 0.5185971762728756, 0.7387724244482365, 0.9539957266911472, 0.6268790890159027, 0.9484513298936408, 0.19106059870844724, 0.37294322569012095, 0.768273699175787, 0.9603262193723192, 0.7782344155463395, 0.21918478504696431, 0.6919859363062251, 0.16665643886050607, 0.7703813467645078, 0.8258993621345541, 0.95, 0.17512283012768398, 0.465715924844207, 0.584719525093333, 0.8801547132641002, 0.6589232685364614, 0.9288817233176625, 0.7636095864061366, 0.8863901597822209, 0.8054829960324975, 0.6966050231571487, 0.8017624348471263, 0.17376294948884566, 0.8465816263983615, 0.25405172071414905, 0.8498388617714197, 0.6752430999754634, 0.33547256444104434, 0.7456820555287096, 0.7955198695646306, 0.8519474723251221, 0.8991089322525483, 0.7397190490641127, 0.1898698576957259, 0.20897892055302023, 0.7289172682469276, 0.8961329498816422, 0.9142150440420558, 0.8385028108342278, 0.9275072085040059, 0.8300987484175841, 0.5706653920848883, 0.2304399172273936, 0.5976160054434143, 0.33614958185742777, 0.5572734228300888, 0.11571105155037738, 0.9303179033192857, 0.9029228697393135, 0.8769394563844016, 0.1530050095886313, 0.9309464619988475, 0.7380892896849034, 0.7579036368724879, 0.8646780713427795, 0.20100673101471161, 0.6935145985948887, 0.9052981975214478, 0.17622729422358632, 0.2611634261830301, 0.9131034995733858, 0.9559045711960699, 0.7616934671206385, 0.190826678770764, 0.047436901458443745, 0.18899412153108855, 0.9005633971565483, 0.07282050049547024, 0.9694667888884985, 0.9396180092663717, 0.24510747256729185, 0.05289780487195389, 0.7564701306650601, 0.9035957587334096, 0.8007377532608353, 0.30965110954140257, 0.19106059870844724, 0.9045061327824662, 0.09397842643822271, 0.14324761107074813, 0.5778624281512992, 0.9205458982768264, 0.6778573524320509, 0.647747611811963, 0.8967062969244837, 0.813346886156975, 0.9384313483327684, 0.30053731520441085, 0.15552574135288055, 0.905623370834902, 0.2833836457877678, 0.4820453800363331, 0.8328707697883151, 0.881081214171351, 0.9333651243255969, 0.5194173337060498, 0.37995838133462534, 0.7581405130195333, 0.7932369845589845, 0.18830443772478472, 0.17935645420431434, 0.957146397188152, 0.4229471593490532, 0.8065312050926059, 0.5756489577449904, 0.8712005145728838, 0.9821490926793564, 0.8006459328388965, 0.8236402396506801, 0.7628175563444883, 0.7234025659657828, 0.8789534291463761, 0.8019309772658032, 0.7783104833280248, 0.17358210415666106, 0.6058662662330025, 0.6352168492171396, 0.9606383615912808, 0.38658638952910923, 0.651002857872442, 0.8617752937444394, 0.465715924844207, 0.8862218408956686, 0.8411630424921515, 0.17306009849448598, 0.21504045290911167, 0.7531039658555844, 0.8925235787478019, 0.8377252262881923, 0.9031188655255564, 0.8979311034464716, 0.8678713905315103, 0.3597554292633871, 0.1898698576957259, 0.9288768296987449, 0.8048240638455388, 0.8429699459955828, 0.7580906740108488, 0.9214086041237446, 0.8696537204746508, 0.196258047201319, 0.9809329721714535, 0.9272928688036179, 0.785678266679733, 0.9423575001497, 0.7233456259389972, 0.8989415087276855, 0.9076691773362666, 0.9030791576151858, 0.8225566992917812, 0.6752430999754634, 0.9693877551020409, 0.7363329989490277, 0.791535775084169, 0.6659792594486125, 0.8748503542446726, 0.7329460104412497, 0.3003650302383845, 0.5838615292604841, 0.7346653932925132, 0.8561251633304875, 0.8306542844684646, 0.18371316509866528, 0.8495995945605064, 0.9973062440915319, 0.9468414240145169, 0.896457542865793, 0.9120050406373192, 0.8480712683551452, 0.6831381239952166, 0.9065499028028898, 0.6823713204162336, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.894141073173163, 0.8926396398301681, 0.9634911520696614, 0.7927590354299714, 0.8323441696598732, 0.8449947440899448, 0.7392800700885873, 0.9642153446749153, 0.9921681817198038, 0.7135449662005422, 0.7280460155792001, 0.8624984184233684, 0.31552114033468087, 0.7708128313590381, 0.7485418846203633, 0.8647876223627424, 0.8580760561968515, 0.9060328132136453, 0.1721051761101499, 0.8480918722778938, 0.6718613566981233, 0.8960971760439522, 0.32893764199003567, 0.9442100823718134, 0.5903927094768876, 0.8212324536702587, 0.8749367586330983, 0.961965530245021, 0.6172616722941056, 0.9314651191074765, 0.8985609679543393, 0.9202827191608316, 0.2922721690947231, 0.8276093177092916, 0.9936419998424288, 0.7446841036831817, 0.7672852470438385, 0.9219932436170655, 0.2805759121612378, 0.050655834272019896, 0.8839836214081072, 0.844848523198068, 0.8038676920269696, 0.3504670723190071, 0.6062472723289427, 0.8068372263029933, 0.05591996212248688, 0.6102608369075813, 0.9343267218430718, 0.9350024698180224, 0.895540655184323, 0.2112694227346503, 0.9548877182085735, 0.8825445333927069, 0.3108704158388348, 0.834628460632581, 0.743146378365523, 0.9666327444353364, 0.8810554316276455, 0.4270664522095719, 0.7229151142178896, 0.7366505750537401, 0.9149306320417432, 0.2629050754742709, 0.8884088183653878, 0.6589232685364614, 0.005889702079302481, 0.7744279532022811, 0.9133731397684426, 0.7785330317264519, 0.7065961844283755, 0.29967132258813983, 0.7935461504124401, 0.8436192804127065, 0.9016850690943105, 0.8362192242070374, 0.6898388579859485, 0.8325509095336996, 0.29036221799147155, 0.8140804887209006, 0.6042816825578725, 0.9139708530670974, 0.4320249206318394, 0.7177225970610405, 0.821935617587803, 0.904352113990756, 0.7616729981544289, 0.9230942705072855, 0.923792940978809, 0.18702390271143615, 0.880039352062868, 0.7155660357121529, 0.6743224497378052, 0.16195862993009424, 0.9449149097946257, 0.9069546174929648, 0.17622729422358632, 0.9049842403132857, 0.3468157398101561, 0.25513699642330545, 0.8662179854796597, 0.8254935742435682, 0.8514674628624775, 0.9344409840288144, 0.16492418410683687, 0.9377427212438103, 0.8334428114259033, 0.1898698576957259]
Finish training and take 2h16m
