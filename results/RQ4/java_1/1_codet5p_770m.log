Namespace(log_name='./RQ5/java_1/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_1/1_codet5p_770m', data_dir='./data/RQ5/java_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'private java.util.List < TYPE_1 > METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , int VAR_3 , TYPE_4 VAR_4 ) { java.util.ArrayList < TYPE_1 > params = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_5 VAR_5 : VAR_1 . METHOD_2 ( ) ) { final java.util.ArrayList < TYPE_6 > VAR_6 = TYPE_7 . METHOD_3 ( VAR_5 , new TYPE_4 [ ] { VAR_4 } ) ; params . add ( new TYPE_1 ( VAR_5 , VAR_2 , VAR_3 , VAR_6 ) ) ; } return params ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'private java.util.List < TYPE_1 > METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , int VAR_3 , TYPE_4 VAR_4 ) { java.util.ArrayList < TYPE_1 > params = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_5 VAR_5 : VAR_1 . METHOD_2 ( ) ) { final java.util.ArrayList < TYPE_6 > VAR_6 = TYPE_7 . METHOD_3 ( VAR_5 , VAR_4 ) ; params . add ( new TYPE_1 ( VAR_5 , VAR_2 , VAR_3 , VAR_6 ) ) ; } return params ; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 29.409
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 28.59 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:28.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 32.2859
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
Namespace(log_name='./RQ5/java_1/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_1/1_codet5p_770m', data_dir='./data/RQ5/java_1_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'private java.util.List < TYPE_1 > METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , int VAR_3 , TYPE_4 VAR_4 ) { java.util.ArrayList < TYPE_1 > params = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_5 VAR_5 : VAR_1 . METHOD_2 ( ) ) { final java.util.ArrayList < TYPE_6 > VAR_6 = TYPE_7 . METHOD_3 ( VAR_5 , new TYPE_4 [ ] { VAR_4 } ) ; params . add ( new TYPE_1 ( VAR_5 , VAR_2 , VAR_3 , VAR_6 ) ) ; } return params ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'private java.util.List < TYPE_1 > METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , int VAR_3 , TYPE_4 VAR_4 ) { java.util.ArrayList < TYPE_1 > params = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_5 VAR_5 : VAR_1 . METHOD_2 ( ) ) { final java.util.ArrayList < TYPE_6 > VAR_6 = TYPE_7 . METHOD_3 ( VAR_5 , VAR_4 ) ; params . add ( new TYPE_1 ( VAR_5 , VAR_2 , VAR_3 , VAR_6 ) ) ; } return params ; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 29.409
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 28.59 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:28.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 32.2859
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 26.85 	 Previous best codebleu 28.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 11.4301
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 54.6 	 Previous best codebleu 28.59
  ********************
 Achieve Best bleu:54.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 15.765
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 62.12 	 Previous best codebleu 54.6
  ********************
 Achieve Best bleu:62.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 12.0021
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 65.92 	 Previous best codebleu 62.12
  ********************
 Achieve Best bleu:65.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 1.0608
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 68.23 	 Previous best codebleu 65.92
  ********************
 Achieve Best bleu:68.23
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 2.2485
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 69.23 	 Previous best codebleu 68.23
  ********************
 Achieve Best bleu:69.23
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 1.0365
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 69.82 	 Previous best codebleu 69.23
  ********************
 Achieve Best bleu:69.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 0.5806
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 70.56 	 Previous best codebleu 69.82
  ********************
 Achieve Best bleu:70.56
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 0.4976
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_1_1/validation.jsonl
  codebleu-4 = 70.91 	 Previous best codebleu 70.56
  ********************
 Achieve Best bleu:70.91
  ********************
reload model from RQ5/java_1/1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_1_1/test.jsonl
  codebleu = 68.42 
  Total = 500 
  Exact Fixed = 3 
[122, 246, 265]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 3 
[122, 246, 265]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[83, 344, 419]
  codebleu = 68.42 
[0.7016265496309229, 0.26242122415182373, 0.7495966635081708, 0.5830912642712589, 0.8410886457286899, 0.7188566932539315, 0.15911559173488035, 0.7453406695688418, 0.31245065054578763, 0.8011782298988499, 0.8214974179111438, 0.8419642515728505, 0.5519305565127863, 0.3577117346243811, 0.7894862445965765, 0.8212033537407276, 0.7523277398705996, 0.9342846849350264, 0.9403575354525664, 0.8082862136108027, 0.41402489810879806, 0.29888181721260015, 0.44915895685140894, 0.9000120435131169, 0.27896395761662757, 0.7923996301648651, 0.7469601268298061, 0.7122205990976278, 0.6151952248646311, 0.774571731962115, 0.8961415113886637, 0.8579781671940192, 0.7068603518764851, 0.9267516766457782, 0.5173666127948744, 0.47449431615123216, 0.21, 0.8768056751919203, 0.7654221347446333, 0.7467173881179517, 0.17700538344381897, 0.5204525623408196, 0.10352635778377761, 0.7794706509817552, 0.45554737673469164, 0.43984676261802935, 0.5204525623408196, 0.976934640027229, 0.5285402641293799, 0.7366535801418781, 0.9269331830469714, 0.17710543619464258, 0.5434253582021804, 0.5615880353000414, 0.32439203977003034, 0.6351051312571125, 0.6492675417979843, 0.8580862993268272, 0.15911559173488035, 0.7828072995497823, 0.7191646915993712, 0.7942701366539271, 0.8927919471646453, 0.6537346667026533, 0.4466896516859603, 0.9471534480091441, 0.9358578468147207, 0.843537340596747, 0.48996874674250523, 0.8063559408133776, 0.7541907802490583, 0.8207755285092051, 0.8088626120193172, 0.5204525623408196, 0.9363184178206276, 0.8332003745297123, 0.7042977257325778, 0.6857140444297682, 0.8049908509575898, 0.7768770267609937, 0.7621674550453241, 0.8693647831726143, 0.9327169633466876, 0.8915143559540244, 0.9852849226532605, 0.9055442427482625, 0.8957659799478881, 0.9187370665691934, 0.9488183079463388, 0.698137569002249, 0.961485854541051, 0.20681743118599583, 0.4712050585703913, 0.9535662529996234, 0.7484562465515003, 0.33893955205393644, 0.8853701119354719, 0.2729752802520826, 0.8339464502355458, 0.8151472726544473, 0.6621956347133192, 0.19193390075532388, 0.9165664080615552, 0.9470071513080363, 0.8774842286953382, 0.7902490824051961, 0.8428302000640212, 0.8813815400934406, 0.7705034503687644, 0.7365340639430238, 0.4468040807457751, 0.5204525623408196, 0.7609511392656239, 0.3648137036233186, 0.898122990476288, 0.3188078253760837, 0.8947233410879221, 0.9600094284589669, 0.5704612328195715, 0.14686807693516812, 0.30539321645230383, 1.0, 0.8007922289844391, 0.9461408346030404, 0.9596411809238716, 0.7188935477431233, 0.7501904166176152, 0.932758510312542, 0.4056243443266174, 0.5504525623408196, 0.44915895685140894, 0.9129875032050465, 0.2697645771466738, 0.8882364743521773, 0.9019546896652846, 0.8041902973389762, 0.8684368528418116, 0.5504525623408196, 0.19372098631424234, 0.6850661340471761, 0.8893137065493062, 0.7574224697589155, 0.7306857787099679, 0.5181146293435839, 0.8555613027817166, 0.6574342277128217, 0.9584247004495543, 0.5993279700821248, 0.871607281170603, 0.14421826611208569, 0.664779776290601, 0.7521995774305402, 0.871415387430881, 0.7583556269641405, 0.9444753607840557, 0.32439203977003034, 0.47252559631739766, 0.7492071148893855, 0.657410880332295, 0.9042446421916496, 0.7656823224223759, 0.7173692510217399, 0.9209893793099855, 0.8623527611587013, 0.8234385994211576, 0.7499767428936083, 0.8678657374052126, 0.0, 0.83084241467313, 0.12463441524890438, 0.8968762462602038, 0.8754463556825656, 0.8567354045301243, 0.8399026867849992, 0.8285441083529881, 0.32742935844964566, 0.8636154656137816, 0.9214852005355179, 0.5598504185364119, 0.05574762823727461, 0.5204525623408196, 0.9171481786623457, 0.9462104606084212, 0.8867171706601364, 0.7264480419399921, 1.0, 0.0687533327226353, 0.2989107981376618, 0.7506228403443113, 0.22860306512497258, 0.236897992876745, 0.8026456274559706, 0.7433706060903413, 0.6703112997540144, 0.8756429067894516, 0.575230737653569, 0.27896395761662757, 0.03388953075391069, 0.29688265066772396, 0.9156873229543501, 0.920847271632746, 0.2082860157672433, 0.8939403219941204, 0.8219617628281854, 0.10824223120235288, 0.9168635615364948, 0.8829817865379768, 0.8726646856765728, 0.6781053145008827, 0.5446644940013464, 0.6110753558851567, 0.011235657881272194, 0.6210624389742789, 0.9484513298936408, 0.18477101144176725, 0.3906232327016752, 0.8051395267912629, 0.9603262193723192, 0.7945631480201435, 0.013278388278388276, 0.7092041668758533, 0.5204525623408196, 0.7703813467645078, 0.82267543745725, 0.95, 0.5204525623408196, 0.27953495387438826, 0.1794696606684134, 0.8801547132641002, 0.5204525623408196, 0.7333474434255911, 0.7636095864061366, 0.9487145883454327, 0.7878359372089682, 0.7501270425469592, 0.6918400950140347, 0.5204525623408196, 0.7388601329850621, 0.24617440796873805, 0.8498388617714197, 0.47449431615123216, 0.45554737673469164, 0.8651706733544999, 0.7866703281288948, 0.8519474723251221, 0.9782294333535642, 0.7730519120573912, 0.44915895685140894, 0.1721904889220881, 0.7179184215887563, 0.9676227264894173, 0.9142150440420558, 0.8761507136068414, 0.6126479622812752, 0.815154955775445, 0.5977946978289542, 0.24760075097288797, 0.8473249685745358, 0.34320968417037434, 0.6250294800833973, 0.11781787059040486, 0.8667248040152794, 0.9029228697393135, 0.8781617655249798, 1.0, 0.9446955185101911, 0.8760135800491213, 0.8130915379555661, 0.8069022986864269, 0.5504525623408196, 0.6924531930616975, 0.9240481975214478, 0.5402543267970705, 0.2867524123750499, 0.7748498009032916, 0.9559045711960699, 0.7193059469252053, 0.15937640511821197, 0.09866439528628082, 0.6937366667892881, 0.803540483161653, 0.10144351161126228, 0.8104011330525134, 0.803846354939513, 0.3648449137853973, 0.09652373294974698, 0.38524075628129806, 0.9035957587334096, 0.8060819624269768, 0.9351406123825403, 0.18477101144176725, 0.7956631332183013, 0.023357555205503356, 0.16118743048204742, 0.5778624281512992, 0.8664629245056451, 0.6992043393714683, 0.647747611811963, 0.9061103812094513, 0.8020158801704518, 0.8109591956041302, 0.4005496164809971, 0.16202645656284342, 0.9073766759987465, 0.9471695160234401, 0.7470090303739755, 0.841875207469964, 0.6726403680004475, 0.8568164816944417, 0.6782596415213834, 0.368369328665829, 0.7450427444100914, 0.8305917930948892, 0.35448469214867717, 0.5402543267970705, 0.957146397188152, 0.49406650680425845, 0.843728854340388, 0.8223936284311923, 0.8712005145728838, 0.916764627201798, 0.7607321869881019, 0.9345537795444867, 0.7677936911393097, 0.2385967255837931, 0.7741007261360716, 0.8019309772658032, 0.7979540041454632, 0.44915895685140894, 0.10305209910931498, 0.6434988093781844, 0.9606383615912808, 0.8371602318125443, 0.651002857872442, 0.8282154153914936, 0.16858262175175878, 0.8862218408956686, 0.8411630424921515, 0.44915895685140894, 0.15974106715443617, 0.7422623548021801, 0.7949109004766297, 0.7447510994211576, 0.9015946729677908, 0.7637125093277769, 0.941246887425869, 0.3001751027939579, 0.44915895685140894, 0.9288768296987449, 0.7719509921222071, 0.644916435580206, 0.7571560504922936, 0.9214086041237446, 0.8531205107407362, 0.6024423384099458, 0.9809329721714535, 0.942663845472012, 0.785678266679733, 0.858336008551728, 0.18137449799434013, 0.8989415087276855, 0.7882961678471544, 0.9523104352365284, 0.8033429247393458, 0.5434253582021804, 0.9093299715548298, 0.7938917818132522, 0.8185391888743567, 0.7999299484331257, 0.8748503542446726, 0.9347001488455178, 0.4230262949248552, 0.5695758149747698, 0.7400913955844939, 0.8912314882918398, 0.3468649068256656, 0.16066687706176688, 0.8458670871319929, 0.9271014990617826, 0.9037543380116166, 0.896457542865793, 0.8957740172211828, 0.8091751557449649, 0.1271907830675226, 0.8734611854236554, 0.7223713204162335, 0.8468379252081462, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6487095547841154, 0.8125819450296324, 0.8591254859048258, 0.9598250197069826, 0.9634911520696614, 0.7927590354299714, 0.8323441696598732, 0.8711249838905324, 0.6641372714292906, 0.9642153446749153, 0.9921681817198038, 0.7135449662005422, 0.7280460155792001, 0.8847785200990763, 0.8762638509930809, 0.7653130811687899, 0.7485418846203633, 0.8457206569057734, 0.9136051415192505, 0.9060328132136453, 0.05477578818858468, 0.8505395682648067, 0.6602080325299485, 0.8341745152833281, 0.32893764199003567, 0.9442100823718134, 0.6721612525183664, 0.8028729803527197, 0.8957718999086999, 0.961965530245021, 0.29688265066772396, 0.8488436137202967, 0.8985609679543393, 0.7553091566674299, 0.6410378892354339, 0.8919778286066379, 0.8518391018422911, 0.3394262600907618, 0.5107005450521716, 0.043864239705777026, 0.74212817232249, 0.4655082719140896, 0.7544341783026602, 0.7207539160821423, 0.34410911990326887, 0.522971034609462, 0.8821296454239946, 0.7466907745857096, 0.049942264678882284, 0.5401743569042998, 0.9343267218430718, 0.8663071368751374, 0.7981150023892631, 0.06699212654307206, 0.9548877182085735, 0.7779671406571498, 0.8228427124746189, 0.5420306098529301, 0.800083937746499, 0.921719438628056, 0.8810554316276455, 0.5643060677130275, 0.6842317563908451, 0.7366505750537401, 0.9149306320417432, 0.6195517635451487, 0.9150679539598123, 0.5204525623408196, 0.6686551030769822, 0.7778724295140381, 0.6891591748305843, 0.8243317050386542, 0.7065961844283755, 0.26516711700007345, 0.7935461504124401, 0.8470010960733048, 0.8924875728510193, 0.8362192242070374, 0.8893062778978682, 0.8187688443406063, 0.9210210949858122, 0.9015200916626787, 0.5834483516641882, 0.9139708530670974, 0.4320249206318394, 0.772616988243183, 0.833896950628271, 0.904352113990756, 0.9152573407816746, 0.8104931498439958, 0.8893823781682744, 0.49657461872946673, 0.9206598027093102, 0.651188477566869, 0.7432816303642229, 0.11400752279106138, 0.9163995434603551, 0.8863108047128694, 0.5402543267970705, 0.8538025850545625, 0.06699413453386091, 0.25513699642330545, 0.852434776462647, 0.8254935742435682, 0.8514674628624775, 0.9250193662902535, 0.15871867356036917, 0.9244093879104769, 0.7372055633450055, 0.44915895685140894]
Finish training and take 1h47m
