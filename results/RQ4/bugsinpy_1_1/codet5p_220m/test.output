class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or
class QuarterOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):         return type(dtindex)._simple_new(             None, freq
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             steps_per_epoch = len(generator)          enqueuer = None          if is_sequence:             steps_per_epoch = len(generator)          if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             steps_per_epoch = len(generator)          if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             steps_per_epoch = len(generator)          if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             steps_per_epoch = len(generator)          if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             steps_per_epoch = len(generator)          if not is_sequence and use_multiprocessing and workers > 1:             warnings
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)
from difflib import get_close_matches  from functools import wraps  from decorator import decorator  import tempfile  import os  from decorator import decorator  from functools import get_close_matches  from difflib import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from functools import get_close_matches  from
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  return dependant def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  return dependant def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)
class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.        class ListParameter(Parameter):         Ensure that list
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,)) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,)) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,)) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,)) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,)) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,)) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest
class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d) class GroupBy(_GroupBy):
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head!= "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag
def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" )+                      if not missing.
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)          for task in tasks:             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers             if task.status == 'RUNNING':             in_workers = assistant or worker in
def _isna_ndarraylike(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):         if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):         if hasattr(obj, "__array__"):          return _isna_
class LinuxHardware(Hardware):              mtab_entries.append(fields)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          mount_size = int(mount_size)          if not mount_size:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return          if not uuids:          return          if not mount:          return          if not device:          return          if not device:          return
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                         'and multiple workers may duplicate your data.'                         'Please consider using the`keras.utils.Sequence'                         'class.'))      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' )      if steps_done == 0:         if is_sequence:              steps_done = len(generator)          else:              raise ValueError('`steps_done=0` is only valid for a generator' )      if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                         'and multiple workers may duplicate your data.'                         'Please consider using the`keras.utils.Sequence'                         'class.'))      if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                         'and multiple workers may duplicate your data.'                         'Please consider using the`keras.utils.Sequence'                         'class.'))      if steps_done == 0:         if is_sequence:              steps_done = len(generator)          else:              raise ValueError('`steps_done=0` is only valid for a generator' )      if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                         'and multiple workers may duplicate your data.'                         'Please consider using the`keras.utils.Sequence'                         'class.'))      if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                         'and multiple workers may duplicate your data.'                         'Please consider using the`keras.utils.Sequence'                         'class.'))
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem
def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url) forms = root.xpath('      if not forms:          raise ValueError("No <form> element found in %s" % response)      formname = formname.strip()      formid = formid.strip()      formnumber = formnumber.strip()      formxpath = formxpath.strip()      if not formname:          raise ValueError("No <form> element found in %s" % response)      if not formid:          raise ValueError("No <form> element found in %s" % response)      if not formnumber:          raise ValueError("No <form> element found in %s" % response)      if not formxpath:          raise ValueError("No <form> element found in %s" % response)      formname = formname.strip()      formid = formid.strip()      formnumber = formnumber.strip()      if not formname:          raise ValueError("No <form> element found in %s" % response)      if not formid:          raise ValueError("No <form> element found in %s" % response)      if not formnumber:          raise ValueError("No <form> element found in %s" % response)      if not formxpath:          raise ValueError("No <form> element found in %s" % response)      formname = formname.strip()      formid = formid.strip()      formnumber = formnumber.strip()      if not formname:          raise ValueError("No <form> element found in %s" % response)      if not formid:          raise ValueError("No <form> element found in %s" % response)      if not formnumber:          raise ValueError("No <form> element found in %s" % response)      if not formxpath:          raise ValueError("No <form> element found in %s" % response)      formname = formname.strip()      formid = formid.strip()      formnumber = formnumber.strip()      if not formname:          raise ValueError("No <form> element found in %s" % response)      if not formid:          raise
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name class APIRouter(routing.Router):                      response
def _isna_ndarraylike_new(obj):  return result  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return  def notna(obj):      return
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Progbar(target=steps)                     if verbose == 1:                  progbar = Prog
class EmptyLineTracker:                  return 0, 0              newlines = 2              if current_line.depth:                  newlines -= 1              if current_line.depth == 1:                  return 0, 0              if current_line.depth == 2:                  return 0, 0              if current_line.depth == 3:                  return 0, 0              if current_line.depth == 4:                  return 0, 0              if current_line.depth == 5:                  return 0, 0              if current_line.depth == 6:                  return 0, 0              if current_line.depth == 7:                  return 0, 0              if current_line.depth == 8:                  return 0, 0              if current_line.depth == 9:                  return 0, 0              if current_line.depth == 10:                  return 0, 0              if current_line.depth == 11:                  return 0, 0              if current_line.depth == 12:                  return 0, 0              if current_line.depth == 13:                  return 0, 0              if current_line.depth == 14:                  return 0, 0              if current_line.depth == 15:                  return 0, 0              if current_line.depth == 16:                  return 0, 0              if current_line.depth == 17:                  return 0, 0              if current_line.depth == 18:                  return 0, 0              if current_line.depth == 19:                  return 0, 0              if current_line.depth == 20:                  return 0, 0              if current_line.depth == 21:                  return 0, 0              if current_line.depth == 22:                  return 0, 0              if current_line.depth == 23:                  return 0, 0              if current_line.depth == 24:                  return 0, 0              if current_line.depth == 25:                  return 0, 0              if current_line.depth == 26:                  return 0, 0              if current_line.depth == 27:                  return 0, 0              if current_line.depth == 28:                  return 0, 0              if current_line.depth == 29:                  return 0, 0              if current_line.depth == 30:                  return 0, 0              if current_line.depth == 31:                  return 0, 0              if current_line.depth == 32:                  return 0, 0              if current_
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
class TestProcessProtocol(protocol.ProcessProtocol):      def __init__(self):          self.deferred = defer.Deferred()         self.out = ''         self.err = ''          self.exitcode = None      def outReceived(self, data):          self.out = data          self.err = ''          self.exitcode = None      def errReceived(self, data):          self.err = data          self.out = ''          self.err = ''          self.exitcode = None      def exitReceived(self, data):          self.exitcode = data          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.exitcode = None      def exitReceived(self, data):          self.exitcode = data          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.out = ''          self.err = ''          self.
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[
def create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields
class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0] == data[1]          result[0] = result[1]          assert data[0]
from pandas.core.dtypes.common import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,     is_period_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,     is_timedelta64_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.generic import (      ABCDataFrame,     ABCDatetimeArray,      ABCExtensionArray,      ABCIndexClass,      ABCMultiIndex,      ABCSeries,     ABCTimedeltaArray,  )  from pandas.core.dtypes.inference import is_list_like  from pandas.core.dtypes.common import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,     is_period_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,     is_timedelta64_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.generic import (      ABCDataFrame,     ABCDatetimeArray,      ABCExtensionArray,      ABCIndexClass,      ABCMultiIndex,      ABCSeries,     ABCTimedeltaArray,  )  from pandas.core.dtypes.inference import is_list_like  from pandas.core.dtypes.common import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:          if disable:              self.iterable =
from pandas.core.frame import DataFrame  from pandas.core.groupby import ops  from pandas.core.groupby.categorical import recode_for_groupby, recode_from_groupby  from pandas.core.indexes.api import CategoricalIndex, Index, MultiIndex  from pandas.core.series import Series  from pandas.io.formats.printing import pprint_thing
def disp_trim(data, length):          if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data
class InfoExtractor(object):      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         try:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format.update(f)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self, spider):         return self.scraper.spider_is_idle(spider)      def spider_is_idle(self,
class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan
class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(to_concat) class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(to_concat) class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(to_concat) class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(to_concat) class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(to_concat) class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(to_concat) class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def close_fd(self, fileobj):          if not fileobj: return                  if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def close(self):          if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def close_fd(self, fileobj):          if not fileobj: return                  if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def close_fd(self, fileobj):          if not fileobj: return                  if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events = list(handler), list(events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          handler, events =
def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)
def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str
class CategoricalBlock(ExtensionBlock):      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep
class ItemMeta(ABCMeta): new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal from pandas.core.dtypes.common import ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_c
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',','')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)      if day_first:      date_str = date_str.replace(',','')     return unified_timestamp(date_str, day_first=False) def extract_timezone(date_str):      date_str = date_str.replace(',','')     return timezone, date_str def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',','')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)      if day_first:      date_str = date_str.replace(',','')     return unified_timestamp(date_str, day_first=False) def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',','')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)      if day_first:      date_str = date_str.replace(',','')     return unified_timestamp(date_str, day_first=False) def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',','')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)      if day_first:      date_str = date_str.replace(',','')     return unified_timestamp(date_str, day_first=False) def unified_timestamp(date_str, day_first=True):      date_str = date_
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)     executor = ProcessPoolExecutor(max_workers=worker_count)      try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_formatting(              if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:          loop.run_until_complete(              schedule_
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (original)"  if write_back == write_back.DIFF:          src_name = f"{src}  (original)"  if write_back == write_back.YES:          src_name = f"{src}  (
def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")          good = self._codes!= -1          if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skipna: TLSConfigis the fixed version  if not good.all():              if skip
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop) class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not
from __future__ import unicode_literals from __future__ import print_function from __future__ import division from __future__ import absolute_import from __future__ import division from __future__ import absolute_import from __future__ import print_function from __future__ import division from __future__ import absolute_import from __future__ import division from __future__ import print_function from __future__ import division from __future__ import absolute_import from __future__ import division from __future__ import division from __future__ import absolute_import from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from __future__ import division from
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition           security_definition = jsonable_encoder(              security_requirement.security_
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          else:             return state.get_pending_tasks()      def is_trivial_worker(self, state):             return state.get_pending_tasks()      def get_pending_tasks(self):             return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          else:             return state.get_pending_tasks()      def get_running_tasks(self):             return six.moves.filter(lambda task: task.status in [RUNNING], self.tasks)          else:             return state.get_running_tasks()      def is_trivial_worker(self, state):             return state.get_running_tasks()      def get_pending_tasks(self):             return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          else:             return state.get_pending_tasks()      def get_running_tasks(self):             return six.moves.filter(lambda task: task.status in [RUNNING], self.tasks)          else:             return state.get_running_tasks()      def is_trivial_worker(self, state):             return state.get_running_tasks()      def get_pending_tasks(self):             return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          else:             return state.get_pending_tasks()      def get_running_tasks(self):             return six.moves.filter(lambda task: task.status in [RUNNING], self.tasks)          else:             return state.get_running_tasks()      def is_trivial_worker(self, state):             return state.get_running_tasks()      def get_pending_tasks(self):             return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          else:             return state.get_pending_tasks()      def get_running_tasks(self):             return six.moves.filter(lambda task: task.status in [RUNNING], self.tasks)          else:             return state.get_running_tasks
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)          def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override start_requests method instead."              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude
class Request(object_ref):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))
def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)
from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.missing import notna  from pandas.core import algorithms  from pandas.core import algorithms  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.missing import notna  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose) class DataFrame(NDFrame):          dtype: object          nv.validate_
class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode =='min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0          if (self.mode =='min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0          if (self.mode =='min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0          if (self.mode =='min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0          if (self.mode =='min' or             (self.mode == 'auto and 'acc' not
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, "same" or "valid".          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(other):              if not op.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return
class ErrorsWithCodes(object):      class ErrorsWithCodes(object):      def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()
class MissingParameterException(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              else:                     raise MissingParameterException(x) class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              else:                     raise MissingParameterException(x) class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              else:                     raise MissingParameterException(x) class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              else:                     raise MissingParameterException(x) class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              else:                     raise MissingParameterException(x) class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              else:                     raise MissingParameterException(x) class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_
class HiveCommandClient(HiveClient):              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              if partition is None:              stdout = run_hive_cmd('use
title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text>)")  info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")
class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value) class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = val_data              return val_enqueuer_gen def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = val_data              return val_enqueuer_gen def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = val_data              return val_enqueuer_gen def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = val_data              return val_enqueuer_gen def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = val_data              return val_enqueuer_gen def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data
try:  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import GzipFile  import six except ImportError  from gzip import
class ImageDataGenerator(object):              The inputs, normalized.         if self.preprocessing_function:             x = self.preprocessing_function(x)          if self.rescale:              x *= self.rescale          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return self
def base_url(url):      def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None if re.match(r'^(?:https?:)?          return path if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path)
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_
class BinGrouper(BaseGrouper):              ngroups,          )      @cache_readonly      def result_index(self):          if len(self.binlabels)!= 0 and isna(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return          if len(self.binlabels)!= 0 and len(self.binlabels[0]):          return
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'      if md5_hash is not None and file_hash is None:          file_hash = md5_hash
class HTTPRequest(HTTPMessage):          headers = dict(self._orig.headers)         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for name, value in headers.items()]         if 'Host' not in headers:              headers['Host'] = url
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences)
class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware) class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.
def should_series_dispatch(left, right, op):     return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises          ------          TypeError TypeError the fixed version!          Raises
class BarPlot(MPLPlot):     def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))         ax.set_xticks(self.tick_pos)         ax.set_xticklabels(ticklabels)          if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          return response         if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x
class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1)) if isinstance(labels.levels[0], MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)
and Index.__new__.  These should not depend on core.internals.  from typing import TYPE_CHECKING, Any, Optional, Sequence, Union, cast  import numpy as np  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from core.internals import Index  from
default 'raise'              )          new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=self.freq)
def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)  def match(command, settings):     def get_new_command
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None      if output_padding is None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return None      if output_padding == None:          if padding == 'valid': return
def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )                     def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_LOADER_CLASS',                             settings.get('SPIDER_MANAGER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):             return v         if v.startswith("'"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:                  '\\\\': '\\\\'}, v)              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\', Pidis the fixed version:
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer, arr_value):              pass                     if is_empty_indexer(indexer
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: TLSConfigis the fixed version def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values class _Window(PandasObject, SelectionMixin):              except (ValueError
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):         if isinstance(maybe_slice, slice):              return self[maybe_slice]         taken = ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )         freq = self.freq if is_period_dtype(self) else None         assert taken.freq == freq, (taken.freq, freq, taken)         return self._shallow_copy(taken, freq=freq)      _can_hold_na = True      _na_value = NaT
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  )  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml  )  @click.option(      "--config",      type=click.Path(         exists=False, file
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)
class XportReader(abc.Iterator):              if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script)
import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common import InfoExtractor  from..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  ) import re  from.common
from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__) if not reactor:     reactor = defer.defer() else:     from twisted.internet
class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:              self.depth -= 1             self._lambda_arguments -= 1              return True          return False
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first
class LocalCache(collections.OrderedDict):         self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value)
from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from.common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import
from.common import InfoExtractor  from..utils import (      compat_urllib_parse,      ExtractorError,  )  def _media_xml_tag(tag): if not tag: return  if not tag: return  if not tag.startswith('<'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'): return  if not tag.startswith('>'): return  if not tag.endswith('>'
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop)              return task      def __await__(self):          task = asyncio.ensure_future(self.serve_coro)
class DataFrame(NDFrame): return new_data     def _combine_match_index(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return          if ops.should_series_dispatch(self, other, func):          return
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return self._set_name(index, inplace=kwargs.get("inplace"))         return super().rename(index=index, **kwargs)      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)     class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return self._set_name(index, inplace=kwargs.get("inplace"))         return super().rename(index=index, **kwargs)      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)     class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return self._set_name(index, inplace=kwargs.get("inplace"))         return super().rename(index=index, **kwargs)      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)     class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return self._
class RedirectMiddleware(BaseRedirectMiddleware):          def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) or                response.status in getattr(spider, 'handle_httpstatus_list', [])):              return response          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == 'HEAD':              return          if request.method == '
