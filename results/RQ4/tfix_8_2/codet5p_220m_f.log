Namespace(log_name='./RQ5/tfix_8_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_8_2/codet5p_220m_f', data_dir='./data/RQ5/tfix_8_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0143
  global_step = 2
  train_loss = 1.6232
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0143
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 9.83 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.01054
  global_step = 3
  train_loss = 1.6316
  ********************
Previous best ppl:1.0143
Achieve Best ppl:1.01054
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 16.77 	 Previous best codebleu 9.83
  ********************
 Achieve Best bleu:16.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00879
  global_step = 4
  train_loss = 1.1155
  ********************
Previous best ppl:1.01054
Achieve Best ppl:1.00879
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 25.54 	 Previous best codebleu 16.77
  ********************
 Achieve Best bleu:25.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00786
  global_step = 5
  train_loss = 0.884
  ********************
Previous best ppl:1.00879
Achieve Best ppl:1.00786
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 29.49 	 Previous best codebleu 25.54
  ********************
 Achieve Best bleu:29.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00721
  global_step = 6
  train_loss = 0.7554
  ********************
Previous best ppl:1.00786
Achieve Best ppl:1.00721
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 29.0 	 Previous best codebleu 29.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00686
  global_step = 7
  train_loss = 0.7344
  ********************
Previous best ppl:1.00721
Achieve Best ppl:1.00686
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 34.8 	 Previous best codebleu 29.49
  ********************
 Achieve Best bleu:34.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00673
  global_step = 8
  train_loss = 0.5051
  ********************
Previous best ppl:1.00686
Achieve Best ppl:1.00673
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 42.92 	 Previous best codebleu 34.8
  ********************
 Achieve Best bleu:42.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00661
  global_step = 9
  train_loss = 0.4524
  ********************
Previous best ppl:1.00673
Achieve Best ppl:1.00661
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 43.69 	 Previous best codebleu 42.92
  ********************
 Achieve Best bleu:43.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00654
  global_step = 10
  train_loss = 0.3812
  ********************
Previous best ppl:1.00661
Achieve Best ppl:1.00654
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 48.01 	 Previous best codebleu 43.69
  ********************
 Achieve Best bleu:48.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00651
  global_step = 11
  train_loss = 0.4044
  ********************
Previous best ppl:1.00654
Achieve Best ppl:1.00651
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 49.72 	 Previous best codebleu 48.01
  ********************
 Achieve Best bleu:49.72
  ********************
reload model from RQ5/tfix_8_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_2/test.jsonl
  codebleu = 45.87 
  Total = 500 
  Exact Fixed = 2 
[330, 469]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 8 
[166, 188, 198, 336, 439, 442, 474, 475]
  ********************
  Total = 500 
  Exact Fixed = 2 
[330, 469]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 8 
[166, 188, 198, 336, 439, 442, 474, 475]
  codebleu = 45.87 
[0.004467251601661405, 0.6932410335616551, 0.5619403469121602, 0.31331096368700495, 0.6058511595444083, 0.8821894623768516, 0.0, 0.47764341900423196, 0.012981040677025227, 0.1482758620689655, 0.5954509413586904, 0.4709924357489944, 0.15393576085345756, 0.7569706667451968, 0.6196949179201965, 0.42315231289059363, 0.5007520143098162, 0.7099766481408452, 0.19685772626843445, 0.3457148616682819, 0.6716511056875383, 0.39323313167877133, 0.16616888802848806, 0.5929824561403508, 0.36185501797118336, 0.00025071589913224434, 0.21197518632249268, 0.13748606554770823, 0.1498798672939852, 0.2855072113105671, 0.30574281427215577, 0.0, 0.3059710472232345, 0.927299329695247, 0.27255205652507636, 0.6068905779332499, 0.0, 0.0788325540705002, 0.7155056700329363, 0.6398412446099295, 0.3067349239306456, 0.8230253102708935, 0.10202520055807765, 0.3434399630195796, 0.7183643415906251, 0.009484931399168412, 0.6570057594483909, 0.274670867776058, 0.2649676652339215, 0.012832037205933667, 0.350381564995214, 0.6433348104458009, 0.30052152664325354, 0.3122474077087937, 0.0, 0.7028480321005652, 0.1540411422937383, 0.29168534789351946, 0.8460094492769958, 0.5346948636390094, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.7698918124013976, 0.0039441765910670275, 0.6197233100748608, 0.30365628137296197, 0.4706502402815602, 0.09554194697229128, 0.28269688894370654, 0.24557667578174203, 0.15554004069650706, 0.8212946104630277, 0.5614571148171295, 0.3084306723908125, 0.2391512081618386, 0.004669260700389105, 0.7127716550262229, 0.34076903099596423, 0.5411181740984977, 0.591516286342532, 0.2839528341415056, 0.6239994697065766, 0.32660140079823796, 0.6359585563603126, 0.5451417837590342, 0.39591846322398483, 0.5844322860768962, 0.4000205799596478, 0.3591365261541136, 0.6108560633180458, 0.27374575900475356, 0.46017329951827773, 0.45579909520741635, 0.0, 0.27435148893726036, 0.8349004181959196, 0.8642415207541971, 0.7962637890346522, 0.18328359266524735, 0.6812985771278143, 0.9164266028701744, 0.7275723927836586, 0.6445931574752105, 0.7834843745068932, 0.0, 0.006833726340267106, 0.8399114554115357, 0.8049637497745419, 0.003361369395482507, 0.5632378151805969, 0.5876935279546878, 0.005415428246897094, 0.2989501986332229, 0.13428968956455645, 0.5808214188221394, 0.21178229269315577, 0.8794087474171386, 0.4487151832996851, 0.7231896021778675, 0.7302547136373172, 0.26040317777899946, 0.6389870100570282, 0.3773869844754326, 0.19856987831095152, 0.6940903658453436, 0.27868486115832364, 0.10614978925528204, 0.13491956704584881, 0.5926121080988309, 0.3483542774650257, 0.6485918913581522, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.4460952454685818, 0.553492655520654, 0.5564826517197291, 0.051839045860280944, 0.4470807683972841, 0.7630292099690186, 0.23677571411131362, 0.5574204713453644, 0.5887126629087569, 0.3992286298565113, 0.37855426133379455, 0.3607039998273758, 0.296119700904085, 0.16650134807357603, 0.19956118766511077, 0.4035665191868353, 0.6612091961731477, 0.5977918533917194, 0.21303964902578626, 0.40704453297391185, 0.7810169160146574, 0.6840296792997195, 0.27724087834942884, 0.2556213017751479, 0.34661999676426425, 0.18476985398077742, 0.16948976858086615, 0.7504004004704901, 0.22664120894423648, 0.5900738748411136, 0.671995337914775, 0.7006847188488577, 0.49215257204284135, 0.06458074436650124, 0.38276055068026643, 0.6868474083243508, 0.5958632581373662, 0.3010117623013667, 0.8890860442169084, 0.40693291435805307, 0.1985327293074886, 0.30785988751928345, 0.5745997086539989, 0.2638468879288072, 0.11453360085481801, 0.059805539696543626, 0.6780323751412478, 0.40021506667847195, 0.21505090890766987, 0.7365936445375989, 0.24551587871136843, 0.6525790759412512, 0.567976832210193, 0.19914946986150234, 0.7725146668513112, 0.0462637414142284, 0.9044606924705487, 0.3992286298565113, 0.6875370381654706, 0.43648868589003154, 0.5758089748131177, 0.00038872397426430183, 0.8837480609952844, 0.35681902651134645, 0.39722643810031, 0.5626976141547512, 0.7070568394457155, 0.40485075092622447, 0.7825255142177938, 0.25092615318227707, 0.6271806220068676, 0.09826768036968808, 0.3854810702802158, 0.4697300318922557, 0.5404489678594299, 0.2992277271333202, 0.9365276486140073, 0.20817981641974054, 0.3083170398975563, 0.9596380983607143, 0.00013890028124437722, 0.6129938455930134, 0.19999999999999998, 0.6061560290246832, 0.7755969629986152, 0.30163222794597694, 0.7061886215274218, 0.731117687292167, 0.17649475021297867, 0.691318859382493, 0.8212015472955612, 0.3526760735547927, 0.2634828394470469, 0.9414428605659417, 0.11558530246943605, 0.5795915067045072, 0.7145087250676192, 0.0, 0.2980967237406818, 0.6844125934903402, 0.2949592603569702, 0.44129911034507874, 0.7698683115100668, 0.1739433012539412, 0.7585119231776787, 0.7471765364160405, 0.4268169073526533, 0.7304967316732975, 0.7263487966630597, 0.3113594314831553, 0.6127572978503959, 0.353591681194957, 0.30716455325379205, 0.0, 0.6350178817306947, 0.23791295422263842, 0.8514814394402155, 0.449449393359003, 0.410425377206737, 0.25555555555555554, 0.7430779662080889, 0.23978058624034976, 0.836942030771336, 0.17456273994841598, 0.753934274260637, 0.5017814354051087, 0.20152515229468537, 0.47634346098211944, 0.7503305249945151, 0.39322493741997167, 0.6624678525854575, 0.08124351547708426, 0.44105656883431654, 0.30008148548488944, 0.3714877971723338, 0.3306926805940149, 0.8307467892419347, 0.1516310084727187, 0.43829433999099154, 0.10861750317168559, 0.6463729364766281, 0.3743365916052602, 0.3075615417534258, 0.30512798203518005, 0.5960450822340708, 0.2917198927956374, 0.4452105198475923, 0.30060157481587746, 0.24515163944586357, 0.8470997505392659, 0.0, 0.5966289756845221, 0.6204607963096059, 0.30375131808614264, 0.4939746495072179, 0.19450818561732375, 0.2918557688264719, 0.6366015466501336, 0.30599117746591425, 0.8232059780715621, 0.39621418984318146, 0.8072615927227478, 0.573043959575943, 0.7011486686075723, 0.5957735304408347, 0.8374634481857526, 0.5761697895341449, 0.7977695539611169, 0.4204817112324464, 0.7649793793296578, 0.27478438017525203, 0.6053766163784955, 0.599721537305827, 0.7752210272627877, 0.6788869388950445, 0.3220998891127413, 0.1344057918034266, 0.0921507718462786, 0.32079604855609684, 0.027305099095430545, 0.16638847618687688, 0.642680735116892, 0.4027121494306726, 0.3263056905651471, 0.06, 0.7869082306429402, 0.169266599215924, 0.5939838617882367, 0.1271033804894816, 0.5308812792845414, 0.954414111805322, 0.2606798974875, 0.0006043228026684158, 0.5908386368273602, 1.0, 0.6622668282704129, 0.32058181511329487, 0.6595818321257165, 0.7189894209825303, 0.8078356986646513, 0.6000494532758354, 0.7598446701796893, 0.8455355915212772, 0.5015372100944879, 0.7591116846422887, 0.6972943442663218, 0.5347191241991722, 0.5652660571987258, 0.8521964460866425, 0.6287796716807859, 0.16076860405102827, 0.10402327436714291, 0.40626185275909443, 0.513951094852316, 0.26582370135238953, 0.8069007178395178, 0.3171038467872285, 0.4147896756607047, 0.5652887122558813, 0.3, 0.4890714508161259, 0.4134505609358237, 0.6745228788727515, 0.5826178744858002, 0.5780999494416426, 0.7451384298635613, 0.3057265995447144, 0.6980083797903092, 0.4614470406899055, 0.24267466951621425, 0.47262335373898007, 0.6495694833158199, 0.8084374616749153, 0.23267827069448496, 0.3218444540406984, 0.444006594916457, 0.46680385389420903, 0.11162972946435354, 0.46092479661255104, 0.38304488103290824, 0.7625515340301889, 0.45101776545562844, 0.44928604604736677, 0.09378119858864568, 0.7905056700329363, 0.011543851366559629, 0.6398103351245982, 0.7767231336445126, 0.3112197366922151, 0.6147178915567153, 0.2435629869037907, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.49802674519351525, 0.5654473178899494, 0.760978313726013, 0.7398520412406208, 0.17731538118686083, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.31860139828537487, 0.3074794208045295, 0.2168392685670143, 0.7433503156010464, 0.18585881606736232, 0.15420113768852392, 0.13855731352393771, 0.6696966979334058, 0.8600057888291379, 0.6754386750930661, 0.09917355371900827, 0.0, 0.7079067953653362, 0.8655056700329364, 0.8, 0.1949245707930345, 0.06260168200413505, 0.5957815081131553, 0.5960864789929613, 0.005405405405405405, 0.0001338887441840652, 0.30530717567486865, 0.4778533416343903, 0.1993183839795576, 0.5383089748131177, 0.5763543320233779, 0.2805377644620578, 0.6453246818220744, 0.5092272948800609, 0.21056545775751523, 0.6160087565692354, 0.18118532992568895, 0.21151332797896238, 0.685303136808272, 0.6911299520289248, 0.3159334506909151, 0.3251212634570997, 0.5032348322172151, 0.6275893856012176, 0.6319730195988179, 0.08928571428571427, 0.40739727760582384, 0.5536911179244788, 0.7041187640690183, 0.20665303670360388, 0.8292906179772745, 0.29732214139143043, 0.6883329804487951, 0.32388037220136395, 0.19404871089985237, 0.5518505299283467, 0.43981837346815045, 0.8379283989138028, 0.8778463326043728, 0.19594320509692176, 0.26363764179415483, 0.08231357869559347, 0.6568173810866085, 0.6651072501642161, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.47486583417284367, 0.30555287524813995, 0.4714698407062744, 0.8677459225855626, 0.15290891386016767, 0.3409695427236469, 0.859664396260277, 0.8114529051148651, 0.4596142043521421, 0.7316423857228858, 0.7195565989965169, 0.4562914879300718, 0.8737480609952845, 0.8969369858475811, 0.1854207716310155, 0.15863416932534052, 0.3182082757472285, 0.1595968766279512, 0.6517965513447935, 0.5127770818711092, 0.8622646994947969, 0.8115900311996962, 0.05429286354966076, 0.09271662386722326, 0.562520658360374, 0.6351804493055584, 0.3914184301045939, 0.00650181359348961, 0.17251988127521883, 0.2365364043248961, 0.4614069132492895, 0.34747218540850605, 0.6580098924657095, 0.5526748771697776, 0.5719259006427047, 0.4775635704485228, 0.5976337070083585, 0.37325991295455374, 0.6666057418239761]
Finish training and take 35m
