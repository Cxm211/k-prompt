Namespace(log_name='./RQ5/tfix_8_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_8_2/codet5p_770m', data_dir='./data/RQ5/tfix_8_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if (remove) {         for (var i = 0, length = this.length; i < length; i++) {           if (!modelMap[(model = this.models[i]).cid]) toRemove.push(model);', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if (remove) {         for (var i = 0; i < this.length; i++) {           if (!modelMap[(model = this.models[i]).cid]) toRemove.push(model);'}]
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.5842613605214974e+268
  global_step = 3
  train_loss = 41.638
  ********************
Previous best ppl:inf
Achieve Best ppl:1.5842613605214974e+268
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 20.12 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:20.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 5.747398400665504e+252
  global_step = 5
  train_loss = 26.0837
  ********************
Previous best ppl:1.5842613605214974e+268
Achieve Best ppl:5.747398400665504e+252
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 36.42 	 Previous best codebleu 20.12
  ********************
 Achieve Best bleu:36.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.3045556027109802e+253
  global_step = 7
  train_loss = 10.8478
  ********************
Previous best ppl:5.747398400665504e+252
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 49.78 	 Previous best codebleu 36.42
  ********************
 Achieve Best bleu:49.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.8850261583176952e+255
  global_step = 9
  train_loss = 5.6438
  ********************
Previous best ppl:5.747398400665504e+252
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 49.05 	 Previous best codebleu 49.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 6.609569828117297e+261
  global_step = 11
  train_loss = 4.8374
  ********************
Previous best ppl:5.747398400665504e+252
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 47.4 	 Previous best codebleu 49.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 6.511953408820804e+265
  global_step = 13
  train_loss = 3.1417
  ********************
Previous best ppl:5.747398400665504e+252
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 45.5 	 Previous best codebleu 49.78
  ********************
early stopping!!!
reload model from RQ5/tfix_8_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_2/test.jsonl
  codebleu = 48.3 
  Total = 500 
  Exact Fixed = 1 
[16]
  Syntax Fixed = 2 
[151, 270]
  Cleaned Fixed = 11 
[35, 153, 166, 172, 193, 198, 297, 417, 435, 439, 469]
  ********************
  Total = 500 
  Exact Fixed = 1 
[16]
  Syntax Fixed = 2 
[151, 270]
  Cleaned Fixed = 11 
[35, 153, 166, 172, 193, 198, 297, 417, 435, 439, 469]
  codebleu = 48.3 
[0.5670445241047504, 0.5247371055557515, 0.6367953537684857, 0.31331096368700495, 0.5960368208864724, 0.8821894623768516, 0.0, 0.5417785626903434, 0.0044279976835573265, 0.11249999999999999, 0.5368474083243506, 0.4333040790136501, 0.15, 0.6236663302242325, 0.6098740828788378, 1.0, 0.5007520143098162, 0.2192125454160942, 0.6885240359080308, 0.351298791626338, 0.6093758801239351, 0.11317222628535983, 0.11656539317399808, 0.39113020033231194, 0.5783068677082133, 0.008533731833720836, 0.7023657005051579, 0.26164945162585174, 0.41888849227698, 0.2855072113105671, 0.7059323615142865, 0.0, 0.4507183101653729, 0.7539889888078515, 0.8790355083665293, 0.5278775644627021, 0.0, 0.10556510016615601, 0.7155056700329363, 0.4928989633943295, 0.5511412946838178, 0.8230253102708935, 0.15501030069068095, 0.5976874345300546, 0.3462959660466284, 0.020944281753461413, 0.6570057594483909, 0.00410958904109589, 0.7776251874771634, 0.06925908416116176, 0.3658845887279942, 0.3632953276692549, 0.3428571428571428, 0.31397510079092045, 0.1297947365831559, 0.5864845415092643, 0.3070782538395746, 0.7571038996052304, 0.31218036781771696, 0.26775185612013497, 0.9315380507550484, 0.6396988442206821, 0.2832680265511309, 0.7698918124013976, 0.1732230072710102, 0.5107768909556616, 0.8091441569283881, 0.4790432550703725, 0.4160815614152497, 0.2266528631527418, 0.9488930040605423, 0.4990979359026785, 0.5407073823185757, 0.5077265232765378, 0.7572267233801039, 0.29035107882031763, 0.01094059405940594, 0.8965241752682729, 0.09249011857707509, 0.5980296435578425, 0.591516286342532, 0.23726320522914013, 0.6508789541441072, 0.5174772567598086, 0.6359585563603126, 0.5451417837590342, 0.4040539277086199, 0.42922099165048344, 0.10293211456273274, 0.35707093522645716, 0.8550899856418173, 0.7402739997668998, 0.6236113404540748, 0.45579909520741635, 0.3100552189599008, 0.7470738524243741, 0.8349004181959196, 0.8770297201566124, 0.7962637890346522, 0.5688724788208753, 0.610915672777911, 0.515277037889455, 0.3361259192183371, 0.6445931574752105, 0.2519744214130703, 0.0, 0.4593867118293433, 0.6683895153674697, 0.7336411455895708, 0.05270446328111167, 0.2228369334452749, 0.5819216413003003, 0.3002631679661971, 0.3079510025390523, 0.509901499184051, 0.5592594653964404, 0.6903510788203177, 0.7809442398765059, 0.398502187828076, 0.7231896021778675, 0.7302547136373172, 0.44924563513412274, 0.36428128208984606, 0.0001585844779721565, 0.09874186186981587, 0.8240264366907621, 0.2011770485941022, 0.30791744832696266, 0.29533965771616255, 0.5446761852217963, 0.5786352937426857, 0.44209087296905836, 0.0, 0.3928296748029187, 0.6667528334695118, 0.4442843622325243, 0.5777099745360312, 0.523099034776898, 0.10356766952267597, 0.4136911179244788, 0.6775851668789229, 0.6387086502123358, 0.5815717636244672, 0.19515848915506873, 0.0901436660123886, 0.3905383491008888, 0.3607039998273758, 0.0, 0.5862322739141566, 0.24554578642825906, 0.6371808959162841, 0.6365638961412512, 0.6983535654700125, 0.18496067116732715, 0.20144865625739522, 0.526376080371026, 0.2450525040405542, 0.5869940359456796, 0.22499999999999998, 0.47952636660960846, 0.48894435922473084, 0.4110337078171185, 0.33277684694674275, 0.5462779984545589, 0.45443627493082295, 0.6020849875039529, 0.5908527155778469, 0.339320402762534, 0.316689598214979, 0.38577998647726336, 0.6868474083243508, 0.5336911179244788, 0.47798912624653667, 0.7922164198377744, 0.21869686247593922, 0.0001593070373637213, 0.41996134394226886, 0.0, 0.6485579753870085, 0.11453360085481801, 0.36686807121046905, 0.6304241997444744, 0.6018621031077366, 0.6046379048692274, 0.7365936445375989, 0.238778721920711, 0.7174650489162135, 0.3, 0.0, 0.7725146668513112, 0.7562262989907678, 0.33564126720436216, 0.41130178148606267, 0.6875370381654706, 0.0239248613969033, 0.5758089748131177, 0.0, 0.8837480609952844, 0.36259198497384604, 0.08342526628029964, 0.5626976141547512, 0.5225513859617287, 0.48857307236002234, 0.1830453549869614, 0.40436432296800573, 0.6271806220068676, 0.4139240818757265, 0.7300713294371353, 0.5892626563791035, 0.6696543229942906, 0.19575085189665523, 0.7859784841477078, 0.24498411116889462, 0.6192538700356924, 0.9066789515243823, 0.037180895916284235, 0.6230627569367759, 0.5536911179244788, 0.9320894171538912, 0.6306606044170897, 0.2811321142495976, 0.7061886215274218, 0.20082062018706492, 0.17649475021297867, 0.4008591117730833, 0.8212015472955612, 0.5811145153819444, 0.7323384287069117, 0.9414428605659417, 0.0, 0.48074569931823535, 0.48704938487975247, 0.009677419354838708, 0.3358044546728667, 0.6844125934903402, 0.8083325080431145, 0.26666666666666666, 0.7698683115100668, 0.09999999999999999, 0.7875054975857689, 0.7471765364160405, 0.1967021360634741, 0.6676628470669137, 0.7263487966630597, 0.5383546389514632, 0.6127572978503959, 0.15600194444844317, 0.7225371723695806, 0.4029164888628578, 0.5333108614657077, 0.38275813271332876, 0.7273452817394238, 0.0213633328808749, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.18353489145754398, 0.6560247841578373, 0.48926472275706656, 0.42685774768136064, 0.5017814354051087, 0.5585140118159326, 0.636151196351536, 0.5135271873414131, 0.0, 0.6624678525854575, 0.33372428676905674, 0.5709909865120066, 0.6876346520405138, 0.6714877971723338, 0.6182031708143285, 0.5488246123790789, 0.6844125934903402, 0.559164241943578, 0.6381394976554917, 0.4909674941015688, 0.1511308101159807, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.0980837653096822, 0.46604306419874064, 0.029721595296592133, 0.8470997505392659, 0.0, 0.6601527915539326, 0.847363188306967, 0.33893907444278737, 0.5753022266329215, 0.17818230393434137, 0.075, 0.6366015466501336, 0.0857142857142857, 0.8232059780715621, 0.1360653527099435, 0.9119641296947072, 0.3556370814404372, 0.4122195129152056, 0.0010711909418936578, 0.5382700514438534, 0.46340630882698075, 0.7977695539611169, 0.44245571892563784, 0.6779434114917402, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.7614689196516977, 0.8642470965743208, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 0.5814500037030308, 0.7063659179388997, 0.145679012345679, 0.30358352628798935, 0.0, 0.7340445690739947, 0.4850661910211653, 0.5074523997842315, 0.0857142857142857, 0.49870480893374824, 0.954414111805322, 0.7812983362937829, 0.0012540847760848727, 0.0, 0.7362981141845485, 0.5738670326935369, 0.2731326666687589, 0.23699875397895173, 0.691828502931038, 0.8078356986646513, 0.39976652129190204, 0.7598446701796893, 0.8455355915212772, 0.5015372100944879, 0.7008642162257277, 0.6752109379370961, 0.44610333356698884, 0.0012428556164596747, 0.042789837348088416, 0.6287796716807859, 0.44507381648898203, 0.5398855599662549, 0.43276625327591534, 0.5193052677964398, 0.6393949501703278, 0.7249586401416586, 0.8944935540076178, 0.5235685683788931, 0.5652887122558813, 0.0, 0.4890714508161259, 0.3653977810633524, 0.6745228788727515, 0.5964858645175034, 0.8766349155411199, 0.7451384298635613, 0.6245836923955046, 0.5960807051999952, 0.4614470406899055, 0.4954070471249812, 0.5932305781811854, 0.5816993464418687, 0.7023311795574012, 0.17687282451787015, 0.6236915825962643, 0.6311119478495857, 0.7536060548993002, 0.32898762131831805, 0.6650142265234396, 0.736074986663424, 0.8250515340301889, 0.2549844434221382, 0.4129740706026147, 0.26283445449679005, 0.7905056700329363, 0.4621787547146431, 0.1054887009468913, 0.6723104602956884, 0.27136677123194985, 0.6363088006476244, 0.5607942439151187, 0.8242749040329442, 0.7530301180787049, 0.8619254788161215, 0.3631280699164892, 0.05666644983978165, 0.6730895310541162, 0.7398520412406208, 0.0, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.25668959821497894, 0.3303143558468694, 0.3241161041677278, 0.468132700259547, 0.7179453925554036, 0.7366238080587815, 0.18585881606736232, 0.39969884422068214, 0.446573008791321, 0.36116520375757855, 0.6814220029643038, 0.6754386750930661, 0.0857142857142857, 0.0, 0.5658837531246206, 0.6200236905792025, 0.5304900558326539, 0.6662022025756482, 0.09713682208624577, 0.6585117129535003, 0.7547391347384329, 0.009855548892787397, 0.00783625026922906, 0.27529609505785896, 0.4778533416343903, 0.4446724306281599, 0.46459392039551944, 0.09999999999999999, 0.6113017814860626, 0.5786580151554076, 0.5092272948800609, 0.21410743157431567, 0.5666056489385841, 0.5293733965850302, 0.21151332797896238, 0.685303136808272, 0.5722818111225448, 0.9381792830507429, 0.6829777303051304, 0.6186273986521214, 0.6275893856012176, 0.6319730195988179, 0.08928571428571427, 0.40739727760582384, 0.39373086345749486, 0.5961389326639888, 0.7844119673784049, 0.702226723380104, 0.17325991295455373, 0.13496676744460995, 0.7148321218614053, 0.19404871089985237, 0.4218738796398989, 0.6995445329739118, 0.8379283989138028, 0.8778463326043728, 0.19574233849963607, 0.6584218632570862, 0.0, 0.6568173810866085, 0.8326781496971694, 0.6462521506344618, 0.7845578251721461, 0.6099380655621088, 0.7500189997069144, 0.29721228898372315, 0.3397740918052187, 0.39673994709686305, 0.6988432397208316, 0.3583507427572924, 0.8626491724949568, 0.5983535654700125, 0.6809070226208283, 0.7316423857228858, 0.6433443143526654, 0.4153535151287477, 0.5358346408073879, 0.69792069322426, 0.02774557403027348, 0.21270941042125957, 0.7220543125543928, 0.8038961967467184, 0.41549921345489016, 0.15, 0.8955980328281301, 0.6931155273806628, 0.07563596602307793, 0.7533946307914341, 0.562520658360374, 0.6901940524102714, 0.46699403594567956, 0.23868348617790436, 0.5282087277319494, 0.6721193723777923, 0.4543533157723756, 0.6986613814884177, 0.5212231080679071, 0.5491454654050718, 0.8632148025904984, 0.0857142857142857, 0.6116808931901063, 0.43333333333333335, 0.6666057418239761]
Finish training and take 33m
