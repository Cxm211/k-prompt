Namespace(log_name='./RQ5/tfix_8_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_8_2/codet5p_220m', data_dir='./data/RQ5/tfix_8_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if (remove) {         for (var i = 0, length = this.length; i < length; i++) {           if (!modelMap[(model = this.models[i]).cid]) toRemove.push(model);', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if (remove) {         for (var i = 0; i < this.length; i++) {           if (!modelMap[(model = this.models[i]).cid]) toRemove.push(model);'}]
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.4058221012890009e+293
  global_step = 2
  train_loss = 43.4421
  ********************
Previous best ppl:inf
Achieve Best ppl:1.4058221012890009e+293
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 10.74 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 5.748942853298881e+274
  global_step = 3
  train_loss = 47.575
  ********************
Previous best ppl:1.4058221012890009e+293
Achieve Best ppl:5.748942853298881e+274
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 18.61 	 Previous best codebleu 10.74
  ********************
 Achieve Best bleu:18.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 8.627770356313104e+260
  global_step = 4
  train_loss = 29.4019
  ********************
Previous best ppl:5.748942853298881e+274
Achieve Best ppl:8.627770356313104e+260
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 22.66 	 Previous best codebleu 18.61
  ********************
 Achieve Best bleu:22.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.4462082123571182e+253
  global_step = 5
  train_loss = 20.1364
  ********************
Previous best ppl:8.627770356313104e+260
Achieve Best ppl:1.4462082123571182e+253
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 27.43 	 Previous best codebleu 22.66
  ********************
 Achieve Best bleu:27.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 6.148971678636971e+245
  global_step = 6
  train_loss = 15.5761
  ********************
Previous best ppl:1.4462082123571182e+253
Achieve Best ppl:6.148971678636971e+245
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 33.62 	 Previous best codebleu 27.43
  ********************
 Achieve Best bleu:33.62
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.3666773003138002e+241
  global_step = 7
  train_loss = 11.41
  ********************
Previous best ppl:6.148971678636971e+245
Achieve Best ppl:2.3666773003138002e+241
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 39.64 	 Previous best codebleu 33.62
  ********************
 Achieve Best bleu:39.64
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 2.1251660998877558e+238
  global_step = 8
  train_loss = 9.9126
  ********************
Previous best ppl:2.3666773003138002e+241
Achieve Best ppl:2.1251660998877558e+238
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 45.14 	 Previous best codebleu 39.64
  ********************
 Achieve Best bleu:45.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 3.398898916855721e+236
  global_step = 9
  train_loss = 7.5029
  ********************
Previous best ppl:2.1251660998877558e+238
Achieve Best ppl:3.398898916855721e+236
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 46.93 	 Previous best codebleu 45.14
  ********************
 Achieve Best bleu:46.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.1220635461062152e+236
  global_step = 10
  train_loss = 6.6243
  ********************
Previous best ppl:3.398898916855721e+236
Achieve Best ppl:1.1220635461062152e+236
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 49.88 	 Previous best codebleu 46.93
  ********************
 Achieve Best bleu:49.88
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0941138391237074e+236
  global_step = 11
  train_loss = 6.7095
  ********************
Previous best ppl:1.1220635461062152e+236
Achieve Best ppl:1.0941138391237074e+236
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 50.21 	 Previous best codebleu 49.88
  ********************
 Achieve Best bleu:50.21
  ********************
reload model from RQ5/tfix_8_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_2/test.jsonl
  codebleu = 48.82 
  Total = 500 
  Exact Fixed = 1 
[16]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 15 
[3, 153, 172, 193, 198, 268, 296, 336, 359, 390, 417, 439, 442, 469, 474]
  ********************
  Total = 500 
  Exact Fixed = 1 
[16]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 15 
[3, 153, 172, 193, 198, 268, 296, 336, 359, 390, 417, 439, 442, 469, 474]
  codebleu = 48.82 
[0.5670445241047504, 0.6932410335616551, 0.6816015466501335, 0.31331096368700495, 0.6058511595444083, 0.8821894623768516, 0.09427051903393222, 0.5417785626903434, 0.10772191840570708, 0.13333333333333333, 0.5368474083243506, 0.5638063820069366, 0.1459406343274025, 0.6957084294388991, 0.6196949179201965, 1.0, 0.5007520143098162, 0.30790831497503884, 0.6885240359080308, 0.25129879162633795, 0.7145082485446812, 0.40243113967436694, 0.24915625771353556, 0.6232430066758361, 0.5783068677082133, 0.008533731833720836, 0.9438645504684042, 0.2583050571155425, 0.6624101654079205, 0.2855072113105671, 0.7059323615142865, 0.0, 0.4507183101653729, 0.8089214008963768, 0.8057884482053539, 0.6068905779332499, 0.0, 0.0006756907818151134, 0.7155056700329363, 0.5060628669920147, 0.5511412946838178, 0.805671205945659, 0.10202520055807765, 0.465560571870452, 0.4939189660937384, 0.0, 0.6570057594483909, 0.274670867776058, 0.694001929950796, 0.180554236235409, 0.3658845887279942, 0.43255724401063667, 0.6558003592103332, 0.3125390679076945, 0.1297947365831559, 0.29505597600090006, 0.3070782538395746, 0.6395188779694462, 0.8460094492769958, 0.45741997253530997, 0.9315380507550484, 0.714297617187952, 0.2832680265511309, 0.7618233428889547, 0.6717087038073983, 0.5107768909556616, 0.10387688783661013, 0.3768732172348278, 0.4112469010663871, 0.2266528631527418, 0.9197028473750204, 0.5283950208311777, 0.5407073823185757, 0.39968950625047095, 0.7572267233801039, 0.29035107882031763, 0.09808487520555739, 0.7315737820210488, 0.33985271067894746, 0.3078044338437184, 0.41046208274963103, 0.2839528341415056, 0.7245977730432864, 0.3084649263402155, 0.6359585563603126, 0.5451417837590342, 0.48115588878443294, 0.42922099165048344, 0.19928064558318875, 0.3591365261541136, 0.6108560633180458, 0.3509642640544365, 0.38869711590386236, 0.45579909520741635, 0.3358820181887793, 0.7470738524243741, 0.8349004181959196, 0.822046535523298, 0.7261706494089959, 0.5688724788208753, 0.3149180088356974, 0.38425613342009035, 0.3648355357735218, 0.6445931574752105, 0.30653213319423056, 0.28588237514650655, 0.4593867118293433, 0.6683895153674697, 0.7336411455895708, 0.05270446328111167, 0.29693699530360973, 0.45809387056116213, 0.020376991225005556, 0.0, 0.509901499184051, 0.8795723049113977, 0.6903510788203177, 0.7798180862046438, 0.3978761649380471, 0.7017610307492962, 0.7302547136373172, 0.6887916417739868, 0.19442588442562006, 0.039781879546349694, 0.0, 0.8035504370427808, 0.2011770485941022, 0.45924633947190363, 0.5246680028432865, 0.5959201143573529, 0.29600083644856684, 0.44209087296905836, 0.0, 0.2045366878501596, 0.6667528334695118, 0.4460952454685818, 0.5777099745360312, 0.7209786656630173, 0.04966615869839919, 0.19999999999999998, 0.6130292099690186, 0.7245741928928766, 0.5815717636244672, 0.47180509653022784, 0.0, 0.43575925891384637, 0.3607039998273758, 0.07147761616420378, 0.17721177026942017, 0.1990909090909091, 0.4854183564853334, 0.11087499999920956, 0.6983535654700125, 0.2106275748607057, 0.2322790318189532, 0.7810169160146574, 0.576856426473012, 0.5869940359456796, 0.22499999999999998, 0.6684064493975892, 0.39347209291868157, 0.4110337078171185, 0.7504004004704901, 0.5462779984545589, 0.2934907229790138, 0.4396009652492904, 0.5908527155778469, 0.339320402762534, 0.01948479195688638, 0.3424171106526596, 0.6868474083243508, 0.5336911179244788, 0.45449078148809957, 0.8452288752239219, 0.3027205279255718, 0.03997824077278507, 0.41996134394226886, 0.5745997086539989, 0.6485579753870085, 0.11453360085481801, 0.36686807121046905, 0.6304241997444744, 0.3970564745608858, 0.5281112350462183, 0.5514655337688695, 0.7713750451807391, 0.6646067803185902, 0.3, 0.0, 0.7725146668513112, 0.6492225367207229, 0.6985688155242894, 0.41130178148606267, 0.6875370381654706, 0.12770020277470315, 0.5263595999090642, 0.0, 0.8837480609952844, 0.5562655346507828, 0.0, 0.5626976141547512, 0.5225513859617287, 0.40485075092622447, 0.7120481432211276, 0.40436432296800573, 0.6271806220068676, 0.4139240818757265, 0.7910956568145167, 0.5892626563791035, 0.6599223799332465, 0.17422820465541622, 0.9365276486140073, 0.24498411116889462, 0.6192538700356924, 0.9596380983607143, 0.01578846901892273, 0.5796605122596801, 0.44619887519287305, 0.9320894171538912, 0.7586059507468779, 0.3031035886312782, 0.6740103526642764, 0.2591859163275857, 0.17649475021297867, 0.4008591117730833, 0.8212015472955612, 0.5232229944754394, 0.5722844654038719, 0.9414428605659417, 0.11558530246943605, 0.6307456993182354, 0.7145087250676192, 0.07499166174133517, 0.4347288551937204, 0.6844125934903402, 0.8083325080431145, 0.5398527106789475, 0.7698683115100668, 0.23225806451612901, 0.7875054975857689, 0.7471765364160405, 0.4268169073526533, 0.4885965394619701, 0.581123005083449, 0.8106391824574766, 0.6127572978503959, 0.24506530357864126, 0.7225371723695806, 0.0, 0.6350178817306947, 0.4077581327133288, 0.7640172373035362, 0.5843476268503004, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.21080723102368715, 0.7564452295444688, 0.17596407016464072, 0.1383954905896922, 0.5017814354051087, 0.19483848185240463, 0.636151196351536, 0.5658229243802022, 0.0008070870465774912, 0.6624678525854575, 0.08124351547708426, 0.6255874957717769, 0.7209679853738471, 0.3714877971723338, 0.6848698374809952, 0.8307467892419347, 0.6844125934903402, 0.19972390808261475, 0.6381394976554917, 0.6463729364766281, 0.3743365916052602, 0.6174969094123448, 0.3079268801719802, 0.5551668790236555, 0.5600468038501167, 0.46839922780359144, 0.40732381855793504, 0.17018000339475464, 0.8470997505392659, 0.09427051903393222, 0.5966289756845221, 0.697363188306967, 0.33893907444278737, 0.4939746495072179, 0.03360085116740899, 0.5773427446886295, 0.6366015466501336, 0.38571273047611965, 0.8232059780715621, 0.41130178148606267, 0.8072615927227478, 0.4024334342860373, 0.48878303577741533, 0.07269000169763126, 0.3009401084073005, 0.46340630882698075, 0.7977695539611169, 0.11876039310621953, 0.6729687842179634, 0.5885667747214658, 0.9238877689380496, 0.6942478573162068, 0.7614689196516977, 0.6293148826219314, 0.05434085966643876, 0.1344057918034266, 0.22368441215365098, 0.30402043454811034, 0.16260912923979673, 0.5814500037030308, 0.12057561993411847, 0.3502184077768308, 0.30358352628798935, 0.0, 0.8378414230005442, 0.14374736540497002, 0.5939838617882367, 0.15511796730160593, 0.667887297922958, 0.954414111805322, 0.26020048451536915, 0.0004045449333929551, 0.00020450096933154148, 0.9432204007219691, 0.5738670326935369, 0.5899406213929941, 0.3830163941834331, 0.7189894209825303, 0.8078356986646513, 0.7419953379147748, 0.7598446701796893, 0.8686125145982003, 0.5015372100944879, 0.7591116846422887, 0.5482702090228039, 0.4127700002336555, 0.004115304829407107, 0.33791853170432357, 0.6287796716807859, 0.5424094289706055, 0.15372172018183922, 0.43276625327591534, 0.5193052677964398, 0.6625167201921691, 0.7249586401416586, 0.7285403649853283, 0.4147896756607047, 0.6782203984945389, 0.0, 0.4890714508161259, 0.41563587630144766, 0.6745228788727515, 0.6585117129535003, 0.5780999494416426, 0.7451384298635613, 0.3057265995447144, 0.6603944867604312, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 0.6495694833158199, 0.7198018405233149, 0.23267827069448496, 0.5042730162949352, 0.6311119478495857, 0.5553163293885757, 0.18430458869537747, 0.5642595261562982, 0.7700435930643703, 0.7934513138791275, 0.2750280055064548, 0.44928604604736677, 0.3053372331674803, 0.7905056700329363, 0.4621787547146431, 0.16509689218126905, 0.8184276974159628, 0.48375502907223766, 0.5044818894255302, 0.5607942439151187, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.5265085296294044, 0.5654473178899494, 0.760978313726013, 0.7398520412406208, 0.0, 0.4592493207167594, 0.5264116554917386, 0.7230706259860853, 0.2012271935492504, 0.5019774510104837, 0.31860139828537487, 0.5390019606744835, 0.597922012470741, 0.752380452288719, 0.13554591715094635, 0.39969884422068214, 0.1286261612635965, 0.6696966979334058, 0.6297267707992438, 0.7635926413190037, 0.0857142857142857, 0.0, 0.7032072043308194, 0.4757294127054047, 0.5304900558326539, 0.6662022025756482, 0.09792390161025119, 0.6585117129535003, 0.5554183564853334, 0.0029702970297029703, 0.15529892970465703, 0.06, 0.4749766675130823, 0.5021287494162977, 0.5383089748131177, 0.5763543320233779, 0.6113017814860626, 0.6786580151554076, 0.3039874013638151, 0.21056545775751523, 0.6160087565692354, 0.4376297585326638, 0.21151332797896238, 0.685303136808272, 0.5722818111225448, 0.7500113278670841, 0.6829777303051304, 0.7123448244422504, 0.6275893856012176, 0.6319730195988179, 0.07363636363636364, 0.40739727760582384, 0.5536911179244788, 0.5752003221506514, 0.7844119673784049, 0.702226723380104, 0.0, 0.6405523211133647, 0.6952135917851546, 0.19404871089985237, 0.5518505299283467, 0.4800113278670841, 0.8379283989138028, 0.8917028689549173, 0.33594700943102185, 0.7773387005649841, 0.0, 0.6568173810866085, 0.6572292041139127, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.7780387576883845, 0.29721228898372315, 0.32153634818406374, 0.2888218008625139, 0.46313604331806835, 0.3583507427572924, 0.8626491724949568, 0.5983535654700125, 0.6809070226208283, 0.7539341600372499, 0.7195565989965169, 0.45420471612839813, 0.8737480609952845, 0.6008337665942295, 0.3, 0.5004526218264369, 0.7220543125543928, 0.1688660126538549, 0.6617190381976104, 0.5135388321351861, 0.8622646994947969, 0.8115900311996962, 0.05515642881387117, 0.4830015338753355, 0.562520658360374, 0.6901940524102714, 0.46699403594567956, 0.23868348617790436, 0.1730180250149207, 0.6721193723777923, 0.4614069132492895, 0.6157955965877151, 0.642418735455043, 0.5674033738135131, 0.8632148025904984, 0.46944553823271534, 0.6116808931901063, 0.09583333333333334, 0.6666057418239761]
Finish training and take 35m
