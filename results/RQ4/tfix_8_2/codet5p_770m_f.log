Namespace(log_name='./RQ5/tfix_8_2/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_8_2/codet5p_770m_f', data_dir='./data/RQ5/tfix_8_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01269
  global_step = 3
  train_loss = 2.0376
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01269
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 17.31 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:17.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00876
  global_step = 5
  train_loss = 1.4457
  ********************
Previous best ppl:1.01269
Achieve Best ppl:1.00876
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 25.78 	 Previous best codebleu 17.31
  ********************
 Achieve Best bleu:25.78
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00936
  global_step = 7
  train_loss = 0.764
  ********************
Previous best ppl:1.00876
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 29.46 	 Previous best codebleu 25.78
  ********************
 Achieve Best bleu:29.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0076
  global_step = 9
  train_loss = 0.61
  ********************
Previous best ppl:1.00876
Achieve Best ppl:1.0076
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 41.37 	 Previous best codebleu 29.46
  ********************
 Achieve Best bleu:41.37
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00649
  global_step = 11
  train_loss = 0.4109
  ********************
Previous best ppl:1.0076
Achieve Best ppl:1.00649
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 50.26 	 Previous best codebleu 41.37
  ********************
 Achieve Best bleu:50.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00648
  global_step = 13
  train_loss = 0.2096
  ********************
Previous best ppl:1.00649
Achieve Best ppl:1.00648
  ********************
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 51.31 	 Previous best codebleu 50.26
  ********************
 Achieve Best bleu:51.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00648
  global_step = 15
  train_loss = 0.1111
  ********************
Previous best ppl:1.00648
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 52.11 	 Previous best codebleu 51.31
  ********************
 Achieve Best bleu:52.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00651
  global_step = 17
  train_loss = 0.0747
  ********************
Previous best ppl:1.00648
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 51.94 	 Previous best codebleu 52.11
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00653
  global_step = 19
  train_loss = 0.0741
  ********************
Previous best ppl:1.00648
BLEU file: ./data/RQ5/tfix_8_2/validation.jsonl
  codebleu-4 = 51.63 	 Previous best codebleu 52.11
  ********************
reload model from RQ5/tfix_8_2/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_2/test.jsonl
  codebleu = 49.33 
  Total = 500 
  Exact Fixed = 2 
[16, 253]
  Syntax Fixed = 2 
[169, 482]
  Cleaned Fixed = 21 
[25, 34, 35, 82, 98, 153, 166, 172, 176, 188, 198, 260, 265, 268, 297, 336, 390, 393, 417, 435, 469]
  ********************
  Total = 500 
  Exact Fixed = 2 
[16, 253]
  Syntax Fixed = 2 
[169, 482]
  Cleaned Fixed = 21 
[25, 34, 35, 82, 98, 153, 166, 172, 176, 188, 198, 260, 265, 268, 297, 336, 390, 393, 417, 435, 469]
  codebleu = 49.33 
[0.4948411429203172, 0.6932410335616551, 0.6367953537684857, 0.21878888247939982, 0.6058511595444083, 0.5641220790021517, 0.0, 0.4479537326131615, 0.17584457862775682, 0.12, 0.4714913986364708, 0.3344346888519393, 0.15393576085345756, 0.4754179779273989, 0.6196949179201965, 1.0, 0.5007520143098162, 0.30790831497503884, 0.661658510351995, 0.36519160551617547, 0.7145082485446812, 0.39323313167877133, 0.25583464080738794, 0.6274961219905688, 0.7168908160692684, 0.006758238160278511, 0.8264242973210627, 0.2583050571155425, 0.3930780358630882, 0.2855072113105671, 0.52541293648124, 0.0, 0.4507183101653729, 0.9620013131992717, 0.8790355083665293, 0.6068905779332499, 0.0, 0.050730605522703656, 0.6054109836476915, 0.6160122780155695, 0.5511412946838178, 0.8106849225916533, 0.10202520055807765, 0.4338099948604608, 0.28896233275834576, 0.2715085296294044, 0.5900732185903352, 0.1894736842105263, 0.7776251874771634, 0.32488394807864973, 0.350381564995214, 0.6515070121475033, 0.5567052722262924, 0.9219902071957782, 0.0, 0.3571381446036521, 0.25119887519287304, 0.5913136822468591, 0.8460094492769958, 0.4669681188022887, 0.9315380507550484, 0.6308950208311778, 0.2832680265511309, 0.7447876600742096, 0.12161705512979955, 0.6197233100748608, 0.5877745357134092, 0.6073843150845792, 0.41039261898653295, 0.2266528631527418, 0.8909688561765989, 0.4990979359026785, 0.8212946104630277, 0.3671488471555514, 0.026606328636027618, 0.2342341573483157, 0.075, 0.6260674443805658, 0.3968503937007874, 0.5952417505999457, 0.5521052192777596, 0.3683950208311778, 0.6895579051414615, 0.4771052192777594, 0.6359585563603126, 0.5451417837590342, 0.21428571428571427, 0.42922099165048344, 0.19557874518739998, 0.3591365261541136, 0.5836243674583652, 0.7402739997668998, 0.46017329951827773, 0.45579909520741635, 0.2725378786154891, 0.6152601704977227, 0.8349004181959196, 0.9251526452920409, 0.7962637890346522, 0.5688724788208753, 0.6812985771278143, 0.7765712201237638, 0.3648355357735218, 0.6445931574752105, 0.26376056336921816, 0.0, 0.4593867118293433, 0.7372665574310813, 0.8049637497745419, 0.006436428655340285, 0.02937913730636981, 0.5876935279546878, 0.34025628111289863, 0.34650852962940437, 0.5365872519916426, 0.5592594653964404, 0.21178229269315577, 0.6087533733413723, 0.2285094299534831, 0.5854800196177911, 0.5816478602423253, 0.39344297891548086, 0.4395742586738879, 0.04285714285714285, 0.11673851653145864, 0.8035504370427808, 0.2011770485941022, 0.26813584036943405, 0.6228110132935121, 0.4976113387647193, 0.7420103581231053, 0.4002460311942325, 0.09999999999999999, 0.3928296748029187, 0.6667528334695118, 0.4460952454685818, 0.553492655520654, 0.36308589829825844, 0.14876752225153467, 0.0, 0.6130292099690186, 0.15410049067123893, 0.5134738346812686, 0.2711510577150145, 0.3992286298565113, 0.2607592589138464, 0.3607039998273758, 0.32940158247728335, 0.5862322739141566, 0.24554578642825906, 0.4854183564853334, 0.6612091961731477, 0.5265085296294044, 0.18510019296607524, 0.40704453297391185, 0.7267531454411045, 0.24612819353071105, 0.2009264654157426, 0.22499999999999998, 0.23439599215703433, 0.21139841078861338, 0.40294136822632043, 0.27159415317991503, 0.6212779984545589, 0.4479362034729103, 0.671995337914775, 0.7006847188488577, 0.5052903220407482, 0.676689598214979, 0.4289051187833622, 0.6868474083243508, 0.5336911179244788, 0.47798912624653667, 0.762098597778367, 0.21869686247593922, 0.41130178148606267, 0.3959691186443218, 0.4794264335959812, 0.8700612974581008, 0.1202478865691037, 0.35852762152173145, 0.6354401436430851, 0.5164927712445991, 0.6046379048692274, 0.29549631980098384, 0.6079083149750388, 0.777439940761444, 0.567976832210193, 0.0, 0.7725146668513112, 0.6670433840511583, 0.7572988249743864, 0.0, 0.5423831631235629, 0.15717295559496658, 0.5263595999090642, 0.007044218708112843, 0.8837480609952844, 0.5977929661935701, 0.040256281112898626, 0.5689454692519331, 0.4897166672635505, 0.48857307236002234, 0.7181903201805009, 0.12664103396353144, 0.6177818063286673, 0.35316742358661557, 0.6367964263009368, 0.4697300318922557, 0.6696543229942906, 0.19575085189665523, 0.4231608975527973, 0.32627945612776854, 0.3055153040476295, 0.9596380983607143, 0.040256281112898626, 0.6129938455930134, 0.04285714285714285, 0.9320894171538912, 0.7755969629986152, 0.4210760489838614, 0.6902373932796317, 0.6032530992618459, 0.17649475021297867, 0.4673851287829819, 0.5171396677809144, 0.6433584595940706, 0.8547211363207963, 0.9414428605659417, 0.0, 0.48074569931823535, 0.8560223473586787, 0.0, 0.4394428274658284, 0.6844125934903402, 0.8083325080431145, 0.33985271067894746, 0.7698683115100668, 0.08017228281241714, 0.8137546760701411, 0.7471765364160405, 0.4099746990190683, 0.6966241432306655, 0.7361845125670947, 0.9056583090096291, 0.6127572978503959, 0.353591681194957, 0.7225371723695806, 0.2644195374242923, 0.5333108614657077, 0.6499918680479313, 0.7997576268531321, 1.0, 0.47909832636948796, 0.22499999999999998, 0.7430779662080889, 0.23978058624034976, 0.836942030771336, 0.5977243774264945, 0.8662528514160328, 0.5017814354051087, 0.5585140118159326, 0.7334322903093697, 0.7503305249945151, 0.4319953379147749, 0.6624678525854575, 0.31683850155049365, 0.6255874957717769, 0.3124072751845855, 0.37379038309331053, 0.6848698374809952, 0.5049392453389381, 0.6844125934903402, 0.37826962089297383, 0.11428571428571427, 0.4909674941015688, 0.504, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.4561779924571592, 0.40732381855793504, 0.8372433448759671, 0.8470997505392659, 0.0, 0.5489264559334668, 0.697363188306967, 0.29399876629308297, 0.4939746495072179, 0.17818230393434137, 0.21841467712957519, 0.6366015466501336, 0.08040754961441697, 0.6187127349184944, 0.007653004076725185, 0.9119641296947072, 0.48857307236002234, 0.5342352368540777, 0.28893704233083406, 0.8374634481857526, 0.3967857076877912, 0.7977695539611169, 0.2873541844077183, 0.7707814021468912, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.8555090515828416, 0.6826080313569398, 0.05434085966643876, 0.1344057918034266, 0.2547947365831559, 0.6682556320550647, 0.5113616550848404, 0.6276136760976885, 0.642680735116892, 0.3502184077768308, 0.41893128298506854, 0.0, 0.7763522613816869, 0.32376236601482183, 0.5702518310230941, 0.4393949501703278, 0.5308812792845414, 0.8944549929803318, 0.7812983362937829, 0.1049464258957323, 0.4372429839446083, 0.9432204007219691, 0.6471411467957033, 0.6273488306532815, 0.6870606775436042, 0.691828502931038, 0.8078356986646513, 0.7419953379147748, 0.8050241598897441, 0.8455355915212772, 0.5015372100944879, 0.598546573447774, 0.8358211452160682, 0.42430121314121655, 0.03652595195999016, 0.8521964460866425, 0.568523154621381, 0.1703865158003642, 0.6021675990462452, 0.5038833860066276, 0.5967328235180057, 0.30782030210549105, 0.8069007178395178, 0.8944935540076178, 0.4147896756607047, 0.5652887122558813, 0.02553191489361702, 0.4890714508161259, 0.4134505609358237, 0.4902029724445522, 0.3523761174236968, 0.3074405864749622, 0.7045675286429856, 0.27953889690093175, 0.6980083797903092, 0.4614470406899055, 0.5027054768195429, 0.5621910495141487, 0.6506136779802395, 0.7894978414909921, 0.21807035804997016, 0.3218444540406984, 0.444006594916457, 0.7536060548993002, 0.32898762131831805, 0.5291886750930661, 0.028944921781748664, 0.4447924256765823, 0.45101776545562844, 0.44928604604736677, 0.3053372331674803, 0.31066114732300565, 0.4621787547146431, 0.16509689218126905, 0.6664205640855709, 0.4743663136253903, 0.6363088006476244, 0.5607942439151187, 0.8242749040329442, 0.7530301180787049, 0.7643220414385394, 0.5265085296294044, 0.5654473178899494, 0.5367782893923102, 0.8481792830507429, 0.1341979980200088, 0.4592493207167594, 0.3671454371215078, 0.6413969930966545, 0.0805618584881895, 0.5019774510104837, 0.1591270383203772, 0.468132700259547, 0.7179453925554036, 0.7433503156010464, 0.18585881606736232, 0.32017228281241716, 0.495439094104668, 0.5533879838772606, 0.802977457495821, 0.6754386750930661, 0.096, 0.0, 0.7514391690819453, 0.8655056700329364, 0.48813114309629924, 0.5846027698772176, 0.09713682208624577, 0.6585117129535003, 0.6661180216547284, 0.3, 0.00783625026922906, 0.06, 0.4749766675130823, 0.7054819616225767, 0.6049522285347575, 0.0, 0.494530864623653, 0.6119913484887409, 0.5092272948800609, 0.11559087884942663, 0.5454747564769881, 0.4376297585326638, 0.2571428571428571, 0.685303136808272, 0.6911299520289248, 0.9381792830507429, 0.6355972336040316, 0.4882147362465425, 0.6275893856012176, 0.29002834618897577, 0.08928571428571427, 0.40739727760582384, 0.12, 0.5961389326639888, 0.1268709855131077, 0.702226723380104, 0.5649383810649133, 0.5744129258538716, 0.6734401247243872, 0.08313927821169673, 0.5471589663515812, 0.6995445329739118, 0.8379283989138028, 0.8778463326043728, 0.26174137339543524, 0.8648103351245982, 0.0, 0.6568173810866085, 0.8326781496971694, 0.7387037834978107, 0.8629576305902529, 0.6099380655621088, 0.7500189997069144, 0.14587668064666204, 0.4714698407062744, 0.8677459225855626, 0.2578336850384971, 0.36347401902513166, 0.8368354605794404, 0.5983535654700125, 0.6809070226208283, 0.7316423857228858, 0.1383097682198144, 0.20370282356615294, 0.4127577177304648, 0.7993157050834909, 0.374320789352244, 0.6722344586267278, 0.31725878284744796, 0.18399490630294932, 0.5320302980861602, 0.5220398231634966, 0.7586172059015853, 0.7845525506958333, 0.05705698933605645, 0.7533946307914341, 0.6570541673204224, 0.6901940524102714, 0.5089439475762831, 0.23868348617790436, 0.3225466491076139, 0.7293003242080285, 0.4614069132492895, 0.6157955965877151, 0.6580098924657095, 0.5491454654050718, 0.6385629301801026, 0.39722643810031, 0.5995702836109532, 0.37325991295455374, 0.6666057418239761]
Finish training and take 56m
