Namespace(log_name='./RQ5/tfix_32_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_32_2/codet5p_220m', data_dir='./data/RQ5/tfix_32_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "if (!this.isConnect) {       throw 'ZilPay is\\'t connection to dApp'     }", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "if (!this.isConnect) {       throw new Error('ZilPay is\\'t connection to dApp')     }"}]
***** Running training *****
  Num examples = 32
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.8641230049532066e+265
  global_step = 5
  train_loss = 43.7329
  ********************
Previous best ppl:inf
Achieve Best ppl:1.8641230049532066e+265
  ********************
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 23.47 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:23.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 2.895644753343638e+244
  global_step = 9
  train_loss = 23.4168
  ********************
Previous best ppl:1.8641230049532066e+265
Achieve Best ppl:2.895644753343638e+244
  ********************
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 45.87 	 Previous best codebleu 23.47
  ********************
 Achieve Best bleu:45.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.6290467324029723e+266
  global_step = 13
  train_loss = 13.5653
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 56.71 	 Previous best codebleu 45.87
  ********************
 Achieve Best bleu:56.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 17
  train_loss = 8.0188
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 57.12 	 Previous best codebleu 56.71
  ********************
 Achieve Best bleu:57.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 21
  train_loss = 5.122
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 58.31 	 Previous best codebleu 57.12
  ********************
 Achieve Best bleu:58.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 25
  train_loss = 3.6624
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 58.23 	 Previous best codebleu 58.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 29
  train_loss = 2.6717
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 58.69 	 Previous best codebleu 58.31
  ********************
 Achieve Best bleu:58.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 33
  train_loss = 1.9353
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 58.4 	 Previous best codebleu 58.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 37
  train_loss = 2.1296
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 59.01 	 Previous best codebleu 58.69
  ********************
 Achieve Best bleu:59.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 41
  train_loss = 1.4472
  ********************
Previous best ppl:2.895644753343638e+244
BLEU file: ./data/RQ5/tfix_32_2/validation.jsonl
  codebleu-4 = 59.11 	 Previous best codebleu 59.01
  ********************
 Achieve Best bleu:59.11
  ********************
reload model from RQ5/tfix_32_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_32_2/test.jsonl
  codebleu = 58.52 
  Total = 500 
  Exact Fixed = 24 
[3, 12, 45, 54, 65, 88, 118, 132, 154, 166, 191, 201, 202, 238, 303, 305, 328, 333, 344, 359, 367, 390, 480, 492]
  Syntax Fixed = 2 
[263, 341]
  Cleaned Fixed = 6 
[35, 126, 200, 237, 469, 475]
  ********************
  Total = 500 
  Exact Fixed = 24 
[3, 12, 45, 54, 65, 88, 118, 132, 154, 166, 191, 201, 202, 238, 303, 305, 328, 333, 344, 359, 367, 390, 480, 492]
  Syntax Fixed = 2 
[263, 341]
  Cleaned Fixed = 6 
[35, 126, 200, 237, 469, 475]
  codebleu = 58.52 
[0.5670445241047504, 0.6932410335616551, 1.0, 0.2122047676692162, 0.6058511595444083, 0.8821894623768516, 0.0, 0.5417785626903434, 0.43011545328563816, 0.12, 0.5368474083243506, 0.9250131914571023, 0.17189979897705582, 0.7569706667451968, 0.6179484048244397, 0.811117687292167, 0.5007520143098162, 0.7099766481408452, 0.6885240359080308, 0.25129879162633795, 0.6716511056875383, 0.39323313167877133, 0.24915625771353556, 0.6232430066758361, 0.5970182644937875, 0.008533731833720836, 0.9438645504684042, 0.26164945162585174, 0.6624101654079205, 0.2855072113105671, 0.7059323615142865, 0.0, 0.5654473178899494, 0.9113854927948712, 0.8790355083665293, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6398412446099295, 0.5511412946838178, 0.805671205945659, 0.10202520055807765, 0.5707115606398924, 1.0, 0.3403395101580171, 0.6570057594483909, 0.274670867776058, 0.8208732553989724, 0.5900038817627358, 0.350381564995214, 0.6515070121475033, 0.6558003592103332, 1.0, 0.1297947365831559, 0.5194637253863024, 0.3070782538395746, 0.6395188779694462, 0.8460094492769958, 0.6052834268841005, 0.9315380507550484, 0.654297617187952, 0.5348435312891645, 0.7618233428889547, 1.0, 0.6197233100748608, 0.6877745357134093, 0.4790432550703725, 0.4199748401004021, 0.28269688894370654, 0.9488930040605423, 0.714297617187952, 0.5426744349179697, 0.39452984161828986, 0.702664818575847, 0.29035107882031763, 0.15, 0.7572581344616534, 0.43317161913164237, 0.6254581948739375, 0.591516286342532, 0.3734424024864518, 0.8049878311476077, 0.7190865275431211, 0.6359585563603126, 0.5451417837590342, 0.48115588878443294, 1.0, 0.8053935031810717, 0.3591365261541136, 0.8550899856418173, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.3056299173484432, 0.7470738524243741, 0.8349004181959196, 0.8639557211113615, 0.7962637890346522, 0.5688724788208753, 0.694617555917012, 0.9164266028701744, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.0, 0.4593867118293433, 0.6683895153674697, 0.7336411455895708, 0.05270446328111167, 0.2878852119582799, 0.5876935279546878, 0.39272004516267667, 0.3079510025390523, 0.509901499184051, 0.8795723049113977, 0.6903510788203177, 0.8896347633344268, 0.5374713058466929, 0.7017610307492962, 0.7302547136373172, 0.6887916417739868, 0.763761398744189, 0.24, 0.2176694832919011, 0.8186530227858186, 0.2011770485941022, 0.45924633947190363, 0.5246680028432865, 0.4976113387647193, 0.3604625696285162, 1.0, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.4460952454685818, 0.5777099745360312, 0.753207079624809, 0.10356766952267597, 0.4136911179244788, 0.78685519109128, 0.7245741928928766, 0.601625348216042, 0.47180509653022784, 0.3992286298565113, 0.5459435933526045, 0.3607039998273758, 0.3561219555899987, 0.5862322739141566, 0.22499999999999998, 0.4854183564853334, 0.546234822395046, 0.500808145396195, 1.0, 0.40825339177238446, 0.7810169160146574, 0.553908332071853, 0.5869940359456796, 0.22499999999999998, 0.6684064493975892, 0.39347209291868157, 0.5274428993282894, 0.7504004004704901, 0.7954279937030855, 0.45443627493082295, 1.0, 0.42888223104428924, 0.339320402762534, 0.316689598214979, 0.40583747375718954, 0.6868474083243508, 0.4822625464959074, 0.4406441546153521, 0.9500330416345475, 0.4065032326897709, 0.6734232449967186, 0.41996134394226886, 0.5745997086539989, 0.6278910313966588, 0.11453360085481801, 0.36686807121046905, 0.6304241997444744, 0.5164927712445991, 0.5281112350462183, 0.7365936445375989, 0.695168617065389, 0.669924448317195, 0.5203577845911456, 0.4453557619745404, 0.7725146668513112, 1.0, 0.9044606924705487, 0.3992286298565113, 0.6875370381654706, 0.43648868589003154, 0.5758089748131177, 0.007044218708112843, 0.7960658705890922, 0.6441764742916929, 0.5608206201870649, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 0.41208749160752645, 0.6271806220068676, 0.4139240818757265, 0.7971354401872718, 0.4697300318922557, 0.6599223799332465, 0.6301401813493988, 0.9365276486140073, 0.32627945612776854, 0.6192538700356924, 0.9596380983607143, 0.174436274930823, 0.5796605122596801, 0.44619887519287305, 0.9320894171538912, 0.7586059507468779, 0.36651542412587346, 0.6740103526642764, 0.731117687292167, 0.15997084006767517, 0.6324885471323345, 0.8212015472955612, 0.5811145153819444, 0.7727819379417207, 0.9414428605659417, 0.14554410932454973, 0.6307456993182354, 0.7147461811855021, 0.0, 0.5992413083109663, 0.6844125934903402, 0.8083325080431145, 0.6119613551150405, 1.0, 0.3965193255633748, 0.7875054975857689, 0.7471765364160405, 0.4268169073526533, 0.7471633983399641, 0.7263487966630597, 0.8106391824574766, 0.6127572978503959, 0.353591681194957, 0.7225371723695806, 0.5042531454411046, 0.6350178817306947, 0.5073028085492076, 0.9663865109019152, 0.6088783494196637, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.2504033576162784, 0.7564452295444688, 0.5814345843863841, 0.7299948711471688, 0.5017814354051087, 0.5585140118159326, 0.8298683115100669, 0.7503305249945151, 0.48469338416142727, 0.6624678525854575, 0.31683850155049365, 0.4501947080651152, 0.7401495338727193, 0.3714877971723338, 0.6848698374809952, 0.8307467892419347, 0.6844125934903402, 0.43829433999099154, 0.6381394976554917, 0.6736456637493553, 0.4016534478992738, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.48950280214956154, 0.40732381855793504, 0.6877459225855624, 0.8470997505392659, 0.0, 0.5966289756845221, 0.8849384145327521, 0.7368213423140443, 0.5753022266329215, 0.17818230393434137, 0.22719082257468703, 0.6366015466501336, 0.6851384298635612, 0.8232059780715621, 0.377190822574687, 0.7911679345786041, 0.573043959575943, 0.7212489793637704, 0.10758390671207611, 0.7503195333207066, 0.5761697895341449, 1.0, 0.44245571892563784, 0.8928704934314164, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.7559607945381612, 0.8255980153481091, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.5029662540715545, 0.5680054404199835, 0.6276136760976885, 0.714297617187952, 0.3502184077768308, 0.30358352628798935, 0.06, 0.8378414230005442, 0.5174693084765555, 0.5702518310230941, 0.6240048954556174, 0.8034645362012123, 0.9617868617067262, 0.7812983362937829, 1.0, 0.4372429839446083, 0.7824539790084621, 0.7053304130816762, 0.5261794110071865, 0.8578047138519911, 0.7189894209825303, 0.656468030647374, 0.5909753125006932, 0.8050241598897441, 0.8686125145982003, 0.5015372100944879, 0.7591116846422887, 0.8074255754853112, 0.4775368663797597, 0.3381394976554918, 0.8808208735688969, 0.6287796716807859, 0.5424094289706055, 0.5398855599662549, 0.5468299419973112, 0.5193052677964398, 0.6625167201921691, 0.7249586401416586, 0.9176675865658077, 0.4147896756607047, 0.5952887122558812, 0.0, 0.3611714488237844, 0.4607209252762136, 0.6745228788727515, 0.9891483218006352, 0.5780999494416426, 0.7451384298635613, 0.3057265995447144, 0.6603944867604312, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 1.0, 0.8084374616749153, 0.23267827069448496, 0.8378414230005442, 0.6311119478495857, 0.7536060548993002, 0.32898762131831805, 0.5642595261562982, 0.7700435930643703, 0.8250515340301889, 0.45101776545562844, 0.44928604604736677, 0.3053372331674803, 0.7905056700329363, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.7916479842275448, 0.6571190145018053, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.8114529051148651, 0.5654473178899494, 0.760978313726013, 0.734763973349975, 0.17731538118686083, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.31860139828537487, 0.5976435605147894, 0.6789868063090441, 0.752380452288719, 0.18585881606736232, 0.39969884422068214, 0.5527989693026047, 0.6354739770278475, 0.8600057888291379, 0.6754386750930661, 0.0857142857142857, 0.0, 0.6551445843345075, 0.8655056700329364, 0.6714771286623475, 0.709115943344826, 0.09713682208624577, 0.5833105633098615, 0.5554183564853334, 0.038139497655491794, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.5021287494162977, 0.5383089748131177, 0.5621609692548745, 0.5282467529717347, 0.6119913484887409, 0.5092272948800609, 0.21056545775751523, 0.6183689771600134, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.6612315527854231, 0.8863585661014859, 0.6829777303051304, 0.7511655122445988, 0.6275893856012176, 0.5692803427643551, 0.08928571428571427, 0.40739727760582384, 0.5036911179244787, 0.6008988678505567, 0.8845572237610086, 0.8292906179772745, 0.636068568378893, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.5518505299283467, 0.6592906179772744, 0.8379283989138028, 0.8917028689549173, 0.33594700943102185, 0.8648103351245982, 0.08484732042396881, 0.6945580343846044, 0.8326781496971694, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.8105747328497521, 0.29371181369212096, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.4432027101248005, 0.859664396260277, 0.6807990952074163, 0.6809070226208283, 0.7316423857228858, 0.7195565989965169, 0.7645618670375556, 0.7710658705890923, 0.7503567123312754, 0.3, 0.6812000145029719, 0.7220543125543928, 0.7571038996052304, 1.0, 0.527190822574687, 0.8622646994947969, 0.6931155273806628, 0.08251128145232256, 0.7936657742154647, 0.562520658360374, 0.6746054822806855, 0.46699403594567956, 0.23868348617790436, 0.5282087277319494, 0.6721193723777923, 0.9891483218006352, 0.6986613814884177, 0.642418735455043, 0.5562895166803407, 0.8632148025904984, 0.47073389088669926, 0.7533655306263078, 0.387966609042281, 0.6666057418239761]
Finish training and take 12m
