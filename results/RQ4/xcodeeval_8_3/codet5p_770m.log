Namespace(log_name='./RQ5/xcodeeval_8_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_8_3/codet5p_770m', data_dir='./data/RQ5/xcodeeval_8_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h>  const int max = 100000; const double eps = 1e-6;  int main( void ) {     unsigned long long dollars;     int tokens[max];     int res[max];     int i, a, b, n;          /* get input */     scanf("%i %i %i", &n, &a, &b);     for( i = 0; i < n; i++ )                 scanf("%i", &tokens[i]);          /* compute the tokens left */     for( i = 0; i < n; i++ ) {         dollars = (double)a/b*tokens[i] + eps;                  if( (dollars*b) % a != 0 )  1/0;                  res[i] = tokens[i] - dollars*b/a;     }          printf("%i", res[0]);     for( i = 1; i < n; i++ )          printf(" %i", res[i]);          return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h>  int main( void ) {     long long dollars;     long long token;     int i, n, a, b;          scanf("%i %i %i", &n, &a, &b);     for( i = 0; i < n; i++ ) {         scanf("%lli", &token);                  dollars = token*a/b;         if( dollars*b % a == 0 )             printf("%lli ", token - dollars*b/a);         else             printf("%lli ", token - dollars*b/a - 1);     }          return 0; }'}]
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.5920418275145752e+225
  global_step = 3
  train_loss = 107.1556
  ********************
Previous best ppl:inf
Achieve Best ppl:1.5920418275145752e+225
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 28.52 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:28.52
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 7.554630139121095e+242
  global_step = 5
  train_loss = 71.7644
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 69.9 	 Previous best codebleu 28.52
  ********************
 Achieve Best bleu:69.9
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.3766722030598613e+280
  global_step = 7
  train_loss = 41.974
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 68.55 	 Previous best codebleu 69.9
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.2190132450905941e+292
  global_step = 9
  train_loss = 25.4817
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 71.28 	 Previous best codebleu 69.9
  ********************
 Achieve Best bleu:71.28
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 2.606052405151883e+297
  global_step = 11
  train_loss = 16.1232
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 71.34 	 Previous best codebleu 71.28
  ********************
 Achieve Best bleu:71.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.648778637816667e+297
  global_step = 13
  train_loss = 14.834
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 71.02 	 Previous best codebleu 71.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 2.9765617819570313e+300
  global_step = 15
  train_loss = 11.1806
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 71.01 	 Previous best codebleu 71.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 2.6645951014531225e+302
  global_step = 17
  train_loss = 8.4383
  ********************
Previous best ppl:1.5920418275145752e+225
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 70.98 	 Previous best codebleu 71.34
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_8_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_8_3/test.jsonl
  codebleu = 71.92 
  Total = 500 
  Exact Fixed = 5 
[75, 223, 308, 322, 451]
  Syntax Fixed = 2 
[81, 177]
  Cleaned Fixed = 1 
[481]
  ********************
  Total = 500 
  Exact Fixed = 5 
[75, 223, 308, 322, 451]
  Syntax Fixed = 2 
[81, 177]
  Cleaned Fixed = 1 
[481]
  codebleu = 71.92 
[0.5003909451993059, 0.9294151499698242, 0.6386890640788627, 0.4507986952593763, 0.7127243662869864, 0.7679881745093895, 0.988076477678953, 0.8394311373107994, 0.7798012813487226, 0.9440112246598258, 0.6622400912006872, 0.7772698324943685, 0.9623971198135532, 0.7166536568788391, 0.784896459386485, 0.58562433057727, 0.9621715437613865, 0.7429804137861776, 0.8054255423928414, 0.8131646133926693, 0.8276821254560949, 0.3446503101961176, 0.9617896364191245, 0.8925560777052948, 0.8319533008515221, 0.0010820181564900134, 0.8434094165290678, 0.6945413184437066, 0.38708231484420597, 0.9606328090250909, 0.8994746786272129, 0.7243558021629983, 0.661830942623481, 0.5431168861528274, 0.2922154774691272, 0.4835000669850591, 0.7027777918150636, 0.9691599747588928, 0.7411039840168427, 0.9733441689001259, 0.4399304988344238, 0.8396088605643368, 0.899379608482729, 0.8921998318682778, 0.6593510546130777, 0.8244899755357031, 0.5338617744031935, 0.8602181220443288, 0.8226980351644886, 0.852977206159188, 0.6349077239728745, 0.9459193905054479, 0.9302172597315774, 0.6485155405556907, 0.9732049512552536, 0.8808373096783533, 0.9088121389774579, 0.9339299975598521, 0.561801583155677, 0.2884494942340172, 0.6659646050056197, 0.5316466612173391, 0.8787266175971837, 0.33460070946115567, 0.8248460806268897, 0.781660877818183, 0.98154990280289, 0.7698824496255008, 0.6866407148315193, 0.915465568076623, 0.5393740157690967, 0.7427760268546069, 0.916403494090893, 0.9853133667641483, 1.0, 0.9402009240478387, 0.9189149325906292, 0.4932733020018213, 0.3639423591237215, 0.8499552034219139, 0.9327804056779383, 0.3816824572563564, 0.7597100033179813, 0.9464690809322596, 0.920139970422319, 0.4724215445506942, 0.608990714278228, 0.9738014890069033, 0.7248773843271845, 0.9822720623046208, 0.28942496496217823, 0.9664937597536958, 0.8952672769095782, 0.8876242548612938, 0.8511121182268683, 0.37642420748881267, 0.9441722498573561, 0.9602694033492566, 0.9319272046508655, 0.8283719455395986, 0.7529046737306214, 0.7063369946392235, 0.9518479807436881, 0.6432666994504355, 0.9253350120367194, 0.881176222647637, 0.86971226089015, 0.934871427061674, 0.7402299514576196, 0.7752117314563925, 0.925790468693755, 0.6107690954848484, 0.4181059355834638, 0.6098636165281349, 0.8778158396993327, 0.5662939894609251, 0.824400945446748, 0.461604635890685, 0.5590904557449987, 0.8887562240561163, 0.3123881029010852, 0.21724876323202302, 0.6424313341672772, 0.9904426405617919, 0.5198919171128822, 0.9094363868933528, 0.9773514349376944, 0.9297985504031074, 0.8060736170490447, 0.5791470604415615, 0.9477648614326466, 0.7102767282014482, 0.5916769991246038, 0.8816150641180378, 0.8109727719001666, 0.9582090124974589, 0.8355015737804767, 0.9055073576552588, 0.4727669414734672, 0.9472873156881443, 0.6333873199069301, 0.7265726336399266, 0.6237731005828068, 0.6259483208762605, 0.3053935253252723, 0.8957685740957428, 0.47213667611022436, 0.15843999506651799, 0.967544928466902, 0.9463882264323074, 0.8588873801436857, 0.8055864673667326, 0.49239571749879285, 0.5459516898043106, 0.9654424477393473, 0.7665708243215428, 0.34513992889724354, 0.5941289590584449, 0.6729263400809514, 0.27677076858201743, 0.580390274877845, 0.7684857402005096, 0.2247554870980143, 0.9742694293836827, 0.8761156573807791, 0.9472207997658679, 0.38502072754729244, 0.9699049873809091, 0.9834700980058935, 0.9610721917709997, 0.7467361709133309, 0.9396221339246325, 0.565676944249317, 0.3728341211055949, 0.5578323336329495, 0.6587073273392418, 0.9521775318277603, 0.7897511138044075, 0.9859793679627393, 0.9139108143266295, 0.5174196783096494, 0.916272560211627, 0.7238383616719015, 0.535206429623671, 0.968840592678585, 0.9052895563342798, 0.9607745973711677, 0.9652619679712691, 0.508243109335715, 0.9456458706993536, 0.6034321191256158, 0.385483565618736, 0.0, 0.8620947595539107, 0.9087123429574342, 0.8218539480382933, 0.4953906170940724, 0.899619566331324, 0.35324778747627306, 0.7918180095546627, 0.3579424928858859, 0.7910965943512587, 0.6266772080766958, 0.9045356516494829, 0.701645902256, 0.7733039571663103, 0.5914384969398494, 0.9665513892195621, 0.7596454142574642, 0.3461479392677451, 0.8699110799666838, 0.35732195151786816, 0.8633690715795768, 0.9232057638382181, 0.9225237666168993, 0.4945239546557928, 0.7938304679800914, 0.9112790289089925, 0.8371407712341709, 0.9100254602133176, 0.8523029357043744, 0.8909108204830978, 1.0, 0.5610229188524035, 0.9602235397377306, 0.7113265609190569, 0.30374499657601767, 0.9794673707391466, 0.8734013068767044, 0.6401493417553263, 0.5029033064648121, 0.792604909968763, 0.6732163514148213, 0.7174012425234068, 0.7336298486605038, 0.8426084577017585, 0.3024540539982831, 0.9512440865930114, 0.9420317788392338, 0.9508626724864289, 0.890935580421016, 0.20787905042836483, 0.9547478616285707, 0.014827337779471383, 0.729220529412195, 0.8084048440855038, 0.9521775318277603, 0.7608889895176245, 0.8815417902924716, 0.914605230296756, 0.9117883158947607, 0.14020214604341588, 0.8491583316973768, 0.3138642748705961, 0.07759093279363852, 0.2941969248494424, 0.8110033734683955, 0.9316289255121708, 0.42689251969146835, 0.46398377548871184, 0.8680712994235078, 0.8910042410146453, 0.9722588250235882, 0.8449844779326559, 0.9168377218613795, 0.7086954249958265, 0.5200719655481933, 0.8078101910282451, 0.8188115370159288, 0.4883323987643829, 0.2942761537144616, 0.6813704892824255, 0.7186306054128173, 0.8879566098203397, 0.9046850524100887, 0.9063468547889519, 0.8923345869421039, 0.7587729023519247, 0.953626076559831, 0.9312591482604915, 0.6587625606931693, 0.3414691117847285, 0.8549602890144714, 0.9166795416411146, 0.9455409660105238, 0.5961867498988478, 0.8548231708514218, 0.43808453932073843, 0.8326098029724313, 0.4113237893793412, 0.9662430550223153, 0.595036047883924, 0.41151709383031154, 0.8065424278669349, 0.8421901762909951, 0.9263268818553201, 0.5742795786446854, 0.4625903675786698, 0.9060784269672213, 0.654357949555753, 0.910835019525245, 0.5319509904078749, 0.909604112322582, 0.9361530539386638, 0.3821796477166763, 0.7586668599419605, 0.8794100936019331, 1.0, 0.6682500806066844, 0.8876077472866535, 0.7402003270206057, 0.3448279680258582, 0.9494075997300704, 0.9242850791613817, 0.17680425967209212, 0.5797660434243985, 0.670700695971357, 0.8735531700555859, 0.8545467071813517, 0.7942436421266241, 0.9838722535113, 0.9765419032562443, 0.3202665169509859, 0.37400385229275235, 0.962676181664404, 0.897868274842214, 0.9783839332393924, 0.5075736355169622, 0.532546034056932, 0.9173075584052859, 0.37944654832808666, 0.3953138893542425, 0.4123043478081776, 0.8740558734023434, 0.2982712119361176, 0.9617625014218179, 0.597398385186501, 0.7169335666834983, 0.47836466842490655, 0.41890123414313674, 0.9821809817650384, 0.7736787852368014, 0.6964096009787752, 0.6521082397979208, 0.9229552563363053, 0.6074300350442166, 0.6040285001208828, 0.8810946359492662, 0.7680099220442433, 0.9623041628773181, 0.5288231788023587, 0.8521108081796271, 0.7325156463466249, 0.19242379301366364, 0.9303593500095784, 0.8787702275332199, 0.8439878665104579, 0.35640416135498465, 0.8391697873648826, 0.293795497072026, 0.9718645902296401, 0.3386718428405343, 0.9171023179179227, 0.9737915388152598, 0.9637474927893142, 0.6693406223166416, 0.7611563185052843, 0.7408235030043473, 0.758988839562897, 0.7369406553821242, 0.9359055149846003, 0.6099316790741642, 0.9331552365394689, 0.6883315662131977, 0.6216371397419055, 0.5951453011501081, 0.9115143735869831, 0.8565940501385656, 0.8847009183301671, 0.9112403027721032, 0.40294507028489834, 0.9613152634801625, 0.7684448062183817, 0.8707818746581966, 0.5506135469714823, 0.6682381920753772, 0.282381412399278, 0.7400752638451388, 0.9293800429166074, 0.9698138929646263, 0.7074381217791966, 0.7905567987043943, 0.9059801416992133, 0.27352354506300447, 0.7559987541863575, 0.9823448177520746, 0.9040449515722547, 0.40730915905680976, 0.9301710411526725, 0.5312651095051248, 0.6831371225580725, 0.9344855036532616, 0.6215157040502399, 0.31090412648665033, 0.8843701717304933, 0.8981156159858223, 0.2776969881491722, 0.0, 0.9352900190279925, 0.421886906571103, 0.5336316494975952, 0.9189645726561522, 0.539612684703451, 0.801591636721631, 0.49935819383309804, 0.8090229732942332, 0.32401118753796765, 0.670628249437884, 0.30509463852336083, 0.753134247857808, 0.9248824030415701, 0.8990210844129167, 0.5456895014045992, 0.934473095392508, 0.619196145402581, 0.9051517121584842, 0.6967063679842646, 0.1459086942388002, 0.8819214421291581, 0.9385995189394607, 0.6220662273323747, 0.3384929898073865, 0.8203734168254198, 0.7664419509057727, 0.9385673313388825, 0.5136075551041939, 0.8001883461221011, 0.22247164132306774, 0.8169189094593757, 0.5951602009255168, 0.8554076307279366, 0.3677005961215266, 0.7966308791524319, 0.7507801222761162, 0.32400931000356986, 0.5666317615036989, 0.7718761136325316, 0.9387635675497785, 0.4183621157481132, 0.6902777777999555, 0.9612499338522498, 0.9751652519535294, 0.7412367177908079, 0.8537985835429875, 0.8021007111795594, 0.666210611458786, 0.8975537705475487, 0.8544775251558401, 0.8375795025499561, 0.8720767024759493, 0.563375733856525, 0.011387238931736576, 0.9633179652022956, 0.4799605515288722, 0.7669842146074164, 0.9722369545163847, 0.45592789552418267, 0.40341415487847787, 0.8627505126882897, 0.9181208637326026, 0.6091844928299013, 0.9386019990279018, 0.5735025861665138, 0.540913512252247, 0.4291632344053259, 0.7141494776867402, 0.947633855490319, 0.9270078518894367, 0.4027266047248116, 0.5771499523476752, 0.9684863847527501, 0.590335920863919, 0.778151370761023, 0.6212359005172369, 0.9451314813109495, 0.8206082150573881, 0.9013279588888281, 0.4076647468368658, 0.8667436416243068, 0.472048350719695, 0.5687023138391892, 0.8837686288402085, 0.7633334189743369, 0.7744889932682666, 0.6511144444174561, 0.47042501815144866, 0.9545187036711082, 0.38702322613559287, 0.8042710150175236, 0.8346804942478532]
Finish training and take 1h5m
