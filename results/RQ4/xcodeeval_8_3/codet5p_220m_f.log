Namespace(log_name='./RQ5/xcodeeval_8_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_8_3/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00057
  global_step = 2
  train_loss = 0.6642
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00057
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 15.57 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:15.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.0006
  global_step = 3
  train_loss = 0.6842
  ********************
Previous best ppl:1.00057
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 41.5 	 Previous best codebleu 15.57
  ********************
 Achieve Best bleu:41.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00057
  global_step = 4
  train_loss = 0.4918
  ********************
Previous best ppl:1.00057
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 44.81 	 Previous best codebleu 41.5
  ********************
 Achieve Best bleu:44.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00054
  global_step = 5
  train_loss = 0.4223
  ********************
Previous best ppl:1.00057
Achieve Best ppl:1.00054
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 54.0 	 Previous best codebleu 44.81
  ********************
 Achieve Best bleu:54.0
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00053
  global_step = 6
  train_loss = 0.3421
  ********************
Previous best ppl:1.00054
Achieve Best ppl:1.00053
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 61.08 	 Previous best codebleu 54.0
  ********************
 Achieve Best bleu:61.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00052
  global_step = 7
  train_loss = 0.3131
  ********************
Previous best ppl:1.00053
Achieve Best ppl:1.00052
  ********************
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 65.87 	 Previous best codebleu 61.08
  ********************
 Achieve Best bleu:65.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00053
  global_step = 8
  train_loss = 0.2781
  ********************
Previous best ppl:1.00052
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 67.91 	 Previous best codebleu 65.87
  ********************
 Achieve Best bleu:67.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00053
  global_step = 9
  train_loss = 0.263
  ********************
Previous best ppl:1.00052
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 68.77 	 Previous best codebleu 67.91
  ********************
 Achieve Best bleu:68.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00053
  global_step = 10
  train_loss = 0.249
  ********************
Previous best ppl:1.00052
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 69.4 	 Previous best codebleu 68.77
  ********************
 Achieve Best bleu:69.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00053
  global_step = 11
  train_loss = 0.2221
  ********************
Previous best ppl:1.00052
BLEU file: ./data/RQ5/xcodeeval_8_3/validation.jsonl
  codebleu-4 = 69.71 	 Previous best codebleu 69.4
  ********************
 Achieve Best bleu:69.71
  ********************
reload model from RQ5/xcodeeval_8_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_8_3/test.jsonl
  codebleu = 71.03 
  Total = 500 
  Exact Fixed = 3 
[214, 269, 322]
  Syntax Fixed = 1 
[177]
  Cleaned Fixed = 1 
[481]
  ********************
  Total = 500 
  Exact Fixed = 3 
[214, 269, 322]
  Syntax Fixed = 1 
[177]
  Cleaned Fixed = 1 
[481]
  codebleu = 71.03 
[0.5003909451993059, 0.9294151499698242, 0.5531434187939529, 0.4507986952593763, 0.5334842031724578, 0.7769952980182466, 0.909668609620123, 0.8394311373107994, 0.7852857892799938, 0.7964471345341, 0.6622400912006872, 0.7772868173384335, 0.9623971198135532, 0.7864631506294787, 0.38769659005485274, 0.5927671877201272, 0.6461637512726355, 0.8086091838614902, 0.8054255423928414, 0.8928539444631554, 0.8276821254560949, 0.3432186595314998, 0.9617896364191245, 0.8925560777052948, 0.8319533008515221, 0.0010820181564900134, 0.9751679833306697, 0.809436882194082, 0.38708231484420597, 0.9321862504515526, 0.952851260737474, 0.7243558021629983, 0.44933760416144625, 0.5495569575790649, 0.9740733453120272, 0.6237143324381418, 0.7336996602379647, 0.5867826311926256, 0.7411039840168427, 0.809382092668695, 0.43871215196390306, 0.8833038098770115, 0.6927064752861193, 0.8921998318682778, 0.6593510546130777, 0.3624212279201404, 0.5338617744031935, 0.5307642080837068, 0.8226980351644886, 0.9714741186824503, 0.6349077239728745, 0.9888765931470758, 0.8913920995182771, 0.7618762140838397, 0.9732049512552536, 0.938955604347008, 0.9088121389774579, 0.9264493284450595, 0.5565094531676825, 0.3889412074119988, 0.324815974116433, 0.48075801910315513, 0.9126538866701963, 0.33460070946115567, 0.9357192313331475, 0.9752926493864778, 0.98154990280289, 0.7858041122445381, 0.7113008970718839, 0.915465568076623, 0.5129346492787287, 0.7769943668010335, 0.9266945793708943, 0.9853133667641483, 0.9191158481207378, 0.7069047988715205, 0.9189149325906292, 0.4932733020018213, 0.9667329410838017, 0.8160829174811354, 0.8979589771065096, 0.3816824572563564, 0.733442446924866, 0.9003077208857198, 0.8729588064267548, 0.4765673051176011, 0.608990714278228, 0.9738014890069033, 0.7248773843271845, 0.9619634759786679, 0.5720930232558139, 0.9871730286979867, 0.9378691266821584, 0.8876242548612938, 0.8009356497917418, 0.37642420748881267, 0.9441722498573561, 0.9602694033492566, 0.9319272046508655, 0.7818573512250176, 0.7599781686282628, 0.7013902966965254, 0.2330057624217975, 0.7462546419750544, 0.9464173775678764, 0.9036755986162253, 0.9134916145765397, 0.8824490530956131, 0.7183458419935205, 0.843779606891953, 0.925790468693755, 0.636874953887632, 0.44301690842111663, 0.6129225799936057, 0.8709544537092064, 0.5662939894609251, 0.8416873135729088, 0.5881328279337963, 0.5590904557449987, 0.5971774267098949, 0.31102049929872344, 0.2329851824758143, 0.5880905716521241, 0.8419027861580808, 0.9876378378993536, 0.9094363868933528, 0.5877208109977439, 0.8844169946212104, 0.8057849666270349, 0.5791470604415615, 0.7003884943280383, 0.7319013516453983, 0.5979624610434242, 0.836544893275637, 0.8345680283779763, 0.9812998208473764, 0.8355015737804767, 0.7358468129209594, 0.4400591474335241, 0.9472873156881443, 0.6333873199069301, 0.7643810428304513, 0.6237731005828068, 0.5886727066963238, 0.3053935253252723, 0.7418681512242402, 0.508002770274875, 0.15843999506651799, 0.9565343127552286, 0.9305992255657882, 0.9084408246013599, 0.776318880268258, 0.4606247605239707, 0.5414012443977789, 0.9799354517153449, 0.7477202956710736, 0.35668442461208505, 0.6711901181086953, 0.7466017902771492, 0.2832762356981138, 0.4235169281322204, 0.7391270648188677, 0.22442794434312324, 0.9742694293836827, 0.8987497922002496, 0.9056406163504835, 0.9818785929874543, 0.9597207225270945, 0.9834700980058935, 0.9551975148162357, 0.2671973809873447, 0.9396221339246325, 0.5713992592551543, 0.34656855725548164, 0.5578323336329495, 0.653856905605865, 0.9521775318277603, 0.7897511138044075, 0.9936607195991256, 0.9139108143266295, 0.5174196783096494, 0.8752308011762593, 0.731695678958928, 0.5341374618194692, 0.968840592678585, 0.9052895563342798, 0.887961195510171, 0.9753403284839008, 0.508243109335715, 0.7908277692013751, 0.3031929728489795, 0.3843652002149621, 0.2768985444038598, 0.8798091526516568, 0.9087123429574342, 0.7897428442056872, 0.5004645722281954, 0.5208866746884949, 0.9490646916957624, 0.7885343223120463, 0.3587406468837636, 0.6369125753167452, 0.6266772080766958, 0.8608146692629707, 0.6703330349752178, 0.7733039571663103, 0.591865368526167, 0.9528960563859521, 0.8533534577368376, 0.26920176208246605, 0.8699110799666838, 0.35732195151786816, 0.5412539275381296, 1.0, 0.8995087661574963, 0.5012127116833381, 0.7938304679800914, 0.9683231065091749, 0.7576176996301744, 0.9175731843799296, 0.8523029357043744, 0.8565478251796333, 0.9447445835540222, 0.5692861362349391, 0.9602235397377306, 0.7370377157385981, 0.29834476798327997, 0.9031947806587897, 0.8544756661410926, 0.6527809207026948, 0.5263304194414254, 0.8123152846477772, 0.734852019183727, 0.7247563047282455, 0.7515690623438132, 0.8426084577017585, 0.29660247436472353, 0.9406995380264958, 0.9198696265840844, 0.9508626724864289, 0.9717066984642257, 0.19285163871557537, 0.956301548538977, 0.014827337779471383, 0.7819105691707486, 0.904590355528839, 0.8836794229901765, 0.7682914974803885, 0.9477354798086446, 0.9276347349756966, 0.9043105995752032, 0.11200213653664287, 0.8491583316973768, 0.8983535848060096, 0.6037252364285755, 0.2941969248494424, 0.5645386383595722, 0.9189200098580517, 0.42689251969146835, 0.46398377548871184, 0.9082446279665759, 0.8910042410146453, 0.9852412520833183, 0.8449844779326559, 0.8932535135680382, 0.7086954249958265, 0.5296868847250604, 0.7654106290713822, 1.0, 0.4883323987643829, 0.2942761537144616, 0.6834344139980126, 0.7200309361219595, 0.9271490169747502, 0.8715107436193149, 0.9029705508895722, 0.8923345869421039, 0.5996254970751413, 0.7914834564811419, 0.9312591482604915, 0.6857318503066976, 0.3329380293736354, 0.7161886343718264, 0.9166795416411146, 0.9455409660105238, 0.5741547548968033, 0.8548231708514218, 0.43808453932073843, 0.9798219468937557, 0.4119890403418556, 0.9780384047260811, 0.594007414149033, 0.41151709383031154, 0.8065424278669349, 0.8763562414963599, 0.9379197320725001, 0.6480774397088013, 0.4178563412555547, 0.9060784269672213, 0.654357949555753, 0.910835019525245, 0.5289514092792647, 0.9495780518930732, 0.394050300144669, 0.37881634285199683, 0.760540475117339, 0.8794100936019331, 0.8897945586059963, 0.7444340059656643, 0.8462838984331946, 0.8165098337008839, 0.3448279680258582, 0.9359073062715482, 0.8795926954939794, 0.20519665134428633, 0.5497925042631905, 0.7134770892891311, 0.8735531700555859, 0.7437923420744814, 0.7942436421266241, 0.6998183310256022, 1.0, 0.31950029715239175, 0.35015564357569384, 0.962676181664404, 0.897868274842214, 0.9783839332393924, 0.474238148279414, 0.6800660980808414, 0.9173075584052859, 0.3612015348342853, 0.3636944021746881, 0.5045975639394058, 0.9273759004975819, 0.4229932863872289, 0.982840810414882, 0.597398385186501, 0.6413765075831755, 0.5092531607280145, 0.4011929009233417, 0.7428394482308103, 0.7736787852368014, 0.7486759848652709, 0.6653351874795173, 0.9524302556873825, 0.6074300350442166, 0.6178552508101587, 0.8810946359492662, 0.7606454622988796, 0.9477010657352742, 0.5288231788023587, 0.8301117571777112, 0.7880782435693265, 0.19242379301366364, 0.7819191793424669, 0.6094440209575142, 0.8439878665104579, 0.35599974604021273, 0.8665928958491977, 0.8718511731916851, 0.28677366028311524, 0.33856809596367465, 0.9749244080949804, 0.9837244566208403, 0.8848900408156215, 0.6349159347983245, 0.7686563185052843, 0.7460110532720265, 0.7495313808815218, 0.7369406553821242, 0.9149764717609801, 0.5780241516155017, 0.6174457136879072, 0.6869534758595228, 0.6216371397419055, 0.6312231658015864, 0.9496316376212464, 0.2183683976094285, 0.6604845753246472, 0.8951877800865564, 0.4155596630877777, 0.4150279820541115, 0.7684448062183817, 0.907558018935852, 0.5506135469714823, 0.5834267775999062, 0.30558263576764216, 0.7095070481130452, 0.9409103501190933, 0.9698138929646263, 0.703409246380515, 0.7905567987043943, 0.9059801416992133, 0.2773793952823077, 0.3125167163996245, 0.9495045733036356, 0.7313112566297147, 0.40730915905680976, 0.9301710411526725, 0.6473967207688989, 0.6831371225580725, 0.9344855036532616, 0.6145389598641935, 0.9327820820761272, 0.9669138743101213, 0.8140690610479137, 0.2776969881491722, 0.6075752522233596, 0.3500987125214333, 0.421886906571103, 0.5748074573837764, 0.9189645726561522, 0.539612684703451, 0.804631742207644, 0.5036312075276035, 0.8063329292166834, 0.3374521240368497, 0.5808422587984978, 0.26421655231043095, 0.6307636789741317, 0.9555682869995965, 0.8990210844129167, 0.5425755720089517, 0.7655757212200781, 0.619196145402581, 0.8791782224567704, 0.6993849093249114, 0.31392972144129466, 0.8806976111458171, 0.7371755813443525, 0.8612778234327079, 0.22771281842669133, 0.8203734168254198, 0.7410450931898308, 0.561515628386571, 0.5099852681505637, 0.7446141056482928, 0.8940132574712745, 0.8169189094593757, 0.5996229338562107, 0.8554076307279366, 0.9707630080515532, 0.9586790625386887, 0.7507801222761162, 0.3330584663396393, 0.5574068797012777, 0.606047697618795, 0.9402418182402292, 0.4211523280742633, 0.6902777777999555, 0.8651757796134328, 0.9616206656038384, 0.7412367177908079, 0.8509360520535629, 0.7847266863687136, 0.666210611458786, 0.794099098182387, 0.8759555092831399, 0.8209169743155011, 0.883875362704373, 0.11032417015561033, 0.010895454655515445, 0.9633179652022956, 0.4323123975875095, 0.7669842146074164, 0.8348131501440001, 0.40795654276837234, 0.41747534088469995, 0.8461699546528594, 0.9181208637326026, 0.616501566000633, 0.859211775569727, 0.5735025861665138, 0.6020153206141936, 0.9487836071815641, 0.7141494776867402, 0.984944616727695, 0.8807963529547436, 0.4027266047248116, 0.6574949929489304, 0.9684863847527501, 0.590335920863919, 0.5881945808234843, 0.6623464465436505, 0.9426242057849843, 0.7177458766709134, 0.9040645444308273, 0.40431997573579515, 0.789687014598824, 0.472048350719695, 0.749078364559882, 0.9386210590467857, 0.9096266419399884, 0.7744889932682666, 0.6361144444174561, 0.46802465703730456, 0.9545187036711082, 0.3726430129234103, 0.8042710150175236, 0.8175264896595079]
Finish training and take 1h2m
