Namespace(log_name='./RQ5/sstubs_8_3/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/sstubs_8_3/codet5p_770m_f', data_dir='./data/RQ5/sstubs_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00273
  global_step = 3
  train_loss = 1.0207
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00273
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 17.07 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:17.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00146
  global_step = 5
  train_loss = 0.4626
  ********************
Previous best ppl:1.00273
Achieve Best ppl:1.00146
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 32.6 	 Previous best codebleu 17.07
  ********************
 Achieve Best bleu:32.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00118
  global_step = 7
  train_loss = 0.1702
  ********************
Previous best ppl:1.00146
Achieve Best ppl:1.00118
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 70.28 	 Previous best codebleu 32.6
  ********************
 Achieve Best bleu:70.28
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00112
  global_step = 9
  train_loss = 0.1298
  ********************
Previous best ppl:1.00118
Achieve Best ppl:1.00112
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 81.39 	 Previous best codebleu 70.28
  ********************
 Achieve Best bleu:81.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00108
  global_step = 11
  train_loss = 0.0721
  ********************
Previous best ppl:1.00112
Achieve Best ppl:1.00108
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 81.69 	 Previous best codebleu 81.39
  ********************
 Achieve Best bleu:81.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00105
  global_step = 13
  train_loss = 0.0783
  ********************
Previous best ppl:1.00108
Achieve Best ppl:1.00105
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 85.27 	 Previous best codebleu 81.69
  ********************
 Achieve Best bleu:85.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00104
  global_step = 15
  train_loss = 0.0308
  ********************
Previous best ppl:1.00105
Achieve Best ppl:1.00104
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 84.92 	 Previous best codebleu 85.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00103
  global_step = 17
  train_loss = 0.0105
  ********************
Previous best ppl:1.00104
Achieve Best ppl:1.00103
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 85.0 	 Previous best codebleu 85.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00103
  global_step = 19
  train_loss = 0.0204
  ********************
Previous best ppl:1.00103
Achieve Best ppl:1.00103
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 85.26 	 Previous best codebleu 85.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00103
  global_step = 21
  train_loss = 0.0208
  ********************
Previous best ppl:1.00103
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 85.31 	 Previous best codebleu 85.27
  ********************
 Achieve Best bleu:85.31
  ********************
reload model from RQ5/sstubs_8_3/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_8_3/test.jsonl
  codebleu = 86.4 
  Total = 500 
  Exact Fixed = 100 
[8, 17, 19, 20, 26, 28, 38, 39, 50, 53, 54, 59, 76, 80, 91, 106, 110, 112, 120, 130, 138, 142, 147, 149, 152, 153, 155, 156, 157, 159, 160, 162, 166, 171, 172, 176, 180, 188, 192, 193, 200, 210, 213, 219, 223, 224, 232, 236, 253, 265, 267, 269, 272, 278, 282, 285, 288, 291, 301, 303, 310, 313, 318, 319, 321, 330, 334, 337, 341, 342, 343, 350, 357, 362, 370, 377, 384, 386, 387, 388, 390, 392, 393, 398, 404, 410, 418, 425, 432, 440, 444, 450, 460, 462, 465, 466, 468, 481, 496, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 100 
[8, 17, 19, 20, 26, 28, 38, 39, 50, 53, 54, 59, 76, 80, 91, 106, 110, 112, 120, 130, 138, 142, 147, 149, 152, 153, 155, 156, 157, 159, 160, 162, 166, 171, 172, 176, 180, 188, 192, 193, 200, 210, 213, 219, 223, 224, 232, 236, 253, 265, 267, 269, 272, 278, 282, 285, 288, 291, 301, 303, 310, 313, 318, 319, 321, 330, 334, 337, 341, 342, 343, 350, 357, 362, 370, 377, 384, 386, 387, 388, 390, 392, 393, 398, 404, 410, 418, 425, 432, 440, 444, 450, 460, 462, 465, 466, 468, 481, 496, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 86.4 
[0.9321212086381336, 0.9272945586059962, 0.9026775857310148, 0.9030970298831136, 0.8336257349196183, 0.7016265496309229, 0.9211131842675646, 1.0, 0.557115595241456, 0.9067749584667979, 0.7057051229072668, 0.9289542494919141, 0.5053936838661389, 0.6766433148075119, 0.7933524811232104, 0.6353004704636518, 1.0, 0.9447883869073515, 1.0, 1.0, 0.3141800655613268, 0.8606751756510949, 0.6577095841418729, 0.9048200716106156, 0.9616102999368312, 1.0, 0.8400553834135902, 1.0, 0.8892385063159653, 0.775479195514087, 0.881955507815954, 0.8853457989463581, 0.9372561368734473, 0.6106880800400042, 0.31020535056772763, 0.920283409368625, 0.9289542494919141, 1.0, 1.0, 0.8186206960614633, 0.8480582060293878, 0.35629753997354174, 0.9517918920357142, 0.7584803831925206, 0.9409857702882689, 0.9543920430664012, 0.936752055107628, 0.9012977329022187, 0.8807613352737074, 1.0, 0.824857970462928, 0.9523104352365284, 1.0, 1.0, 0.850084905673596, 0.2650468709052251, 0.9289542494919141, 0.896916781503533, 1.0, 0.8292665603297107, 0.9495565333799167, 0.9468351265733028, 0.8606751756510949, 0.9215126923513757, 0.36780544774696344, 0.9031002677247391, 0.8037884497232345, 0.7491375710149024, 0.38264339942798953, 0.8910871854638854, 0.5513161792248773, 0.8221311800146563, 0.9635294111338131, 0.805438392944647, 0.9683839208477296, 1.0, 0.8725176323961279, 0.8704544662650214, 0.7130825083995453, 1.0, 0.9108242908590016, 0.7401266881590898, 0.9441518237028776, 0.8676184836694505, 0.27260257043057684, 0.966410184624177, 0.9207265089204273, 0.920283409368625, 0.920283409368625, 0.8892385063159653, 1.0, 0.4813721606173971, 0.9680975247490764, 0.8309401076758502, 0.8258022364276514, 0.9289542494919141, 0.7764656730962519, 0.9830124435719327, 0.9238877689380496, 0.9067749584667979, 0.9207265089204273, 0.9215110032256559, 0.8080522026094046, 0.9217146126934004, 0.8977573831682686, 1.0, 0.9026775857310148, 0.8247901310766182, 0.9406018863753673, 1.0, 0.6635747265011052, 1.0, 0.9026321368482364, 0.7324080115911205, 0.9215126923513757, 0.9635294111338131, 0.9323598782946632, 0.8851642960040433, 0.9422672588258918, 1.0, 0.31020535056772763, 0.9141000841539659, 0.9499861168443409, 0.761594612074824, 0.8338376642187639, 0.9660272508877419, 0.8313123185424158, 0.9118239030379824, 0.9044642401339957, 1.0, 0.9635294111338131, 0.9318351265733027, 0.5051206437303207, 0.9277243873888863, 0.966410184624177, 0.7434726595893233, 0.7932076705050406, 1.0, 0.9600553834135903, 0.6863560579461042, 0.8959900265439149, 1.0, 0.6544266909065675, 0.9330331470954285, 0.8016164190158708, 0.9562237378841005, 1.0, 0.7057051229072668, 1.0, 0.6309099006070308, 0.9567814728909636, 1.0, 1.0, 0.4813721606173971, 1.0, 1.0, 1.0, 0.9725011474210121, 1.0, 1.0, 0.9238394500691973, 1.0, 0.9501570703145379, 0.9289542494919141, 0.9352133283258579, 1.0, 0.30987023960223115, 0.6637638661040244, 0.956711228908929, 0.9449436332701645, 1.0, 1.0, 0.8336257349196183, 0.9170576952964131, 0.8377253763997345, 1.0, 0.9120033353993269, 0.9523104352365284, 0.8667343878768172, 1.0, 0.9096188406411201, 0.8955980328281301, 0.9217146126934004, 0.9289542494919141, 0.9490051755223876, 0.7526011619539674, 0.7526011619539674, 1.0, 0.6206862179768846, 0.772496321151799, 0.9321212086381336, 1.0, 1.0, 0.9354021130737269, 0.8588898303189523, 0.9053599694294705, 0.888132097598265, 0.9107159398088138, 0.9215126923513757, 1.0, 0.9056583090096291, 0.9217146126934004, 0.9065130582854939, 0.8064626894135283, 0.7916213020297636, 0.7720403060113344, 0.9289542494919141, 0.8777538763403605, 0.8655056700329364, 1.0, 0.6919689216458761, 0.8119808925419361, 1.0, 0.9725011474210121, 0.307221699738228, 0.5850662816271308, 0.901968614021996, 0.8804983047658983, 1.0, 0.892387748772268, 0.9584627887351178, 0.8947836446510709, 1.0, 1.0, 0.8892385063159653, 0.6502973371873174, 0.9683839208477296, 0.7032349297460765, 0.9497338034253828, 0.8933633833099063, 0.9497338034253828, 1.0, 0.901968614021996, 0.763188430183241, 0.7845131640307421, 1.0, 0.7585533552507637, 0.9582523400106997, 0.7016265496309229, 0.4466839945231893, 0.9635294111338131, 0.7365989766063477, 0.6184635986584919, 0.9376717074141803, 0.3118442564164614, 0.8910871854638854, 0.49030930450406585, 0.9361684767997713, 0.9217146126934004, 0.9264832677103618, 0.8336063484341889, 0.9437985500759485, 1.0, 0.8408348810255788, 0.9289542494919141, 0.8080522026094046, 0.6058424651692407, 0.8959900265439149, 0.966410184624177, 0.9600948805844232, 0.9564084233825603, 0.8382738479048725, 0.7927296331266827, 0.837637083531956, 1.0, 0.9104979088289846, 1.0, 0.9521775318277603, 1.0, 0.9053599694294705, 0.8474334174872993, 1.0, 0.8450599127771485, 0.8149441283737979, 0.32160693326868633, 0.9048182160290064, 0.9331580236066618, 1.0, 0.813149993477295, 0.9450753346970411, 0.966410184624177, 1.0, 0.9660272508877419, 0.9143879239397519, 1.0, 0.936752055107628, 0.9470228248649237, 1.0, 0.8215389757230824, 0.8974634481857526, 1.0, 0.936752055107628, 0.9165239329369728, 0.9215126923513757, 0.9600553834135903, 0.9217146126934004, 0.8831780262633904, 0.820504416790904, 0.8724358431483958, 0.6096462831167461, 1.0, 0.38568966389323306, 1.0, 0.761594612074824, 0.840759919488681, 0.9107159398088138, 0.5850662816271308, 0.7927995280655955, 0.9441518237028776, 1.0, 0.9323598782946632, 0.8773012763185428, 1.0, 0.9620033353993269, 0.7663187022564415, 0.820504416790904, 0.831256097916082, 1.0, 1.0, 0.27846913096568277, 1.0, 0.43204628787814087, 0.87388928720571, 0.8248056172114834, 0.966410184624177, 0.3279261760870241, 0.9441518237028776, 0.9045508804845042, 0.9683839208477296, 1.0, 0.9172668798784964, 0.7854270470202793, 0.9657517567313276, 1.0, 0.8191739701096303, 0.8320856318465295, 1.0, 0.8186292246753781, 0.6540920459296338, 0.9217146126934004, 1.0, 1.0, 1.0, 0.9289542494919141, 0.9325400085710811, 0.9217146126934004, 0.3059993007742159, 0.9498733743967636, 0.5051417653851575, 1.0, 0.859279267136342, 0.6930671323843829, 0.8917028689549173, 0.8462187025051897, 0.9217146126934004, 0.9583676774082095, 1.0, 0.5288056133514245, 0.7847797329235424, 0.9289542494919141, 0.9498733743967636, 1.0, 0.9468351265733028, 0.9683839208477296, 0.9635294111338131, 0.9217146126934004, 0.8648136681143916, 0.9289542494919141, 0.9498733743967636, 1.0, 0.9472873156881443, 0.948939479110037, 0.9217146126934004, 0.8572252889399417, 0.9572615927227479, 0.9289542494919141, 1.0, 0.6206862179768846, 0.9289542494919141, 0.9690470390182255, 0.31039297408614097, 0.9026775857310148, 0.9289542494919141, 1.0, 0.9342884434309551, 1.0, 1.0, 1.0, 0.847020929079007, 0.9443117408372359, 0.862347908057336, 1.0, 1.0, 0.9439664040171851, 0.8821296454239946, 0.9141000841539659, 0.9261420415269246, 1.0, 0.9427895563342799, 0.7821411805480895, 0.8531511596695294, 0.8287066478120482, 0.8235572543611509, 1.0, 0.966410184624177, 0.8846386294351021, 0.9217146126934004, 0.27846913096568277, 0.33799710319663573, 1.0, 0.7757925551610506, 0.966410184624177, 0.775479195514087, 0.9546221339246326, 0.9090440252597953, 0.8975252971187101, 0.940387748772268, 1.0, 0.9357730833493334, 0.8892385063159653, 0.5288056133514245, 0.31351865472205076, 0.7960310701098643, 0.9289542494919141, 1.0, 0.9800683463319189, 0.936752055107628, 0.6637638661040244, 0.9357730833493334, 0.9498733743967636, 0.9217146126934004, 1.0, 0.9498733743967636, 0.76588180077509, 0.9307091932148608, 0.8462187025051897, 0.940101844638672, 0.9365276486140073, 0.9498733743967636, 1.0, 0.7852780098784685, 0.920283409368625, 0.9299292663393219, 1.0, 0.9289542494919141, 0.6206862179768846, 0.9435321610524647, 0.9437985500759485, 0.9219188985107609, 1.0, 0.9498733743967636, 0.966410184624177, 0.888951063482764, 0.9443755095409017, 0.9468351265733028, 0.8965296466431494, 0.876923058378295, 0.9141000841539659, 0.9215126923513757, 1.0, 0.9407613352737074, 1.0, 0.6106880800400042, 0.8357117295209777, 1.0, 1.0, 0.27846913096568277, 1.0, 0.9396965873089294, 0.9330331470954285, 0.5343307614409688, 0.8821296454239946, 0.870013292995709, 0.802438385352046, 0.9090440252597953, 0.802438385352046, 0.9217146126934004, 0.9631318837364407, 0.9289542494919141, 0.8462187212440857, 1.0, 0.8238232230166638, 0.35629753997354174, 0.9289542494919141, 0.792380000217789, 0.9344907401578124, 0.6485596519019184, 0.8533420267585983, 0.9289542494919141, 0.9546221339246326, 0.9498733743967636, 0.9583676774082095, 0.8535087661549325, 0.4928582934065027, 0.8572077307599157, 1.0, 0.9376717074141803, 1.0, 0.8821296454239946, 0.23763144990206952]
Finish training and take 1h17m
Namespace(log_name='./RQ5/sstubs_8_3/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/sstubs_8_3/codet5p_770m_f', data_dir='./data/RQ5/sstubs_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00407
  global_step = 2
  train_loss = 1.1328
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00407
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 17.6 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:17.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00223
  global_step = 3
  train_loss = 1.0912
  ********************
Previous best ppl:1.00407
Achieve Best ppl:1.00223
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 19.35 	 Previous best codebleu 17.6
  ********************
 Achieve Best bleu:19.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00172
  global_step = 4
  train_loss = 0.4276
  ********************
Previous best ppl:1.00223
Achieve Best ppl:1.00172
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 28.03 	 Previous best codebleu 19.35
  ********************
 Achieve Best bleu:28.03
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00144
  global_step = 5
  train_loss = 0.3071
  ********************
Previous best ppl:1.00172
Achieve Best ppl:1.00144
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 31.92 	 Previous best codebleu 28.03
  ********************
 Achieve Best bleu:31.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00128
  global_step = 6
  train_loss = 0.1858
  ********************
Previous best ppl:1.00144
Achieve Best ppl:1.00128
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 43.93 	 Previous best codebleu 31.92
  ********************
 Achieve Best bleu:43.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00123
  global_step = 7
  train_loss = 0.1046
  ********************
Previous best ppl:1.00128
Achieve Best ppl:1.00123
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 64.93 	 Previous best codebleu 43.93
  ********************
 Achieve Best bleu:64.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00121
  global_step = 8
  train_loss = 0.0934
  ********************
Previous best ppl:1.00123
Achieve Best ppl:1.00121
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 75.36 	 Previous best codebleu 64.93
  ********************
 Achieve Best bleu:75.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00121
  global_step = 9
  train_loss = 0.0749
  ********************
Previous best ppl:1.00121
Achieve Best ppl:1.00121
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 75.89 	 Previous best codebleu 75.36
  ********************
 Achieve Best bleu:75.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0012
  global_step = 10
  train_loss = 0.0564
  ********************
Previous best ppl:1.00121
Achieve Best ppl:1.0012
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 77.93 	 Previous best codebleu 75.89
  ********************
 Achieve Best bleu:77.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0012
  global_step = 11
  train_loss = 0.056
  ********************
Previous best ppl:1.0012
Achieve Best ppl:1.0012
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 78.33 	 Previous best codebleu 77.93
  ********************
 Achieve Best bleu:78.33
  ********************
reload model from RQ5/sstubs_8_3/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_8_3/test.jsonl
  codebleu = 81.46 
  Total = 500 
  Exact Fixed = 93 
[8, 17, 19, 20, 26, 28, 38, 39, 50, 53, 76, 80, 91, 106, 110, 112, 120, 130, 138, 142, 149, 152, 153, 155, 156, 159, 160, 162, 166, 171, 172, 176, 180, 188, 189, 192, 193, 200, 210, 213, 219, 223, 224, 236, 253, 267, 269, 272, 278, 282, 288, 291, 301, 303, 313, 319, 321, 330, 334, 337, 341, 342, 343, 350, 362, 370, 377, 378, 384, 386, 387, 388, 390, 392, 393, 398, 404, 410, 418, 425, 432, 440, 444, 446, 450, 460, 462, 465, 466, 468, 481, 496, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[99]
  ********************
  Total = 500 
  Exact Fixed = 93 
[8, 17, 19, 20, 26, 28, 38, 39, 50, 53, 76, 80, 91, 106, 110, 112, 120, 130, 138, 142, 149, 152, 153, 155, 156, 159, 160, 162, 166, 171, 172, 176, 180, 188, 189, 192, 193, 200, 210, 213, 219, 223, 224, 236, 253, 267, 269, 272, 278, 282, 288, 291, 301, 303, 313, 319, 321, 330, 334, 337, 341, 342, 343, 350, 362, 370, 377, 378, 384, 386, 387, 388, 390, 392, 393, 398, 404, 410, 418, 425, 432, 440, 444, 446, 450, 460, 462, 465, 466, 468, 481, 496, 498]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[99]
  codebleu = 81.46 
[0.8902637847731272, 0.9272945586059962, 0.7729007577788838, 0.9495565333799167, 0.21866014349720522, 0.6656223719552603, 0.31904920988427427, 1.0, 0.6408391522189848, 0.17831319933284095, 0.6031190556313669, 0.9289542494919141, 0.4752621989496865, 0.6766433148075119, 0.7933524811232104, 0.7623175707022043, 1.0, 0.9447883869073515, 1.0, 1.0, 0.7316694889691049, 0.8606751756510949, 0.6577095841418729, 0.8560949409597712, 0.9616102999368312, 1.0, 0.8400553834135902, 1.0, 0.8167274641821936, 0.9514967635709539, 0.3275720527894636, 0.33255550196014716, 0.9372561368734473, 0.8769465098454015, 0.8274557585670506, 0.2581966957215153, 0.23134922722104162, 1.0, 1.0, 0.340636039232948, 0.8480582060293878, 0.3540396192874326, 0.9517918920357142, 0.7584803831925206, 0.8897574591522375, 0.9543920430664012, 0.8954305487814924, 0.9012977329022187, 0.851499440293822, 1.0, 0.7835279332483667, 0.85119134561276, 1.0, 0.9636496648476234, 0.850084905673596, 0.8909862111271478, 0.15041730480471188, 0.844284181867484, 0.9636496648476234, 0.901968614021996, 0.758857880653734, 0.9349697756418187, 0.8606751756510949, 0.9215126923513757, 0.5227748565038876, 0.8826792660464275, 0.7922967326236081, 0.29573486599726895, 0.9325815531825292, 0.8910871854638854, 0.3548518477075675, 0.8221311800146563, 0.9459070762117685, 0.805438392944647, 0.9683839208477296, 1.0, 0.6438855906161014, 0.8704544662650214, 0.706098469699026, 1.0, 0.9108242908590016, 0.6288166360170014, 0.9441518237028776, 0.8676184836694505, 0.8674435206185069, 0.966410184624177, 0.9207265089204273, 0.2581966957215153, 0.2581966957215153, 0.8167274641821936, 1.0, 0.9317899201148767, 0.9412418786397232, 0.8000245406125166, 0.8255195881312929, 0.2192773856784755, 0.7390861550091488, 0.9830124435719327, 0.9586307963613738, 0.17831319933284095, 0.9207265089204273, 0.9286730262313432, 0.7886024135688656, 0.9217146126934004, 0.5108651780469062, 1.0, 0.9026775857310148, 0.8550592998169957, 0.8839588406233774, 1.0, 0.6635747265011052, 1.0, 0.8712559885401261, 0.669074191877338, 0.9215126923513757, 0.9459070762117685, 0.9323598782946632, 0.898610628990282, 0.9044279138103133, 1.0, 0.8274557585670506, 0.8767099682404029, 0.8842313913178967, 0.8088598314985247, 0.8080113468050034, 0.9492321316796177, 0.779672232463307, 0.8875721495437094, 0.9044642401339957, 1.0, 0.9459070762117685, 0.9111462462300539, 0.5254938990406912, 0.987026696724848, 0.966410184624177, 0.7891346536757111, 0.7939008788146948, 1.0, 0.9600553834135903, 0.8515505278871711, 0.946462071715882, 1.0, 0.44905835601370364, 0.9257703904953272, 0.8016164190158708, 0.9562237378841005, 0.9636496648476234, 0.6031190556313669, 1.0, 0.5921434792714542, 0.9166945353206586, 1.0, 1.0, 0.9317899201148767, 1.0, 1.0, 0.9636496648476234, 0.9725011474210121, 1.0, 1.0, 0.9238394500691973, 1.0, 0.8796947272705897, 0.2192773856784755, 0.9352133283258579, 1.0, 0.874146718465499, 0.9514967635709539, 0.956711228908929, 0.9317146511344261, 1.0, 1.0, 0.21866014349720522, 0.9170576952964131, 0.8377253763997345, 1.0, 0.8606130108993177, 0.9023338728659691, 0.8667343878768172, 1.0, 0.8030515870907278, 0.6469122790389242, 0.20178867681479457, 0.9289542494919141, 0.9490051755223876, 0.7526011619539674, 0.7526011619539674, 1.0, 1.0, 0.7425110464214316, 0.9321212086381336, 1.0, 1.0, 0.9194869803498233, 0.8502218606010179, 0.6496438669012632, 0.888132097598265, 0.8433961293276255, 0.9215126923513757, 1.0, 0.8301906072743601, 0.21623461575357328, 0.9093978589879124, 0.7799929854363867, 0.804537523915227, 0.7720403060113344, 0.2192773856784755, 0.8777538763403605, 0.7360547879689501, 1.0, 0.7294689216458761, 0.7263295307859631, 1.0, 0.9725011474210121, 0.82996315174193, 0.5850662816271308, 0.901968614021996, 0.7840431752560024, 1.0, 0.892387748772268, 0.9204343426305499, 0.6902544562065016, 1.0, 1.0, 0.8167274641821936, 0.6642742735155721, 0.9683839208477296, 0.6859684943964812, 0.9497338034253828, 0.8933633833099063, 0.9497338034253828, 0.96612097182042, 0.901968614021996, 0.8317663828924111, 0.7845131640307421, 1.0, 0.901672684198352, 0.8497181474174771, 0.6868474083243508, 0.43181014614221946, 0.9459070762117685, 0.7365989766063477, 0.7561237077100311, 0.9376717074141803, 0.8748583244748018, 0.8910871854638854, 0.5134585381657101, 0.9169189006410692, 0.2041072311786718, 0.9583676774082095, 0.8336063484341889, 0.9437985500759485, 1.0, 0.6922127104874237, 0.5339024140221744, 0.7886024135688656, 0.8816015466501335, 0.946462071715882, 0.966410184624177, 0.8968428693921784, 0.9397417567158937, 0.8382738479048725, 0.7927296331266827, 0.8424756869867742, 0.9636496648476234, 0.8923757554522853, 1.0, 0.9521775318277603, 1.0, 0.6496438669012632, 0.8149209426020443, 1.0, 0.7202546562142633, 0.20012365594661552, 0.32160693326868633, 0.9213340161631496, 0.8991369640971377, 1.0, 0.7865670749519025, 0.9450753346970411, 0.966410184624177, 1.0, 0.9492321316796177, 0.6810313328422608, 0.9636496648476234, 0.9097725521291267, 0.9470228248649237, 1.0, 0.9521775318277603, 0.8974634481857526, 1.0, 0.9222725521291268, 0.897936047397687, 0.9215126923513757, 0.9600553834135903, 0.2041072311786718, 0.9245037448286011, 0.8021717237765789, 0.9513403413335417, 0.6156860958663175, 1.0, 0.38568966389323306, 1.0, 0.8088598314985247, 0.8149223254631701, 0.8433961293276255, 0.5850662816271308, 0.5947460858027493, 0.8696019816102547, 0.753123150364972, 0.9323598782946632, 0.8638988585762717, 1.0, 0.3198770387992647, 0.7109854004939737, 0.8021717237765789, 0.831256097916082, 0.9636496648476234, 1.0, 0.778736086530805, 1.0, 0.9023939815882147, 0.87388928720571, 0.8088652431854009, 0.966410184624177, 0.3447610883553041, 0.9441518237028776, 0.8429702623655579, 0.9683839208477296, 1.0, 0.9427895563342799, 0.328446825825717, 0.8724942788782983, 1.0, 0.7913624457588009, 0.8320856318465295, 1.0, 0.8186292246753781, 0.9343980242513754, 0.2008636546254698, 1.0, 1.0, 1.0, 0.1336058768160965, 0.9325400085710811, 0.20413805854938205, 0.6407476497212097, 0.9498733743967636, 0.5051417653851575, 1.0, 0.859279267136342, 0.6966426466671506, 0.8917028689549173, 0.6632276284673109, 0.3715485161010401, 0.9425949077241198, 0.9636496648476234, 0.5288056133514245, 0.7636665014767161, 0.8130711447701425, 0.9498733743967636, 1.0, 0.9318351265733027, 0.9683839208477296, 0.9459070762117685, 0.20143435109098323, 0.8888828333105372, 0.49730737477045545, 0.9498733743967636, 1.0, 0.8849009450858702, 0.948939479110037, 0.4859338784244227, 0.6910408933064586, 0.9572615927227479, 0.2192773856784755, 1.0, 1.0, 0.2436313581016512, 0.831362477723079, 0.7591394663246853, 0.7729007577788838, 0.23134922722104162, 1.0, 0.884288443430955, 1.0, 1.0, 1.0, 0.847020929079007, 0.9443117408372359, 0.862347908057336, 1.0, 1.0, 0.8849021050669312, 0.8821296454239946, 0.8767099682404029, 0.8999295507835838, 1.0, 0.9364713827761666, 0.7821411805480895, 0.8531511596695294, 0.8592315791676481, 0.8092648168353909, 1.0, 0.966410184624177, 0.782966282462418, 0.20036474819927036, 0.778736086530805, 0.6643580458280314, 1.0, 0.7553529537862455, 0.966410184624177, 0.6725705865318179, 0.9546221339246326, 0.8186691030330882, 0.7999526096773, 0.8841007521231434, 1.0, 0.9107730833493335, 0.8167274641821936, 0.5288056133514245, 0.4470539305662686, 0.46080426005109126, 0.5606885861009054, 1.0, 0.9800683463319189, 0.936752055107628, 0.9514967635709539, 0.9107730833493335, 0.9498733743967636, 0.24587464939512496, 1.0, 0.9498733743967636, 0.731164568837595, 0.8489095125038026, 0.6632276284673109, 0.940101844638672, 0.9165276486140073, 0.9498733743967636, 1.0, 0.3811651728129726, 0.2581966957215153, 0.9493239030379825, 1.0, 0.12951236875172675, 1.0, 0.9172498814207914, 0.9437985500759485, 0.9616102999368312, 1.0, 0.9498733743967636, 0.966410184624177, 0.868055092450922, 0.3273056062682239, 0.9468351265733028, 0.8965296466431494, 0.3206271394870877, 0.9141000841539659, 0.9215126923513757, 1.0, 0.548193227829207, 1.0, 0.8769465098454015, 0.936752055107628, 1.0, 1.0, 0.778736086530805, 1.0, 0.9685667308405901, 0.7849379363556296, 0.23547169727047174, 0.8821296454239946, 0.9266684096158422, 0.7835565737918848, 0.8492582054292956, 0.7835565737918848, 0.22930073168413706, 0.9631318837364407, 0.49730737477045545, 0.9272945586059962, 1.0, 0.9565292378112795, 0.3540396192874326, 0.5339024140221744, 0.792380000217789, 0.9344907401578124, 0.3811945138381043, 0.895040008571081, 0.5339024140221744, 0.9546221339246326, 0.9498733743967636, 0.9583676774082095, 0.7311793828804349, 0.7391200591044521, 0.8572077307599157, 1.0, 0.9376717074141803, 1.0, 0.8821296454239946, 0.8213196549536046]
Finish training and take 1h21m
