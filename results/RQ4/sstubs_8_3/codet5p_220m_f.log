Namespace(log_name='./RQ5/sstubs_8_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_8_3/codet5p_220m_f', data_dir='./data/RQ5/sstubs_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00339
  global_step = 2
  train_loss = 0.9552
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00339
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 6.43 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:6.43
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00216
  global_step = 3
  train_loss = 0.9421
  ********************
Previous best ppl:1.00339
Achieve Best ppl:1.00216
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 14.46 	 Previous best codebleu 6.43
  ********************
 Achieve Best bleu:14.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00173
  global_step = 4
  train_loss = 0.4669
  ********************
Previous best ppl:1.00216
Achieve Best ppl:1.00173
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 24.07 	 Previous best codebleu 14.46
  ********************
 Achieve Best bleu:24.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00143
  global_step = 5
  train_loss = 0.3254
  ********************
Previous best ppl:1.00173
Achieve Best ppl:1.00143
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 28.59 	 Previous best codebleu 24.07
  ********************
 Achieve Best bleu:28.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00128
  global_step = 6
  train_loss = 0.1877
  ********************
Previous best ppl:1.00143
Achieve Best ppl:1.00128
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 37.45 	 Previous best codebleu 28.59
  ********************
 Achieve Best bleu:37.45
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00117
  global_step = 7
  train_loss = 0.1562
  ********************
Previous best ppl:1.00128
Achieve Best ppl:1.00117
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 53.33 	 Previous best codebleu 37.45
  ********************
 Achieve Best bleu:53.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00111
  global_step = 8
  train_loss = 0.1498
  ********************
Previous best ppl:1.00117
Achieve Best ppl:1.00111
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 73.17 	 Previous best codebleu 53.33
  ********************
 Achieve Best bleu:73.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00108
  global_step = 9
  train_loss = 0.0824
  ********************
Previous best ppl:1.00111
Achieve Best ppl:1.00108
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 81.63 	 Previous best codebleu 73.17
  ********************
 Achieve Best bleu:81.63
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00107
  global_step = 10
  train_loss = 0.0863
  ********************
Previous best ppl:1.00108
Achieve Best ppl:1.00107
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 83.46 	 Previous best codebleu 81.63
  ********************
 Achieve Best bleu:83.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00107
  global_step = 11
  train_loss = 0.0927
  ********************
Previous best ppl:1.00107
Achieve Best ppl:1.00107
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
  codebleu-4 = 85.58 	 Previous best codebleu 83.46
  ********************
 Achieve Best bleu:85.58
  ********************
reload model from RQ5/sstubs_8_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/sstubs_8_3/test.jsonl
  codebleu = 85.25 
  Total = 500 
  Exact Fixed = 45 
[17, 73, 116, 120, 126, 130, 131, 138, 149, 152, 156, 157, 159, 160, 162, 189, 193, 200, 213, 223, 232, 236, 241, 253, 269, 278, 283, 285, 291, 301, 330, 334, 342, 350, 362, 365, 378, 384, 386, 404, 432, 446, 450, 460, 466]
  Syntax Fixed = 1 
[297]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 45 
[17, 73, 116, 120, 126, 130, 131, 138, 149, 152, 156, 157, 159, 160, 162, 189, 193, 200, 213, 223, 232, 236, 241, 253, 269, 278, 283, 285, 291, 301, 330, 334, 342, 350, 362, 365, 378, 384, 386, 404, 432, 446, 450, 460, 466]
  Syntax Fixed = 1 
[297]
  Cleaned Fixed = 0 
[]
  codebleu = 85.25 
[0.9321212086381336, 0.8959902461818039, 0.9493239030379825, 0.9495565333799167, 0.2576349807117979, 0.6488378866864628, 0.9211131842675646, 0.9500416069012747, 0.42648587344647043, 0.8859478236166698, 0.23821952302946187, 0.9289542494919141, 0.5053936838661389, 0.6766433148075119, 0.7933524811232104, 0.8632148025904984, 1.0, 0.9447883869073515, 0.9225510753875747, 0.9500416069012747, 0.7108260466284402, 0.8224864091306889, 0.6577095841418729, 0.9470228248649237, 0.9616102999368312, 0.9468301376782615, 0.8400553834135902, 0.9468301376782615, 0.7840629381472555, 0.9514967635709539, 0.881955507815954, 0.24509318780557446, 0.9372561368734473, 0.3167644169687551, 0.8642470965743208, 0.559110204539902, 0.9289542494919141, 0.9468301376782615, 0.9468301376782615, 0.37080132206011496, 0.8961079319106557, 0.31790557332069963, 0.9517918920357142, 0.8368293296348667, 0.9546221339246326, 0.9041734668686527, 0.8954305487814924, 0.8546248763868238, 0.19276160999761313, 0.9468301376782615, 0.9642836546885394, 0.9523104352365284, 0.9468301376782615, 0.9468301376782615, 0.850084905673596, 0.9610055919835154, 0.9289542494919141, 0.8999789643332932, 0.9468301376782615, 0.9477454832552463, 0.9083643249569027, 0.9468351265733028, 0.8224864091306889, 0.8545437930833992, 0.17721725520116569, 0.651342871130222, 0.9260485387213218, 0.3877465256350663, 0.3652079525194293, 0.8910871854638854, 0.6777218023625289, 0.8748733743967635, 1.0, 0.805438392944647, 0.9683839208477296, 0.9468301376782615, 0.8628832369820862, 0.8704544662650214, 0.7130825083995453, 0.9468301376782615, 0.9108242908590016, 0.9312991012532417, 0.9441518237028776, 0.7924541110463985, 0.8619179987828729, 0.966410184624177, 0.9207265089204273, 0.559110204539902, 0.559110204539902, 0.7840629381472555, 0.353602565942495, 0.9317899201148767, 0.9680975247490764, 0.8000245406125166, 0.7339735519853505, 0.9289542494919141, 0.8999137044664197, 0.9477454832552463, 0.9146180809762077, 0.8859478236166698, 0.9207265089204273, 0.9558104142545454, 0.944274958466798, 0.9217146126934004, 0.5307659323385833, 0.9468301376782615, 0.9026775857310148, 0.908115627804579, 0.8134658863594189, 0.9468301376782615, 0.3157287009265204, 0.9468301376782615, 0.9026321368482364, 0.9098501992036327, 0.8545437930833992, 1.0, 0.9323598782946632, 0.8466799550259199, 0.7987474775335555, 1.0, 0.8642470965743208, 0.9141000841539659, 0.9038898481683997, 0.47993974221201385, 0.9327938350960072, 1.0, 0.8313123185424158, 0.9118239030379824, 0.9044642401339957, 1.0, 1.0, 0.8743913462290018, 0.8147166794810943, 0.987026696724848, 0.966410184624177, 0.9145980101874507, 0.9274843030447293, 1.0, 0.9600553834135903, 0.9493239030379825, 0.971284912875797, 0.9468301376782615, 0.1678806968688521, 0.5043387359084706, 0.8016164190158708, 0.9562237378841005, 0.9468301376782615, 0.23821952302946187, 1.0, 0.6949992150849486, 0.9567814728909636, 1.0, 0.9468301376782615, 0.9317899201148767, 0.9468301376782615, 1.0, 1.0, 0.9725011474210121, 1.0, 1.0, 0.8487134384339086, 1.0, 0.8829091833737499, 0.9289542494919141, 0.9352133283258579, 0.9468301376782615, 0.8673804501268787, 0.9514967635709539, 0.956711228908929, 0.9449436332701645, 0.8337841808515074, 0.9468301376782615, 0.2576349807117979, 0.8867923019455073, 0.8377253763997345, 0.9468301376782615, 0.8977185688720069, 0.9523104352365284, 0.8667343878768172, 0.9468301376782615, 0.9266829146927709, 0.7582676383434297, 0.9217146126934004, 0.9289542494919141, 0.9377704204852682, 0.7526011619539674, 0.7526011619539674, 0.9468301376782615, 1.0, 0.772496321151799, 0.9321212086381336, 0.9500416069012747, 1.0, 0.7643600793481078, 0.891318859382493, 0.790639009147663, 0.9077668318893848, 0.7552130730048433, 0.8545437930833992, 1.0, 0.8414554294087475, 0.9217146126934004, 0.9106965620392573, 0.8064626894135283, 0.9259424422574842, 0.8365954173448955, 0.9289542494919141, 0.8917317601325887, 0.8655056700329364, 0.9468301376782615, 0.33799505679730246, 0.8642470965743208, 1.0, 0.9725011474210121, 0.9655267507815484, 0.49387098229972204, 0.9477454832552463, 0.8804983047658983, 0.9468301376782615, 0.892387748772268, 0.9584627887351178, 0.8947836446510709, 1.0, 0.9468301376782615, 0.7840629381472555, 0.6642742735155721, 0.9683839208477296, 0.6881309740726359, 0.8837241867662415, 0.8933633833099063, 0.8837241867662415, 1.0, 0.9477454832552463, 0.7942663828924112, 0.8439975336408694, 1.0, 0.901672684198352, 0.9582523400106997, 0.7016265496309229, 0.24000157336236694, 1.0, 0.7369342414615079, 0.7880116169536691, 0.9689347101697146, 0.8839519845504367, 0.8910871854638854, 0.9600553834135903, 0.8089262892827052, 0.9217146126934004, 0.9583676774082095, 0.9525078218537795, 0.9437985500759485, 1.0, 0.8655948908847075, 0.9289542494919141, 0.944274958466798, 0.8816015466501335, 0.971284912875797, 0.966410184624177, 0.9600948805844232, 0.9564084233825603, 0.8249091621790023, 0.8557768311681513, 0.9456972075000312, 0.9468301376782615, 0.9104979088289846, 0.3471330732795079, 0.9521775318277603, 1.0, 0.790639009147663, 0.8474334174872993, 0.3428456545380965, 0.8711309068006623, 0.9217146126934004, 0.9524183912964097, 0.9665788286155268, 0.8991369640971377, 1.0, 0.6329475608578975, 0.9616136179279984, 0.966410184624177, 0.9468301376782615, 1.0, 0.9143879239397519, 1.0, 0.936752055107628, 0.9470228248649237, 0.9161007909599712, 0.9415948169054367, 0.5919997361822817, 1.0, 0.936752055107628, 0.897936047397687, 0.8545437930833992, 0.9600553834135903, 0.9217146126934004, 0.9622725659469971, 0.9493239030379825, 0.8791028584168332, 0.6435126018401478, 1.0, 0.9544887439837764, 0.9225510753875747, 0.47993974221201385, 0.9449216567144265, 0.7552130730048433, 0.49387098229972204, 0.9612141455329344, 0.9441518237028776, 0.22970760740607388, 0.8427695539611171, 0.9576902216791403, 0.9468301376782615, 0.9620033353993269, 0.7468709816695149, 0.9493239030379825, 0.831256097916082, 0.9468301376782615, 0.3428456545380965, 0.9537546369958738, 0.9468301376782615, 0.9565292378112795, 0.3184766059771862, 0.9517918920357142, 0.966410184624177, 0.3683508638118963, 0.9441518237028776, 0.8344239311421204, 0.9683839208477296, 1.0, 0.9427895563342799, 0.8886420415269245, 0.8826216840840828, 1.0, 0.8521105690558852, 0.8320856318465295, 0.9468301376782615, 0.8730911354138612, 0.6837973464127917, 0.9217146126934004, 0.9468301376782615, 1.0, 0.9468301376782615, 0.9289542494919141, 0.9325400085710811, 0.9217146126934004, 0.8821296454239946, 0.9498733743967636, 0.5248326023429876, 1.0, 0.859279267136342, 0.8761939126718362, 0.9076642269253836, 0.9493239030379825, 0.9217146126934004, 0.9202598925620489, 0.9468301376782615, 0.786355374034859, 0.7538576425176133, 0.9289542494919141, 0.9498733743967636, 1.0, 0.9084991874065247, 0.9683839208477296, 1.0, 0.9217146126934004, 0.9576902216791403, 0.9289542494919141, 0.9498733743967636, 0.9468301376782615, 0.9472873156881443, 0.948939479110037, 0.9217146126934004, 0.9558104142545454, 0.9572615927227479, 0.9289542494919141, 0.9468301376782615, 1.0, 0.9289542494919141, 0.9690470390182255, 0.8, 0.9493239030379825, 0.9289542494919141, 1.0, 0.8285954077614559, 1.0, 0.9468301376782615, 0.9468301376782615, 0.8130629786392027, 0.3332868433872681, 0.8204912180636579, 0.9500416069012747, 0.9468301376782615, 0.916219287905375, 0.8502947691857088, 0.9141000841539659, 0.8999295507835838, 0.9468301376782615, 0.5276213253954563, 0.7698443759088608, 0.8531511596695294, 0.9540286538025624, 0.3024309048238601, 1.0, 0.966410184624177, 0.3456639875170132, 0.9217146126934004, 0.9537546369958738, 0.30908051104784984, 0.9468301376782615, 0.7712684376771003, 0.966410184624177, 0.9514967635709539, 0.9375434840463579, 0.8889354577543411, 0.7820842106531639, 0.9178662308912058, 0.9161007909599712, 0.8691302219929937, 0.7840629381472555, 0.786355374034859, 0.9052050509144789, 0.30897618472573485, 0.9289542494919141, 0.9468301376782615, 0.9800683463319189, 0.8813634612200358, 0.9514967635709539, 0.8691302219929937, 0.9498733743967636, 0.9217146126934004, 1.0, 0.9498733743967636, 0.7831318837364406, 0.9092001332540531, 0.9493239030379825, 0.940101844638672, 0.9165276486140073, 0.9498733743967636, 0.9468301376782615, 0.33332923507457834, 0.559110204539902, 0.9493239030379825, 0.9468301376782615, 0.9289542494919141, 1.0, 0.9172498814207914, 0.9437985500759485, 0.9616102999368312, 1.0, 0.9498733743967636, 0.966410184624177, 0.9673871209776717, 0.7665614887924286, 0.9468351265733028, 0.8965296466431494, 0.31082900960558024, 0.9141000841539659, 0.8545437930833992, 1.0, 0.9114994402938219, 0.9468301376782615, 0.3167644169687551, 0.936752055107628, 0.9468301376782615, 1.0, 0.9537546369958738, 0.9468301376782615, 0.9685667308405901, 0.9330331470954285, 0.94236547382835, 0.3069395337601283, 0.9266684096158422, 0.9471695160234401, 0.8889354577543411, 0.9471695160234401, 0.9217146126934004, 0.9057440814691728, 0.9289542494919141, 0.7544685445117228, 0.9500416069012747, 0.9565292378112795, 0.31790557332069963, 0.9289542494919141, 0.9525078218537795, 0.3472996542387723, 0.3397535997124671, 0.252463536082756, 0.9289542494919141, 0.9546221339246326, 0.9498733743967636, 0.9202598925620489, 0.8489597693000923, 0.9685667308405901, 0.33507136498457635, 0.9468301376782615, 0.9689347101697146, 0.852565642646234, 0.8502947691857088, 0.23455875651995364]
Finish training and take 1h0m
Namespace(log_name='./RQ5/sstubs_8_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/sstubs_8_3/codet5p_220m_f', data_dir='./data/RQ5/sstubs_8_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00339
  global_step = 2
  train_loss = 1.0202
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00339
  ********************
BLEU file: ./data/RQ5/sstubs_8_3/validation.jsonl
