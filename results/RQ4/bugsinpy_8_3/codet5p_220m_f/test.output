class TestProcessProtocol(protocol.ProcessProtocol): def __init__(self):          self.deferred = defer.Deferred()         self.out = ''         self.err = ''         self.exitcode = None      def outReceived(self, data):          self.deferred.resolve(data)      def errReceived(self, data):          self.deferred.reject(data)
def get_file(fname,          Path to the downloaded file      if cache_dir is None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5_hash          hash_algorithm = 'md5'      if md5_hash is not None and file_hash is None:      file_hash = md5
class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy(_GroupBy):                 result = algorithms.take_nd(obj.values, result) if post_processing:                 result = post_processing(result, inferences) class GroupBy
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)          executor = ProcessPoolExecutor(max_workers=worker_count)      try:              loop.run_until_complete(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_format
import logging logger = logging.getLogger(__name__)  title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text>)") info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")
from __future__ import unicode_literals from __future__ import print_function from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_literals from __future__ import division from __future__ import absolute_import from __future__ import unicode_
def read_conllx(input_data, use_morphology=False, n=0):                  continue                  try:                      id_ = int(id_) - 1                     head = (int(head) if head != "0" else id_                      dep = "ROOT" if dep == "root"                      tag = pos if tag == "_" else tag                      tag = tag + "__" if use_morphology else tag                      tag = tag + "__"
class ObjectBlock(Block):                  if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return self
class QuarterOffset(DateOffset):         shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(QuarterOffset):         class BQuarterEnd(Quarter
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass class Block(PandasObject):          check_setitem_lengths(indexer
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_down
class ItemMeta(ABCMeta): new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None      if output_padding is None:          if padding == 'valid':          return None      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(dim_size, stride_size, kernel_size, padding, output_padding)      return deconv_length(
 try:     from gzip import GzipFile  import six except ImportError:  from gzip import GzipFile  import six 
def add_codes(err_cls):             class ErrorsWithCodes(object):             def __getattribute(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()
class BarPlot(MPLPlot):          def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))         ax.set_xticks(self.tick_pos)          if name is not None and self.use_index:              ax.set_xlabel(name)
class _MergeOperation:                  )                  ]                 left_keys = [self.left.index.values]                 if left_drop:                 self.left = self.left._drop_labels_or_levels(left_drop)
class Series(base.IndexOpsMixin, generic.NDFrame):         kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         ) if non_mapping: return self._set_name(index, inplace=kwargs.get("inplace", False))         return super().rename(index=index, **kwargs)         @Substitution(**_shared_doc_kwargs)         @Appender(generic.NDFrame.reindex.__doc__)
class BaseAsyncIOLoop(IOLoop):                  if all_fds:                  self.close_fd(fileobj)                  self.asyncio_loop.close()def add_handler(self, fd, handler, events):                  fd, fileobj = self.split_fd(fd)
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def _create_testcase(method, testcase_name):          def eb_wrapper(failure):              case = _create_testcase(method, testcase_name)             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addTestCase(case, exc_info)          request.callback = eb_wrapper          def _create_testcase(method, testcase_name):          def eb_wrapper(failure):              case = _create_testcase(method, testcase_name)             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addTestCase(case, exc_info)          request.callback = eb_wrapper          def _create_testcase(method, testcase_name):          def eb_wrapper(failure):              case = _create_testcase(method, testcase_name)             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addTestCase(case, exc_info)          request.callback = eb_wrapper          def _create_testcase(method, testcase_name):          def eb_wrapper(failure):              case = _create_testcase(method, testcase_name)             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addTestCase(case, exc_info)          request.callback = eb_wrapper          def _create_testcase(method, testcase_name):          def eb_wrapper(failure):              case = _create_testcase(method, testcase_name)             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addTestCase(case, exc_info)          request.callback = eb_wrapper          def _create_testcase(method, testcase_name):          def eb_wrapper(failure):              case = _create_testcase(method, testcase_name)             exc
class HiveCommandClient(HiveClient):              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string,          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'same'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'valid'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'valid'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'same'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'valid'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'same'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'valid'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'same'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'valid'.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                  output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps)
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
 from difflib import get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_close_matches, get_
def base_url(url): def urljoin(base, path): if not isinstance(path, compat_str) or not path: return None if re.match(r'^(?:https?:)?          return path if not isinstance(path, compat_str) or not path: return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.match(r'^(?:https?:)?          return None      return compat_urlparse.urljoin(base, path) if re.
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)          if mask.any():                 ret[mask]              return ret          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return _cat_compare_op(op)          if is_scalar(op):                 return
and Index.__new__.  These should not depend on core.internals.  from typing import TYPE_CHECKING, Any, Optional, Sequence, Union, cast  import numpy as np from typing import TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE_CHECKING, TYPE
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else else return jsonable_encoder(response_content)
def _isna_ndarraylike_old(obj) return result  def notna(obj):      Detect non-missing values for an array-like object.  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_old(obj)  return result  def _isna_ndarraylike_
class Worker(object):             return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          def is_trivial_worker(self, state):             return state.get_pending_tasks()
 class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self): if len(self.binlabels) != 0: return self.result_index() class BinGrouper(BaseGrouper):          ngroups,          @cache_readonly def result_index(self):
_dict.get(representation_id, {}).update(f)                              formats.append({location_key(initialization_url): initialization_url})                              formats.extend(representation_ms_info['fragments'])                         try:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                      except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format.update(f)                      else:                             self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats
class AsyncioServer:      task = asyncio.ensure_future(coro, loop=self.loop)      return task      def __await__(self):      task = asyncio.ensure_future(self.serve_coro, loop=self.loop)      return task
class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._codes != -1          if not good.all():             if skipna:                  return np.nan class Categorical(ExtensionArray, PandasObject):                  good = self._
class FastParquetImpl(BaseImpl):              if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda lambda:          else:              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")
class ReduceLROnPlateau(Callback):             self.mode = 'auto'             if (self.mode == 'min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = -np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0
class Sanic:                      if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)
default 'raise'              ) new_dates = new_dates.view(DT64NS_DTYPE) dtype = tz_to_dtype(tz) return self._simple_new(new_dates, dtype=dtype, freq=self.freq) 
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                      val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:
class CentralPlannerScheduler(Scheduler):             tasks.sort(key=self._rank(), reverse=True)              for task in tasks:             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING' and in_workers:
class TestPeriodIndex(DatetimeLike): idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN") ts = Series(np.random.randn(len(idx)), index=idx) result = ts[2007] expected = ts[idx == "2007"] tm.assert_series_equal(result, expected)
from .common import InfoExtractor  def _media_xml_tag(tag):  from ..utils import compat_urlparse, ExtractorError  def _extract_xml_tag(tag):  from ..utils import compat_html_parse, ExtractorError  def _real_extract(url):  url = compat_urlparse(url)  mobj = _extract_xml_tag(tag)  if mobj and not mobj.startswith('http'):   mobj = compat_html_parse(mobj)   mobj = mobj.replace('http://', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.replace('http://www.', '')  mobj = mobj.
class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect, old_cmd, self.script) class CorrectedCommand(object): compatibility_call(self.side_effect
class BaseComparisonOpsTests(BaseOpsUtil):          assert result is NotImplemented          else:          raise pytest.skip(f"{type(data).__name} does not implement __eq__")
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:              self.iterable = iterable              self.disable = kwargs              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n =
def unified_timestamp(date_str, day_first=True):     date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)     timezone, date_str = extract_timezone(date_str)     return unified_timestamp(date_str, day_first=False)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return unified_timestamp(date_str, day_first=True)     return
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin          )         if isinstance(maybe_slice, slice):         return self[maybe_slice]         taken = ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )         freq = self.freq if is_period_dtype(self)         assert taken.freq == freq, (taken.freq, taken)         return self._shallow_copy(taken, freq=freq)
def get_flat_dependant(dependant: Dependant) -> Dependant:     def is_scalar_field(field: Field)     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )     def is_scalar_sequence_field(field: Field)     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )     return dependant def get_flat_dependant(dependant: Dependant) -> Dependant:     def is_scalar_field(field: Field)     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )     def is_scalar_sequence_field(field: Field)     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )     return dependant def get_flat_dependant(dependant: Dependant) -> Dependant:     def is_scalar_field(field: Field)     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )     def is_scalar_sequence_field(field: Field)     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )     return dependant def get_flat_dependant(dependant: Depend
class DataFrame(NDFrame, dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)
def should_series_dispatch(left, right, op):     return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left, right, op):     return True     if should_series_dispatch(left,
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)         return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
class create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values().values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANT_1:             new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          return new_field
from twisted.internet import defer, reactor, logging  logger = logging.getLogger(__name__)  defer, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor, reactor
class RedirectMiddleware(BaseRedirectMiddleware):          def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', [])          if request.method == 'HEAD':          if (request.meta.get('dont_redirect', False or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(
from pandas.core.groupby.categorical import recode_for_groupby, recode_from_groupby  from pandas.core.index.api import CategoricalIndex, Index, MultiIndex  from pandas.core.series.api import Series  from pandas.io.formats.printing import pprint_thing  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing.categorical import pprint_categorical  from pandas.io.formats.printing
class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1]) result = op(ser, dti) assert result.name == names[2] class TestSeriesComparison: dti = dti.tz_localize("US/Central") ser = Series(dti).rename(names[1
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude
class CollectionRequirement: manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']              version = manifest['version']              dependencies = manifest['dependencies']          else: display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (name,))
def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)    
def js_to_json(code):             if v in ('true', 'false', 'null'):             return v          if v.startswith('"'):             return v         if v.startswith(''"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\', lambda m: m.replace('\\', '\\\\', 1))              v = v[1:-1]              v =
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key, self.freq)              return series[loc]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(key, self.freq)                  return series[reso]             except (TypeError, ValueError):              pass              asdt, reso = parse_time_string(key, self.freq)              if reso is None:                  reso = self._get_string_slice(
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name = f"{src}  (original)"      return True      return False      if write_back == write_back.YES:         with open(src_name, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name = f"{src_name}  (original)"      return True      return False      if write_back == write_back.YES:         with open(src_name, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name = f"{src_name}  (original)"      return True      return False      if write_back == write_back.YES:         with open(src_name, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name = f"{src_name}  (original)"      return True      return False      if write_back == write_back.YES:         with open(src_name, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name = f"{src_name}  (original)"      return True      return False      if write_back == write_back.YES:         with open(src_name, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name = f"{src_name}  (original)"      return True      return False      if write_back == write_back.YES:         with open(src_name, "w", encoding=src_buffer.encoding)              f.write(dst_contents)      elif write_back == write_back.DIFF:      src_name =
def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message)  def print_tensor(x, message=''):     return tf.Print(x, [x], message
def run_script(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )         return proc.wait()         def run_script_with_context(script_path, cwd='.'):         shell=run_thru_shell,         cwd=cwd      )        
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []     batch_sizes = []     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:     warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))
def melt(          else:             value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)             if not missing.empty:             raise KeyError(                      "The following 'value_vars' are not present in"
def _isna_new(obj):      elif hasattr(obj, "__array__"):      return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):      elif hasattr(obj, "__array__"):      return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return _isna_ndarraylike(obj)      else:         return obj is None  def _isna_ndarraylike(obj):      if isinstance(obj, np.ndarray):     return
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")          good = self._codes != -1 if not good.all():
def _urlencode(seq, enc): def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url) forms = root.xpath('      if not forms:      raise ValueError("No <form> element found in %s" % response)
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_LOADER_CLASS',                             settings.get('SPIDER_MANAGER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())
class NumericIndex(Index):          name = data.name          return cls._simple_new(subarr, name=name) @Appender(_index_shared_docs["_maybe_cast_slice_bound"]) def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, Dict          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=True,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition
class WebSocketHandler(tornado.web.RequestHandler): .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value)         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_open(self -> None:          if self.ws_connection:          self.ws_connection.open()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(self -> None:          if self.ws_connection:          self.ws_connection.close()         def on_connection_close(
 + "\033[0m"      return data + "\033[1m"      return data + "\033[2m"      return data + "\033[3m"      return data + "\033[4m"      return data + "\033[5m"      return data + "\033[6m"      return data + "\033[7m"      return data + "\033[8m"      return data + "\033[9m"      return data + "\033[10m"      return data + "\033[11m"      return data + "\033[12m"      return data + "\033[13m"      return data + "\033[14m"      return data + "\033[15m"      return data + "\033[16m"      return data + "\033[17m"      return data + "\033[18m"      return data + "\033[19m"      return data + "\033[20m"      return data + "\033[21m"      return data + "\033[22m"      return data + "\033[23m"      return data + "\033[24m"      return data + "\033[25m"      return data + "\033[26m"      return data + "\033[27m"      return data + "\033[28m"      return data + "\033[29m"      return data + "\033[30m"      return data + "\033[31m"      return data + "\033[32m"      return data + "\033[33m"      return data + "\033[34m"      return data + "\033[35m"      return data + "\033[36m"      return data + "\033[37m"      return data + "\033[38m"      return data + "\033[39m"      return data + "\033[40m"      return data + "\033[41m"      return data + "\033[42m"      return data + "\033[43m"      return data + "\033[44m"      return data + "\033[45m"      return data + "\033[46m"      return data + "\033[47m"      return data + "\033[48m"      return data + "\033[49m"      return data + "\033[50m"      return data +
 import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.missing import notna  from pandas.core import algorithms  from pandas.core.algorithms import unique  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.missing import notna  from pandas.core import algorithms  from pandas.core.algorithms import unique  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.dtypes.generic import notna  from pandas.core.
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):                 to_concat = []                 for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError, err:                 raise ValueError(f"Key {key} not in level {level}")                 to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                             ' and multiple workers may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if is_sequence:             steps_per_epoch = len(generator)             enqueuer = None              try:                 enqueuer = generator.run(steps_per_epoch=steps_per_epoch, batch_size=batch_size)             except Exception as e:                 warnings.warn(                 UserWarning('An error occurred while running the generator with `steps_per_epoch=steps_per_epoch`'                             ' and `batch_size=batch_size` may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if is_sequence:             val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         if is_sequence:             steps_per_epoch = len(generator)             enqueuer = None              try:                 enqueuer = generator.run(steps_per_epoch=steps_per_epoch, batch_size=batch_size)             except Exception as e:                 warnings.warn(                 UserWarning('An error occurred while running the generator with `steps_per_epoch=steps_per_epoch`'                             ' and `batch_size=batch_size` may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if is_sequence:             val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         if is_sequence:             steps_per_epoch = len(generator)             enqueuer = None              try:                 enqueuer = generator.run(steps_per_epoch=steps_per_epoch, batch_size=batch_size)             except Exception as e:                 warnings.warn(                 UserWarning('An error occurred while running the generator with `
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name
class Series(base.IndexOpsMixin, generic.NDFrame):             from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:             to_concat = [self, to_append]          return concat(to_concat)
class RedirectMiddleware(BaseRedirectMiddleware):         if 'Location' not in response.headers or response.status not in allowed_status:         return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)
class DataFrame(NDFrame): return new_data     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine_match_index(self, other, func): if ops.should_series_dispatch(self, other, func): return     def _combine
class EmptyLineTracker:                  return 0, 0              newlines = 2              if current_line.depth                  newlines -= 1              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line.depth                  return 0, 0              if current_line
class HTTPRequest(HTTPMessage):              headers = dict(self._orig.headers)              if 'Host' not in in headers['Host']              headers['Host'] = url.netloc.split('@')[-1]
class FloatBlock(FloatOrComplexBlock):          )          formatter.get_result_as_array()          def should_store(self, value) -> bool: return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
class Index(IndexOpsMixin, PandasObject):             multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0]
from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  from tensorflow.python.ops import functional_ops as functional  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon,
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)         self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)
class BaseReshapingTests(BaseExtensionTests):          result[0]          assert data[0] == result[1]          assert data[1] == result[2]          assert data[2] == result[3]          assert data[3] == result[4]          assert data[4] == result[5]          assert data[5] == result[6]          assert data[6] == result[7]          assert data[7] == result[8]          assert data[8] == result[9]          assert data[9] == result[10]          assert data[10] == result[11]          assert data[11] == result[12]          assert data[12] == result[13]          assert data[13] == result[14]          assert data[14] == result[15]          assert data[15] == result[16]          assert data[16] == result[17]          assert data[17] == result[18]          assert data[18] == result[19]          assert data[19] == result[20]          assert data[20] == result[21]          assert data[21] == result[22]          assert data[22] == result[23]          assert data[23] == result[24]          assert data[24] == result[25]          assert data[25] == result[26]          assert data[26] == result[27]          assert data[27] == result[28]          assert data[28] == result[29]          assert data[29] == result[30]          assert data[30] == result[31]          assert data[31] == result[32]          assert data[32] == result[33]          assert data[33] == result[34]          assert data[34] == result[35]          assert data[35] == result[36]          assert data[36] == result[37]          assert data[37] == result[38]          assert data[38] == result[39]          assert data[39] == result[40]          assert data[40] == result[41]          assert data[41] == result[42]          assert data[42] == result[43]          assert data[43] == result[44]          assert data[44] == result[45
[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[np.isinf(values)]         values[
          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError         
def validate_baseindexer_support(func_name: Optional[str] -> None) -> None:          "median",          "std",          "var",          "kurt",          "quantile",     
class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return the normalized (hashable/immutable) value.
class Request(object_ref):     s = safe_url_string(url, self.encoding)     self._url = escape_ajax(s)     if ':' not in self._url: raise ValueError('Missing scheme in request url: %s' % self._url)
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)      def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override start_requests method is deprecated."              )              for url in self.start_urls:              yield self.make_requests_from_url(url)
class XportReader(abc.Iterator):             if isinstance(filepath_or_buffer, (str, bytes)):             self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:             pass             self.filepath_or_buffer = BytesIO(contents)
def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)
def get_objs_combined_axis(          The axis to extract indexes from.      intersect : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)
 = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None
class ImageDataGenerator(object):              The inputs, normalized.         if self.preprocessing_function:             x = self.preprocessing_function(x)          if self.rescale:              x *= self.rescale          if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale         if self.samplewise_center:             x *= self.samplewise_center         if self.preprocessing_function:             x = self.preprocessing_function(x
(      find_xpath_attr,     fix_xml_all_ampersand,  )
from pandas.core.dtypes.common import ensure_platform_int,      is_bool,     is_bool_dtype,     is_categorical,     is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal
def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                      include_none=include_none,                      custom
class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1) - 1)) class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(
class GroupBy(_GroupBy):              ).sortlevel()              if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": True}              return output.reindex(**d)
class LocalCache(collections.OrderedDict):         self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name
class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab_entries class LinuxHardware(Hardware):     mtab_entries.append(fields)     return mtab
class ExecutionEngine(object):      d = self.scraper.enqueue_scrape(response, request, spider)      d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                              extra={'spider': spider, 'failure': f}))      return d      def spider_is_idle(self, spider):      d = self.scraper.enqueue_scrape(response, request, spider)      d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                              extra={'spider': spider, 'failure': f}))      return d
.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result = result.unstack(data, fill_value=fill_value)              clocs = [v if i > v else v - 1 for v in clocs]              return result.unstack(data, fill_value=fill_value)              result
_dtype,      is_list_like_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.inference import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,     is_period_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,     is_timedelta64_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.inference import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,     is_period_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,     is_timedelta64_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.inference import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,     is_period_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,     is_timedelta64_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.inference import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is
class BracketTracker:             if self._lambda_arguments and leaf.type == token.COLON              self.depth -= 1             self._lambda_arguments              return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True
 def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) +
class MissingParameterException(object):             if self.has_value:             return self.value              elif self.is_bool:             return False              elif self.is_list:             return []
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1), data_format='channels_first'):          x, tf_data_format = _preprocess_conv2d_input(x, data_format)          if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1), data_format='channels_first'):          x, tf_data_format = _preprocess_conv2d_input(x, data_format)          if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1), data_format='channels_first'):          x, tf_data_format = _preprocess_conv2d_input(x, data_format)          if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1), data_format='channels_first'):          x, tf_data_format = _preprocess_conv2d_input(x, data_format)          if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1), data_format='channels_first'):          x, tf_data_format = _preprocess_conv2d_input(x, data_format)          if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1), data_format='channels_first'):          x, tf_data_format = _preprocess_conv2
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True      ),      is_eager=True     
