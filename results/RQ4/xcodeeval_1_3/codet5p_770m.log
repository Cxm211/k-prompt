Namespace(log_name='./RQ5/xcodeeval_1_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_1_3/codet5p_770m', data_dir='./data/RQ5/xcodeeval_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> long long dp[500010],i,N,ans,tmp,mid; int main(){     for(scanf("%I64d",&N),i=1;i<=N;i++)         scanf("%I64d", &tmp), dp[i]=dp[i-1]+tmp;     mid=2*dp[i-1]/3;     if(!(dp[i-1]%3))         for(i=2;i<N;i++) ans+=dp[i]==mid;     printf("%I64d\\n", ans); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> long long dp[500010],i,N,ans,tmp,f,mid,cnt; int main(){     for(scanf("%I64d",&N),i=1;i<=N;i++)         scanf("%I64d",&tmp), dp[i]=dp[i-1]+tmp;     mid=2*(f=dp[i-1]/3);     if(!(dp[i-1]%3))         for(i=1;i<N;i++) ans+=(dp[i]==mid)*cnt, cnt+=dp[i]==f;     printf("%I64d\\n", ans); }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.0650151438050115e+244
  global_step = 2
  train_loss = 111.8658
  ********************
Previous best ppl:inf
Achieve Best ppl:3.0650151438050115e+244
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 37.5 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:37.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 6.047032050273436e+220
  global_step = 3
  train_loss = 114.8209
  ********************
Previous best ppl:3.0650151438050115e+244
Achieve Best ppl:6.047032050273436e+220
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 56.02 	 Previous best codebleu 37.5
  ********************
 Achieve Best bleu:56.02
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 3.5349271200190977e+216
  global_step = 4
  train_loss = 48.8545
  ********************
Previous best ppl:6.047032050273436e+220
Achieve Best ppl:3.5349271200190977e+216
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 68.44 	 Previous best codebleu 56.02
  ********************
 Achieve Best bleu:68.44
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 4.8483903483444675e+212
  global_step = 5
  train_loss = 26.9024
  ********************
Previous best ppl:3.5349271200190977e+216
Achieve Best ppl:4.8483903483444675e+212
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 70.87 	 Previous best codebleu 68.44
  ********************
 Achieve Best bleu:70.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.1110928156848991e+216
  global_step = 6
  train_loss = 9.9543
  ********************
Previous best ppl:4.8483903483444675e+212
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 71.57 	 Previous best codebleu 70.87
  ********************
 Achieve Best bleu:71.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.29714863570713e+222
  global_step = 7
  train_loss = 5.6356
  ********************
Previous best ppl:4.8483903483444675e+212
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 72.22 	 Previous best codebleu 71.57
  ********************
 Achieve Best bleu:72.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.6936826307165083e+229
  global_step = 8
  train_loss = 4.646
  ********************
Previous best ppl:4.8483903483444675e+212
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 71.98 	 Previous best codebleu 72.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.5085681540764485e+239
  global_step = 9
  train_loss = 3.9238
  ********************
Previous best ppl:4.8483903483444675e+212
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 71.84 	 Previous best codebleu 72.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 3.57943974947945e+245
  global_step = 10
  train_loss = 2.7424
  ********************
Previous best ppl:4.8483903483444675e+212
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 71.71 	 Previous best codebleu 72.22
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_1_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_3/test.jsonl
  codebleu = 72.56 
  Total = 500 
  Exact Fixed = 3 
[10, 322, 451]
  Syntax Fixed = 2 
[81, 177]
  Cleaned Fixed = 1 
[481]
  ********************
  Total = 500 
  Exact Fixed = 3 
[10, 322, 451]
  Syntax Fixed = 2 
[81, 177]
  Cleaned Fixed = 1 
[481]
  codebleu = 72.56 
[0.48121215044590926, 0.9294151499698242, 0.6386890640788627, 0.4507986952593763, 0.7167784203410406, 0.7679881745093895, 0.9131313070760732, 0.8394311373107994, 0.7852857892799938, 0.9811390010032224, 0.6622400912006872, 0.7772868173384335, 0.944969638955907, 0.6796040603412238, 0.8242836546885394, 0.5778619422291421, 0.9621715437613865, 0.7398136234006146, 0.8054255423928414, 0.8135665604814388, 0.5338789776284102, 0.3446503101961176, 0.9617896364191245, 0.8925560777052948, 0.8319533008515221, 0.0009061008383891935, 0.8434094165290678, 0.7089270802000895, 0.38708231484420597, 0.9606328090250909, 0.9180978958523331, 0.7227047954515889, 0.6569729679812651, 0.5431168861528274, 0.31729680887259115, 0.4965050364189206, 0.7336996602379647, 0.9691599747588928, 0.7411039840168427, 0.9733441689001259, 0.4386719175987259, 0.8788715653818208, 0.9422467314785667, 0.8921998318682778, 0.6593510546130777, 0.8616680155829046, 0.5338617744031935, 0.8602181220443288, 0.8226980351644886, 0.9358086045306386, 0.6349077239728745, 0.9765120913566805, 0.955626584171517, 0.8294779183542209, 0.9715785475583607, 0.9048525797314828, 0.9088121389774579, 0.9339299975598521, 0.566797592245256, 0.2884494942340172, 0.6594863233493391, 0.5415454260109173, 0.9126538866701963, 0.33460070946115567, 0.8248460806268897, 0.9752926493864778, 0.976819232671998, 0.7698824496255008, 0.7016234777170453, 0.907235765663017, 0.5140532373556195, 0.7769943668010335, 0.9266945793708943, 0.9853133667641483, 0.9191158481207378, 0.9097593629600539, 0.9189149325906292, 0.4932733020018213, 0.9835575139456028, 0.8563674297442561, 0.9327804056779383, 0.3816824572563564, 0.7597100033179813, 0.9616100740405664, 0.920139970422319, 0.4315647710177651, 0.608990714278228, 0.9738014890069033, 0.7248773843271845, 0.9822720623046208, 0.28942496496217823, 0.9664937597536958, 0.8952672769095782, 0.8603084300966128, 0.8511121182268683, 0.37642420748881267, 0.922263545444175, 0.9602694033492566, 0.9007029307261634, 0.8283719455395986, 0.7529046737306214, 0.9019464684129248, 0.9518479807436881, 0.7462546419750544, 0.841280442437645, 0.9036755986162253, 0.89687501719581, 0.934871427061674, 0.7219385539808008, 0.843779606891953, 0.8304057168977106, 0.636874953887632, 0.4181059355834638, 0.6077745179229681, 0.8790857766490099, 0.5662939894609251, 0.8481107547515385, 0.5881328279337963, 0.5590904557449987, 0.883086511284442, 0.34541274340654515, 0.21361352760251168, 0.6424313341672772, 0.9904426405617919, 0.5198919171128822, 0.9094363868933528, 0.9773514349376944, 0.9297985504031074, 0.802202097529589, 0.5567644426317497, 0.9450125678546649, 0.7308436256253621, 0.5934773614711584, 0.8816150641180378, 0.8123306966611903, 0.9775596710533672, 0.8355015737804767, 0.9081295060348795, 0.5474913051472977, 0.8495225732261329, 0.6333873199069301, 0.7643810428304513, 0.6237731005828068, 0.6259483208762605, 0.3053935253252723, 0.901175714210833, 0.47213667611022436, 0.15843999506651799, 0.9565343127552286, 0.9630021049296447, 0.8502617988384281, 0.8055864673667326, 0.49239571749879285, 0.5459516898043106, 0.9799354517153449, 0.7507257616996814, 0.3371764154031742, 0.6711901181086953, 0.7466017902771492, 0.2832762356981138, 0.580390274877845, 0.7577774779855018, 0.2247554870980143, 0.9742694293836827, 0.8789418017653541, 0.8879016404601738, 0.38502072754729244, 0.9699049873809091, 0.9834700980058935, 0.9551975148162357, 0.7467361709133309, 0.9396221339246325, 0.5401362238014022, 0.3728341211055949, 0.5578323336329495, 0.6504035526374041, 0.9521775318277603, 0.7897511138044075, 0.9807231032051993, 0.9139108143266295, 0.5174196783096494, 0.916272560211627, 0.7847099524235768, 0.535206429623671, 0.9429003490355619, 0.9052895563342798, 0.9607745973711677, 0.9583066026877074, 0.508243109335715, 0.9456458706993536, 0.6019819112269543, 0.3843652002149621, 0.49615384615384617, 0.8798091526516568, 0.9087123429574342, 0.801280412486297, 0.5062338029974262, 0.8638142897936805, 0.3574528301245649, 0.7758687640846222, 0.3579424928858859, 0.7910965943512587, 0.6266772080766958, 0.8990889204422448, 0.7156414320739874, 0.7733039571663103, 0.591865368526167, 0.9472064627007573, 0.7596454142574642, 0.3461479392677451, 0.8474656290142348, 0.36229474528149785, 0.8678976437379708, 0.9347275769474129, 0.9496735450823421, 0.4931509314951914, 0.7938304679800914, 0.9474669064833015, 0.7576176996301744, 0.9171073467519609, 0.8523029357043744, 0.8909108204830978, 0.939990366803559, 0.5604039895828797, 0.9602235397377306, 0.7846655469155479, 0.30374499657601767, 0.9711525005974759, 0.8734013068767044, 0.6401493417553263, 0.5029033064648121, 0.7852333138116714, 0.6732163514148213, 0.7174012425234068, 0.7336298486605038, 0.8426084577017585, 0.2548797337195095, 0.9466921306062679, 0.9420317788392338, 0.9508626724864289, 0.890935580421016, 0.20199669748718835, 0.9547478616285707, 0.014827337779471383, 0.7650431430220939, 0.904590355528839, 0.9262207935809244, 0.7746476439683496, 0.9256906539786105, 0.8886456676058869, 0.8950956014816437, 0.11477533045927665, 0.8491583316973768, 0.3138642748705961, 0.07759093279363852, 0.2985898520376627, 0.8000333050108104, 0.9263033253166333, 0.42689251969146835, 0.46398377548871184, 0.8795785087031951, 0.8910042410146453, 0.9722588250235882, 0.8449844779326559, 0.9214583338078037, 0.7086954249958265, 0.5200719655481933, 0.8078101910282451, 0.8188115370159288, 0.4883323987643829, 0.2942761537144616, 0.7321741424629815, 0.7200309361219595, 0.9271490169747502, 0.9055371956451799, 0.9179223973142112, 0.8923345869421039, 0.7648353685198632, 0.9793105773213144, 0.81522144998351, 0.6261746941082218, 0.3329380293736354, 0.8553117284030648, 0.9166795416411146, 0.9455409660105238, 0.5191264343087988, 0.8548231708514218, 0.43808453932073843, 0.8874329537885239, 0.4119890403418556, 0.9780384047260811, 0.595036047883924, 0.41151709383031154, 0.8065424278669349, 0.7785211041425664, 0.8949507881827607, 0.5688676768256914, 0.4990931771166349, 0.9060784269672213, 0.654357949555753, 0.9110292935350885, 0.5319509904078749, 0.8590818158786591, 0.9489374752009117, 0.3821796477166763, 0.7586668599419605, 0.8132985363028591, 0.8897945586059963, 0.7444340059656643, 0.8876077472866535, 0.8290622858030623, 0.3448279680258582, 0.9494075997300704, 0.9241588430999468, 0.20519665134428633, 0.5385083676708351, 0.7243913306646597, 0.8735531700555859, 0.8545467071813517, 0.7942436421266241, 0.9838722535113, 0.9765419032562443, 0.2992138853720385, 0.40672551960118875, 0.962676181664404, 0.8882050598183766, 0.9750131467225385, 0.5075736355169622, 0.5626774547371682, 0.9173075584052859, 0.3854398044418995, 0.4260664847736758, 0.41313650351244335, 0.893306721467412, 0.2982712119361176, 0.9617625014218179, 0.5728333815281874, 0.8966288012426236, 0.47836466842490655, 0.3705477276166575, 0.9821809817650384, 0.7736787852368014, 0.7486759848652709, 0.6521082397979208, 0.951769503559424, 0.6074300350442166, 0.5771597273003104, 0.8810946359492662, 0.7606454622988796, 0.974183904376235, 0.5288231788023587, 0.8469172469187838, 0.7760709676860538, 0.19242379301366364, 0.9303593500095784, 0.9319984976298381, 0.8439878665104579, 0.34296476892608474, 0.6605719155667432, 0.293795497072026, 0.9698700071476711, 0.33856809596367465, 0.9749244080949804, 0.9737915388152598, 0.987950922923978, 0.6693406223166416, 0.7611563185052843, 0.7460110532720265, 0.758988839562897, 0.7369406553821242, 0.9359055149846003, 0.5780241516155017, 0.9331552365394689, 0.6939430078420002, 0.6350077342091638, 0.6028754964779925, 0.9496316376212464, 0.84299747006287, 0.8931139245494264, 0.892600841772144, 0.4155596630877777, 0.9613152634801625, 0.7684448062183817, 0.907558018935852, 0.5506135469714823, 0.6694971009355826, 0.30840064646069615, 0.7580685995528176, 0.9409103501190933, 0.9698138929646263, 0.703409246380515, 0.7905567987043943, 0.9059801416992133, 0.27352354506300447, 0.7559987541863575, 0.9823448177520746, 0.9040449515722547, 0.40730915905680976, 0.9301710411526725, 0.5312651095051248, 0.6831371225580725, 0.9344855036532616, 0.6137052920264272, 0.10089414443444605, 0.9669138743101213, 0.8790363374505846, 0.2776969881491722, 0.48529564938642145, 0.9647434403630581, 0.421886906571103, 0.5479621679765283, 0.9189645726561522, 0.539612684703451, 0.804631742207644, 0.49935819383309804, 0.841210527867613, 0.3374521240368497, 0.6607259507908624, 0.9233388471345003, 0.765190464139593, 0.9569630499588484, 0.8990210844129167, 0.5456895014045992, 0.9478278042336812, 0.619196145402581, 0.9051517121584842, 0.6993849093249114, 0.1459086942388002, 0.8819214421291581, 0.9385995189394607, 0.6201992213394104, 0.350135820216791, 0.8899151396534579, 0.7648837329883056, 0.954238606427035, 0.5181203323339844, 0.8001883461221011, 0.251863754094517, 0.7840611593037248, 0.5951602009255168, 0.8554076307279366, 0.3787964080732937, 0.967235610505617, 0.7507801222761162, 0.33476190458371013, 0.5666317615036989, 0.5352238827052526, 0.9620828238000163, 0.4257173650321103, 0.6902777777999555, 0.9677706157693222, 0.9822516624818802, 0.7412367177908079, 0.8787838168173723, 0.8021007111795594, 0.666210611458786, 0.9098749234098644, 0.9193257381912645, 0.8375795025499561, 0.883875362704373, 0.5290842345678486, 0.011803874246352868, 0.9633179652022956, 0.4799605515288722, 0.7669842146074164, 0.9722369545163847, 0.45592789552418267, 0.41747534088469995, 0.8627505126882897, 0.9146063593427887, 0.6097430799469337, 0.9386019990279018, 0.5016513842398902, 0.6437828458487911, 0.4362302049836076, 0.7141494776867402, 0.944244861555185, 0.9270078518894367, 0.40379284641258495, 0.5750475616489092, 0.9715419204381248, 0.590335920863919, 0.778151370761023, 0.6212359005172369, 0.9426242057849843, 0.8206082150573881, 0.9013279588888281, 0.4076647468368658, 0.8667436416243068, 0.472048350719695, 0.5687023138391892, 0.9386210590467857, 0.7639957175620053, 0.7744889932682666, 0.6511144444174561, 0.46802465703730456, 0.9545187036711082, 0.38702322613559287, 0.8042710150175236, 0.5638521359608164]
Finish training and take 1h10m
Namespace(log_name='./RQ5/xcodeeval_1_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_1_3/codet5p_770m', data_dir='./data/RQ5/xcodeeval_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> long long dp[500010],i,N,ans,tmp,mid; int main(){     for(scanf("%I64d",&N),i=1;i<=N;i++)         scanf("%I64d", &tmp), dp[i]=dp[i-1]+tmp;     mid=2*dp[i-1]/3;     if(!(dp[i-1]%3))         for(i=2;i<N;i++) ans+=dp[i]==mid;     printf("%I64d\\n", ans); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> long long dp[500010],i,N,ans,tmp,f,mid,cnt; int main(){     for(scanf("%I64d",&N),i=1;i<=N;i++)         scanf("%I64d",&tmp), dp[i]=dp[i-1]+tmp;     mid=2*(f=dp[i-1]/3);     if(!(dp[i-1]%3))         for(i=1;i<N;i++) ans+=(dp[i]==mid)*cnt, cnt+=dp[i]==f;     printf("%I64d\\n", ans); }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.0650151438050115e+244
  global_step = 2
  train_loss = 111.8658
  ********************
Previous best ppl:inf
Achieve Best ppl:3.0650151438050115e+244
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
