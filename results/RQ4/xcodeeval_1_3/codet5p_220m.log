Namespace(log_name='./RQ5/xcodeeval_1_3/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_1_3/codet5p_220m', data_dir='./data/RQ5/xcodeeval_1_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h> long long dp[500010],i,N,ans,tmp,mid; int main(){     for(scanf("%I64d",&N),i=1;i<=N;i++)         scanf("%I64d", &tmp), dp[i]=dp[i-1]+tmp;     mid=2*dp[i-1]/3;     if(!(dp[i-1]%3))         for(i=2;i<N;i++) ans+=dp[i]==mid;     printf("%I64d\\n", ans); }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h> long long dp[500010],i,N,ans,tmp,f,mid,cnt; int main(){     for(scanf("%I64d",&N),i=1;i<=N;i++)         scanf("%I64d",&tmp), dp[i]=dp[i-1]+tmp;     mid=2*(f=dp[i-1]/3);     if(!(dp[i-1]%3))         for(i=1;i<N;i++) ans+=(dp[i]==mid)*cnt, cnt+=dp[i]==f;     printf("%I64d\\n", ans); }'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.6102169090679018e+298
  global_step = 2
  train_loss = 142.4051
  ********************
Previous best ppl:inf
Achieve Best ppl:1.6102169090679018e+298
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 21.47 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:21.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.958398065045953e+289
  global_step = 3
  train_loss = 146.3175
  ********************
Previous best ppl:1.6102169090679018e+298
Achieve Best ppl:3.958398065045953e+289
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 60.0 	 Previous best codebleu 21.47
  ********************
 Achieve Best bleu:60.0
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 5.482363105725929e+285
  global_step = 4
  train_loss = 82.7275
  ********************
Previous best ppl:3.958398065045953e+289
Achieve Best ppl:5.482363105725929e+285
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 60.27 	 Previous best codebleu 60.0
  ********************
 Achieve Best bleu:60.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 4.1810940375821394e+282
  global_step = 5
  train_loss = 51.4688
  ********************
Previous best ppl:5.482363105725929e+285
Achieve Best ppl:4.1810940375821394e+282
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 63.13 	 Previous best codebleu 60.27
  ********************
 Achieve Best bleu:63.13
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0151414121145492e+281
  global_step = 6
  train_loss = 36.5451
  ********************
Previous best ppl:4.1810940375821394e+282
Achieve Best ppl:1.0151414121145492e+281
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 67.4 	 Previous best codebleu 63.13
  ********************
 Achieve Best bleu:67.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 4.4336741460317995e+280
  global_step = 7
  train_loss = 30.4636
  ********************
Previous best ppl:1.0151414121145492e+281
Achieve Best ppl:4.4336741460317995e+280
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 68.15 	 Previous best codebleu 67.4
  ********************
 Achieve Best bleu:68.15
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 8.713823620190252e+280
  global_step = 8
  train_loss = 28.6813
  ********************
Previous best ppl:4.4336741460317995e+280
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 70.47 	 Previous best codebleu 68.15
  ********************
 Achieve Best bleu:70.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 4.0594699064166944e+281
  global_step = 9
  train_loss = 18.5321
  ********************
Previous best ppl:4.4336741460317995e+280
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 70.77 	 Previous best codebleu 70.47
  ********************
 Achieve Best bleu:70.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 5.328000218060964e+282
  global_step = 10
  train_loss = 22.7562
  ********************
Previous best ppl:4.4336741460317995e+280
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 71.07 	 Previous best codebleu 70.77
  ********************
 Achieve Best bleu:71.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 3.209348850424588e+283
  global_step = 11
  train_loss = 13.9339
  ********************
Previous best ppl:4.4336741460317995e+280
BLEU file: ./data/RQ5/xcodeeval_1_3/validation.jsonl
  codebleu-4 = 71.07 	 Previous best codebleu 71.07
  ********************
reload model from RQ5/xcodeeval_1_3/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_3/test.jsonl
  codebleu = 70.5 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[322]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 1 
[322]
  codebleu = 70.5 
[0.5003909451993059, 0.8321752526767051, 0.6386890640788627, 0.39489399233351397, 0.7129541686380121, 0.758661460452385, 0.8845199737446907, 0.8201412977761033, 0.7798012813487226, 0.7978401528638444, 0.6468820023714645, 0.7688413678752155, 0.957277749370421, 0.7296645362554622, 0.7970123315164127, 0.5579881435274828, 0.9529444560679083, 0.7417743076518157, 0.7850826791102017, 0.7406588954281603, 0.8209810121992888, 0.3104312045519388, 0.8879772569474149, 0.883013273500731, 0.8319533008515221, 0.0010540273849368343, 0.845146354884855, 0.726328670004605, 0.2616147517893536, 0.7423516492307571, 0.765702530043391, 0.5728704284951145, 0.500575902968613, 0.535570988828813, 0.33588280534535514, 0.4965050364189206, 0.6949197933786447, 0.9691599747588928, 0.7411039840168427, 0.9733441689001259, 0.43914826411179686, 0.8788715653818208, 0.7557448157624085, 0.8394133511819559, 0.6593510546130777, 0.8679108010685463, 0.5299449360480865, 0.8594485486384797, 0.8226980351644886, 0.9013987259135601, 0.6273299903845604, 0.9475141044823088, 0.906517320647392, 0.8659592116467298, 0.9672505019251096, 0.9007069025058358, 0.8973853716300615, 0.8879534929157553, 0.566797592245256, 0.2773868961029912, 0.5015791867146502, 0.48075801910315513, 0.9329607793262014, 0.33460070946115567, 0.9386443155880784, 0.9383272615623113, 0.976819232671998, 0.7602051546645454, 0.6870563930732505, 0.907235765663017, 0.500085208534374, 0.7251477903807675, 0.9150252687655285, 0.9587638728557637, 0.9191158481207378, 0.9067378704097662, 0.9189149325906292, 0.4947752996761327, 0.9835575139456028, 0.8005198244376465, 0.8399560790374813, 0.36947045857034255, 0.7556896465937578, 0.8935957716853726, 0.9015641755367891, 0.43008913751304784, 0.6029569844031324, 0.9370006374872937, 0.6923038770955816, 0.9311325077565562, 0.2797821253233517, 0.9526091160438181, 0.8468134260245599, 0.8856263720128676, 0.843409256989499, 0.35203378886389197, 0.9312971942645913, 0.8659413898346563, 0.8829054572687063, 0.8120188161714624, 0.7311314873586284, 0.8962750139737339, 0.9518479807436881, 0.69461217794586, 0.841280442437645, 0.9037598255711303, 0.863312640597331, 0.8455421183965908, 0.7115582389431178, 0.843779606891953, 0.9174152551338901, 0.6264792527850046, 0.46132374693989764, 0.6429686096678264, 0.7957899020095283, 0.5544006406441137, 0.6156493998402052, 0.5881328279337963, 0.5564332438328972, 0.8764331132270073, 0.34541274340654515, 0.19047659005142478, 0.6452628988066307, 0.9706027722222594, 0.4906275086498332, 0.8961070515960101, 0.9728059803922399, 0.9297985504031074, 0.7990141360130947, 0.623190003134112, 0.9498936808462883, 0.7384230907758329, 0.5934773614711584, 0.8816150641180378, 0.8315564921333688, 0.957733207910364, 0.8178054068965699, 0.7224322152449658, 0.35463694097209875, 0.9472873156881443, 0.6333873199069301, 0.7249170095047828, 0.6237731005828068, 0.6037646858819163, 0.31633680726190805, 0.878776535559528, 0.4439432005485672, 0.15843999506651799, 0.4693243990967115, 0.8977009632829398, 0.8163940643677927, 0.7961838575725315, 0.4523605727198724, 0.5414012443977789, 0.9714543121419643, 0.4123396309781585, 0.3530119889852382, 0.6687301156021817, 0.7466017902771492, 0.27668506727707776, 0.580390274877845, 0.6901630584423446, 0.2266081501599424, 0.9668019361294558, 0.8924409160071864, 0.8954351471954556, 0.3332469793411241, 0.9661342790856922, 0.9604132788912321, 0.8792161733832224, 0.745575939750925, 0.9042106929634075, 0.5665318481677857, 0.3701849078093676, 0.5241784586583151, 0.6396643414457834, 0.8882381329172488, 0.6773080161311059, 0.982012642435071, 0.8975842989637222, 0.5039999312444778, 0.9093383768829296, 0.7065344713081076, 0.5298839774968003, 0.9665462050951112, 0.7480403344915619, 0.9454726521675199, 0.9557000143002845, 0.4177634666349561, 0.8037701117005612, 0.6019819112269543, 0.37010185172310794, 0.37846153846153846, 0.8560769328087214, 0.8853573111343924, 0.7963996489402061, 0.48909674621731847, 0.7298839713467837, 0.33612290803278533, 0.766060689148458, 0.35789698790386626, 0.7703947977884202, 0.2918023572656316, 0.8874757756206368, 0.6629900830231886, 0.7683853967816683, 0.5914384969398494, 0.8591395979767211, 0.8533534577368376, 0.3212886704686576, 0.8623693520002298, 0.3460171531607763, 0.5426888686107147, 0.7640684555747085, 0.9371886013440851, 0.4990419371024124, 0.7688400492281791, 0.6476935463586878, 0.744833918292807, 0.932013028343579, 0.8319580552564085, 0.8831205463231013, 0.8988515731044794, 0.5632600476130775, 0.9602235397377306, 0.7151109832577704, 0.30374499657601767, 0.9797722962055422, 0.8265709713187376, 0.6401493417553263, 0.5219303023482933, 0.7641606915638797, 0.6949461854097343, 0.7007515643026158, 0.7146994271555224, 0.8195189672453422, 0.3024540539982831, 0.8129404752221872, 0.7742876417461901, 0.9508626724864289, 0.87452049965634, 0.20409509770038606, 0.9547478616285707, 0.01517517578455145, 0.7295821779648639, 0.869508796604259, 0.9262207935809244, 0.7262299833489934, 0.9477354798086446, 0.914605230296756, 0.9205560066851318, 0.1149731797575114, 0.8541263808534807, 0.28687767968099553, 0.017505264819623778, 0.27496816217037867, 0.8000333050108104, 0.9263033253166333, 0.42689251969146835, 0.44301407500511275, 0.8916878010398698, 0.8815062222100687, 0.9632543555813096, 0.8409936421826247, 0.89265986166131, 0.7075772173461479, 0.5041492533147997, 0.7905138482631402, 0.8188115370159288, 0.45696355597234656, 0.2794339588993276, 0.7258923826404009, 0.7216618615588383, 0.9271490169747502, 0.5864109502755911, 0.9178903531608789, 0.8094704181661518, 0.5959467892452219, 0.757662877081034, 0.9312591482604915, 0.6857318503066976, 0.3280969792193928, 0.9103138141821812, 0.9129766621063571, 0.9058890894871514, 0.5961867498988478, 0.899901653091776, 0.4961446226974958, 0.925318105928959, 0.41674952183105474, 0.9780384047260811, 0.440878935486296, 0.38592115105210545, 0.8037271990298229, 0.8421901762909951, 0.8949507881827607, 0.6361177455623226, 0.43562640132464864, 0.9060784269672213, 0.6532752946103628, 0.8761409812158019, 0.5118727536064708, 0.8752607189265235, 0.29970655914956634, 0.37186219069652265, 0.7539629833226005, 0.8794100936019331, 0.7612276760259724, 0.6152288042194233, 0.8876077472866535, 0.7612175082939892, 0.345589415122532, 0.9494075997300704, 0.9241588430999468, 0.20795986290351345, 0.5705311712768084, 0.6865763614542033, 0.8691803396787734, 0.8545467071813517, 0.7942436421266241, 0.746040982845569, 0.9533083872651615, 0.3079770810699627, 0.3898608981971201, 0.962676181664404, 0.8330146631653947, 0.8210317568143306, 0.4920183697765157, 0.623918586401055, 0.9138963276622132, 0.39551609310457536, 0.38325941519524764, 0.41313650351244335, 0.8904677341355778, 0.27295548814296766, 0.9617625014218179, 0.5894464681454045, 0.92724279693536, 0.5116980017582399, 0.3920758677486021, 0.9579731776440357, 0.765989015232347, 0.7362157150033519, 0.6591827885544952, 0.9784487939574704, 0.5973035577720847, 0.5530879252539567, 0.8810946359492662, 0.798101791113383, 0.9667068307090809, 0.5962093089487768, 0.7889853709258723, 0.7705816030959198, 0.16282450501676263, 0.8954962027142548, 0.8787702275332199, 0.8242339580668097, 0.35640416135498465, 0.8665928958491977, 0.2651624245768782, 0.6384231267345484, 0.34295899444245803, 0.935400079034808, 0.6201643825401286, 0.8978190871846536, 0.6693406223166416, 0.7474779056231708, 0.7273429106232598, 0.7389699307645374, 0.6885499172346777, 0.9359055149846003, 0.5377443037726098, 0.8267277437736402, 0.743362110462453, 0.6132267247469232, 0.6312231658015864, 0.919336582478808, 0.8343102984929338, 0.8598280498194824, 0.9126504243038784, 0.38036846607339503, 0.9613152634801625, 0.7684448062183817, 0.8492121819843856, 0.5370234386019257, 0.6092720039669848, 0.3032022908035358, 0.7500450879704729, 0.9215150153038906, 0.9698138929646263, 0.6885937243191476, 0.7729310954846317, 0.8199310293957243, 0.4314132388171852, 0.7537535386522394, 0.9823448177520746, 0.8941036489426529, 0.40730915905680976, 0.9072261533844175, 0.5101642930426153, 0.669855206066504, 0.9344855036532616, 0.6176769729835196, 0.8921420184079258, 0.9282085814438639, 0.8790363374505846, 0.2776969881491722, 0.3925324675324675, 0.9516188698125516, 0.421886906571103, 0.5448994420689934, 0.8838724984326742, 0.39640195455610505, 0.8018202590925926, 0.5036312075276035, 0.8186766545432983, 0.3402791419393206, 0.6718831637886337, 0.9293930619270312, 0.7705711494398761, 0.9279266438006974, 0.8943099780390755, 0.5401749445139642, 0.7729853237743907, 0.6015716472945073, 0.9000344953975268, 0.7228352296252503, 0.11730410677753136, 0.8901520821853084, 0.9385995189394607, 0.868233902276383, 0.35248263205427854, 0.8203734168254198, 0.7644848744897429, 0.954238606427035, 0.5220942811428554, 0.7771335395143664, 0.23769670050246333, 0.7285571483911315, 0.5658836834897989, 0.838101333740499, 0.36221282405072497, 0.7087326775862578, 0.7379095703278673, 0.35986195950882166, 0.5621350369178544, 0.7730594084031194, 0.9230919581189487, 0.4211523280742633, 0.6961079141390056, 0.9148000137550478, 0.826781601169819, 0.7412367177908079, 0.8761229884130892, 0.8021007111795594, 0.6533041248054541, 0.7598019962153529, 0.957243053310842, 0.8122375569053466, 0.8815717366146187, 0.5320358854621969, 0.012341891798825813, 0.9633179652022956, 0.47850125813409494, 0.7669842146074164, 0.8793541593896029, 0.45592789552418267, 0.3916393450048932, 0.8545755589429918, 0.9146063593427887, 0.542413680784654, 0.8665816355113989, 0.5555845372343139, 0.6032513232892225, 0.9223036106888496, 0.7141494776867402, 0.944244861555185, 0.7161847710700192, 0.40108981287989276, 0.5837591450491273, 0.9605574380222852, 0.5766111583274198, 0.8059911647917903, 0.6530925310027635, 0.9067226579874064, 0.8206082150573881, 0.891230747648696, 0.459510635950531, 0.8512319604847998, 0.472048350719695, 0.5517306924072882, 0.912968284479533, 0.9087577586268546, 0.7690962059099671, 0.6511144444174561, 0.3692814767537853, 0.9545187036711082, 0.42111413522650193, 0.791896579535458, 0.9583597990063684]
Finish training and take 38m
