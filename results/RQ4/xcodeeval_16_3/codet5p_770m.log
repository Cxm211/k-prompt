Namespace(log_name='./RQ5/xcodeeval_16_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_16_3/codet5p_770m', data_dir='./data/RQ5/xcodeeval_16_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <stdio.h>  #include <math.h>    static inline int min(int a, int b) { return (a < b) ? a : b; }    void testcase() {      // a / B      int x, y;      scanf("%d%d", &x, &y);      // n^2 - 1 <= x      int peak = min(sqrt(4 * (1 + x)) / 2, y);      int ans = 0;      for (int i = 1; i < peak; ++i) {          int lower = i * (i + 2);          int upper = min(x - x % i, y * i + i);          int times = (upper - lower) / i + 1;          ans += times;      }      printf("%d\\n", ans);      return;  }    int main() {      int t;      scanf("%d", &t);      for (int i = 0; i < t; ++i) testcase();      return 0;  }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include <stdio.h>  #include <math.h>  #define ll long long    static inline ll min(ll a, ll b) { return (a < b) ? a : b; }    void testcase() {      // a / B      ll x, y;      scanf("%lld%lld", &x, &y);      // n^2 - 1 <= x      ll peak = min(sqrt(4 * (1 + x)) / 2, y);      ll ans = 0;      for (int i = 1; i < peak; ++i) {          ll lower = i * (i + 2);          ll upper = min(x - x % i, y * i + i);          ll times = (upper - lower) / i + 1;          ans += times;      }      printf("%lld\\n", ans);      return;  }    int main() {      int t;      scanf("%d", &t);      for (int i = 0; i < t; ++i) testcase();      return 0;  }'}]
***** Running training *****
  Num examples = 16
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 4.7717119784510865e+227
  global_step = 5
  train_loss = 98.2638
  ********************
Previous best ppl:inf
Achieve Best ppl:4.7717119784510865e+227
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 66.26 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:66.26
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.4993855920985436e+295
  global_step = 9
  train_loss = 62.5848
  ********************
Previous best ppl:4.7717119784510865e+227
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 70.14 	 Previous best codebleu 66.26
  ********************
 Achieve Best bleu:70.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 13
  train_loss = 37.2293
  ********************
Previous best ppl:4.7717119784510865e+227
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.58 	 Previous best codebleu 70.14
  ********************
 Achieve Best bleu:71.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 17
  train_loss = 21.7876
  ********************
Previous best ppl:4.7717119784510865e+227
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 70.93 	 Previous best codebleu 71.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 21
  train_loss = 15.381
  ********************
Previous best ppl:4.7717119784510865e+227
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.38 	 Previous best codebleu 71.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 25
  train_loss = 8.5184
  ********************
Previous best ppl:4.7717119784510865e+227
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.33 	 Previous best codebleu 71.58
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_16_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_16_3/test.jsonl
  codebleu = 72.77 
  Total = 500 
  Exact Fixed = 3 
[322, 412, 451]
  Syntax Fixed = 1 
[81]
  Cleaned Fixed = 1 
[481]
  ********************
  Total = 500 
  Exact Fixed = 3 
[322, 412, 451]
  Syntax Fixed = 1 
[81]
  Cleaned Fixed = 1 
[481]
  codebleu = 72.77 
[0.5003909451993059, 0.9294151499698242, 0.6386890640788627, 0.4507986952593763, 0.7167784203410406, 0.7679881745093895, 0.988076477678953, 0.8394311373107994, 0.719699071780217, 0.9545358769986374, 0.6164950417794137, 0.7772868173384335, 0.9623971198135532, 0.736036573233421, 0.8242836546885394, 0.5927671877201272, 0.9621715437613865, 0.7428715295728313, 0.7702492094781257, 0.8135665604814388, 0.8276821254560949, 0.3446503101961176, 0.9617896364191245, 0.8925560777052948, 0.5936726906928713, 0.0017274651254391895, 0.8434094165290678, 0.7089270802000895, 0.3600824707101461, 0.9606328090250909, 0.952851260737474, 0.7249876253142388, 0.6569729679812651, 0.5431168861528274, 0.2999940037739095, 0.4964785177490538, 0.7027777918150636, 0.9348002448738693, 0.7411039840168427, 0.9866836999577304, 0.43871215196390306, 0.8833038098770115, 0.8973872683386734, 0.9104066605958898, 0.6593510546130777, 0.8616680155829046, 0.5338617744031935, 0.8602181220443288, 0.8226980351644886, 0.9358086045306386, 0.6349077239728745, 0.9651979920898561, 0.9153590562183367, 0.8647848479583772, 0.9732049512552536, 0.8966007884524483, 0.9088121389774579, 0.9339299975598521, 0.561801583155677, 0.2884494942340172, 0.695853159280218, 0.5415454260109173, 0.547538231779396, 0.33460070946115567, 0.9512811420549903, 0.9752926493864778, 0.9799704917277337, 0.7858041122445381, 0.7113008970718839, 0.915465568076623, 0.4836106725411139, 0.7769943668010335, 0.9206179105612011, 0.9853133667641483, 0.9191158481207378, 0.9402009240478387, 0.9189149325906292, 0.4932733020018213, 0.9717037265508326, 0.9544361042227616, 0.9327804056779383, 0.3816824572563564, 0.7597100033179813, 0.9616100740405664, 0.9319685888076141, 0.45623963021965236, 0.608990714278228, 0.8510333427247143, 0.7248773843271845, 0.9784487939574704, 0.28942496496217823, 0.9664937597536958, 0.8952672769095782, 0.8876242548612938, 0.6174211828129103, 0.3737335723876971, 0.9441722498573561, 0.9602694033492566, 0.8465058198732305, 0.8283719455395986, 0.7529046737306214, 0.9019464684129248, 0.9518479807436881, 0.7013825995946199, 0.9499450445139372, 0.9036755986162253, 0.89687501719581, 0.934871427061674, 0.7219385539808008, 0.843779606891953, 0.925790468693755, 0.636874953887632, 0.4181059355834638, 0.6077745179229681, 0.8900074689819688, 0.5662939894609251, 0.8481107547515385, 0.5881328279337963, 0.5384141038898764, 0.8887562240561163, 0.30111901344367975, 0.2154366420897607, 0.6424313341672772, 0.9904426405617919, 0.9876378378993536, 0.9094363868933528, 0.9500151453400968, 0.9297985504031074, 0.802202097529589, 0.5791470604415615, 0.9443091575627018, 0.708987745987874, 0.5920295413051476, 0.8816150641180378, 0.7905316145044854, 0.9616895841800797, 0.8355015737804767, 0.9055073576552588, 0.4774063517864672, 0.8815883572997729, 0.6333873199069301, 0.7643810428304513, 0.6237731005828068, 0.618971576690214, 0.3053935253252723, 0.8107628994160092, 0.47213667611022436, 0.15843999506651799, 0.9328833739047961, 0.9630021049296447, 0.8669302726487816, 0.8055864673667326, 0.4642707174987929, 0.5459516898043106, 0.38219655012668907, 0.7665708243215428, 0.35174423017740575, 0.6711901181086953, 0.7466017902771492, 0.2832762356981138, 0.580390274877845, 0.7684857402005096, 0.2247554870980143, 0.9570521617810392, 0.8789418017653541, 0.9472207997658679, 0.9872442369212986, 0.9699049873809091, 0.9834700980058935, 0.9610721917709997, 0.7467361709133309, 0.9396221339246325, 0.5480309606435074, 0.3728341211055949, 0.5106302882633955, 0.653856905605865, 0.9254463017658894, 0.7897511138044075, 0.9795706376252826, 0.9139108143266295, 0.5174196783096494, 0.9074999665144187, 0.731695678958928, 0.535206429623671, 0.968840592678585, 0.9052895563342798, 0.9607745973711677, 0.9652619679712691, 0.508243109335715, 0.9732316438707951, 0.5230353956587164, 0.3843652002149621, 0.0, 0.8798091526516568, 0.9087123429574342, 0.8218539480382933, 0.5062338029974262, 0.9485128510887966, 0.9527602685578562, 0.7061239996954074, 0.38323926417182946, 0.7723465943512586, 0.5576755585419275, 0.9045356516494829, 0.7156414320739874, 0.7733039571663103, 0.591865368526167, 0.9756645028428577, 0.8533534577368376, 0.3461479392677451, 0.8699110799666838, 0.33410986488889494, 0.8678976437379708, 0.8391126036853808, 0.9496735450823421, 0.5012127116833381, 0.7938304679800914, 0.9683231065091749, 0.7576176996301744, 0.9100254602133176, 0.8396612812521356, 0.8909108204830978, 0.9447445835540222, 0.5604039895828797, 0.9602235397377306, 0.7793412065446155, 0.30374499657601767, 0.9848726663737328, 0.8734013068767044, 0.6569901043761641, 0.5075905017189635, 0.8123152846477772, 0.9009882112827006, 0.7247563047282455, 0.7336298486605038, 0.8426084577017585, 0.2250497585936182, 0.9661469642895737, 0.9420317788392338, 0.9508626724864289, 0.890935580421016, 0.20787905042836483, 0.956301548538977, 0.01475343653439011, 0.7650431430220939, 0.904590355528839, 0.9521775318277603, 0.7459040842816722, 0.9477354798086446, 0.9276347349756966, 0.9363924664229649, 0.11200213653664287, 0.8491583316973768, 0.3138642748705961, 0.07759093279363852, 0.2941969248494424, 0.788911997279355, 0.9548574842083983, 0.42689251969146835, 0.46398377548871184, 0.8866527473204353, 0.8910042410146453, 0.9722588250235882, 0.8449844779326559, 0.9184585843480406, 0.7086954249958265, 0.5200719655481933, 0.8078101910282451, 0.8188115370159288, 0.4883323987643829, 0.2837030440685242, 0.739351212728503, 0.7200309361219595, 0.8831505037407669, 0.9088627028234778, 0.9179223973142112, 0.8923345869421039, 0.7587729023519247, 0.9721318740804274, 0.9312591482604915, 0.6857318503066976, 0.3329380293736354, 0.8226645052550549, 0.9166795416411146, 0.9455409660105238, 0.5741547548968033, 0.8548231708514218, 0.3239840742940912, 0.9452315454123918, 0.4119890403418556, 0.9780384047260811, 0.595036047883924, 0.41151709383031154, 0.8065424278669349, 0.8421901762909951, 0.9379197320725001, 0.5462809876827743, 0.4990931771166349, 0.8482588666380302, 0.654357949555753, 0.8842156297612047, 0.5289514092792647, 0.9495780518930732, 0.9300739067458841, 0.3765487756747352, 0.760540475117339, 0.8794100936019331, 0.6924331352088553, 0.7444340059656643, 0.8876077472866535, 0.8290622858030623, 0.3448279680258582, 0.9494075997300704, 0.8738993738909158, 0.20519665134428633, 0.2851872277062614, 0.7243913306646597, 0.8735531700555859, 0.8545467071813517, 0.7711647668262053, 0.9785165539587144, 0.9765419032562443, 0.2992138853720385, 0.3889408365645352, 0.962676181664404, 0.897868274842214, 0.9870433547342701, 0.5075736355169622, 0.5790261988125887, 0.9173075584052859, 0.3854398044418995, 0.4260664847736758, 0.41143799701648776, 0.9273759004975819, 0.2982712119361176, 0.9617625014218179, 0.597398385186501, 0.8622108446982941, 0.4182619133895703, 0.3949626673550144, 0.9821809817650384, 0.7533903892395735, 0.6964096009787752, 0.6521082397979208, 0.956563145990218, 0.6074300350442166, 0.6178552508101587, 0.828076584683292, 0.7606454622988796, 0.974183904376235, 0.5288231788023587, 0.8521108081796271, 0.7880782435693265, 0.19242379301366364, 0.9303593500095784, 0.6717690412879314, 0.8439878665104579, 0.35640416135498465, 0.8330563375832075, 0.293795497072026, 0.9726718946789215, 0.3354936965685169, 0.9749244080949804, 0.9737915388152598, 0.987950922923978, 0.6693406223166416, 0.7686563185052843, 0.7460110532720265, 0.758988839562897, 0.7369406553821242, 0.9125361942131389, 0.5780241516155017, 0.9331552365394689, 0.6595338581073896, 0.6350077342091638, 0.6013715798045949, 0.9496316376212464, 0.8667057770035502, 0.9292941776343798, 0.8638287232610085, 0.4155596630877777, 0.9613152634801625, 0.7684448062183817, 0.907558018935852, 0.5176603832937504, 0.668171817709041, 0.4130042192803641, 0.7580685995528176, 0.9409103501190933, 0.9698138929646263, 0.7074381217791966, 0.7905567987043943, 0.9059801416992133, 0.27352354506300447, 0.7559987541863575, 0.9823448177520746, 0.9040449515722547, 0.40730915905680976, 0.8948688296336118, 0.5301303934515195, 0.6831371225580725, 0.9344855036532616, 0.5916821777968526, 0.318272931874597, 0.9669138743101213, 0.8884019621450376, 0.2776969881491722, 0.1878787878787879, 0.9647434403630581, 0.421886906571103, 0.5701879332826507, 1.0, 0.539612684703451, 0.7849467304243832, 0.49935819383309804, 0.841210527867613, 0.3374521240368497, 0.6546396265607987, 0.30509463852336083, 0.7756256761612133, 0.9569630499588484, 0.8990210844129167, 0.5456895014045992, 0.938595895727768, 0.619196145402581, 0.9057365595839495, 0.6993849093249114, 0.1459086942388002, 0.8819214421291581, 0.9385995189394607, 0.6201992213394104, 0.3606607079181603, 0.8899151396534579, 0.7362076822493754, 0.954238606427035, 0.5096816017675542, 0.7895116166935833, 0.251863754094517, 0.8169189094593757, 0.5951602009255168, 0.8554076307279366, 0.9833603222298068, 0.967235610505617, 0.7507801222761162, 0.3330584663396393, 0.5666317615036989, 0.7718761136325316, 0.957228209570204, 0.38073683577706224, 0.6902777777999555, 0.979859116245265, 0.9822516624818802, 0.7412367177908079, 0.8943391240835281, 0.8021007111795594, 0.666210611458786, 0.9163391393733058, 0.8994265578444094, 0.8121228540451371, 0.883875362704373, 0.535964272693835, 0.011803874246352868, 0.9633179652022956, 0.4799605515288722, 0.7669842146074164, 0.9497847158980881, 0.3562457073408314, 0.41747534088469995, 0.8627505126882897, 0.9181208637326026, 0.619988254124251, 0.9386019990279018, 0.5735025861665138, 0.653157845848791, 0.9331089552213916, 0.7141494776867402, 0.947633855490319, 0.9270078518894367, 0.4027266047248116, 0.5750475616489092, 0.9684863847527501, 0.590335920863919, 0.8151829986495325, 0.6106194384013883, 0.9426242057849843, 0.8206082150573881, 0.9040645444308273, 0.4076647468368658, 0.8667436416243068, 0.472048350719695, 0.5687023138391892, 0.9386210590467857, 0.9096266419399884, 0.7744889932682666, 0.5743800354971557, 0.46802465703730456, 0.9545187036711082, 0.4207598960402934, 0.8042710150175236, 0.9475769384486536]
Finish training and take 53m
