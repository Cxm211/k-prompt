Namespace(log_name='./RQ5/xcodeeval_16_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_16_3/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_16_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00053
  global_step = 3
  train_loss = 0.5808
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00053
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 40.65 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:40.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00052
  global_step = 5
  train_loss = 0.4322
  ********************
Previous best ppl:1.00053
Achieve Best ppl:1.00052
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 59.74 	 Previous best codebleu 40.65
  ********************
 Achieve Best bleu:59.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00048
  global_step = 7
  train_loss = 0.3221
  ********************
Previous best ppl:1.00052
Achieve Best ppl:1.00048
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.08 	 Previous best codebleu 59.74
  ********************
 Achieve Best bleu:71.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00049
  global_step = 9
  train_loss = 0.2799
  ********************
Previous best ppl:1.00048
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.59 	 Previous best codebleu 71.08
  ********************
 Achieve Best bleu:71.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00051
  global_step = 11
  train_loss = 0.2315
  ********************
Previous best ppl:1.00048
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.71 	 Previous best codebleu 71.59
  ********************
 Achieve Best bleu:71.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00052
  global_step = 13
  train_loss = 0.2182
  ********************
Previous best ppl:1.00048
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.44 	 Previous best codebleu 71.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00052
  global_step = 15
  train_loss = 0.1993
  ********************
Previous best ppl:1.00048
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.53 	 Previous best codebleu 71.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00051
  global_step = 17
  train_loss = 0.1717
  ********************
Previous best ppl:1.00048
BLEU file: ./data/RQ5/xcodeeval_16_3/validation.jsonl
  codebleu-4 = 71.57 	 Previous best codebleu 71.71
  ********************
reload model from RQ5/xcodeeval_16_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_16_3/test.jsonl
  codebleu = 73.09 
  Total = 500 
  Exact Fixed = 3 
[214, 322, 451]
  Syntax Fixed = 1 
[177]
  Cleaned Fixed = 2 
[284, 481]
  ********************
  Total = 500 
  Exact Fixed = 3 
[214, 322, 451]
  Syntax Fixed = 1 
[177]
  Cleaned Fixed = 2 
[284, 481]
  codebleu = 73.09 
[0.5100944582540017, 0.8321752526767051, 0.6386890640788627, 0.4507986952593763, 0.6729000307632179, 0.7769952980182466, 0.988076477678953, 0.8394311373107994, 0.7852857892799938, 0.9545358769986374, 0.6622400912006872, 0.7655704185494574, 0.9538617747264153, 0.7864631506294787, 0.8242836546885394, 0.599771780721065, 0.9525263224938354, 0.7606664568520785, 0.7702492094781257, 0.8501074912680264, 0.8276821254560949, 0.3461454902662245, 0.9617896364191245, 0.8925560777052948, 0.8319533008515221, 0.0010820181564900134, 0.9751679833306697, 0.7089270802000895, 0.3600824707101461, 0.9330254280600032, 0.934742138346936, 0.7243558021629983, 0.6587166967295006, 0.5495569575790649, 0.3525064583419274, 0.4964785177490538, 0.7336996602379647, 0.9691599747588928, 0.7411039840168427, 0.9866836999577304, 0.43871215196390306, 0.8672930125939636, 0.9172537695953207, 0.8503438363789719, 0.6593510546130777, 0.8616680155829046, 0.5338617744031935, 0.8142809247152252, 0.8226980351644886, 0.9714741186824503, 0.6349077239728745, 0.9888765931470758, 0.9202770890052219, 0.73504268677293, 0.959903658996969, 0.8815005618972627, 0.9088121389774579, 0.9166308649225363, 0.5565094531676825, 0.3889412074119988, 0.697719790104478, 0.48075801910315513, 0.547538231779396, 0.33460070946115567, 0.9159363235701456, 0.9752926493864778, 0.98154990280289, 0.7858041122445381, 0.7016234777170453, 0.915465568076623, 0.5129346492787287, 0.7769943668010335, 0.9268555414820475, 0.9853133667641483, 0.9191158481207378, 0.9072279836100083, 0.9189149325906292, 0.4947752996761327, 0.9667329410838017, 0.851558311920717, 0.8979589771065096, 0.3816824572563564, 0.7597100033179813, 0.9003077208857198, 0.8729588064267548, 0.45623963021965236, 0.6332013218278199, 0.9738014890069033, 0.7175583895703044, 0.9341669952741447, 0.2874092843485814, 0.9871730286979867, 0.9378691266821584, 0.8876242548612938, 0.8009356497917418, 0.37642420748881267, 0.9441722498573561, 0.9602694033492566, 0.9319272046508655, 0.8283719455395986, 0.7652053474549476, 0.9019464684129248, 0.7989881734438138, 0.7316548538486924, 0.9464173775678764, 0.9036755986162253, 0.8595567140383655, 0.8824490530956131, 0.7219385539808008, 0.843779606891953, 0.925790468693755, 0.6264792527850046, 0.44301690842111663, 0.6118154056069615, 0.8900074689819688, 0.5662939894609251, 0.839880762888503, 0.5863648401109702, 0.5590904557449987, 0.8587231303601761, 0.25541397064879995, 0.2154366420897607, 0.6349075543134048, 0.9904426405617919, 0.9619015394020021, 0.9094363868933528, 0.9728059803922399, 0.8844169946212104, 0.802202097529589, 0.5791470604415615, 0.9477648614326466, 0.7384230907758329, 0.6005171839156628, 0.8758659351981772, 0.8345680283779763, 0.9812998208473764, 0.8355015737804767, 0.9055073576552588, 0.6095574224256186, 0.8815883572997729, 0.6333873199069301, 0.7643810428304513, 0.6237731005828068, 0.6259483208762605, 0.3053935253252723, 0.8868499446671998, 0.47213667611022436, 0.15843999506651799, 0.9041115411179401, 0.9630021049296447, 0.9084408246013599, 0.8055864673667326, 0.49239571749879285, 0.5459516898043106, 0.9799354517153449, 0.7665708243215428, 0.35174423017740575, 0.6711901181086953, 0.7466017902771492, 0.2832762356981138, 0.4508756001895507, 0.7271424618675996, 0.22464504564776028, 0.9570521617810392, 0.8987497922002496, 0.9764700147580929, 0.9872442369212986, 0.9563686680222498, 0.9834700980058935, 0.9551975148162357, 0.7467361709133309, 0.9091329886749024, 0.5401362238014022, 0.34656855725548164, 0.5106302882633955, 0.653856905605865, 0.9521775318277603, 0.7897511138044075, 0.9825644410992302, 0.9139108143266295, 0.5174196783096494, 0.916272560211627, 0.343189527419997, 0.5341374618194692, 0.968840592678585, 0.9052895563342798, 0.928993377894699, 0.9667912869693065, 0.508243109335715, 0.9732316438707951, 0.6019819112269543, 0.3843652002149621, 0.004705882352941176, 0.8476239295414961, 0.9087123429574342, 0.7897428442056872, 0.5046352925095559, 0.5568709244136449, 0.9493437591648197, 0.7885343223120463, 0.35780881211287907, 0.6258631776973891, 0.5576755585419275, 0.8734382716678093, 0.6581780890189421, 0.7733039571663103, 0.5914384969398494, 0.9357386553220302, 0.8533534577368376, 0.3530037086004747, 0.8699110799666838, 0.35732195151786816, 0.9087427909672332, 1.0, 0.9333581362475458, 0.5012127116833381, 0.7857651620497359, 0.9521086994838612, 0.7576176996301744, 0.888065037565753, 0.8523029357043744, 0.8527242219233501, 0.9447445835540222, 0.5740795726965989, 0.9005892571211114, 0.7151109832577704, 0.30374499657601767, 0.9819888979099367, 0.8544756661410926, 0.6401493417553263, 0.4904476445761063, 0.7802715766354296, 0.6616378816549005, 0.7029697117595877, 0.7336298486605038, 0.8426084577017585, 0.29660247436472353, 0.9406995380264958, 0.9314156079190896, 0.9508626724864289, 0.9899172392844513, 0.20461399075573877, 0.9547478616285707, 0.015382234728165137, 0.7803845286608786, 0.904590355528839, 0.9521775318277603, 0.7808145991250481, 0.9477354798086446, 0.914605230296756, 0.9025188240315642, 0.11200213653664287, 0.8491583316973768, 0.28687767968099553, 0.6066526280331083, 0.2941969248494424, 0.8110033734683955, 0.9316289255121708, 0.42689251969146835, 0.46398377548871184, 0.9082446279665759, 0.8910042410146453, 0.9852412520833183, 0.8449844779326559, 0.8835311556871228, 0.7086954249958265, 0.5296868847250604, 0.7654106290713822, 0.8188115370159288, 0.45291082885460726, 0.2942761537144616, 0.6834344139980126, 0.7200309361219595, 0.9271490169747502, 0.8715107436193149, 0.9179223973142112, 0.8923345869421039, 0.7601507876079147, 0.837924636310246, 0.9312591482604915, 0.6857318503066976, 0.3329380293736354, 0.8553117284030648, 0.9322192718709021, 0.9455409660105238, 0.5621108679477544, 0.8548231708514218, 0.43808453932073843, 0.9798219468937557, 0.4119890403418556, 0.9780384047260811, 0.594007414149033, 0.41151709383031154, 0.8065424278669349, 0.8763562414963599, 0.9227369501332858, 0.5667558470674415, 0.4990931771166349, 0.9060784269672213, 0.654357949555753, 0.8476486927050157, 0.5289514092792647, 0.9495780518930732, 0.41615305393866364, 0.3821796477166763, 0.760540475117339, 0.8794100936019331, 0.8897945586059963, 0.7213673227775241, 0.8876077472866535, 0.803243714058054, 0.3448279680258582, 0.9494075997300704, 0.9241588430999468, 0.20519665134428633, 0.5485112701832704, 0.6893743593808941, 0.8735531700555859, 0.8545467071813517, 0.7942436421266241, 0.8976812966245387, 1.0, 0.3202665169509859, 0.40359771610469364, 0.962676181664404, 0.897868274842214, 0.9402653343653977, 0.5075736355169622, 0.577391816361255, 0.9173075584052859, 0.3958158965912856, 0.374733445167721, 0.5045975639394058, 0.9273759004975819, 0.29772728019452893, 0.982840810414882, 0.597398385186501, 0.7545387581592976, 0.5092531607280145, 0.38567795075722866, 0.9821809817650384, 0.7736787852368014, 0.7486759848652709, 0.6653351874795173, 0.9784487939574704, 0.6025278677762487, 0.6178552508101587, 0.828076584683292, 0.7606454622988796, 0.974183904376235, 0.5299298150532961, 0.8521108081796271, 0.7826728955268443, 0.19242379301366364, 0.9303593500095784, 0.8787702275332199, 0.8439878665104579, 0.35599974604021273, 0.8665928958491977, 0.9168638777441369, 0.9712034950107209, 0.3386718428405343, 0.9749244080949804, 0.9837244566208403, 0.9651530694682628, 0.6693406223166416, 0.7611563185052843, 0.7460110532720265, 0.7298979204581357, 0.7369406553821242, 0.9359055149846003, 0.5817741516155017, 0.8870056335772818, 0.8061470224375229, 0.6216371397419055, 0.6312231658015864, 0.8988386999883251, 0.8424698441897294, 0.5546507682422561, 0.8891382375368331, 0.4155596630877777, 0.9613152634801625, 0.7684448062183817, 0.907558018935852, 0.5162245353052077, 0.5727571998867057, 0.30573977368865607, 0.7486405069388824, 0.9409103501190933, 0.9698138929646263, 0.703409246380515, 0.7905567987043943, 0.8608263557643544, 0.2730822878883405, 0.7559987541863575, 0.9644727771943793, 0.8983633146672891, 0.40730915905680976, 0.9301710411526725, 0.5279674737073239, 0.6831371225580725, 0.9344855036532616, 0.5847054336108061, 0.9255630678241955, 0.8843701717304933, 0.8981156159858223, 0.2776969881491722, 0.6021019576702131, 0.9647434403630581, 0.421886906571103, 0.5701879332826507, 0.9189645726561522, 0.539612684703451, 0.7849467304243832, 0.5036312075276035, 0.841210527867613, 0.3374521240368497, 0.6607259507908624, 0.9233388471345003, 0.7756256761612133, 0.977246012599333, 0.8990210844129167, 0.5425755720089517, 0.7709223542020225, 0.619196145402581, 0.9019915513069043, 0.6993849093249114, 0.1459086942388002, 0.8806976111458171, 0.9385995189394607, 0.8676734624744935, 0.23187090803373867, 0.8203734168254198, 0.7362076822493754, 0.9417054785646468, 0.5338572016270736, 0.8001883461221011, 0.8906979678251827, 0.8169189094593757, 0.5996229338562107, 0.8321526885806367, 0.9833603222298068, 0.7966308791524319, 0.719222144083394, 0.3330584663396393, 0.5529282734377579, 0.7718761136325316, 0.9562136766381757, 0.42865827350977254, 0.6902777777999555, 0.993565204007768, 0.9822516624818802, 0.7412367177908079, 0.8157507247238265, 0.782864984241054, 0.666210611458786, 0.9163391393733058, 0.8759555092831399, 0.8375795025499561, 0.883875362704373, 0.11576688801061685, 0.011387238931736576, 0.9043551167162223, 0.4799605515288722, 0.7669842146074164, 0.9569036126054264, 0.45592789552418267, 0.41747534088469995, 0.8627505126882897, 0.9181208637326026, 0.5839423318089887, 0.7567061343242648, 0.5735025861665138, 0.6020153206141936, 0.8816853757825234, 0.7141494776867402, 0.984944616727695, 0.8878866375240382, 0.4027266047248116, 0.5750475616489092, 0.9684863847527501, 0.590335920863919, 0.7444398919794659, 0.6648575127928055, 0.913538001078352, 0.8055309396220718, 0.9020881289138165, 0.38308084616782967, 0.8667436416243068, 0.472048350719695, 0.5671802873979482, 0.9386210590467857, 0.9096266419399884, 0.7744889932682666, 0.6511144444174561, 0.46802465703730456, 0.9545187036711082, 0.38702322613559287, 0.8042710150175236, 0.9680933173062176]
Finish training and take 46m
