Namespace(log_name='./RQ5/tfix_16_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_16_2/codet5p_220m_f', data_dir='./data/RQ5/tfix_16_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01173
  global_step = 3
  train_loss = 1.8748
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01173
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 13.88 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:13.88
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.0075
  global_step = 5
  train_loss = 1.2659
  ********************
Previous best ppl:1.01173
Achieve Best ppl:1.0075
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 30.01 	 Previous best codebleu 13.88
  ********************
 Achieve Best bleu:30.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00639
  global_step = 7
  train_loss = 0.8702
  ********************
Previous best ppl:1.0075
Achieve Best ppl:1.00639
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 49.82 	 Previous best codebleu 30.01
  ********************
 Achieve Best bleu:49.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00596
  global_step = 9
  train_loss = 0.5961
  ********************
Previous best ppl:1.00639
Achieve Best ppl:1.00596
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.84 	 Previous best codebleu 49.82
  ********************
 Achieve Best bleu:55.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.0059
  global_step = 11
  train_loss = 0.3734
  ********************
Previous best ppl:1.00596
Achieve Best ppl:1.0059
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 56.24 	 Previous best codebleu 55.84
  ********************
 Achieve Best bleu:56.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00585
  global_step = 13
  train_loss = 0.3264
  ********************
Previous best ppl:1.0059
Achieve Best ppl:1.00585
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 54.97 	 Previous best codebleu 56.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.0058
  global_step = 15
  train_loss = 0.2282
  ********************
Previous best ppl:1.00585
Achieve Best ppl:1.0058
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.43 	 Previous best codebleu 56.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00578
  global_step = 17
  train_loss = 0.2298
  ********************
Previous best ppl:1.0058
Achieve Best ppl:1.00578
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.31 	 Previous best codebleu 56.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00571
  global_step = 19
  train_loss = 0.1855
  ********************
Previous best ppl:1.00578
Achieve Best ppl:1.00571
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.63 	 Previous best codebleu 56.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0057
  global_step = 21
  train_loss = 0.2368
  ********************
Previous best ppl:1.00571
Achieve Best ppl:1.0057
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.61 	 Previous best codebleu 56.24
  ********************
reload model from RQ5/tfix_16_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_16_2/test.jsonl
  codebleu = 54.11 
  Total = 500 
  Exact Fixed = 11 
[16, 47, 54, 132, 349, 354, 367, 391, 457, 469, 500]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 15 
[3, 145, 166, 172, 188, 193, 198, 268, 330, 336, 417, 435, 439, 442, 474]
  ********************
  Total = 500 
  Exact Fixed = 11 
[16, 47, 54, 132, 349, 354, 367, 391, 457, 469, 500]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 15 
[3, 145, 166, 172, 188, 193, 198, 268, 330, 336, 417, 435, 439, 442, 474]
  codebleu = 54.11 
[0.5670445241047504, 0.6845073826793884, 0.6816015466501335, 0.2122047676692162, 0.6058511595444083, 0.8821894623768516, 0.0, 0.4165645430686046, 0.008635889938037818, 0.11249999999999999, 0.5368474083243506, 0.00010785790618315851, 0.15393576085345756, 0.6472222666043883, 0.49146272601903207, 1.0, 0.6193949501703278, 0.2642680213004776, 0.5149193791727077, 0.6974942116108808, 0.7145082485446812, 0.3317256155367455, 0.25583464080738794, 0.6232430066758361, 0.5970182644937875, 0.006758238160278511, 0.3077042504432333, 0.23985520596237853, 0.40956559394603126, 0.2855072113105671, 0.5149770358022868, 0.0, 0.5654473178899494, 0.7221965444238074, 0.5025534004220087, 0.6068905779332499, 0.0, 0.10556510016615601, 0.6539315065112679, 0.6398412446099295, 0.5511412946838178, 0.805671205945659, 0.08149139863647084, 0.40366685053304824, 0.7183643415906251, 0.31747481206286177, 1.0, 0.274670867776058, 0.8208732553989724, 0.5900038817627358, 0.350381564995214, 0.6515070121475033, 0.6558003592103332, 1.0, 0.0, 0.5194637253863024, 0.3070782538395746, 0.6395188779694462, 0.9318213423140442, 0.6232902973738775, 0.9315380507550484, 0.654297617187952, 0.2832680265511309, 0.7618233428889547, 0.6997799682734636, 0.6197233100748608, 0.14394169237252197, 0.4683275038174648, 0.4199748401004021, 0.2266528631527418, 0.934857916341244, 0.1301328225772834, 0.8508222909971193, 0.39452984161828986, 0.702664818575847, 0.23900409159064706, 0.15, 0.8773551649307303, 0.39718089591628425, 0.5380234635084377, 0.591516286342532, 0.14295774662907662, 0.36662249548760817, 0.7383139646397308, 0.6359585563603126, 0.5451417837590342, 0.48115588878443294, 0.5066408688157863, 0.3355442463646316, 0.3591365261541136, 0.6108560633180458, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.2725378786154891, 0.7470738524243741, 0.8349004181959196, 0.8639557211113615, 0.7902466426203973, 0.5177002534662967, 0.6812985771278143, 0.9164266028701744, 0.7275723927836586, 0.6445931574752105, 0.7834843745068932, 0.0, 0.2402562811128986, 0.8399114554115357, 0.8049637497745419, 0.05270446328111167, 0.2878852119582799, 0.5876935279546878, 0.39272004516267667, 0.30365138677226156, 0.509901499184051, 0.0, 0.2178166887244068, 0.8794087474171386, 0.5143209096122012, 0.7231896021778675, 0.7302547136373172, 0.7807350977435055, 0.5195870202871274, 0.3773869844754326, 0.005672998526019732, 0.4298070594261493, 0.27868486115832364, 0.8797888055545642, 0.5246680028432865, 0.3202688209577117, 0.4488041447621831, 1.0, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.4460952454685818, 0.35069935771252403, 0.5564826517197291, 0.1526136760976885, 0.44815878639981177, 0.6130292099690186, 0.7246000172152951, 0.5574204713453644, 0.4595867868440112, 0.41130178148606267, 0.5885650943899094, 0.3607039998273758, 0.3561219555899987, 0.5862322739141566, 0.26493187938965956, 0.4035665191868353, 0.3833616795223091, 0.47507995820083293, 0.18510019296607524, 0.40825339177238446, 0.6758332538184946, 0.6840296792997195, 0.27724087834942884, 0.22499999999999998, 0.6684064493975892, 0.39347209291868157, 0.4619754204180907, 0.7504004004704901, 0.7954279937030855, 0.4479362034729103, 0.671995337914775, 0.7191441569283883, 0.5052903220407482, 0.21118173670864582, 0.6011496215861661, 0.6868474083243508, 0.5336911179244788, 0.47798912624653667, 0.8601018446386719, 0.40693291435805307, 0.6734232449967186, 0.3959691186443218, 0.5745997086539989, 0.6278910313966588, 0.11321771168817468, 0.36686807121046905, 0.6780323751412478, 0.40021506667847195, 0.8533946307914342, 0.7775381978884832, 0.7713750451807391, 0.6525790759412512, 0.567976832210193, 0.5954710388424542, 0.7725146668513112, 0.7562262989907678, 0.94333927756915, 0.41130178148606267, 0.4996402791078279, 0.43648868589003154, 0.5852573831682686, 0.007044218708112843, 0.8837480609952844, 0.751375045180739, 0.39722643810031, 0.5626976141547512, 0.7070568394457155, 0.40485075092622447, 0.7825255142177938, 0.40436432296800573, 0.6271806220068676, 0.4139240818757265, 0.7656774499782346, 0.4697300318922557, 0.6599223799332465, 0.4090633164381116, 0.9365276486140073, 0.39462621654037877, 0.3074925800041956, 0.9596380983607143, 0.037180895916284235, 0.5796605122596801, 0.44619887519287305, 0.9320894171538912, 0.7755969629986152, 0.4210760489838614, 0.6740103526642764, 0.731117687292167, 0.04993436505953147, 0.691318859382493, 0.8212015472955612, 0.7834452289722301, 0.762134846251658, 0.9414428605659417, 0.11558530246943605, 0.5864902989394949, 0.7145087250676192, 0.0, 0.4347288551937204, 0.6844125934903402, 0.8083325080431145, 0.5985639363699742, 0.7698683115100668, 0.2475891977778692, 0.6774950945205829, 0.7471765364160405, 0.4268169073526533, 0.7304967316732975, 0.6629572906529959, 0.9056583090096291, 0.6913784572286918, 0.39249739838955333, 0.7225371723695806, 0.45355396855740826, 0.6350178817306947, 0.5073028085492076, 0.7984935680651688, 0.0, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.23978058624034976, 0.836942030771336, 0.401473698622807, 0.7496603249119216, 0.5017814354051087, 0.5585140118159326, 0.4082567795920036, 0.7503305249945151, 0.36522840682175095, 0.6624678525854575, 0.31683850155049365, 0.6255874957717769, 0.7568044193179739, 0.3714877971723338, 0.6515365041476618, 0.8307467892419347, 0.6844125934903402, 0.43829433999099154, 0.5953125, 0.6463729364766281, 0.34272438732934996, 0.8222267233801039, 0.7142845202249919, 0.5960450822340708, 0.5600468038501167, 0.4561779924571592, 0.47093320489949897, 0.6877459225855624, 0.8470997505392659, 0.0, 0.4367500239658253, 0.8849384145327521, 0.6547551174518553, 0.4859699405327176, 0.06557708600314388, 0.2900371026939149, 0.6366015466501336, 0.38866241237317, 0.7137387506067986, 0.2465085296294044, 0.8072615927227478, 0.573043959575943, 0.7011486686075723, 0.20642809963595843, 0.16458897152331436, 0.1595246283016026, 0.7977695539611169, 0.5818498519004407, 0.7649793793296578, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.8555090515828416, 0.6109185002122562, 0.3220998891127413, 0.14043945566372723, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 0.569257524437746, 0.08828427124746191, 0.3502184077768308, 0.6045187520612343, 0.06, 0.7869082306429402, 0.2233775576161466, 0.5702518310230941, 0.6240048954556174, 0.8034645362012123, 0.954414111805322, 0.6970127275140043, 0.1049464258957323, 0.20169077412725672, 0.7386691030330881, 0.7359735741809917, 0.5042263356787084, 0.6870606775436042, 0.7189894209825303, 0.656468030647374, 0.6000494532758354, 0.8050241598897441, 0.8455355915212772, 0.41494648485747626, 0.8033946307914341, 0.6981023533844464, 0.5347191241991722, 0.5811483383242801, 0.8521964460866425, 0.5892647227570667, 0.5424094289706055, 0.22389678890316622, 0.5038833860066276, 1.0, 0.6393949501703278, 0.5805102295464017, 0.9176675865658077, 0.4147896756607047, 1.0, 0.20351805215206997, 0.4890714508161259, 0.38011458888741184, 0.6745228788727515, 0.5528635017332217, 0.5780999494416426, 0.7451384298635613, 0.3057265995447144, 0.9024484160469517, 0.4614470406899055, 0.32772705003045166, 0.3445672601793484, 1.0, 0.8084374616749153, 0.23267827069448496, 0.8378414230005442, 0.37099885503952457, 0.46680385389420903, 0.3134982604054203, 0.4460613780866912, 0.7700435930643703, 0.7625515340301889, 0.45101776545562844, 0.5754121037621087, 0.6923473261888204, 0.7905056700329363, 0.4621787547146431, 0.16509689218126905, 0.7767231336445126, 0.7730817392743747, 0.5638264212064281, 0.5607942439151187, 0.7211277277287549, 0.3050368478785238, 0.6995292956215062, 0.39835356547001255, 1.0, 0.760978313726013, 0.7398520412406208, 0.17731538118686083, 0.4592493207167594, 0.7210821183929019, 0.5785629301801026, 0.10029870059345264, 0.5019774510104837, 0.15606585053046712, 0.5302675647941586, 0.8799789818939843, 0.752380452288719, 0.18585881606736232, 0.39969884422068214, 0.6567867859128023, 0.6354739770278475, 0.6340021561968084, 0.5520581748828688, 0.09999999999999999, 0.0, 0.3886662005977344, 0.8655056700329364, 0.6116932084176985, 0.5846027698772176, 0.09713682208624577, 0.6585117129535003, 0.41591864534318224, 0.038139497655491794, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.7054819616225767, 0.5383089748131177, 0.28587921208276085, 0.5282467529717347, 0.7062207385179242, 0.5092272948800609, 0.21056545775751523, 0.6183689771600134, 0.46544814211381313, 0.21151332797896238, 0.685303136808272, 0.7288312733021203, 0.9381792830507429, 0.6829777303051304, 0.7123448244422504, 0.6275893856012176, 0.6319730195988179, 0.08928571428571427, 0.40739727760582384, 0.5536911179244788, 0.5758314530928041, 0.7503143105742338, 0.8292906179772745, 0.29494667314911716, 0.6883329804487951, 0.7148321218614053, 0.05216242273033751, 0.5518505299283467, 0.6592906179772744, 0.8379283989138028, 0.8420598436025485, 0.24373379387306737, 0.8648103351245982, 0.0828714707479615, 1.0, 0.8326781496971694, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.6242583006373346, 0.329957331730546, 0.4714698407062744, 0.7945462129468169, 0.5420052553931844, 0.3583507427572924, 0.8368354605794404, 0.8114529051148651, 0.6809070226208283, 0.6181302051050597, 0.7195565989965169, 0.797935247551725, 0.8737480609952845, 0.7712732534255593, 0.3, 0.3893061591353758, 0.7392647227570666, 0.8038961967467184, 0.6517965513447935, 0.5107338113451334, 0.8955980328281301, 0.6924012535180504, 0.05489548592171942, 0.4592359915981675, 0.562520658360374, 0.6901940524102714, 0.5089439475762831, 0.23868348617790436, 0.42997442672976427, 0.7135160729534664, 0.4614069132492895, 0.6157955965877151, 0.47992233211507185, 0.24561881818205022, 0.8632148025904984, 0.5714177340432305, 0.7856607594664533, 0.36791104518876483, 1.0]
Finish training and take 16m
