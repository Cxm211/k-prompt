Namespace(log_name='./RQ5/tfix_16_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_16_2/codet5p_770m', data_dir='./data/RQ5/tfix_16_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if(markers){   args.markers = Array();   for(i=0; i < markers.length; i++) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if(markers){   args.markers = [];   for(i=0; i < markers.length; i++) {'}]
***** Running training *****
  Num examples = 16
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
Namespace(log_name='./RQ5/tfix_16_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_16_2/codet5p_770m', data_dir='./data/RQ5/tfix_16_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if(markers){   args.markers = Array();   for(i=0; i < markers.length; i++) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if(markers){   args.markers = [];   for(i=0; i < markers.length; i++) {'}]
***** Running training *****
  Num examples = 16
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 9.444885710835665e+254
  global_step = 5
  train_loss = 33.2636
  ********************
Previous best ppl:inf
Achieve Best ppl:9.444885710835665e+254
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 33.98 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:33.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 9.47732087024156e+248
  global_step = 9
  train_loss = 12.7771
  ********************
Previous best ppl:9.444885710835665e+254
Achieve Best ppl:9.47732087024156e+248
  ********************
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.37 	 Previous best codebleu 33.98
  ********************
 Achieve Best bleu:55.37
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.6827759541973863e+280
  global_step = 13
  train_loss = 4.5687
  ********************
Previous best ppl:9.47732087024156e+248
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.88 	 Previous best codebleu 55.37
  ********************
 Achieve Best bleu:55.88
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.6997719692341154e+280
  global_step = 17
  train_loss = 1.7582
  ********************
Previous best ppl:9.47732087024156e+248
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.31 	 Previous best codebleu 55.88
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 2.749561578381256e+276
  global_step = 21
  train_loss = 1.0583
  ********************
Previous best ppl:9.47732087024156e+248
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.89 	 Previous best codebleu 55.88
  ********************
 Achieve Best bleu:55.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.2118518694448493e+284
  global_step = 25
  train_loss = 0.4258
  ********************
Previous best ppl:9.47732087024156e+248
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.87 	 Previous best codebleu 55.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 2.1498822740533154e+292
  global_step = 29
  train_loss = 0.4934
  ********************
Previous best ppl:9.47732087024156e+248
BLEU file: ./data/RQ5/tfix_16_2/validation.jsonl
  codebleu-4 = 55.8 	 Previous best codebleu 55.89
  ********************
early stopping!!!
reload model from RQ5/tfix_16_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_16_2/test.jsonl
  codebleu = 55.55 
  Total = 500 
  Exact Fixed = 21 
[17, 45, 47, 103, 118, 132, 154, 190, 201, 202, 238, 253, 263, 290, 303, 305, 313, 333, 336, 367, 457]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 7 
[35, 126, 153, 237, 328, 390, 469]
  ********************
  Total = 500 
  Exact Fixed = 21 
[17, 45, 47, 103, 118, 132, 154, 190, 201, 202, 238, 253, 263, 290, 303, 305, 313, 333, 336, 367, 457]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 7 
[35, 126, 153, 237, 328, 390, 469]
  codebleu = 55.55 
[0.5670445241047504, 0.5247371055557515, 0.6367953537684857, 0.31331096368700495, 0.9433946642877236, 0.7919235659219761, 0.0, 0.5417785626903434, 0.43011545328563816, 0.12, 0.5368474083243506, 0.543709349483924, 0.40537805984662106, 0.53101263279552, 0.4087382860570157, 0.811117687292167, 1.0, 0.29228321907283183, 0.6885240359080308, 0.25129879162633795, 0.7145082485446812, 0.40243113967436694, 0.24915625771353556, 0.6232430066758361, 0.5970182644937875, 0.008533731833720836, 0.6354815207521537, 0.26101526962702754, 0.6624101654079205, 0.2855072113105671, 0.7059323615142865, 0.0, 0.4507183101653729, 0.9113854927948712, 0.8142975021172099, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6398412446099295, 0.5511412946838178, 0.805671205945659, 0.3420252005580776, 0.6202064481033123, 1.0, 0.22739727760582387, 1.0, 0.274670867776058, 0.3578263397059, 0.5900038817627358, 0.350381564995214, 0.6433348104458009, 0.6558003592103332, 0.9096887238103473, 0.08221258381761304, 0.5194637253863024, 0.3070782538395746, 0.6563937419855969, 0.8460094492769958, 0.6232902973738775, 0.9315380507550484, 0.654297617187952, 0.2832680265511309, 0.7618233428889547, 0.8072090910976335, 0.5587320009154486, 0.6877745357134093, 0.4132027926585744, 0.4199748401004021, 0.2266528631527418, 0.93334273708204, 0.14040253646355694, 0.5426744349179697, 0.7337480609952843, 0.832226723380104, 0.29035107882031763, 0.15, 0.8965241752682729, 0.43317161913164237, 0.6254581948739375, 0.591516286342532, 0.3734424024864518, 0.5711359782538347, 0.47499345139276594, 0.6225257887752799, 0.5451417837590342, 0.48115588878443294, 0.9293190884995292, 0.8053935031810717, 0.44058280681232764, 0.5658448779697635, 0.7402739997668998, 0.44517512468103604, 0.45579909520741635, 0.03637156789728707, 0.60936317038474, 0.8349004181959196, 0.8639557211113615, 0.822755485176792, 0.5688724788208753, 0.7792011322130665, 0.7765712201237638, 1.0, 0.6445931574752105, 0.7744515423179406, 0.0, 0.2402562811128986, 0.6683895153674697, 0.46179005171090737, 0.09741278879265103, 0.16025628111289864, 0.5876935279546878, 0.34025628111289863, 0.3079510025390523, 0.7053031368082721, 0.8795723049113977, 0.6903510788203177, 0.8896347633344268, 0.5374713058466929, 0.7017610307492962, 0.7302547136373172, 0.49936514445587815, 0.628200415633154, 0.24, 0.2176694832919011, 0.8186530227858186, 0.2011770485941022, 0.36813584036943403, 0.5246680028432865, 0.4595610896763082, 0.5067877730717336, 1.0, 0.12869111792447885, 0.39089314466834374, 0.6667528334695118, 0.4460952454685818, 0.3560867914013002, 0.523099034776898, 0.10356766952267597, 0.4136911179244788, 0.78685519109128, 0.7245741928928766, 0.5574204713453644, 0.6182924787290234, 0.3992286298565113, 0.5459435933526045, 0.3607039998273758, 0.676689598214979, 0.5862322739141566, 0.22499999999999998, 0.4854183564853334, 0.6365638961412512, 0.6983535654700125, 1.0, 0.29306704220365265, 0.7810169160146574, 0.576856426473012, 0.42951506189692296, 0.19999999999999998, 0.6684064493975892, 0.39347209291868157, 0.3148605110103055, 0.7504004004704901, 0.6212779984545589, 0.3775131980077461, 0.6038653593066965, 0.6851384298635612, 0.339320402762534, 0.316689598214979, 0.40621281109105456, 0.4551179673016059, 0.4822625464959074, 0.4406441546153521, 0.8203497781195837, 0.4065032326897709, 0.6734232449967186, 0.41996134394226886, 0.5745997086539989, 0.6278910313966588, 0.11453360085481801, 0.4858502876677805, 0.6304241997444744, 0.6018621031077366, 0.6046379048692274, 0.7365936445375989, 0.5951645181513807, 0.7671436083045311, 0.5203577845911456, 0.4453557619745404, 1.0, 0.7562262989907678, 0.6207211852446728, 0.3992286298565113, 0.4924077683967948, 0.43648868589003154, 0.5758089748131177, 0.007044218708112843, 0.7960658705890922, 0.6441764742916929, 0.021791405915364114, 1.0, 1.0, 0.40485075092622447, 0.7181903201805009, 0.8393695805229611, 0.6271806220068676, 0.4139240818757265, 0.6594544572565134, 0.4697300318922557, 0.6599223799332465, 0.4090633164381116, 0.6374705325762731, 0.1567859634729023, 0.6192538700356924, 0.9036449000698414, 0.174436274930823, 0.38302840324738974, 0.24, 0.9320894171538912, 0.7586059507468779, 0.36651542412587346, 0.6740103526642764, 0.731117687292167, 0.15997084006767517, 0.4008591117730833, 0.8212015472955612, 0.3280411661440926, 0.930386475290021, 0.9414428605659417, 0.11558530246943605, 0.48074569931823535, 0.7147461811855021, 0.0, 0.5992413083109663, 0.6844125934903402, 0.6434429789154809, 0.6119613551150405, 1.0, 0.3965193255633748, 0.7875054975857689, 0.7471765364160405, 0.4664006141196524, 0.630686584078196, 0.7263487966630597, 0.6213661362476086, 0.6298602502474342, 0.189991187756759, 0.7225371723695806, 0.5042531454411046, 0.5333108614657077, 0.5073028085492076, 0.8753186505731576, 1.0, 0.6636363636363636, 0.19999999999999998, 0.7430779662080889, 0.2504033576162784, 0.7564452295444688, 0.7951959058230954, 0.5779045553295564, 0.5017814354051087, 0.5585140118159326, 0.9250131914571023, 0.7503305249945151, 0.4766895982149789, 0.6624678525854575, 0.33372428676905674, 0.4929264451629831, 0.7401495338727193, 0.3714877971723338, 0.6848698374809952, 0.8307467892419347, 0.6844125934903402, 0.19972390808261475, 0.6381394976554917, 0.6463729364766281, 0.3743365916052602, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.39650783308755094, 0.4561694688162282, 0.29884397323472855, 0.6877459225855624, 0.8470997505392659, 0.0, 0.5966289756845221, 0.8849384145327521, 0.594295065161796, 1.0, 0.17818230393434137, 0.1905109696242391, 0.6366015466501336, 0.2046783687082245, 0.8232059780715621, 0.377190822574687, 0.6051632287777107, 0.49878377166916454, 0.7212489793637704, 0.4728429175459463, 0.8374634481857526, 0.5761697895341449, 1.0, 0.0461620487864034, 0.8928704934314164, 0.5885667747214658, 0.9238877689380496, 0.6942478573162068, 0.7559607945381612, 0.8255980153481091, 0.0, 0.1344057918034266, 0.8114529051148651, 0.3953131521590735, 0.5680054404199835, 0.6276136760976885, 0.714297617187952, 0.3502184077768308, 0.5721180064546281, 0.06666666666666667, 0.7340445690739947, 0.5340021561968085, 0.5702518310230941, 0.6240048954556174, 0.8034645362012123, 0.9211619479436641, 0.7812983362937829, 0.7837480609952844, 0.4372429839446083, 0.8634603372368668, 0.6100989929128371, 0.6273488306532815, 0.8578047138519911, 0.6386365314528626, 0.8078356986646513, 1.0, 0.8050241598897441, 0.8686125145982003, 0.5015372100944879, 0.5381816623263027, 0.7568990335893562, 0.4127700002336555, 0.0003475785050774528, 0.6768905471506659, 0.6287796716807859, 0.44507381648898203, 0.5569664494026063, 0.5468299419973112, 0.32283693344527487, 0.6625167201921691, 0.7249586401416586, 0.9176675865658077, 0.45024959573777057, 0.6478970276399474, 0.0, 0.4890714508161259, 0.5225548103389284, 0.6745228788727515, 0.5964858645175034, 0.4341373526797966, 0.756047661202274, 0.6038636468722998, 0.6603944867604312, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 1.0, 0.8084374616749153, 0.12915142402115995, 0.8263522613816869, 0.6311119478495857, 0.7536060548993002, 0.32898762131831805, 0.5642595261562982, 0.7700435930643703, 0.8250515340301889, 0.45101776545562844, 0.12, 0.3053372331674803, 0.7746218390368413, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.7916479842275448, 0.47093283317282664, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.8619254788161215, 0.6983535654700125, 0.5654473178899494, 0.760978313726013, 0.734763973349975, 0.1341979980200088, 0.4592493207167594, 0.7210821183929019, 0.7230706259860853, 0.10029870059345264, 0.5019774510104837, 0.31860139828537487, 0.2874560093019307, 0.7179453925554036, 0.7433503156010464, 0.18585881606736232, 0.39969884422068214, 0.3261184633367756, 0.41846432469397654, 0.6413698717053808, 0.6754386750930661, 0.0, 0.0, 0.7032072043308194, 0.8655056700329364, 0.24752119854989618, 0.709115943344826, 0.3659679909174146, 0.5833105633098615, 0.3061180216547285, 0.038139497655491794, 0.00783625026922906, 0.2815400648057302, 0.4778533416343903, 0.42273091745181135, 0.46459392039551944, 0.5047415132329038, 0.5027320193766102, 0.6119913484887409, 0.5092272948800609, 0.21056545775751523, 0.6160087565692354, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.6612315527854231, 0.8863585661014859, 0.6829777303051304, 0.7511655122445988, 0.6275893856012176, 0.8050605857172723, 0.08928571428571427, 0.40739727760582384, 0.5036911179244787, 0.22521040894986327, 0.8845572237610086, 0.702226723380104, 0.636068568378893, 0.6883329804487951, 0.34561606251791466, 0.19404871089985237, 0.13407144859326844, 0.6319795729625858, 0.8379283989138028, 0.8917028689549173, 0.33594700943102185, 0.8648103351245982, 0.08484732042396881, 1.0, 0.7178361513284952, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.5215425702265616, 0.29371181369212096, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.3583507427572924, 0.859664396260277, 0.6807990952074163, 0.6809070226208283, 0.7539341600372499, 0.7195565989965169, 0.7802568700480011, 0.7710658705890923, 0.6994023466303121, 0.3, 0.00018391908051612737, 0.7392647227570666, 0.594095094904146, 0.6376032890192349, 0.5265085296294044, 0.6202056140796331, 0.6931155273806628, 0.0817743687432679, 0.6785036503543816, 0.562520658360374, 0.6746054822806855, 0.46699403594567956, 0.23868348617790436, 0.7337480609952843, 0.6721193723777923, 0.4695633649758072, 0.6157955965877151, 0.642418735455043, 0.5562895166803407, 0.646516427917569, 0.29722643810031, 0.7533655306263078, 0.387966609042281, 0.4347288804035845]
Finish training and take 19m
