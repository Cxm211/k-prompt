Namespace(log_name='./RQ5/tfix_900_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_900_3/codet5p_770m', data_dir='./data/RQ5/tfix_900_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'var inputPipesOf = inputPipes.bind(null, pipe)     , parentTaskIds = []', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'var inputPipesOf = inputPipes.bind(null, pipe)   var parentTaskIds = []'}]
***** Running training *****
  Num examples = 900
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 226
  train_loss = 17.0816
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 59.9 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:59.9
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 451
  train_loss = 8.8293
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 63.54 	 Previous best codebleu 59.9
  ********************
 Achieve Best bleu:63.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 676
  train_loss = 4.409
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 58.7 	 Previous best codebleu 63.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 901
  train_loss = 2.6704
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 62.01 	 Previous best codebleu 63.54
  ********************
early stopping!!!
reload model from RQ5/tfix_900_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_900_3/test.jsonl
  codebleu = 63.61 
  Total = 500 
  Exact Fixed = 106 
[5, 6, 13, 18, 19, 20, 24, 25, 44, 46, 47, 54, 59, 62, 65, 67, 69, 77, 78, 80, 85, 93, 97, 99, 101, 105, 109, 111, 114, 117, 121, 123, 129, 131, 134, 146, 147, 148, 151, 152, 153, 156, 162, 170, 187, 188, 191, 198, 199, 205, 212, 217, 223, 226, 229, 233, 242, 253, 255, 258, 271, 276, 283, 284, 291, 295, 297, 300, 303, 304, 308, 310, 317, 320, 324, 331, 339, 344, 345, 356, 357, 359, 389, 391, 394, 403, 407, 413, 418, 424, 441, 446, 448, 449, 452, 453, 457, 465, 475, 484, 487, 488, 489, 493, 495, 497]
  Syntax Fixed = 10 
[14, 41, 60, 74, 238, 252, 290, 371, 406, 411]
  Cleaned Fixed = 7 
[9, 120, 159, 268, 285, 417, 445]
  ********************
  Total = 500 
  Exact Fixed = 106 
[5, 6, 13, 18, 19, 20, 24, 25, 44, 46, 47, 54, 59, 62, 65, 67, 69, 77, 78, 80, 85, 93, 97, 99, 101, 105, 109, 111, 114, 117, 121, 123, 129, 131, 134, 146, 147, 148, 151, 152, 153, 156, 162, 170, 187, 188, 191, 198, 199, 205, 212, 217, 223, 226, 229, 233, 242, 253, 255, 258, 271, 276, 283, 284, 291, 295, 297, 300, 303, 304, 308, 310, 317, 320, 324, 331, 339, 344, 345, 356, 357, 359, 389, 391, 394, 403, 407, 413, 418, 424, 441, 446, 448, 449, 452, 453, 457, 465, 475, 484, 487, 488, 489, 493, 495, 497]
  Syntax Fixed = 10 
[14, 41, 60, 74, 238, 252, 290, 371, 406, 411]
  Cleaned Fixed = 7 
[9, 120, 159, 268, 285, 417, 445]
  codebleu = 63.61 
[0.2790159959288229, 0.8170015046833874, 0.7456907908606256, 0.4787139878886564, 0.7450395226130992, 0.9271179107073657, 0.7130804239841513, 0.11582292438020222, 0.6119613551150405, 0.824344816363836, 0.19726550033812454, 0.7067366858513033, 0.6640033451354017, 0.756047661202274, 0.39323313167877133, 0.6913784572286918, 0.5033638986642797, 1.0, 1.0, 1.0, 0.8214203428813993, 0.3743172882353815, 0.13846153846153847, 0.83881529495591, 1.0, 0.2504033576162784, 0.5196206410170191, 0.15747669278081078, 0.5252284068217509, 0.110722640539825, 0.535133493667276, 0.6142482969348242, 0.7048941706002465, 0.4198769757196277, 0.35408412750050705, 0.8149139863647084, 0.3062558977392416, 0.8504403342456683, 0.6934531647858191, 0.43438472627011904, 0.8518324738936913, 0.22267396568917747, 0.6057649027320717, 0.9891483218006352, 0.5481721142074378, 1.0, 1.0, 0.6642742735155721, 0.5712490372843609, 0.8951959058230954, 0.5123984208296868, 0.5266057619745405, 0.8223873319410013, 0.7351751543429909, 0.677190822574687, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 0.9520446947173573, 0.9383259097532135, 1.0, 0.5610771314034941, 0.5897692121354159, 0.8229808431453671, 0.48289849585465594, 1.0, 0.6170182644937875, 1.0, 0.7539630945946609, 0.16545744323923328, 0.20369111792447886, 0.08978100888954238, 0.9584407378958149, 0.5007846217557572, 0.23785310412867677, 1.0, 1.0, 0.3479500938643075, 0.8503143105742339, 0.6530747837026651, 0.6387289085944345, 0.26090620551116817, 0.7194516087418251, 1.0, 0.9179350389391119, 0.0, 0.3353314785118311, 0.0485646584311115, 0.5566895982149789, 0.4570583972426556, 0.7299948711471688, 1.0, 0.839865482182728, 0.4134404736596011, 0.4484520389345378, 1.0, 0.028318180717115303, 1.0, 0.6088783494196637, 0.8114529051148651, 0.4965144687034782, 0.7752216631749811, 0.0, 0.8249365300761395, 0.30480616432215846, 0.48167268419835185, 0.467913762972279, 1.0, 0.5024101654079205, 1.0, 0.6785036503543816, 0.35473913473843294, 1.0, 0.2924537101693822, 0.5566638701334943, 0.9474466761527047, 0.16950123401194037, 0.19999999999999998, 0.08953276868214524, 1.0, 0.9068864709786153, 0.9472083239018387, 0.10906101778358473, 0.8840188084091536, 0.7256392993228769, 0.4118819918812901, 0.5551676892768422, 1.0, 0.32616770616700436, 1.0, 0.7386991709927695, 0.634297617187952, 1.0, 0.22619887519287302, 0.4544627617245256, 0.7697319914623539, 0.5000222006460707, 0.6052067080970429, 0.6937464703296872, 0.4698683469384751, 0.5960707868709735, 0.27528728007290226, 0.5440521407974296, 0.7396601242701673, 1.0, 1.0, 1.0, 0.7974634481857525, 0.49014713870993193, 1.0, 1.0, 1.0, 0.40135160729534664, 0.7728222909971192, 0.8249365300761395, 0.6241730049753154, 0.8245957586820076, 0.7290594020931371, 0.5266057619745405, 0.5348435312891645, 0.7863340021370451, 0.5461243465154161, 0.5301818996067954, 0.7424282823720908, 0.6745836923955046, 0.44478640618236376, 0.35511870560120445, 0.6957231649810731, 0.7135428903906851, 0.6336131842675646, 0.5941888941101804, 0.4831458551130241, 0.5532531256091296, 0.6477517173276517, 0.7202172177276029, 0.5882210413806648, 0.41522228363818015, 0.8273136413238535, 0.510409269308891, 0.7269320077439674, 0.6879133182048369, 0.7302007404183433, 0.8426568563575418, 0.7546391146496889, 0.3812999423756144, 1.0, 1.0, 0.40080814539619514, 0.5609260915878896, 1.0, 0.5728633222115624, 0.7359097981643703, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.8123223032135625, 0.8114529051148651, 1.0, 0.39873667339437135, 0.15429293535427072, 0.6863884298635612, 0.7000353517077935, 0.6867787885259955, 1.0, 0.6459247942853364, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.31888879608349985, 0.21245371016938225, 0.9891483218006352, 0.9080331470954286, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6663998078842303, 0.4474101908974676, 0.48779329396875104, 0.6252358465506889, 0.35624947398505524, 1.0, 0.3819511489967859, 0.6387851138365109, 1.0, 0.6271806220068676, 0.4971121850651573, 1.0, 0.5660528353107435, 0.39359500830217886, 0.7862551025473414, 0.9891483218006352, 0.47035576197454043, 0.7521271372862586, 0.7154629787573281, 0.6726527033303111, 0.7970182644937875, 0.6056583090096291, 0.6029055732198362, 0.3698963377183132, 1.0, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6533039205036821, 0.7080707346422974, 0.1588669502253369, 0.4435027248795795, 0.8113118214155393, 0.04101911673989318, 0.9657549444990421, 1.0, 0.6179144359460748, 0.782115426602226, 0.4223835094667513, 0.0, 1.0, 0.8046451547488727, 0.09418181350952061, 0.5376172770255981, 0.8476757297426298, 0.8016000841539659, 0.604436274930823, 0.565895296523178, 0.6645982898765359, 0.0, 0.6993751196103626, 0.6310308074685265, 0.367641751923923, 1.0, 0.8082416075216149, 0.6463338290119179, 0.177190822574687, 0.13636363636363635, 0.7135428903906851, 0.6084150066900985, 0.5156424393838233, 0.11349072297901378, 0.7564084664881232, 0.6519830178464932, 0.6506923930808135, 1.0, 1.0, 0.8516684096158422, 0.4141117485802601, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.8459894670707158, 1.0, 0.29286074313859833, 0.8398078253515011, 0.7940821326000729, 0.7826555560719357, 0.027660853979899265, 1.0, 0.8835687037030036, 0.5843539719797703, 1.0, 0.43701826449378744, 0.6179062905764697, 0.7135428903906851, 0.8249365300761395, 0.8206847188488577, 0.3844698211793555, 0.8065781220782022, 1.0, 0.40660822240095185, 0.8936825710002676, 0.7133927634579131, 0.3433562648816456, 0.7460474584273435, 0.8162863460162433, 0.46526859680566335, 0.7531697895341449, 0.9318657024016066, 0.5290092325728539, 0.6247768518453196, 1.0, 0.5537998353931795, 0.8255980153481091, 0.6969247006410559, 1.0, 0.37549356386576216, 0.599029654397516, 0.6831170190578055, 0.5189589454524783, 0.5382922926202521, 0.4160581146410312, 1.0, 0.7750950271331211, 0.4959953038804974, 0.72455988025806, 0.4707909983708979, 0.5841414787295285, 0.8140165767392193, 0.7144948346694379, 1.0, 0.8765344915115144, 0.6352900380997268, 0.7329853689231081, 0.7668758886842345, 1.0, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5423688216151067, 0.38304611372110686, 0.47786341412302047, 0.6721193723777923, 0.6999214919756145, 0.7234622423171753, 0.629920929034602, 0.6737569575259978, 1.0, 1.0, 0.8193461763670171, 1.0, 0.5140913145175611, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.5504109403593369, 0.7391886750930661, 0.4407283286877167, 0.5669580702376444, 0.34003623200632915, 0.28452740980526503, 0.756047661202274, 0.40960005732452515, 0.3918436547458276, 0.7397718799592691, 0.5099766481408452, 0.11453360085481801, 0.5308989025444404, 0.6892870539097846, 0.8103529851697227, 0.545545649326698, 0.8042470965743207, 0.6530367547828486, 0.7802383125555377, 0.04448415542647708, 0.4121673239558469, 0.6742613142686853, 0.7892647227570666, 0.8623473261888206, 1.0, 0.6193453441662242, 1.0, 0.4444597200395934, 0.5781687723560363, 0.8114529051148651, 0.3807256007926331, 0.8165273869902545, 0.6581346216373467, 0.6742857383161394, 0.15807201856685674, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 1.0, 0.7313559408133775, 0.7017410285928236, 0.969001580992227, 1.0, 0.4054183564853334, 0.34575990398983625, 0.5991554821598547, 0.9365276486140073, 0.691735368923108, 1.0, 0.7440965622438329, 0.40209781994840155, 0.40733923790399773, 0.6816015466501335, 1.0, 0.602487595382981, 0.2175320073157329, 0.23683637774429345, 0.6428938087244226, 0.3242197269682706, 1.0, 0.6501690122312549, 0.15147944874811742, 0.19272438732935, 0.6418894161880858, 0.5149139863647084, 0.38892361152314225, 0.4085332823846201, 0.8661547359011423, 0.15472876794829787, 0.688062756936776, 0.39999999999999997, 0.7023594991536818, 0.7429370522068693, 0.6847340670900772, 0.7636815104017958, 0.4933895153674698, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 0.6522551430667551, 1.0, 0.5316562756993463, 1.0, 1.0, 0.7004611223221533, 0.42038175275163125, 1.0, 1.0, 0.24895420055876946, 0.28890659929471596, 0.6578607431385983, 0.8114529051148651, 0.7322642870443095, 0.8334272760895116, 0.6745758964627063, 0.003996009641464269, 0.839616326453424, 0.47102574599540464, 0.6797190631118583, 1.0, 0.34509049865171215, 0.5075783901825247, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.37746298540922907, 0.8777227983675986, 0.6835302798798335, 0.7398815472813818, 0.8114529051148651, 0.4255577241698508, 0.33591281745819057, 0.12, 0.8010669062882427, 0.3698677180019313, 0.766682914692771, 0.46843920200122524, 0.57631529495591, 1.0, 0.5428238436964714, 0.4616935697845572, 1.0, 1.0, 1.0, 0.4229578192624543, 0.566221018567181, 0.6186389706521688, 1.0, 0.7634373166195929, 1.0, 0.6492633713864637, 1.0, 0.8691371695837178, 0.20034796016239415, 0.27085988727733457]
Finish training and take 18m
