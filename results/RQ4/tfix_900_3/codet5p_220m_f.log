Namespace(log_name='./RQ5/tfix_900_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_900_3/codet5p_220m_f', data_dir='./data/RQ5/tfix_900_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 900 training instances 
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00419
  global_step = 113
  train_loss = 0.8058
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00419
  ********************
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 59.48 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:59.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00417
  global_step = 225
  train_loss = 0.4087
  ********************
Previous best ppl:1.00419
Achieve Best ppl:1.00417
  ********************
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 61.06 	 Previous best codebleu 59.48
  ********************
 Achieve Best bleu:61.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00425
  global_step = 337
  train_loss = 0.266
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 60.94 	 Previous best codebleu 61.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00474
  global_step = 449
  train_loss = 0.1821
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 59.78 	 Previous best codebleu 61.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00488
  global_step = 561
  train_loss = 0.1235
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 63.27 	 Previous best codebleu 61.06
  ********************
 Achieve Best bleu:63.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00505
  global_step = 673
  train_loss = 0.0851
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 62.96 	 Previous best codebleu 63.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00506
  global_step = 785
  train_loss = 0.0789
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 64.01 	 Previous best codebleu 63.27
  ********************
 Achieve Best bleu:64.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00533
  global_step = 897
  train_loss = 0.0506
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 62.82 	 Previous best codebleu 64.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0053
  global_step = 1009
  train_loss = 0.0408
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 64.27 	 Previous best codebleu 64.01
  ********************
 Achieve Best bleu:64.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00532
  global_step = 1121
  train_loss = 0.0324
  ********************
Previous best ppl:1.00417
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 64.54 	 Previous best codebleu 64.27
  ********************
 Achieve Best bleu:64.54
  ********************
reload model from RQ5/tfix_900_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_900_3/test.jsonl
  codebleu = 64.37 
  Total = 500 
  Exact Fixed = 109 
[3, 5, 6, 13, 18, 19, 20, 25, 29, 44, 46, 47, 54, 59, 61, 62, 65, 67, 75, 77, 78, 79, 80, 81, 85, 93, 101, 105, 109, 110, 111, 115, 121, 123, 126, 129, 130, 131, 133, 134, 139, 151, 152, 153, 156, 159, 162, 176, 178, 188, 191, 198, 199, 205, 217, 223, 226, 229, 233, 236, 242, 252, 253, 258, 262, 265, 271, 284, 295, 297, 298, 300, 302, 308, 310, 317, 331, 336, 339, 340, 344, 345, 357, 359, 371, 387, 396, 403, 406, 407, 418, 420, 424, 439, 440, 441, 445, 446, 449, 453, 457, 468, 475, 477, 487, 488, 489, 493, 497]
  Syntax Fixed = 5 
[28, 41, 182, 411, 425]
  Cleaned Fixed = 7 
[120, 138, 182, 280, 285, 417, 425]
  ********************
  Total = 500 
  Exact Fixed = 109 
[3, 5, 6, 13, 18, 19, 20, 25, 29, 44, 46, 47, 54, 59, 61, 62, 65, 67, 75, 77, 78, 79, 80, 81, 85, 93, 101, 105, 109, 110, 111, 115, 121, 123, 126, 129, 130, 131, 133, 134, 139, 151, 152, 153, 156, 159, 162, 176, 178, 188, 191, 198, 199, 205, 217, 223, 226, 229, 233, 236, 242, 252, 253, 258, 262, 265, 271, 284, 295, 297, 298, 300, 302, 308, 310, 317, 331, 336, 339, 340, 344, 345, 357, 359, 371, 387, 396, 403, 406, 407, 418, 420, 424, 439, 440, 441, 445, 446, 449, 453, 457, 468, 475, 477, 487, 488, 489, 493, 497]
  Syntax Fixed = 5 
[28, 41, 182, 411, 425]
  Cleaned Fixed = 7 
[120, 138, 182, 280, 285, 417, 425]
  codebleu = 64.37 
[0.3852352687934718, 0.8170015046833874, 1.0, 0.4787139878886564, 1.0, 1.0, 0.6845648861187565, 0.11582292438020222, 0.5981716191316424, 0.6779253872262, 0.19726550033812454, 0.7067366858513033, 1.0, 0.6863884298635612, 0.39323313167877133, 0.6127572978503959, 0.45189927805897867, 1.0, 1.0, 1.0, 0.8214203428813993, 0.3743172882353815, 0.15, 0.8228420677375816, 1.0, 0.2772943579237247, 0.7949680697811121, 0.7107915119421391, 1.0, 0.110722640539825, 0.6423715976280558, 0.6142482969348242, 0.7994534636574044, 0.4198769757196277, 0.35408412750050705, 0.8149139863647084, 0.8275841908785402, 0.8504403342456683, 0.6934531647858191, 0.5074639622547265, 0.9377649116482698, 0.22267396568917747, 0.4134709869742218, 0.9891483218006352, 0.5481721142074378, 1.0, 1.0, 0.5892742735155722, 0.46877089663917104, 0.8951959058230954, 0.5524581952101042, 0.5266057619745405, 0.8223873319410013, 1.0, 0.677190822574687, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 0.6148576874551961, 1.0, 1.0, 0.23537461062620735, 0.4803494323607689, 1.0, 0.7487325657010794, 1.0, 0.5036465025036173, 0.9517008915274616, 0.7539630945946609, 0.24361462489811037, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 1.0, 0.20613487118166404, 1.0, 1.0, 1.0, 1.0, 0.8249365300761395, 0.7265018183047776, 0.18, 0.803950001904677, 1.0, 0.9179350389391119, 0.0, 0.3822410046649487, 0.0485646584311115, 0.5675986891240699, 0.3916882592503279, 0.7299948711471688, 1.0, 0.9137960674324628, 0.3915187569135013, 0.4484520389345378, 0.7459627599846212, 0.039536122964966045, 0.6488884298635612, 0.6088783494196637, 0.8114529051148651, 0.40324167440714376, 0.8350700737155115, 0.0, 0.8249365300761395, 0.30480616432215846, 0.48167268419835185, 0.467913762972279, 1.0, 1.0, 1.0, 0.6785036503543816, 0.35473913473843294, 0.5379694287810409, 0.8114529051148651, 0.5566638701334943, 0.6851303805216359, 0.2610380151684039, 0.0, 0.08953276868214524, 1.0, 0.31859326906298235, 1.0, 0.10906101778358473, 0.8840188084091536, 1.0, 0.4118819918812901, 0.726624382466148, 1.0, 0.9891483218006352, 1.0, 0.5107646604974312, 1.0, 1.0, 0.22619887519287302, 0.44196276172452564, 0.4831321633398339, 0.8382851365602284, 1.0, 0.6937464703296872, 0.5187838131148098, 0.64272438732935, 0.27320579161919023, 0.5827005454349591, 0.7396601242701673, 0.7530045026259591, 0.47578247752410807, 0.523507649142383, 0.5760694567816493, 0.5063942770541421, 1.0, 1.0, 1.0, 0.40135160729534664, 0.7728222909971192, 0.8249365300761395, 0.734603718873536, 0.8245957586820076, 1.0, 0.5266057619745405, 0.5348435312891645, 1.0, 0.5461243465154161, 0.5301818996067954, 0.7424282823720908, 0.6888694066812189, 0.44478640618236376, 0.41511870560120445, 0.6957231649810731, 0.18652595195999017, 0.6336131842675646, 0.5386096543033837, 0.4831458551130241, 0.5532531256091296, 0.6477517173276517, 1.0, 0.5882210413806648, 1.0, 0.8273136413238535, 0.6190738228228467, 0.7269320077439674, 0.84248072287024, 0.7302007404183433, 0.8426568563575418, 0.8516700572634182, 0.3812999423756144, 0.8784253679044904, 1.0, 0.4231498701419104, 0.5609260915878896, 1.0, 0.5728633222115624, 0.7359097981643703, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.42391409499168653, 0.8114529051148651, 1.0, 0.39873667339437135, 0.15429293535427072, 0.8191441569283882, 0.8002608097855715, 0.6867787885259955, 1.0, 0.6459247942853364, 0.9226212781081518, 0.0731716191316424, 0.46418189365627316, 0.31888879608349985, 0.21245371016938225, 0.3972934177879658, 0.9080331470954286, 0.44665344789927386, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6663998078842303, 0.42493877026999405, 0.619750295236949, 0.6487021300892772, 0.35624947398505524, 1.0, 0.6072359854134853, 0.6387851138365109, 1.0, 0.6271806220068676, 0.4766759794022652, 1.0, 0.43016109313909, 0.39359500830217886, 0.6146317849491512, 0.9891483218006352, 0.47035576197454043, 0.7815563138499123, 1.0, 0.6089886858900315, 0.7448443514503091, 0.3938266805419558, 0.6029055732198362, 0.5410631333323874, 1.0, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6952637111045539, 0.8407059481807464, 0.1717728294285068, 0.4435027248795795, 0.8113118214155393, 0.04030522491224135, 1.0, 1.0, 0.6179144359460748, 0.47535257334060044, 0.5223835094667514, 0.0, 1.0, 0.7355980328281302, 0.09418181350952061, 0.5376172770255981, 0.834704902378399, 0.8016000841539659, 0.604436274930823, 0.8249365300761395, 0.6875131192847503, 0.0, 0.7142857142857142, 0.6310308074685265, 0.367641751923923, 1.0, 0.7582416075216148, 0.6463338290119179, 0.22719082257468703, 0.15789473684210525, 0.5058144125160369, 0.7439654153934541, 0.7570701463285548, 0.11349072297901378, 0.873209263429523, 0.06423223587777374, 0.6506923930808135, 0.6041077305454495, 1.0, 0.8516684096158422, 0.4141117485802601, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.5910593910145905, 0.8244788648855748, 0.29286074313859833, 0.8398078253515011, 0.7639700877821456, 0.7826555560719357, 0.027660853979899265, 1.0, 1.0, 0.5843539719797703, 1.0, 0.43701826449378744, 1.0, 0.15837703500732306, 0.3814205333654446, 0.7191441569283882, 0.3844698211793555, 0.5824615640430578, 1.0, 0.40660822240095185, 1.0, 0.8095727631302025, 0.3561219555899987, 0.8225095086407594, 0.8997846296433376, 0.5644004745782011, 0.7531697895341449, 0.9318657024016066, 0.5290092325728539, 0.6060574827359351, 0.5678911585054794, 0.5537998353931795, 0.8255980153481091, 0.6969247006410559, 0.5019100417936252, 0.37549356386576216, 0.599029654397516, 0.8269967312248111, 0.46253094342499124, 0.6068567648108864, 0.4452247813076979, 1.0, 0.6742738870938194, 0.4959953038804974, 0.72455988025806, 0.4707909983708979, 0.8114529051148651, 0.9195522364276514, 0.7144948346694379, 1.0, 1.0, 0.712432895242584, 0.7329853689231081, 0.6928455688060347, 1.0, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.47786341412302047, 0.38637311324227014, 0.6467214894577404, 0.7889213653390947, 0.629920929034602, 0.5924295517447509, 0.3426136760976885, 1.0, 0.9608461673226849, 1.0, 0.5773753982148186, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.5226136760976885, 0.7391886750930661, 0.4407283286877167, 0.6655012758056489, 0.34003623200632915, 0.28452740980526503, 1.0, 0.5967321681828902, 0.3918436547458276, 0.8701746619282504, 0.5099766481408452, 0.11453360085481801, 0.536560588211956, 0.6892870539097846, 0.8102736261199102, 0.545545649326698, 0.8042470965743207, 0.6164734436957756, 0.5650407172771029, 0.04448415542647708, 0.3154745710868131, 0.3994083999106591, 1.0, 0.42549908952165627, 0.8685222066525069, 0.6193453441662242, 0.8174230148919821, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 1.0, 0.7009116410097869, 0.6742857383161394, 0.15807201856685674, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 1.0, 0.8007265089204272, 0.7881248632151466, 1.0, 1.0, 0.4054183564853334, 0.30791559625159903, 0.8433369370338967, 0.9365276486140073, 0.691735368923108, 0.6424957217109725, 0.7440965622438329, 0.40209781994840155, 0.40400364537321126, 0.6816015466501335, 1.0, 0.602487595382981, 1.0, 0.23683637774429345, 0.664322380152994, 0.3372632052291401, 1.0, 0.9821771168817468, 0.6903510788203177, 0.19272438732935, 0.6418894161880858, 0.5149139863647084, 0.7233004498085515, 0.4085332823846201, 0.8661547359011423, 0.414297617187952, 0.688062756936776, 0.39999999999999997, 0.7023594991536818, 0.6453274394589772, 0.4505226685069079, 1.0, 1.0, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 1.0, 1.0, 0.6151628493773373, 0.6901940524102714, 1.0, 0.6733027629890207, 0.42038175275163125, 0.8542712839031905, 1.0, 0.39206686043974615, 0.28890659929471596, 0.6578607431385983, 0.8114529051148651, 0.4650586376515653, 0.8334272760895116, 0.6615804724100134, 0.003996009641464269, 0.8253119306693903, 0.47102574599540464, 0.677288551000834, 0.4242309802201044, 0.34509049865171215, 0.41118547774125425, 0.8249365300761395, 0.4016265496309229, 0.45473913473843297, 0.6160987310372124, 0.8777227983675986, 0.6731015007059578, 0.7208750701213389, 0.8114529051148651, 0.643309301212212, 1.0, 0.12, 0.8560255464372679, 0.3698677180019313, 0.766682914692771, 0.46843920200122524, 0.36393172467891854, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 1.0, 1.0, 1.0, 0.41969276730357064, 0.566221018567181, 0.6186389706521688, 1.0, 0.6834373166195928, 0.7396773803089467, 0.6492633713864637, 1.0, 0.8691371695837178, 0.20034796016239415, 0.3158229243802022]
Finish training and take 26m
Namespace(log_name='./RQ5/tfix_900_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_900_3/codet5p_220m_f', data_dir='./data/RQ5/tfix_900_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 900 training instances 
***** Running training *****
  Num examples = 900
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00429
  global_step = 113
  train_loss = 0.8075
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00429
  ********************
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 60.55 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:60.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00437
  global_step = 225
  train_loss = 0.4332
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 61.0 	 Previous best codebleu 60.55
  ********************
 Achieve Best bleu:61.0
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00433
  global_step = 337
  train_loss = 0.2913
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 62.83 	 Previous best codebleu 61.0
  ********************
 Achieve Best bleu:62.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00467
  global_step = 449
  train_loss = 0.1996
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 61.1 	 Previous best codebleu 62.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00493
  global_step = 561
  train_loss = 0.1419
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 61.89 	 Previous best codebleu 62.83
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00488
  global_step = 673
  train_loss = 0.0991
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 63.17 	 Previous best codebleu 62.83
  ********************
 Achieve Best bleu:63.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00507
  global_step = 785
  train_loss = 0.0683
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 62.36 	 Previous best codebleu 63.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00521
  global_step = 897
  train_loss = 0.0532
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 63.61 	 Previous best codebleu 63.17
  ********************
 Achieve Best bleu:63.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00541
  global_step = 1009
  train_loss = 0.0433
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 63.67 	 Previous best codebleu 63.61
  ********************
 Achieve Best bleu:63.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00546
  global_step = 1121
  train_loss = 0.0353
  ********************
Previous best ppl:1.00429
BLEU file: ./data/RQ5/tfix_900_3/validation.jsonl
  codebleu-4 = 63.97 	 Previous best codebleu 63.67
  ********************
 Achieve Best bleu:63.97
  ********************
reload model from RQ5/tfix_900_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_900_3/test.jsonl
  codebleu = 63.48 
  Total = 500 
  Exact Fixed = 103 
[5, 6, 13, 14, 19, 20, 24, 25, 29, 44, 47, 54, 55, 59, 60, 61, 62, 65, 67, 75, 77, 78, 79, 80, 81, 85, 93, 101, 109, 111, 123, 130, 131, 133, 134, 139, 151, 152, 153, 162, 176, 178, 182, 187, 188, 191, 198, 199, 212, 217, 223, 226, 229, 233, 252, 253, 255, 258, 262, 265, 271, 284, 295, 298, 308, 310, 317, 331, 336, 339, 344, 345, 357, 359, 365, 374, 381, 396, 403, 406, 407, 413, 418, 424, 426, 439, 440, 441, 445, 446, 448, 449, 453, 457, 465, 468, 475, 484, 487, 488, 489, 493, 497]
  Syntax Fixed = 6 
[28, 41, 43, 291, 340, 411]
  Cleaned Fixed = 5 
[120, 138, 159, 285, 417]
  ********************
  Total = 500 
  Exact Fixed = 103 
[5, 6, 13, 14, 19, 20, 24, 25, 29, 44, 47, 54, 55, 59, 60, 61, 62, 65, 67, 75, 77, 78, 79, 80, 81, 85, 93, 101, 109, 111, 123, 130, 131, 133, 134, 139, 151, 152, 153, 162, 176, 178, 182, 187, 188, 191, 198, 199, 212, 217, 223, 226, 229, 233, 252, 253, 255, 258, 262, 265, 271, 284, 295, 298, 308, 310, 317, 331, 336, 339, 344, 345, 357, 359, 365, 374, 381, 396, 403, 406, 407, 413, 418, 424, 426, 439, 440, 441, 445, 446, 448, 449, 453, 457, 465, 468, 475, 484, 487, 488, 489, 493, 497]
  Syntax Fixed = 6 
[28, 41, 43, 291, 340, 411]
  Cleaned Fixed = 5 
[120, 138, 159, 285, 417]
  codebleu = 63.48 
[0.3852352687934718, 0.8170015046833874, 0.5739877791532703, 0.4787139878886564, 1.0, 1.0, 0.7386443337585873, 0.11582292438020222, 0.5981716191316424, 0.6779253872262, 0.19726550033812454, 0.7067366858513033, 1.0, 1.0, 0.39323313167877133, 0.6127572978503959, 0.45189927805897867, 0.44412484527547247, 1.0, 1.0, 0.8214203428813993, 0.3743172882353815, 0.15, 1.0, 1.0, 0.2772943579237247, 0.7949680697811121, 0.7107915119421391, 1.0, 0.110722640539825, 0.535133493667276, 0.6142482969348242, 0.7048941706002465, 0.4198769757196277, 0.1884635074738072, 0.8149139863647084, 0.25777885407084644, 0.8504403342456683, 0.6934531647858191, 0.5074639622547265, 0.9377649116482698, 0.22267396568917747, 0.6302333034257401, 0.9891483218006352, 0.5481721142074378, 0.5952887122558812, 1.0, 0.5892742735155722, 0.46877089663917104, 0.8951959058230954, 0.5524581952101042, 0.5266057619745405, 0.8223873319410013, 1.0, 0.8249365300761395, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 1.0, 1.0, 1.0, 0.23537461062620735, 0.5897692121354159, 1.0, 0.7487325657010794, 1.0, 0.6170182644937875, 0.9517008915274616, 0.7539630945946609, 0.2752284068217509, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 1.0, 0.20613487118166404, 1.0, 1.0, 1.0, 1.0, 0.8249365300761395, 0.561772186391564, 0.18, 0.803950001904677, 1.0, 0.9179350389391119, 0.0, 0.2966597993138361, 0.06999322985968293, 0.5675986891240699, 0.20821851560994864, 0.7299948711471688, 1.0, 0.9517918920357142, 0.3915187569135013, 0.4484520389345378, 0.46043936376930283, 0.028318180717115303, 0.686863907300452, 0.6088783494196637, 0.8114529051148651, 0.40324167440714376, 0.6948645203178383, 0.0, 0.050492779251189994, 0.30480616432215846, 0.48167268419835185, 0.467913762972279, 1.0, 0.5024101654079205, 1.0, 0.6785036503543816, 0.35473913473843294, 0.49284570208700074, 0.2924537101693822, 0.5566638701334943, 0.6851303805216359, 0.2610380151684039, 0.19999999999999998, 0.08953276868214524, 0.714297617187952, 0.9068864709786153, 1.0, 0.10906101778358473, 0.9139712635151394, 0.8150666162955742, 0.2661180216547285, 0.726624382466148, 0.5890213683206489, 0.9891483218006352, 1.0, 0.7386991709927695, 1.0, 1.0, 0.22619887519287302, 0.44196276172452564, 0.7697319914623539, 0.8382851365602284, 1.0, 0.6937464703296872, 0.5187838131148098, 0.5960707868709735, 0.27320579161919023, 0.5440521407974296, 0.7396601242701673, 0.7530045026259591, 0.47578247752410807, 0.523507649142383, 0.6060694567816493, 0.5063942770541421, 1.0, 1.0, 1.0, 0.35436935077271325, 0.7728222909971192, 0.45236374240186505, 0.734603718873536, 0.42970304404388293, 0.9215567956810036, 0.5266057619745405, 0.5348435312891645, 1.0, 0.5461243465154161, 0.5301818996067954, 0.7424282823720908, 0.6745836923955046, 0.44478640618236376, 0.41511870560120445, 0.6957231649810731, 0.18652595195999017, 0.5432390392078216, 0.5941888941101804, 0.4831458551130241, 0.5532531256091296, 0.6477517173276517, 1.0, 0.5882210413806648, 1.0, 0.8273136413238535, 0.6190738228228467, 0.7269320077439674, 0.795720901409837, 0.7302007404183433, 0.8426568563575418, 0.8516700572634182, 0.3812999423756144, 1.0, 1.0, 0.4231498701419104, 0.2559983304118015, 1.0, 0.5728633222115624, 0.7359097981643703, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.32235479041432746, 0.8114529051148651, 1.0, 0.39873667339437135, 0.15429293535427072, 0.6863884298635612, 0.8201517014631392, 0.6867787885259955, 0.6241223232336472, 0.225962006708376, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.31888879608349985, 0.21245371016938225, 0.9891483218006352, 0.9080331470954286, 0.44665344789927386, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6663998078842303, 0.42493877026999405, 0.46096577652487136, 0.59820323220824, 0.35624947398505524, 1.0, 0.6072359854134853, 0.6387851138365109, 1.0, 0.6271806220068676, 0.4766759794022652, 1.0, 0.43016109313909, 0.39359500830217886, 0.7862551025473414, 0.9891483218006352, 0.47035576197454043, 0.7521271372862586, 0.5274151448167971, 0.6089886858900315, 0.699716658571939, 0.5389213653390948, 0.6029055732198362, 0.5410631333323874, 0.7387824622428694, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6952637111045539, 0.8596354401872717, 0.1717728294285068, 0.4435027248795795, 0.8113118214155393, 0.04030522491224135, 1.0, 1.0, 0.6179144359460748, 1.0, 0.5223835094667514, 0.0, 1.0, 0.8046451547488727, 0.16826111698077859, 0.5376172770255981, 0.834704902378399, 0.8016000841539659, 0.604436274930823, 0.8249365300761395, 0.7855975357012536, 0.0, 0.7142857142857142, 0.6310308074685265, 0.367641751923923, 1.0, 0.7582416075216148, 0.6463338290119179, 0.22719082257468703, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.7570701463285548, 0.09999999999999999, 0.7906536181776866, 0.1779525474706411, 0.6506923930808135, 0.6041077305454495, 1.0, 0.8516684096158422, 0.4141117485802601, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.6339165338717334, 0.7912022751182011, 0.29286074313859833, 0.8398078253515011, 0.7639700877821456, 0.7826555560719357, 0.027660853979899265, 0.5550049310474542, 1.0, 0.5843539719797703, 0.6197856242749173, 0.43701826449378744, 0.7704282310037642, 0.2778574746187954, 0.3814205333654446, 0.8206847188488577, 0.3844698211793555, 0.6107000692160132, 1.0, 0.40660822240095185, 1.0, 0.8095727631302025, 0.3433562648816456, 0.8225095086407594, 0.8997846296433376, 0.5644004745782011, 0.7531697895341449, 0.9318657024016066, 0.5290092325728539, 0.6060574827359351, 0.5678911585054794, 0.5537998353931795, 0.8255980153481091, 0.6392333266349789, 0.5019100417936252, 0.37549356386576216, 0.6148191280817265, 0.6794696234291855, 0.46253094342499124, 0.5651627516802897, 0.4452247813076979, 1.0, 0.7750950271331211, 0.4959953038804974, 0.72455988025806, 0.4707909983708979, 0.8114529051148651, 0.9195522364276514, 0.7144948346694379, 1.0, 0.9501570703145379, 0.712432895242584, 0.7329853689231081, 0.6928455688060347, 1.0, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5423688216151067, 0.38304611372110686, 0.47786341412302047, 0.7135160729534664, 0.6467214894577404, 0.7889213653390947, 0.629920929034602, 0.6737569575259978, 0.3426136760976885, 1.0, 0.9608461673226849, 1.0, 0.5773753982148186, 0.4147896756607047, 0.8155450160863305, 0.47908545381695616, 0.370321911798854, 1.0, 0.7391886750930661, 0.4407283286877167, 0.6655012758056489, 0.34003623200632915, 0.28452740980526503, 0.8191441569283882, 0.5701073447261734, 0.4366371358026476, 1.0, 0.5099766481408452, 0.11453360085481801, 0.5308989025444404, 0.6892870539097846, 0.8102736261199102, 0.545545649326698, 1.0, 0.6164734436957756, 0.5650407172771029, 0.05127041479686025, 0.26320947721364635, 0.6742613142686853, 0.7892647227570666, 0.3956576079714798, 0.8685222066525069, 0.6193453441662242, 0.8174230148919821, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 1.0, 0.5331086030237067, 0.6742857383161394, 0.35437970233785665, 0.5763240578218234, 0.13841238567827724, 0.5519698034749387, 1.0, 0.8007265089204272, 0.7881248632151466, 1.0, 1.0, 0.4054183564853334, 0.34575990398983625, 0.8433369370338967, 0.9365276486140073, 0.691735368923108, 0.8383329804487951, 0.7440965622438329, 0.40209781994840155, 0.40400364537321126, 0.6816015466501335, 1.0, 0.602487595382981, 0.21948160223692786, 0.23683637774429345, 0.664322380152994, 0.3372632052291401, 1.0, 0.8657029465524806, 0.8114529051148651, 0.19272438732935, 0.6278821760519093, 0.5149139863647084, 0.7233004498085515, 0.4085332823846201, 0.8661547359011423, 0.414297617187952, 0.688062756936776, 0.39999999999999997, 0.7023594991536818, 0.7429370522068693, 0.6847340670900772, 1.0, 1.0, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 1.0, 1.0, 0.6151628493773373, 1.0, 1.0, 0.6733027629890207, 0.42038175275163125, 0.8542712839031905, 1.0, 0.24895420055876946, 0.28890659929471596, 0.6578607431385983, 0.8114529051148651, 0.626199275169943, 0.8334272760895116, 0.6137893328919128, 0.003996009641464269, 0.8253119306693903, 0.47102574599540464, 0.6452078183025891, 1.0, 0.34509049865171215, 0.28012068183501126, 0.8249365300761395, 0.4016265496309229, 0.45473913473843297, 0.2876464056338297, 0.8777227983675986, 0.6731015007059578, 0.7398815472813818, 0.8114529051148651, 0.643309301212212, 0.30215269230653224, 0.12, 0.8560255464372679, 0.3698677180019313, 0.766682914692771, 0.46843920200122524, 0.36393172467891854, 1.0, 0.6137392159029667, 0.5450269031178905, 1.0, 1.0, 1.0, 0.41969276730357064, 0.566221018567181, 0.6186389706521688, 1.0, 0.6704260829330185, 0.7396773803089467, 0.6492633713864637, 1.0, 0.8691371695837178, 0.20034796016239415, 0.7158229243802021]
Finish training and take 11m
