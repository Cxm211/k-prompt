Namespace(log_name='./RQ5/tfix_300_3/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_300_3/codet5p_770m_f', data_dir='./data/RQ5/tfix_300_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 300 training instances 
***** Running training *****
  Num examples = 300
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00428
  global_step = 76
  train_loss = 0.8957
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00428
  ********************
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 59.1 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:59.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00449
  global_step = 151
  train_loss = 0.364
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 59.65 	 Previous best codebleu 59.1
  ********************
 Achieve Best bleu:59.65
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00471
  global_step = 226
  train_loss = 0.152
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 60.36 	 Previous best codebleu 59.65
  ********************
 Achieve Best bleu:60.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00513
  global_step = 301
  train_loss = 0.0834
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 59.39 	 Previous best codebleu 60.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00552
  global_step = 376
  train_loss = 0.056
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 60.81 	 Previous best codebleu 60.36
  ********************
 Achieve Best bleu:60.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00529
  global_step = 451
  train_loss = 0.0786
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 60.78 	 Previous best codebleu 60.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00576
  global_step = 526
  train_loss = 0.0249
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 61.16 	 Previous best codebleu 60.81
  ********************
 Achieve Best bleu:61.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.0059
  global_step = 601
  train_loss = 0.0124
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 60.9 	 Previous best codebleu 61.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00597
  global_step = 676
  train_loss = 0.0096
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 60.61 	 Previous best codebleu 61.16
  ********************
reload model from RQ5/tfix_300_3/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_300_3/test.jsonl
  codebleu = 60.73 
  Total = 500 
  Exact Fixed = 69 
[5, 13, 19, 44, 47, 59, 62, 65, 67, 85, 93, 99, 101, 109, 111, 117, 129, 130, 131, 134, 139, 147, 151, 152, 153, 162, 170, 187, 188, 190, 191, 197, 198, 199, 205, 217, 225, 226, 229, 252, 253, 258, 265, 266, 269, 271, 284, 303, 308, 317, 333, 345, 358, 359, 371, 374, 403, 430, 436, 441, 445, 446, 449, 453, 465, 475, 487, 488, 493]
  Syntax Fixed = 3 
[28, 236, 237]
  Cleaned Fixed = 5 
[9, 120, 159, 268, 417]
  ********************
  Total = 500 
  Exact Fixed = 69 
[5, 13, 19, 44, 47, 59, 62, 65, 67, 85, 93, 99, 101, 109, 111, 117, 129, 130, 131, 134, 139, 147, 151, 152, 153, 162, 170, 187, 188, 190, 191, 197, 198, 199, 205, 217, 225, 226, 229, 252, 253, 258, 265, 266, 269, 271, 284, 303, 308, 317, 333, 345, 358, 359, 371, 374, 403, 430, 436, 441, 445, 446, 449, 453, 465, 475, 487, 488, 493]
  Syntax Fixed = 3 
[28, 236, 237]
  Cleaned Fixed = 5 
[9, 120, 159, 268, 417]
  codebleu = 60.73 
[0.0631948944578052, 0.8170015046833874, 0.7456907908606256, 0.4787139878886564, 1.0, 0.940598945729278, 0.7130804239841513, 0.11582292438020222, 0.6119613551150405, 0.6779253872262, 0.14858020165241986, 0.7067366858513033, 1.0, 0.49718705940327845, 0.39323313167877133, 0.6321782328024446, 0.5033638986642797, 0.44412484527547247, 1.0, 0.9207265089204273, 0.8214203428813993, 0.3743172882353815, 0.15, 0.6793670579146192, 0.5826136760976885, 0.2772943579237247, 0.7949680697811121, 0.6805831851375559, 0.5406130222063663, 0.110722640539825, 0.7256321424853589, 0.6142482969348242, 0.7114217991038729, 0.46599916037959765, 0.3326555560719357, 0.3532530992618459, 0.5106018213494752, 0.8504403342456683, 0.6934531647858191, 0.47732238227990853, 0.763761398744189, 0.22267396568917747, 0.5277537539048389, 0.9891483218006352, 0.5481721142074378, 0.5652887122558813, 1.0, 0.30636551088734876, 0.46877089663917104, 0.8951959058230954, 0.5123984208296868, 0.5266057619745405, 0.8223873319410013, 0.622877007215058, 0.677190822574687, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 0.6148576874551961, 0.608455162308236, 1.0, 0.5610771314034941, 0.5897692121354159, 1.0, 0.502988841340978, 1.0, 0.5970182644937875, 0.9517008915274616, 0.7539630945946609, 0.21094269253603662, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 0.5057182222660676, 0.20613487118166404, 0.4887096364908238, 0.548663900608423, 0.643291057814614, 0.7328492172088266, 0.6530747837026651, 0.7265018183047776, 0.19999999999999998, 0.803950001904677, 1.0, 0.9179350389391119, 0.0, 0.2966597993138361, 0.062347779025847205, 0.5675986891240699, 0.4570583972426556, 0.7299948711471688, 1.0, 0.9190411344533542, 0.3915187569135013, 0.4484520389345378, 0.029003718286821547, 0.016782765293716924, 1.0, 0.483668171350054, 0.8114529051148651, 0.5150788862839089, 0.8199440232953434, 0.0, 0.04035071477803321, 0.22499999999999998, 0.48167268419835185, 0.467913762972279, 1.0, 0.5024101654079205, 1.0, 0.2913778039631588, 0.35473913473843294, 0.414297617187952, 0.2924537101693822, 0.5566638701334943, 1.0, 0.16950123401194037, 0.19999999999999998, 0.08953276868214524, 0.33036017439379955, 0.9068864709786153, 0.571216493718466, 0.10906101778358473, 0.8840188084091536, 0.7402231631196423, 0.4118819918812901, 0.726624382466148, 1.0, 0.9891483218006352, 1.0, 0.6632147281466566, 0.6379694287810409, 1.0, 0.22619887519287302, 0.4544627617245256, 0.4831321633398339, 0.7907114354093481, 1.0, 0.6937464703296872, 0.42878381311480984, 0.5960707868709735, 0.27320579161919023, 0.5440521407974296, 0.7396601242701673, 0.7530045026259591, 1.0, 0.523507649142383, 0.5760694567816493, 0.5063942770541421, 1.0, 1.0, 1.0, 0.40135160729534664, 0.7728222909971192, 0.5632387151836264, 0.734603718873536, 0.8245957586820076, 0.9215567956810036, 0.5266057619745405, 0.5348435312891645, 1.0, 0.5461243465154161, 0.5301818996067954, 0.7262862588837676, 0.6745836923955046, 0.44478640618236376, 0.41511870560120445, 0.6957231649810731, 0.7135428903906851, 0.6336131842675646, 0.5941888941101804, 0.3412452357473688, 0.4455034335264345, 0.6477517173276517, 0.7202172177276029, 0.5882210413806648, 0.574642566556635, 0.7450106674614723, 0.6042302157829817, 0.7269320077439674, 0.5694088271166025, 0.7302007404183433, 0.8426568563575418, 0.6632787668520421, 0.15126528209192489, 1.0, 1.0, 0.4231498701419104, 0.8114529051148651, 1.0, 0.5728633222115624, 0.4861953421646677, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 1.0, 0.8114529051148651, 1.0, 0.3051003097580078, 0.10929293535427073, 0.6863884298635612, 0.8201517014631392, 0.6867787885259955, 1.0, 0.32342195343189895, 0.7288049970779993, 0.0731716191316424, 0.4307250470563342, 0.37050628892829995, 0.21245371016938225, 0.5228929981441417, 0.9080331470954286, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6586922980028074, 0.4111029451844598, 0.619750295236949, 0.6487021300892772, 0.48897284678550934, 0.5960626227910577, 0.49811694316607, 1.0, 1.0, 0.6271806220068676, 0.4971121850651573, 1.0, 0.43016109313909, 0.39359500830217886, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.8821296454239946, 0.8601527033303111, 0.7448443514503091, 0.3938266805419558, 0.6029055732198362, 0.31558280681232764, 0.7816396051000123, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6952637111045539, 0.5995715891800304, 0.1717728294285068, 0.8728963827959195, 0.6736538901224837, 0.04030522491224135, 1.0, 1.0, 0.6179144359460748, 0.47535257334060044, 0.4223835094667513, 0.0, 1.0, 0.7355980328281302, 0.16826111698077859, 0.5376172770255981, 0.8476757297426298, 0.8016000841539659, 0.604436274930823, 0.8249365300761395, 1.0, 0.0, 0.6993751196103626, 1.0, 0.367641751923923, 1.0, 0.7582416075216148, 0.4471186075211674, 0.177190822574687, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.8020701463285549, 0.11349072297901378, 0.8260711388991167, 0.06423223587777374, 0.7504680456833208, 0.6041077305454495, 1.0, 0.5059563456668401, 0.4141117485802601, 0.29123935575300347, 0.6899766481408451, 0.7828427124746189, 0.5700516985195162, 0.8244788648855748, 0.29286074313859833, 0.8398078253515011, 0.7122892408726511, 0.5782030614679, 0.027660853979899265, 0.5550049310474542, 0.8358445557986962, 0.5843539719797703, 0.6197856242749173, 0.43701826449378744, 0.7601650731090275, 0.7135428903906851, 0.3853313725311576, 0.7839520803542337, 0.3374011779985145, 0.5543773857041603, 1.0, 0.40660822240095185, 0.8794984978839551, 0.8312424689833122, 0.3227886222566654, 0.8567608976319174, 0.8997846296433376, 0.5644004745782011, 0.6314544371692268, 0.9318657024016066, 0.5290092325728539, 0.6060574827359351, 0.5678911585054794, 0.5537998353931795, 0.8255980153481091, 0.5840819416647886, 0.5019100417936252, 0.37549356386576216, 0.6463980754501475, 0.8168977233058528, 0.5189589454524783, 0.6068567648108864, 0.4343002319965344, 0.656247522058613, 0.6903571668875679, 1.0, 0.3682936672819867, 0.4707909983708979, 0.5841414787295285, 0.9195522364276514, 0.7144948346694379, 0.4709261928229648, 0.8791851067962002, 0.6352900380997268, 0.7329853689231081, 0.8591190003999107, 0.5942976171879519, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.8642531454411047, 0.3826556713691657, 0.6999214919756145, 0.5674524112007323, 0.8027763266824628, 0.6737569575259978, 0.3426136760976885, 0.26822487472667034, 1.0, 1.0, 0.49479306890352615, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.5389773124613249, 0.7391886750930661, 0.4407283286877167, 0.638050295413492, 0.34003623200632915, 0.3022109370915457, 1.0, 0.49195511101803757, 0.4366371358026476, 1.0, 0.3105028374306678, 0.11453360085481801, 0.536560588211956, 0.6892870539097846, 0.8102736261199102, 0.545545649326698, 0.8042470965743207, 0.6530367547828486, 0.7802383125555377, 0.04448415542647708, 0.49605468697269356, 0.6742613142686853, 0.5084177428513625, 0.7678137368502398, 0.8685222066525069, 0.6493657312404237, 0.8174230148919821, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 0.8165273869902545, 0.6941813926327731, 0.7308835793300246, 0.35437970233785665, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 1.0, 0.8007265089204272, 0.7017410285928236, 0.7316224703619254, 0.8604352575955447, 0.4054183564853334, 0.34575990398983625, 0.8433369370338967, 0.6627238167185998, 0.691735368923108, 0.5084373166195929, 0.8214029683767206, 0.40209781994840155, 0.6996345753156019, 0.6816015466501335, 0.7972388343569652, 0.602487595382981, 0.21948160223692786, 0.23683637774429345, 0.6066968461360432, 0.3372632052291401, 0.940598945729278, 0.5015730288994655, 0.15510767240135345, 0.19272438732935, 0.6418894161880858, 0.5149139863647084, 1.0, 0.4085332823846201, 0.8661547359011423, 0.15472876794829787, 0.688062756936776, 0.39999999999999997, 1.0, 0.7429370522068693, 0.5679431365730555, 0.8742687840283396, 0.6841474506952241, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 1.0, 1.0, 0.6151628493773373, 0.6901940524102714, 1.0, 0.6733027629890207, 0.42038175275163125, 0.8542712839031905, 1.0, 0.24895420055876946, 0.28890659929471596, 0.6220642771726803, 0.5353082743436026, 0.7322642870443095, 0.8334272760895116, 0.6615804724100134, 0.003996009641464269, 0.8253119306693903, 0.47102574599540464, 0.8034443090225696, 1.0, 0.34509049865171215, 0.5075783901825247, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.25349826040542034, 0.6835302798798335, 0.7398815472813818, 0.8114529051148651, 0.643309301212212, 0.30215269230653224, 0.075, 0.8560255464372679, 0.3698677180019313, 0.766682914692771, 0.6230619919314705, 0.571611084602222, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 1.0, 1.0, 0.8180779662080888, 0.2334865604657932, 0.566221018567181, 0.6186389706521688, 1.0, 0.6651212197111355, 0.7396773803089467, 0.6492633713864637, 0.7734355371870574, 0.8691371695837178, 0.20034796016239415, 0.7158229243802021]
Finish training and take 20m
