Namespace(log_name='./RQ5/tfix_300_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_300_3/codet5p_770m', data_dir='./data/RQ5/tfix_300_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'if (!arguments[0] || !/mousemove|scroll/.test(arguments[0].type)) {    console.log("User presence detected!");', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'if (now - last > 1000) {    saveState();    console.log("User presence detected!");'}]
***** Running training *****
  Num examples = 300
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 76
  train_loss = 19.865
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 58.6 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:58.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 151
  train_loss = 9.1143
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 58.21 	 Previous best codebleu 58.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 226
  train_loss = 5.0257
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 60.34 	 Previous best codebleu 58.6
  ********************
 Achieve Best bleu:60.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 301
  train_loss = 2.7324
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 59.6 	 Previous best codebleu 60.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 376
  train_loss = 1.5532
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_300_3/validation.jsonl
  codebleu-4 = 57.74 	 Previous best codebleu 60.34
  ********************
early stopping!!!
reload model from RQ5/tfix_300_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_300_3/test.jsonl
  codebleu = 59.25 
  Total = 500 
  Exact Fixed = 68 
[13, 19, 29, 44, 53, 55, 59, 62, 65, 67, 69, 84, 85, 93, 97, 105, 109, 111, 117, 123, 129, 131, 139, 146, 147, 151, 152, 153, 157, 162, 170, 187, 188, 199, 217, 225, 226, 229, 252, 253, 258, 265, 271, 276, 284, 295, 303, 308, 317, 321, 336, 340, 358, 359, 365, 374, 386, 403, 417, 424, 441, 449, 453, 487, 488, 489, 493, 494]
  Syntax Fixed = 1 
[345]
  Cleaned Fixed = 4 
[5, 120, 134, 345]
  ********************
  Total = 500 
  Exact Fixed = 68 
[13, 19, 29, 44, 53, 55, 59, 62, 65, 67, 69, 84, 85, 93, 97, 105, 109, 111, 117, 123, 129, 131, 139, 146, 147, 151, 152, 153, 157, 162, 170, 187, 188, 199, 217, 225, 226, 229, 252, 253, 258, 265, 271, 276, 284, 295, 303, 308, 317, 321, 336, 340, 358, 359, 365, 374, 386, 403, 417, 424, 441, 449, 453, 487, 488, 489, 493, 494]
  Syntax Fixed = 1 
[345]
  Cleaned Fixed = 4 
[5, 120, 134, 345]
  codebleu = 59.25 
[0.29236190630031955, 0.8170015046833874, 0.7456907908606256, 0.3668284850162606, 0.6864484244424076, 0.8697471734123385, 0.7130804239841513, 0.11582292438020222, 0.6094129773172783, 0.6779253872262, 0.19726550033812454, 0.7067366858513033, 0.6640033451354017, 0.49718705940327845, 0.39323313167877133, 0.6321782328024446, 0.5033638986642797, 0.44412484527547247, 1.0, 0.8244011513502163, 0.8214203428813993, 0.3743172882353815, 0.13846153846153847, 0.5055188769352307, 0.5826136760976885, 0.2504033576162784, 0.4759105789848658, 0.15747669278081078, 1.0, 0.14820673147239685, 0.535133493667276, 0.6142482969348242, 0.7134655991716751, 0.4534117477921851, 0.2630576862511726, 0.8149139863647084, 0.4074903033093302, 0.8504403342456683, 0.6934531647858191, 0.41673766744658963, 0.763761398744189, 0.22267396568917747, 0.48013765364088845, 0.9891483218006352, 0.5481721142074378, 0.5652887122558813, 0.763145605438578, 0.30021233660012925, 0.46877089663917104, 0.8951959058230954, 0.5123984208296868, 0.5536911179244788, 1.0, 0.32435175818106565, 0.8249365300761395, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 0.6148576874551961, 0.7535182593233194, 1.0, 0.5610771314034941, 0.5897692121354159, 0.8229808431453671, 0.48289849585465594, 1.0, 0.5970182644937875, 1.0, 0.7539630945946609, 0.13474082862310607, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 0.5007846217557572, 0.20613487118166404, 0.45349864579828547, 0.548663900608423, 0.643291057814614, 0.6724062327149246, 0.6530747837026651, 0.6387289085944345, 0.18, 0.8621370741236649, 1.0, 0.9179350389391119, 0.0, 0.2966597993138361, 0.0485646584311115, 0.4161219555899987, 0.4570583972426556, 0.7299948711471688, 1.0, 0.814064525684886, 0.4491365250784265, 0.4484520389345378, 1.0, 0.016782765293716924, 0.6488884298635612, 0.483668171350054, 0.4326263272143769, 0.4965144687034782, 0.8300594074301477, 0.0, 0.8249365300761395, 0.1714285714285714, 0.48167268419835185, 0.542913762972279, 1.0, 0.5024101654079205, 1.0, 0.2761663443693547, 0.35473913473843294, 0.414297617187952, 0.2924537101693822, 0.5566638701334943, 0.9474466761527047, 0.16950123401194037, 0.19999999999999998, 0.08953276868214524, 0.33036017439379955, 0.9068864709786153, 0.9472083239018387, 0.10906101778358473, 0.8840188084091536, 0.8150666162955742, 0.4118819918812901, 0.726624382466148, 1.0, 0.32616770616700436, 1.0, 0.7386991709927695, 0.8674961219905688, 0.9223302456108291, 0.22619887519287302, 0.4544627617245256, 0.4831321633398339, 0.5822125353044847, 1.0, 0.6937464703296872, 0.45590150397397095, 0.5960707868709735, 0.27528728007290226, 0.5440521407974296, 0.6853543509327785, 1.0, 1.0, 0.523507649142383, 0.47252224575738944, 0.5063942770541421, 1.0, 1.0, 1.0, 0.40135160729534664, 0.7728222909971192, 0.5632387151836264, 1.0, 0.8245957586820076, 0.6080404866961566, 0.5266057619745405, 0.5348435312891645, 0.7863340021370451, 0.5461243465154161, 0.5301818996067954, 0.7084639193171354, 0.6745836923955046, 0.44478640618236376, 0.41511870560120445, 0.6957231649810731, 0.7135428903906851, 0.5886820492533873, 0.4858412195420705, 0.3412452357473688, 0.5532531256091296, 0.6477517173276517, 0.7202172177276029, 0.5882210413806648, 0.6706847188488577, 0.7450106674614723, 0.5058738381912614, 0.7269320077439674, 0.5726160404010894, 0.7302007404183433, 0.6593084555095938, 0.6885381235394885, 0.3812999423756144, 1.0, 1.0, 0.4468640641327328, 0.5609260915878896, 0.050923035847610806, 0.5728633222115624, 0.5400083948023668, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.8123223032135625, 0.6126392797557267, 1.0, 0.3187366733943714, 0.15429293535427072, 0.6863884298635612, 0.7296508600957852, 0.6867787885259955, 0.4739790984471416, 0.6459247942853364, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.31888879608349985, 0.2265709289904188, 0.516068568378893, 0.7508693878955064, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6586922980028074, 0.4111029451844598, 0.5177932939687511, 0.6487021300892772, 0.35624947398505524, 0.5960626227910577, 0.49811694316607, 1.0, 1.0, 0.6271806220068676, 0.4971121850651573, 1.0, 0.5660528353107435, 0.39359500830217886, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.3595801478019146, 0.6726527033303111, 0.7448443514503091, 0.6056583090096291, 0.6029055732198362, 0.31558280681232764, 0.46928679425385467, 0.0, 0.3538685674110905, 0.34062237195526035, 0.4650446435589134, 0.7080707346422974, 0.1717728294285068, 0.4435027248795795, 0.565629122056696, 0.04101911673989318, 1.0, 1.0, 0.6179144359460748, 0.4089903936086955, 0.4223835094667513, 0.0, 1.0, 0.6924012535180504, 0.16826111698077859, 0.5376172770255981, 0.7618415878378562, 0.8016000841539659, 0.604436274930823, 0.8249365300761395, 0.6124900507484021, 0.0, 0.5666932084176984, 0.6310308074685265, 0.367641751923923, 1.0, 0.8082416075216149, 0.5395164303202522, 0.177190822574687, 0.13636363636363635, 0.7135428903906851, 0.7439654153934541, 0.8020701463285549, 0.11349072297901378, 0.8212928138736779, 0.30039204052282054, 0.6506923930808135, 0.6041077305454495, 1.0, 0.8710525186025484, 0.39464243597592485, 0.24302916939320657, 0.31788607385502077, 0.7828427124746189, 0.6553451053003048, 0.8244788648855748, 0.29286074313859833, 0.8398078253515011, 0.7835249822766466, 0.7826555560719357, 0.027660853979899265, 0.5550049310474542, 0.820109647033614, 0.5843539719797703, 0.5447856242749173, 0.43701826449378744, 0.5948813510810405, 0.7135428903906851, 0.3814205333654446, 0.7839520803542337, 0.3374011779985145, 0.6107000692160132, 1.0, 0.40660822240095185, 0.816225472769031, 0.8312424689833122, 0.3227886222566654, 0.7316558493019588, 0.8251411716151402, 0.46526859680566335, 0.7531697895341449, 0.9318657024016066, 0.5290092325728539, 0.6060574827359351, 0.5678911585054794, 1.0, 0.8255980153481091, 0.6392333266349789, 0.5110656166644442, 0.37549356386576216, 0.6463980754501475, 0.8135886907324694, 0.5189589454524783, 0.5382922926202521, 0.4343002319965344, 0.656247522058613, 0.6742738870938194, 0.5551981770755083, 0.72455988025806, 0.4707909983708979, 0.8114529051148651, 0.8140165767392193, 0.7144948346694379, 0.4709261928229648, 1.0, 0.6352900380997268, 0.7329853689231081, 0.7668758886842345, 0.5942976171879519, 0.9228963827959196, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.5975308759151072, 0.6721193723777923, 0.6999214919756145, 0.6781982935333077, 0.629920929034602, 0.6737569575259978, 0.3426136760976885, 0.5883089748131176, 0.8455636134664264, 1.0, 0.49479306890352615, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 1.0, 0.7391886750930661, 0.4407283286877167, 0.6103404231788209, 0.34003623200632915, 0.3022109370915457, 0.6863884298635612, 0.39864117861539017, 0.4366371358026476, 1.0, 0.240137731452083, 0.11453360085481801, 0.5926173418488229, 0.6892870539097846, 0.7539027915539327, 0.545545649326698, 0.8042470965743207, 0.6530367547828486, 0.7802383125555377, 0.04448415542647708, 0.3154745710868131, 0.8651704051777762, 0.5084177428513625, 0.3956576079714798, 0.8685222066525069, 0.6193453441662242, 0.8174230148919821, 0.4444597200395934, 0.5314834531180896, 0.34272438732934996, 0.3807256007926331, 0.7173892751631783, 0.6941813926327731, 0.7308835793300246, 0.35437970233785665, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 1.0, 0.6863559408133775, 0.7017410285928236, 0.7787918207406566, 0.8604352575955447, 0.4054183564853334, 0.30791559625159903, 0.7818538953900607, 0.7405951011625707, 0.691735368923108, 0.5084373166195929, 0.8214029683767206, 0.40209781994840155, 0.40400364537321126, 1.0, 0.7972388343569652, 0.602487595382981, 0.24658767523161879, 0.23683637774429345, 0.6428938087244226, 0.3372632052291401, 1.0, 0.7923043045126105, 0.16509970681865313, 0.19272438732935, 0.6418894161880858, 0.8182199885463513, 0.7729570038010705, 0.4085332823846201, 0.8661547359011423, 0.15472876794829787, 0.5613093728470866, 0.39999999999999997, 0.7023594991536818, 0.026640868815786287, 0.5679431365730555, 0.8742687840283396, 0.48861662650065896, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 0.7674961219905688, 0.5030700081467023, 0.5316562756993463, 0.6901940524102714, 1.0, 0.6733027629890207, 0.39869451224821295, 0.8542712839031905, 1.0, 0.39206686043974615, 0.28890659929471596, 0.6578607431385983, 0.41080814539619515, 0.5508853710142922, 0.8334272760895116, 0.6418857377391443, 0.003996009641464269, 0.8253119306693903, 0.47102574599540464, 0.5811145153819444, 0.37826402883081917, 0.34509049865171215, 0.5075783901825247, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.7317849927876572, 0.6835302798798335, 0.7398815472813818, 0.3798418629627377, 0.643309301212212, 0.30215269230653224, 0.12, 0.8010669062882427, 0.7016265496309229, 0.766682914692771, 0.46843920200122524, 0.571611084602222, 0.756047661202274, 0.48129480963601345, 0.42880120128837773, 1.0, 1.0, 1.0, 0.4009071748370381, 0.7494203690984217, 0.6186389706521688, 1.0, 1.0, 0.7396773803089467, 0.6492633713864637, 0.7279095254199106, 0.8691371695837178, 0.20034796016239415, 0.7158229243802021]
Finish training and take 14m
