Namespace(log_name='./RQ5/tfix_32_3/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_32_3/codet5p_770m_f', data_dir='./data/RQ5/tfix_32_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 32 training instances 
***** Running training *****
  Num examples = 32
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00705
  global_step = 9
  train_loss = 1.6435
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00705
  ********************
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 29.24 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:29.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00549
  global_step = 17
  train_loss = 0.6747
  ********************
Previous best ppl:1.00705
Achieve Best ppl:1.00549
  ********************
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 54.19 	 Previous best codebleu 29.24
  ********************
 Achieve Best bleu:54.19
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00519
  global_step = 25
  train_loss = 0.2539
  ********************
Previous best ppl:1.00549
Achieve Best ppl:1.00519
  ********************
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 53.94 	 Previous best codebleu 54.19
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00528
  global_step = 33
  train_loss = 0.1438
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.2 	 Previous best codebleu 54.19
  ********************
 Achieve Best bleu:58.2
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00556
  global_step = 41
  train_loss = 0.1
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.27 	 Previous best codebleu 58.2
  ********************
 Achieve Best bleu:58.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00567
  global_step = 49
  train_loss = 0.0553
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.39 	 Previous best codebleu 58.27
  ********************
 Achieve Best bleu:58.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00569
  global_step = 57
  train_loss = 0.0261
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.19 	 Previous best codebleu 58.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00572
  global_step = 65
  train_loss = 0.0201
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.5 	 Previous best codebleu 58.39
  ********************
 Achieve Best bleu:58.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00574
  global_step = 73
  train_loss = 0.018
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.09 	 Previous best codebleu 58.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00577
  global_step = 81
  train_loss = 0.0197
  ********************
Previous best ppl:1.00519
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.3 	 Previous best codebleu 58.5
  ********************
reload model from RQ5/tfix_32_3/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_32_3/test.jsonl
  codebleu = 55.73 
  Total = 500 
  Exact Fixed = 23 
[21, 47, 53, 55, 59, 65, 111, 117, 130, 131, 147, 153, 188, 198, 199, 253, 271, 308, 317, 344, 359, 449, 493]
  Syntax Fixed = 5 
[67, 99, 162, 383, 489]
  Cleaned Fixed = 2 
[159, 268]
  ********************
  Total = 500 
  Exact Fixed = 23 
[21, 47, 53, 55, 59, 65, 111, 117, 130, 131, 147, 153, 188, 198, 199, 253, 271, 308, 317, 344, 359, 449, 493]
  Syntax Fixed = 5 
[67, 99, 162, 383, 489]
  Cleaned Fixed = 2 
[159, 268]
  codebleu = 55.73 
[0.13302463133083384, 0.8170015046833874, 0.8214597355701927, 0.4787139878886564, 0.7561984520017521, 0.940598945729278, 0.7130804239841513, 0.11582292438020222, 0.5981716191316424, 0.6364204911514356, 0.3499927730653973, 0.7067366858513033, 0.7986647134866534, 0.5470621842784034, 0.39323313167877133, 0.49298321124950273, 0.394249753204603, 0.3951417837590342, 0.19377717010542167, 0.9207265089204273, 1.0, 0.3743172882353815, 0.3529411764705882, 0.6793670579146192, 0.5826136760976885, 0.2772943579237247, 0.7122141752955726, 0.12474637457734936, 0.5406130222063663, 0.0214411564631476, 0.535133493667276, 0.436350307234642, 0.7114217991038729, 0.3601842665079529, 0.3326555560719357, 0.7487076447163913, 0.2695461009583294, 0.8504403342456683, 0.6934531647858191, 0.49269306161613136, 0.7661785491189476, 0.22267396568917747, 0.4302333034257401, 0.2386459180704551, 0.5712490372843609, 0.5952887122558812, 1.0, 0.4607142567944371, 0.46877089663917104, 0.2968222263370741, 0.5123984208296868, 0.5266057619745405, 1.0, 0.622877007215058, 0.8249365300761395, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 0.7939840333597292, 0.521299561845173, 0.4585617401550842, 0.24, 0.4803494323607689, 1.0, 0.502988841340978, 0.9556325881310865, 0.5970182644937875, 0.9517008915274616, 0.5416581477675209, 0.2752284068217509, 0.20369111792447886, 0.08978100888954238, 0.7306621243949691, 0.5057182222660676, 0.23785310412867677, 0.4887096364908238, 0.548663900608423, 0.5341859296251839, 0.7064927606229161, 0.6530747837026651, 0.7265018183047776, 0.18, 0.7372230552852055, 0.4412416483083722, 0.9179350389391119, 0.0, 0.2966597993138361, 0.06285037271682578, 0.5675986891240699, 0.4570583972426556, 0.7299948711471688, 0.4376733077091023, 0.7285325926380868, 0.3915187569135013, 0.4484520389345378, 0.7459627599846212, 0.016782765293716924, 0.8164329076215759, 0.48045217012600516, 0.13150369788356012, 0.5150788862839089, 0.8199440232953434, 0.0, 0.07719082257468701, 0.3536911179244788, 0.48167268419835185, 0.467913762972279, 0.7764804587657133, 0.5024101654079205, 1.0, 0.6785036503543816, 0.04551384262951512, 0.27986152408713605, 0.23799950041899015, 0.5415063788058132, 1.0, 0.1674868491617659, 0.19999999999999998, 0.0, 0.33036017439379955, 0.9068864709786153, 0.571216493718466, 0.01826980517838122, 0.8840188084091536, 0.8150666162955742, 0.4118819918812901, 0.5551676892768422, 0.7562262989907678, 0.9891483218006352, 1.0, 0.7386991709927695, 0.5639528341415055, 0.7768898072067754, 0.22619887519287302, 0.4544627617245256, 0.7853313927871439, 0.7961659808638935, 0.23452984161828982, 0.5676407079074275, 0.2247960593523654, 0.5960707868709735, 0.27320579161919023, 0.5440521407974296, 0.5189575999632741, 0.7530045026259591, 1.0, 0.523507649142383, 0.6060694567816493, 0.5063942770541421, 0.7059117582516798, 0.7767788352354288, 1.0, 0.40135160729534664, 0.7728222909971192, 0.02118840166103055, 0.734603718873536, 0.8245957586820076, 0.9215567956810036, 0.5266057619745405, 0.5348435312891645, 0.9453360085481803, 0.5461243465154161, 0.5301818996067954, 0.7750198530224517, 0.6745836923955046, 0.4363518046738012, 0.35511870560120445, 0.5606672680560794, 0.3536911179244788, 0.6040629296941564, 0.46766697226592346, 0.4831458551130241, 0.5532531256091296, 0.7534666892164803, 0.7202172177276029, 0.7436210610284252, 0.6706847188488577, 0.6274913759237848, 0.6190738228228467, 0.6653504253402676, 0.5992305511647729, 0.7302007404183433, 0.8426568563575418, 0.6814650334361707, 0.3512443785220981, 0.6583603643329422, 1.0, 0.4231498701419104, 0.49716968796987465, 0.04816003385784584, 0.3606672680560794, 0.7218723908052287, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.5266551101233702, 0.8114529051148651, 1.0, 0.39873667339437135, 0.07683567216420958, 0.6863884298635612, 0.8201517014631392, 0.1898355357735218, 0.4739790984471416, 0.3242663937199117, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.32660632863602757, 0.21245371016938225, 0.3972934177879658, 0.9080331470954286, 0.322412788792651, 0.8249783514952764, 0.2067414939499072, 0.8837352944498882, 0.6663998078842303, 0.42493877026999405, 0.619750295236949, 0.6487021300892772, 0.35624947398505524, 0.5960626227910577, 0.6072359854134853, 0.6387851138365109, 0.6093189247482212, 0.6271806220068676, 0.46132529083669765, 0.8541030238941549, 0.43016109313909, 0.39359500830217886, 0.694620505777644, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.5274151448167971, 0.6726527033303111, 0.7448443514503091, 0.6056583090096291, 0.6029055732198362, 0.3965630043849799, 0.7816396051000123, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6952637111045539, 0.5886335096920794, 0.145244327318979, 0.4435027248795795, 0.5211842759207581, 0.23985271067894742, 0.7470248478272412, 1.0, 0.6179144359460748, 0.44531416678032165, 0.4223835094667513, 0.0, 0.8728963827959195, 0.6039652722584431, 0.09418181350952061, 0.5376172770255981, 0.7618415878378562, 0.8016000841539659, 0.604436274930823, 0.565895296523178, 0.6875131192847503, 0.0, 0.6993751196103626, 0.6499702347088693, 0.4162135382912301, 1.0, 0.7582416075216148, 0.73659581790699, 0.177190822574687, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.8020701463285549, 0.11349072297901378, 0.8212928138736779, 0.29671568060541254, 0.6314976354111284, 0.6041077305454495, 0.45676707546873774, 0.49549612161417456, 0.3210022750677429, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.6553451053003048, 0.5629982017754408, 0.29286074313859833, 0.8398078253515011, 0.7721214735047168, 0.5782030614679, 0.0, 0.5550049310474542, 0.8358445557986962, 0.5843539719797703, 0.6197856242749173, 0.43701826449378744, 0.7704282310037642, 0.10704421870811284, 0.3814205333654446, 0.7839520803542337, 0.3844698211793555, 0.6107000692160132, 1.0, 0.40220728137560635, 0.8794984978839551, 0.8312424689833122, 0.20770229485423067, 0.8274001021378707, 0.8997846296433376, 0.5644004745782011, 0.7531697895341449, 0.9318657024016066, 0.5290092325728539, 0.6060574827359351, 0.5678911585054794, 0.5537998353931795, 0.6788869388950445, 0.6392333266349789, 0.5019100417936252, 0.3008285762477364, 0.630608601765937, 0.678707375819837, 0.5189589454524783, 0.6206773872041823, 0.3677581118175097, 0.6371175344107181, 0.6742738870938194, 0.4959953038804974, 0.3127595472330722, 0.4707909983708979, 0.4065085296294044, 0.9195522364276514, 0.7144948346694379, 0.4709261928229648, 0.5921235082342631, 0.6352900380997268, 0.7329853689231081, 0.8591190003999107, 0.6902131912598308, 0.664297617187952, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.41554175867060084, 0.7135160729534664, 0.6999214919756145, 0.6781982935333077, 0.629920929034602, 0.6737569575259978, 0.3426136760976885, 0.5883089748131176, 0.9608461673226849, 1.0, 0.5052475236131788, 0.4147896756607047, 0.6077342705098803, 0.5190854538169563, 0.370321911798854, 0.43554564932669804, 0.7358553417597328, 0.3742061765763506, 0.5848231354762568, 0.34003623200632915, 0.12201368865751627, 0.6530550965302279, 0.6149139863647084, 0.4366371358026476, 0.6989171443189292, 0.3105028374306678, 0.11321771168817468, 0.5047692794313927, 0.6892870539097846, 0.7101527915539326, 0.49997666751308223, 0.8042470965743207, 0.6530367547828487, 0.8907381942373906, 0.04448415542647708, 0.21299924700136813, 0.6354739770278475, 0.5217365165784443, 0.6025902297209078, 0.6592906179772744, 0.603341816321497, 0.8174230148919821, 0.4444597200395934, 0.5221706258633101, 0.34272438732934996, 0.3807256007926331, 0.8165273869902545, 0.701625248426111, 0.5770682924306744, 0.35437970233785665, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 0.5341258702031846, 0.8007265089204272, 0.7017410285928236, 0.7787918207406566, 0.8604352575955447, 0.15356651918683534, 0.38426308558838673, 0.8433369370338967, 0.5894606988288391, 0.691735368923108, 0.5084373166195929, 0.8214029683767206, 0.40209781994840155, 0.2336922166210181, 0.9584407378958149, 0.3281244318068927, 0.602487595382981, 0.18844621046785182, 0.23683637774429345, 0.664322380152994, 0.3372632052291401, 0.940598945729278, 0.8347788278270061, 0.3539261751720011, 0.19272438732935, 0.6418894161880858, 0.4353746106262073, 0.7233004498085515, 0.4085332823846201, 0.8661547359011423, 0.15472876794829787, 0.5308752469777882, 0.39999999999999997, 0.4802706849724713, 0.7429370522068693, 0.25252164710137054, 0.8742687840283396, 0.6937628353106088, 0.554739134738433, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 0.8228963827959196, 0.5126972987746131, 0.6151628493773373, 0.6901940524102714, 1.0, 0.6733027629890207, 0.37509948078494726, 0.8542712839031905, 0.7548295193530756, 0.3376773158211448, 0.28890659929471596, 0.6578607431385983, 0.41080814539619515, 0.6855308023502449, 0.8334272760895116, 0.6142101264169657, 0.003449439476672805, 0.8253119306693903, 0.47102574599540464, 0.677288551000834, 0.3926725860358069, 0.34509049865171215, 0.41979179561989455, 0.22719082257468703, 0.4016265496309229, 0.29511870560120446, 0.3008557137029968, 0.7466086300715167, 0.355067633384039, 0.7020875907084383, 0.4326263272143769, 0.643309301212212, 0.30215269230653224, 0.12, 0.8560255464372679, 0.6488378866864628, 0.766682914692771, 0.6028785751316023, 0.36393172467891854, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 0.5907820443144609, 0.5364230699554865, 0.762178754714643, 0.621238225076177, 0.7494203690984217, 0.6186389706521688, 1.0, 0.6988219320042083, 0.6820100022269966, 0.6492633713864637, 0.7279095254199106, 0.8691371695837178, 0.1942247917643758, 0.7158229243802021]
Finish training and take 27m
