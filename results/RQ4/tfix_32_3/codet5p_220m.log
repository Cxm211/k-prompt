Namespace(log_name='./RQ5/tfix_32_3/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_32_3/codet5p_220m', data_dir='./data/RQ5/tfix_32_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'function addCommentsToSgf(callback) {     Comment.find({ kifu: kifu._id })', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'var addCommentsToSgf = function (callback) {     Comment.find({ kifu: kifu._id })'}]
***** Running training *****
  Num examples = 32
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 4.505870317802953e+267
  global_step = 5
  train_loss = 40.0878
  ********************
Previous best ppl:inf
Achieve Best ppl:4.505870317802953e+267
  ********************
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 24.06 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:24.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 4.477495789787686e+222
  global_step = 9
  train_loss = 19.4501
  ********************
Previous best ppl:4.505870317802953e+267
Achieve Best ppl:4.477495789787686e+222
  ********************
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 48.39 	 Previous best codebleu 24.06
  ********************
 Achieve Best bleu:48.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 3.631339870320411e+250
  global_step = 13
  train_loss = 12.5822
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 54.85 	 Previous best codebleu 48.39
  ********************
 Achieve Best bleu:54.85
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.6881993628013534e+296
  global_step = 17
  train_loss = 7.4029
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 56.49 	 Previous best codebleu 54.85
  ********************
 Achieve Best bleu:56.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 21
  train_loss = 6.1414
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 57.15 	 Previous best codebleu 56.49
  ********************
 Achieve Best bleu:57.15
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 25
  train_loss = 4.0516
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 57.54 	 Previous best codebleu 57.15
  ********************
 Achieve Best bleu:57.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 29
  train_loss = 3.3037
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.21 	 Previous best codebleu 57.54
  ********************
 Achieve Best bleu:58.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 33
  train_loss = 2.3973
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.31 	 Previous best codebleu 58.21
  ********************
 Achieve Best bleu:58.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 37
  train_loss = 2.3119
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.31 	 Previous best codebleu 58.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 41
  train_loss = 1.7975
  ********************
Previous best ppl:4.477495789787686e+222
BLEU file: ./data/RQ5/tfix_32_3/validation.jsonl
  codebleu-4 = 58.31 	 Previous best codebleu 58.31
  ********************
reload model from RQ5/tfix_32_3/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_32_3/test.jsonl
  codebleu = 56.31 
  Total = 500 
  Exact Fixed = 26 
[47, 53, 59, 67, 80, 109, 111, 117, 130, 131, 153, 162, 169, 198, 217, 253, 271, 308, 359, 374, 389, 449, 453, 487, 489, 493]
  Syntax Fixed = 2 
[291, 383]
  Cleaned Fixed = 7 
[85, 92, 120, 147, 268, 417, 445]
  ********************
  Total = 500 
  Exact Fixed = 26 
[47, 53, 59, 67, 80, 109, 111, 117, 130, 131, 153, 162, 169, 198, 217, 253, 271, 308, 359, 374, 389, 449, 453, 487, 489, 493]
  Syntax Fixed = 2 
[291, 383]
  Cleaned Fixed = 7 
[85, 92, 120, 147, 268, 417, 445]
  codebleu = 56.31 
[0.3852352687934718, 0.8170015046833874, 0.861672684198352, 0.4787139878886564, 0.5642010748634618, 0.8697471734123385, 0.7130804239841513, 0.11582292438020222, 0.6092782220842599, 0.6779253872262, 0.3499927730653973, 0.7067366858513033, 0.5274043937673059, 0.49718705940327845, 0.39323313167877133, 0.6502572978503959, 0.5734684198586062, 0.3951417837590342, 0.7161547359011423, 0.9207265089204273, 0.832900393434221, 0.3743172882353815, 0.12, 0.5055188769352307, 0.4970106305726538, 0.2504033576162784, 0.7949680697811121, 0.13798319601526085, 0.39985271067894745, 0.110722640539825, 0.535133493667276, 0.6142482969348242, 0.7134655991716751, 0.4198769757196277, 0.3326555560719357, 0.8149139863647084, 0.2767918763734297, 0.8504403342456683, 0.6934531647858191, 0.45203178509364844, 0.763761398744189, 0.22267396568917747, 0.5052333034257401, 0.5229326815734496, 0.7639443592247308, 0.5952887122558812, 1.0, 0.602585911416486, 0.46877089663917104, 0.8951959058230954, 0.5123984208296868, 0.5266057619745405, 1.0, 0.32435175818106565, 0.677190822574687, 0.861482988138697, 0.8524766967683619, 0.8581179629766709, 1.0, 0.6148576874551961, 0.6903478637642433, 0.4585617401550842, 0.345025714877925, 0.5897692121354159, 0.3714373445095627, 0.38618478058475125, 1.0, 0.5970182644937875, 0.6168173810866084, 0.7539630945946609, 0.00323247279301676, 0.20369111792447886, 0.08978100888954238, 0.7880383905616808, 0.5007846217557572, 0.20613487118166404, 0.5421737770198148, 0.548663900608423, 0.643291057814614, 0.8503143105742339, 0.6530747837026651, 0.6387289085944345, 0.26090620551116817, 0.669579146013357, 0.6372358081500218, 0.9179350389391119, 0.0, 0.2966597993138361, 0.0485646584311115, 0.5675986891240699, 0.4570583972426556, 0.8662528514160328, 0.4376733077091023, 0.814064525684886, 0.4134404736596011, 0.4484520389345378, 0.7459627599846212, 0.016782765293716924, 0.6488884298635612, 0.483668171350054, 0.4326263272143769, 0.4690750900518629, 0.8199440232953434, 0.0, 0.07719082257468701, 0.1714285714285714, 0.48167268419835185, 0.467913762972279, 1.0, 0.5024101654079205, 1.0, 0.6785036503543816, 0.35473913473843294, 0.414297617187952, 0.2924537101693822, 0.5356162234813034, 0.9474466761527047, 0.16950123401194037, 0.19999999999999998, 0.08953276868214524, 0.10648766222679504, 0.9068864709786153, 0.6228161758510766, 0.0, 0.8840188084091536, 0.8150666162955742, 0.4118819918812901, 0.5551676892768422, 0.6197905990898798, 0.9891483218006352, 1.0, 0.8433946307914342, 0.4985223736324468, 0.7768898072067754, 0.22619887519287302, 0.4544627617245256, 0.7760858398620529, 0.5822125353044847, 0.23452984161828982, 0.3899460133142397, 0.7043784173580552, 0.5960707868709735, 0.27528728007290226, 0.5440521407974296, 0.7396601242701673, 0.7530045026259591, 0.8618213423140443, 0.523507649142383, 0.5760694567816493, 0.5063942770541421, 0.7059117582516798, 0.6992691049170493, 1.0, 0.40135160729534664, 0.7728222909971192, 0.5632387151836264, 0.734603718873536, 0.8245957586820076, 0.3688360643960851, 0.5266057619745405, 0.5348435312891645, 0.7863340021370451, 0.5461243465154161, 0.5301818996067954, 0.6295976978911988, 0.6745836923955046, 0.44478640618236376, 0.35511870560120445, 1.0, 0.3536911179244788, 0.6336131842675646, 0.4858412195420705, 0.4831458551130241, 0.4939491227305387, 0.6477517173276517, 0.7202172177276029, 0.5882210413806648, 0.6706847188488577, 0.8273136413238535, 0.510409269308891, 0.6653504253402676, 0.5992305511647729, 0.7302007404183433, 0.8426568563575418, 0.6632787668520421, 0.3812999423756144, 0.6956009081078438, 0.836150319044902, 0.41925376624580646, 0.49716968796987465, 0.050923035847610806, 0.5728633222115624, 0.7359097981643703, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.5266551101233702, 0.8114529051148651, 0.5656223719552603, 0.39873667339437135, 0.15429293535427072, 0.6863884298635612, 0.7296508600957852, 0.6867787885259955, 0.4739790984471416, 0.6459247942853364, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.5442680213004776, 0.21245371016938225, 0.3972934177879658, 0.9080331470954286, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6663998078842303, 0.42493877026999405, 0.6530836285702823, 0.6487021300892772, 0.35624947398505524, 0.5960626227910577, 0.6072359854134853, 0.6387851138365109, 0.6093189247482212, 0.6271806220068676, 0.46132529083669765, 0.8541030238941549, 0.5660528353107435, 0.39359500830217886, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.5274151448167971, 0.6726527033303111, 0.7448443514503091, 0.6056583090096291, 0.6029055732198362, 0.3965630043849799, 0.7387824622428694, 0.0, 0.21066726805607944, 0.5944836924880659, 0.6208906511898491, 0.7080707346422974, 0.15553373183372082, 0.4435027248795795, 0.49431633594533775, 0.04101911673989318, 0.7796468115916302, 1.0, 0.6179144359460748, 0.4089903936086955, 0.4223835094667513, 0.0, 0.6591642516004086, 0.7355980328281302, 0.09418181350952061, 0.5376172770255981, 0.5721360784673309, 0.8016000841539659, 0.604436274930823, 0.565895296523178, 0.6124900507484021, 0.0, 0.8189207115002721, 0.6310308074685265, 0.367641751923923, 1.0, 0.8082416075216149, 0.7980256131811054, 0.177190822574687, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.3802025788978515, 0.11349072297901378, 0.8212928138736779, 0.0939805501712093, 0.6506923930808135, 0.6041077305454495, 0.45676707546873774, 0.49549612161417456, 0.3210022750677429, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.6150037638754932, 0.8821296454239946, 0.29286074313859833, 0.8398078253515011, 0.8920886407883755, 0.5782030614679, 0.021299942375614375, 0.5550049310474542, 0.820109647033614, 0.5843539719797703, 0.5447856242749173, 0.43701826449378744, 0.7215433553637871, 0.15837703500732306, 0.3814205333654446, 0.7839520803542337, 0.3844698211793555, 0.5903958808688652, 1.0, 0.40220728137560635, 0.816225472769031, 0.8312424689833122, 0.3189508954085808, 0.7460474584273435, 0.8251411716151402, 0.46526859680566335, 0.7531697895341449, 0.875675965679132, 0.37181359647129447, 0.6060574827359351, 0.41461455835343747, 0.5537998353931795, 0.5135589861564948, 0.6392333266349789, 0.5110656166644442, 0.2882742839999841, 0.6148191280817265, 0.678707375819837, 0.5189589454524783, 0.44076500003648533, 0.4248142755031662, 0.6865490061448141, 0.6742738870938194, 0.4959953038804974, 0.2972040950695999, 0.4707909983708979, 0.5841414787295285, 0.8140165767392193, 0.7144948346694379, 0.4709261928229648, 0.6419590907344903, 0.6352900380997268, 0.7329853689231081, 0.7668758886842345, 0.5942976171879519, 0.6738495662857232, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.47786341412302047, 0.6721193723777923, 0.6999214919756145, 0.6781982935333077, 0.629920929034602, 0.5924295517447509, 0.3426136760976885, 0.5612317572223963, 0.8369932351905466, 1.0, 0.5140913145175611, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.43554564932669804, 0.7391886750930661, 0.4407283286877167, 0.5669580702376444, 0.34003623200632915, 0.3022109370915457, 0.6863884298635612, 0.42778187550634333, 0.4366371358026476, 1.0, 0.23322807389765116, 0.11453360085481801, 0.30313336828616455, 0.6892870539097846, 0.7965481359238318, 0.545545649326698, 0.8042470965743207, 0.6427510345668692, 0.8907381942373906, 0.04448415542647708, 0.3154745710868131, 0.6355353074494696, 0.5217365165784443, 0.8623473261888206, 1.0, 0.6193453441662242, 0.8174230148919821, 0.0, 0.5271706258633102, 0.34272438732934996, 0.3807256007926331, 0.8165273869902545, 0.7009116410097869, 0.7728490312188327, 0.35437970233785665, 0.4766063286360276, 0.11841238567827725, 0.5519698034749387, 0.5509782586537075, 0.6863559408133775, 0.7583127394270307, 0.7787918207406566, 0.8604352575955447, 0.4054183564853334, 0.1736289878148608, 0.8433369370338967, 0.6106034482537167, 0.6024344723004497, 0.5084373166195929, 0.8696187021511392, 0.40209781994840155, 0.40400364537321126, 0.6816015466501335, 0.7972388343569652, 0.602487595382981, 0.24658767523161879, 0.0857142857142857, 0.6428938087244226, 0.3372632052291401, 0.940598945729278, 0.7923043045126105, 0.43819881886061685, 0.19272438732935, 0.6418894161880858, 0.460395956404703, 0.5260009416319087, 0.4085332823846201, 0.8661547359011423, 0.15472876794829787, 0.6899719219176503, 0.39999999999999997, 0.7023594991536818, 0.7429370522068693, 0.6847340670900772, 0.8742687840283396, 0.48861662650065896, 0.554739134738433, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 0.6522551430667551, 0.6375167201921692, 0.5316562756993463, 0.6901940524102714, 1.0, 0.6733027629890207, 0.37509948078494726, 0.8542712839031905, 1.0, 0.24895420055876946, 0.28890659929471596, 0.6578607431385983, 0.41080814539619515, 0.7322642870443095, 0.8334272760895116, 0.6615804724100134, 0.003996009641464269, 0.8253119306693903, 0.47102574599540464, 0.5811145153819444, 0.3926725860358069, 0.46628336334230014, 0.5075783901825247, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.7317849927876572, 0.355067633384039, 0.7398815472813818, 0.4326263272143769, 0.643309301212212, 0.30215269230653224, 0.12, 0.8294519950448036, 0.7016265496309229, 0.766682914692771, 0.6028785751316023, 0.36393172467891854, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 1.0, 0.46975640328881985, 1.0, 0.545231229371765, 0.7494203690984217, 0.6186389706521688, 1.0, 0.6988219320042083, 0.7396773803089467, 0.6492633713864637, 0.7279095254199106, 0.8691371695837178, 0.20034796016239415, 0.7158229243802021]
Finish training and take 14m
