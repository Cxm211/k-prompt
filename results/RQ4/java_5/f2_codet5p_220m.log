Namespace(log_name='./RQ5/java_5/f2_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_5/f2_codet5p_220m', data_dir='./data/RQ5/java_5_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 0
  eval_ppl = 1.00068
  global_step = 2
  train_loss = 1.1083
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00068
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 10.46 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 1
  eval_ppl = 1.00049
  global_step = 3
  train_loss = 0.6748
  ********************
Previous best ppl:1.00068
Achieve Best ppl:1.00049
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 13.15 	 Previous best codebleu 10.46
  ********************
 Achieve Best bleu:13.15
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 2
  eval_ppl = 1.00042
  global_step = 4
  train_loss = 0.5836
  ********************
Previous best ppl:1.00049
Achieve Best ppl:1.00042
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 28.12 	 Previous best codebleu 13.15
  ********************
 Achieve Best bleu:28.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 3
  eval_ppl = 1.0004
  global_step = 5
  train_loss = 0.3536
  ********************
Previous best ppl:1.00042
Achieve Best ppl:1.0004
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 31.97 	 Previous best codebleu 28.12
  ********************
 Achieve Best bleu:31.97
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 4
  eval_ppl = 1.00039
  global_step = 6
  train_loss = 0.2344
  ********************
Previous best ppl:1.0004
Achieve Best ppl:1.00039
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 42.93 	 Previous best codebleu 31.97
  ********************
 Achieve Best bleu:42.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 5
  eval_ppl = 1.00038
  global_step = 7
  train_loss = 0.3585
  ********************
Previous best ppl:1.00039
Achieve Best ppl:1.00038
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 49.3 	 Previous best codebleu 42.93
  ********************
 Achieve Best bleu:49.3
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 6
  eval_ppl = 1.00037
  global_step = 8
  train_loss = 0.1648
  ********************
Previous best ppl:1.00038
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 51.51 	 Previous best codebleu 49.3
  ********************
 Achieve Best bleu:51.51
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 7
  eval_ppl = 1.00037
  global_step = 9
  train_loss = 0.2625
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 54.95 	 Previous best codebleu 51.51
  ********************
 Achieve Best bleu:54.95
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 8
  eval_ppl = 1.00037
  global_step = 10
  train_loss = 0.2456
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 56.14 	 Previous best codebleu 54.95
  ********************
 Achieve Best bleu:56.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 9
  eval_ppl = 1.00037
  global_step = 11
  train_loss = 0.1707
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 56.71 	 Previous best codebleu 56.14
  ********************
 Achieve Best bleu:56.71
  ********************
reload model from RQ5/java_5/f2_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_2/test.jsonl
  codebleu = 59.11 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[58, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[58, 344, 419]
  codebleu = 59.11 
[0.7016265496309229, 0.282237747431991, 0.7244385909782253, 0.41854621781321566, 0.8017839755713734, 0.3002209127619963, 0.20028952928854593, 0.6482925669271551, 0.2526041351649016, 0.7457018199401371, 0.9217179344125859, 0.7326457602700618, 0.4891407487676397, 0.33084607844644304, 0.7467969289220364, 0.3366978605247982, 0.3617235339418958, 0.8601188370424977, 0.3911495971232658, 0.7277313197041092, 0.8535087661549325, 0.2825445323372062, 0.1973450913036684, 0.7195422307059687, 0.26019140674766816, 0.5737997352686671, 0.7496626570929712, 0.24035478356411344, 0.5740994509755263, 0.7824191709399051, 0.6414813941400557, 0.6873662420545538, 0.6573324263927034, 0.3309398743680336, 0.30037795017905866, 0.21341125928003388, 0.9490051755223876, 0.7645481686846878, 0.3143475237262214, 0.6731590030090238, 0.1837555964676141, 0.20168622482466905, 0.3356275386616757, 0.8082679233225165, 0.2779947779662872, 0.23339533504521645, 0.1981704993085723, 0.7779120389404082, 0.9061537357063023, 0.6564919628282537, 0.34882047931217625, 0.4020839132815655, 0.21732158704181234, 0.32821388405371343, 0.944274958466798, 0.7258893392566115, 0.6371825065524551, 0.8294121954691612, 0.20028952928854593, 0.6748882581046782, 0.4188929946451023, 0.6307188510165458, 0.8233775338229194, 0.48737568099684603, 0.5618772912941226, 0.3255125934226603, 0.9228675103159067, 0.7518329266471542, 0.7319733117136124, 0.3448606407688256, 0.8875485500759485, 0.7506179768038621, 0.8214154016134527, 0.20168622482466905, 0.8009059128228648, 0.24148514067817678, 0.6459742111511437, 0.40965170545232765, 0.7781257324303039, 0.35052708016320144, 0.6466437069569624, 0.667134265678519, 0.7375260136440842, 0.8431171619150728, 0.8505699755371832, 0.6280485066726383, 0.751041907378877, 0.7724187456276157, 0.4076126695643083, 0.8631712921939972, 0.27298295835725306, 0.2076254204373124, 0.2141946642537291, 0.828406709800946, 0.8385638969479294, 0.8439975336408694, 0.5096543857275134, 0.6027009825614271, 0.7155677885532417, 0.6638449600070209, 0.7007209176866561, 0.20463708202371544, 0.7904982000185357, 0.9295084003434915, 0.8284020522463414, 0.818604796702507, 0.3238656644471753, 0.7854061653794722, 0.693092311325318, 0.8181224979957988, 0.2580930237147756, 0.1973450913036684, 0.8004855843549328, 0.2747562734254884, 0.3156787108389226, 0.3198975661697442, 0.7955598954540631, 0.8117498885831333, 0.4642907335502081, 0.27614805235407797, 0.9456202538775613, 0.7851806793787677, 0.6464759750524514, 0.9276537602120205, 0.38590527818314213, 0.6787558427106583, 0.6969222374225061, 0.31042479977733406, 0.38884149469802637, 0.1981704993085723, 0.2008919110495603, 0.8387326192522637, 0.3195506575207539, 0.9360251493185239, 0.5532439560483272, 0.26319677344405495, 0.6815766919825572, 0.1982264732929227, 0.7094783559068917, 0.5985460132137562, 0.34643952564335334, 0.6877483868598919, 0.772496321151799, 0.8854896075481384, 0.7570780601549043, 0.3142907371678166, 0.842653053572799, 0.5553706305752664, 0.6285616962601648, 0.263857145015134, 0.26078068367531076, 0.27450356699556455, 0.3530490441891116, 0.3110894892369726, 0.8279924547765871, 0.944274958466798, 0.49025574515829035, 0.7776475554643918, 0.6149021387093628, 0.3681205982882073, 0.9117966772276285, 0.22318167254517068, 0.7867864477290623, 0.7913676992477093, 0.7321529777184321, 0.7720408278336869, 0.2997686178693369, 0.9312991012532417, 0.5963093922280552, 0.27907947633305746, 0.7548340382627486, 0.8561505784124128, 0.6144232842628481, 0.6236313401524359, 0.7662036643795969, 0.32670549139481675, 0.8790904185947994, 0.2525633302580863, 0.37794425135743503, 0.09460679306446071, 0.2001482378217965, 0.38676076101202067, 0.5999629827846928, 0.3428340348531573, 0.7397251267678653, 0.26359478163341105, 0.4535640797757121, 0.27843798867015623, 0.3226700176690205, 0.8748733743967635, 0.23994804977779344, 0.8065512050171801, 0.3497039249558208, 0.31168869929367926, 0.7958913835066506, 0.7060089851548503, 0.26019140674766816, 0.26402567863653426, 0.7830011774921302, 0.8669569460602955, 0.8373887697347597, 0.936752055107628, 0.7851773831786442, 0.6479164016556753, 0.4180716148185418, 0.7707021960448603, 0.9511489077184989, 0.365032998226354, 0.6125773298541886, 0.780336643289784, 0.6151756185522186, 0.9419870730414177, 0.5239015601562096, 0.8483909315531839, 0.3201358252899485, 0.16379358360586094, 0.8195828071607292, 0.8471311821354086, 0.3188716543126748, 0.801169789534145, 0.5963432384610431, 0.2001482378217965, 0.7103676483086313, 0.8543977368573089, 0.38108374813042417, 0.2008919110495603, 0.30582559562746786, 0.21341125928003388, 0.7663246823325051, 0.1981704993085723, 0.8115270103245333, 0.6208716301688832, 0.3184943230303296, 0.7932914145722472, 0.6690874155175593, 0.8165963509339259, 0.2001482378217965, 0.7703047559758772, 0.2973590387132516, 0.5807497370951503, 0.21341125928003388, 0.4535640797757121, 0.6894914896837585, 0.7015164389259478, 0.8446086634415404, 0.7560928611949997, 0.6160079286053539, 0.2008919110495603, 0.1883314394699334, 0.30649302032887493, 0.8228893013961295, 0.9142150440420558, 0.7653614989599026, 0.819466384813053, 0.9104948835879048, 0.5032931854692144, 0.7725040894073211, 0.5385006436585963, 0.35858999518689294, 0.2301823188756478, 0.15418037934276801, 0.8568065239230183, 0.7428177153679325, 0.7764055613060061, 0.27260257043057684, 0.3457038805765606, 0.8225394466165354, 0.8114146497860415, 0.35669822433002185, 0.19831992742702956, 0.6901038563004188, 0.30245973960748945, 0.21341125928003388, 0.3604949800248019, 0.804363294869249, 0.8338530280205303, 0.7438832657679467, 0.2602549960773328, 0.9439975336408695, 0.26669868391409063, 0.7694496510087083, 0.30666220593032234, 0.8936294012021844, 0.7161353506408099, 0.8910871854638854, 0.9439975336408695, 0.6750976407504361, 0.8381189319740585, 0.7124674132929127, 0.7962984695478379, 0.3201358252899485, 0.7415109501172948, 0.23248078696380914, 0.21520181301880278, 0.5735283309621977, 0.37216441324851646, 0.33636712982345907, 0.5755233043849655, 0.836813479542287, 0.6651427788067421, 0.8712818103255136, 0.6059404980008516, 0.8132148025904986, 0.28873084754189704, 0.9471695160234401, 0.5054220034129565, 0.8047569965165327, 0.40154259854604424, 0.8788248884697676, 0.4830242583054791, 0.3794530767161612, 0.6331832283319798, 0.702947721035756, 0.3525003214314568, 0.21341125928003388, 0.39302712040712906, 0.6134135653510895, 0.2099448308231857, 0.8527432083605055, 0.7369455784266192, 0.6877773626094266, 0.7159255999851685, 0.8987229835300921, 0.7456265966096753, 0.6674432364375052, 0.8610447563885792, 0.5701275767706582, 0.732974089137441, 0.1973696076606292, 0.8565079112765963, 0.5912143005982238, 0.07480208191789525, 0.353704879775432, 0.5001385173166933, 0.8637506510554597, 0.30037795017905866, 0.8712299947771849, 0.25713278728915906, 0.19831992742702956, 0.18318911859768797, 0.8257451932547546, 0.7225044929148478, 0.7167237596778511, 0.7557992754099112, 0.771971362719331, 0.91970861258227, 0.9583676774082095, 0.2008919110495603, 0.7196849690792362, 0.761899634152513, 0.7459504764441578, 0.45668988116974407, 0.8663590716880905, 0.24433948037152053, 0.2777819052581376, 0.842292699197474, 0.8606384653875616, 0.7249616363996148, 0.7805351417808857, 0.6632208135509337, 0.8940560390899916, 0.8384600776366871, 0.8997135775382634, 0.7989531841146771, 0.21732158704181234, 0.39124949776856544, 0.7517334539095855, 0.3596040863011446, 0.7650493179217401, 0.7228819421809687, 0.8607242266137565, 0.9222725521291268, 0.30934200663888306, 0.29167519336191744, 0.8512843905586343, 0.2615954836836465, 0.26972193375038406, 0.7455583360178761, 0.34779676280149024, 0.9413014552018506, 0.22832477346115895, 0.6308153423200319, 0.7975293719902012, 0.673106131305089, 0.9231744182181167, 0.2994716964100343, 0.8200980608690891, 0.7293532020251807, 0.7703947670703939, 0.6794151141339883, 0.5267390358451736, 0.7703222712043629, 0.7774816465700545, 0.9035797722087997, 0.9336324324973175, 0.7649495090201577, 0.7562937394396303, 0.2188381789657648, 0.4323977767696198, 0.8328468877464585, 0.3775625809931694, 0.62166112251739, 0.31747806331918965, 0.8251189694211294, 0.37122241636505826, 0.29320605391949006, 0.7338722873630522, 0.8131793069362292, 0.8268323327051484, 0.8427374226430995, 0.31244597064055657, 0.3447000868699527, 0.5804463678624519, 0.6668556867136646, 0.32554383166405926, 0.7962751212087769, 0.6087825592068604, 0.6961646707977531, 0.8622417623629541, 0.4116650618311092, 0.7830011774921302, 0.7809789214708449, 0.6550147785138103, 0.8150233339421971, 0.6366721044530332, 0.29287847837743947, 0.30538338115904434, 0.6072445121255134, 0.5420424463805805, 0.8463835931264172, 0.6969650785813177, 0.47572112338149514, 0.8042532168310335, 0.3464927175190078, 0.8748733743967635, 0.32160693326868633, 0.3246727074898911, 0.6286302467743045, 0.7130825083995453, 0.8489800868205266, 0.8749250476050281, 0.3440652220390481, 0.7211648306470044, 0.4535640797757121, 0.898871187454698, 0.7952178398348333, 0.3275132317608889, 0.7527539342948406, 0.7558054094868673, 0.9297574173227554, 0.6838905656991746, 0.4457532475034466, 0.31548825358581534, 0.3368187482806173, 0.8236528441863966, 0.26184777968217093, 0.8374226883180389, 0.19831992742702956, 0.49954950157097433, 0.5708970570787804, 0.8850230971469417, 0.24936800794332245, 0.29589784684814585, 0.7538576425176133, 0.7268501559820937, 0.7548015882539172, 0.6406627132837781, 0.7816113575802897, 0.7515987471642543, 0.8692892652606987, 0.7875223626918182, 0.8565687425019681, 0.8608773653481296, 0.8643563915310817, 0.2712409226423623, 0.6643480585380116, 0.8626359234445131, 0.6764630638499591, 0.7937539341675677, 0.85818607897772, 0.815791063279768, 0.18768125056823864, 0.8001554015962489, 0.23679804018802814, 0.649758753550971, 0.9495565333799167, 0.7928862649902179, 0.8303674988265835, 0.21341125928003388, 0.7951925101759935, 0.4535640797757121, 0.6069014166959923, 0.8305745044405066, 0.7403658679909041, 0.8342487100106875, 0.7909258713245991, 0.19954501518196285, 0.8837311438245481, 0.7978812937743411, 0.2008919110495603]
Finish training and take 33m
