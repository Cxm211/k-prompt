Namespace(log_name='./RQ5/java_5/3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_5/3_codet5p_220m', data_dir='./data/RQ5/java_5_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '}       private final PassFactory checkGlobalNames =        new PassFactory("Check names", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '}       final PassFactory checkGlobalNames =        new PassFactory("Check names", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {'}]
***** Running training *****
  Num examples = 5
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 44.2277
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 11.93 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:11.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 44.7701
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 32.87 	 Previous best codebleu 11.93
  ********************
 Achieve Best bleu:32.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 20.2405
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 45.71 	 Previous best codebleu 32.87
  ********************
 Achieve Best bleu:45.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 12.9449
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 63.98 	 Previous best codebleu 45.71
  ********************
 Achieve Best bleu:63.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 8.2235
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 71.94 	 Previous best codebleu 63.98
  ********************
 Achieve Best bleu:71.94
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 6.675
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 73.82 	 Previous best codebleu 71.94
  ********************
 Achieve Best bleu:73.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 3.77
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 75.5 	 Previous best codebleu 73.82
  ********************
 Achieve Best bleu:75.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 4.8622
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 78.71 	 Previous best codebleu 75.5
  ********************
 Achieve Best bleu:78.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 2.7132
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 79.4 	 Previous best codebleu 78.71
  ********************
 Achieve Best bleu:79.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 1.5877
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 79.39 	 Previous best codebleu 79.4
  ********************
reload model from RQ5/java_5/3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_3/test.jsonl
  codebleu = 77.21 
  Total = 500 
  Exact Fixed = 26 
[23, 36, 42, 47, 74, 112, 130, 131, 138, 181, 222, 226, 228, 230, 237, 241, 248, 270, 273, 315, 329, 339, 348, 458, 489, 500]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[58, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 26 
[23, 36, 42, 47, 74, 112, 130, 131, 138, 181, 222, 226, 228, 230, 237, 241, 248, 270, 273, 315, 329, 339, 348, 458, 489, 500]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 3 
[58, 344, 419]
  codebleu = 77.21 
[0.0841258702031846, 0.294441011668617, 0.8340268246875062, 0.5830912642712589, 0.8410886457286899, 0.9000092746228069, 0.966410184624177, 0.7238925475243609, 0.3072110927368262, 0.8011782298988499, 0.9685944421474288, 0.8593411581323525, 0.669580236704477, 0.343727450945313, 0.9003430139277828, 0.8343261869604057, 0.7523277398705996, 0.9342846849350264, 0.9621816403645371, 0.8673567485716778, 0.4996427791823768, 0.25857245489813463, 1.0, 0.9000120435131169, 0.5227529782049001, 0.8985819684179537, 0.8424600720346243, 0.7318447632403485, 0.6151952248646311, 0.7984153870943118, 0.8961415113886637, 0.8579781671940192, 0.7321748033833095, 0.27558659699020227, 0.792956903878446, 1.0, 0.545241467478512, 0.8745519666015812, 0.9410056506290296, 0.7443217176118664, 0.6940078756137626, 1.0, 0.5592196473310664, 0.8345641796204135, 0.37410033058463965, 0.4631047202409472, 1.0, 0.976934640027229, 0.8311925768061224, 0.7366535801418781, 0.9269331830469714, 0.37410033058463965, 0.2681150062890573, 0.5972988141875188, 0.32405006051403207, 0.5184288227875373, 0.6993730085347135, 0.8855924534744011, 0.966410184624177, 0.8704933355439815, 0.7996905614254997, 0.7935433613911154, 0.9094559456673585, 0.683747344676549, 0.525037320912514, 0.9471534480091441, 0.9607233175453542, 0.843537340596747, 0.8629693959820761, 0.8063559408133776, 0.8875485500759485, 0.8286690242372425, 0.8963926483875864, 1.0, 0.9337703133336674, 0.8762476125170207, 0.711979924212595, 0.5422875137889253, 0.9348495609386918, 0.8590221290282165, 0.7621674550453241, 0.8693647831726143, 0.6930766359364882, 0.9109429421150292, 0.9852849226532605, 0.7519894248327487, 0.7438613337267472, 0.9197682751991088, 0.9488183079463388, 0.9587461859192787, 0.961485854541051, 0.24252546929754926, 0.2719709748681333, 0.9535662529996234, 0.8530965304100651, 0.8545521993874243, 0.8853701119354719, 0.8321624083223698, 0.8696882992240296, 0.8151472726544473, 0.6916772828650979, 0.9548905452619663, 0.868178877541268, 0.9285342957365641, 0.8745588911913525, 0.8853058630953912, 0.8550242556943874, 0.8813815400934406, 0.7674080702465693, 0.837841768268662, 0.22320314745567632, 1.0, 0.931919905317208, 0.8318604630520261, 0.898122990476288, 0.8744433206869813, 0.8947233410879221, 0.9600094284589669, 0.590871653728197, 0.23530484437207808, 0.35979301188527607, 0.9418090107609731, 0.8765208719204487, 0.9461408346030404, 0.9571016094529847, 0.7188935477431233, 0.7335237499509486, 0.932758510312542, 0.48666825572275857, 1.0, 1.0, 0.9105568726695912, 0.8987135835101596, 0.9431219690661039, 0.9298499422299327, 0.8521403038247229, 0.8684368528418116, 1.0, 0.8275239936524709, 0.6842490991938389, 0.8868397852593601, 0.7898482563516439, 0.24857356249961626, 0.847733087519547, 0.8936973938003048, 0.7399046460615835, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.014551758735342566, 0.25839300770760326, 0.40367302623134316, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.32405006051403207, 0.4382171945538378, 0.8882724617610109, 0.6550690221321874, 0.8721459350488947, 0.3060760492325353, 0.7173692510217399, 0.938879555515129, 0.8620700454998175, 0.8234385994211576, 0.8412239301694939, 0.8678657374052126, 0.9312991012532417, 0.83084241467313, 0.059457761795062194, 0.9041340728227794, 0.8884670476301071, 0.8567354045301243, 0.933286970639994, 0.8285441083529881, 0.3207411299507977, 0.9499046047578625, 0.9214852005355179, 0.6121576162732347, 0.07607726785842973, 1.0, 0.9114698188792376, 0.7676410336466819, 0.8867171706601364, 0.814665991132224, 1.0, 0.783801630906283, 0.29079785541333014, 0.7048910897361526, 0.591651271701471, 0.2365243418529688, 0.8353382676021841, 0.8218670339508176, 0.9465491069053984, 0.8586435655918875, 0.4794267326955328, 0.5227529782049001, 0.1934892316923661, 0.7830011774921302, 0.9156873229543501, 0.920847271632746, 0.936752055107628, 0.8939403219941204, 0.7988848397512625, 0.9399989903319628, 0.9165318450152771, 0.9735542568395896, 0.9055256568213023, 0.6781053145008827, 0.816318859382493, 0.798272609744273, 0.9734190821154947, 0.6268790890159027, 0.9484513298936408, 0.20178636748337778, 0.44133890352712457, 0.8516665453656083, 0.925675085910995, 0.7945631480201435, 0.801169789534145, 0.7162570708021344, 1.0, 0.7703813467645078, 0.3622914768846951, 0.95, 1.0, 0.792956903878446, 1.0, 0.8801547132641002, 1.0, 0.9288817233176625, 0.7636095864061366, 0.9487145883454327, 0.7878359372089682, 0.7360764107540074, 0.8017624348471263, 1.0, 0.8465816263983615, 0.30745335446905736, 0.8498388617714197, 1.0, 0.3901069579874911, 0.8651706733544999, 0.8247308245173888, 0.9011976714326457, 0.8921042672694857, 0.7730519120573912, 1.0, 0.9517008915274616, 0.8093542785781114, 0.9377051985043877, 0.9142150440420558, 0.8580012590324053, 0.9173467324769831, 0.93092593470538, 0.5929022108454984, 0.9447883869073515, 0.5328522761377882, 0.15130651924653368, 0.5550107088205123, 0.1383424991648719, 0.8154063109839143, 0.9029228697393135, 0.8882492511578264, 0.8697838061014673, 0.9344052063492188, 0.8760135800491213, 0.8344531615285882, 0.8646780713427795, 1.0, 0.8027136845880052, 0.9052981975214478, 1.0, 0.29739605984612727, 0.9131034995733858, 0.9559045711960699, 0.2723684552145271, 0.31717471734651254, 0.30793433386770624, 0.28779209640198006, 0.9005633971565483, 0.18441272589615826, 0.9694667888884985, 0.9396180092663717, 0.5849350193566214, 0.30793433386770624, 0.6879557781873078, 0.9035957587334096, 0.8007377532608353, 0.9305504218364138, 0.20178636748337778, 0.6940399860279578, 0.10804289408544776, 0.21728739078237788, 0.5778624281512992, 0.9205458982768264, 0.6992043393714683, 0.6336496538690435, 0.940888866830786, 0.8178334933130291, 0.9384313483327684, 0.602746669476317, 0.6083418857548727, 0.9790427622042994, 0.9471695160234401, 0.3923440990020852, 0.841875207469964, 0.9234334473517907, 0.9468368593144163, 0.6693140996033659, 0.8884049563995104, 0.7757388605257256, 0.7932369845589845, 0.33667502438294916, 1.0, 0.957146397188152, 0.8549033669907233, 0.8430191247683193, 0.9162509302160007, 0.8624584208490358, 0.8909899484569659, 0.8006459328388965, 0.9015447973123007, 0.7804110476521025, 0.7225963853375947, 0.9205890436191755, 0.8019309772658032, 0.8188865946509148, 1.0, 0.5682540766407544, 0.6443664684483896, 0.9606383615912808, 0.8679908487707813, 0.651002857872442, 0.8926703816314219, 0.792956903878446, 0.8980659480857067, 0.8411630424921515, 1.0, 0.22152948098577457, 0.5544233577589087, 0.8925235787478019, 0.8377252262881923, 0.9237373188990914, 0.8979311034464716, 0.941246887425869, 0.3597554292633871, 1.0, 0.9288768296987449, 0.8114803769129251, 0.8641269634908533, 0.7571560504922936, 0.9214086041237446, 0.8696537204746508, 0.6889920012154405, 0.9809329721714535, 0.931600566509587, 0.785678266679733, 0.9423575001497, 0.7297862281282066, 0.8989415087276855, 0.8575743267954499, 0.3280744660611017, 0.8627830017322995, 0.2681150062890573, 0.9693877551020409, 0.835222816247519, 0.8232478901372681, 0.852734238473497, 0.8748503542446726, 0.9347001488455178, 0.9222725521291268, 0.5695758149747698, 0.7400913955844939, 0.8912314882918398, 0.3211286748191717, 0.3203295251205849, 0.9170972299520759, 0.9973062440915319, 0.9468414240145169, 0.896457542865793, 0.9120050406373192, 0.9452585311430763, 0.666856131305089, 0.9396360626487428, 0.7023713204162336, 0.9684326167255213, 0.7925041454075072, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.894141073173163, 0.9598250197069826, 0.9634911520696614, 0.8712775503716599, 0.8323441696598732, 0.9089251984809752, 0.5017231480080151, 0.9642153446749153, 0.9921681817198038, 0.7135449662005422, 0.7268959478520645, 0.9093270194127461, 0.8796566184255739, 0.7708128313590381, 0.7420624686143404, 0.9177534780225702, 0.9136051415192505, 0.9043217962861809, 0.9289542494919141, 0.8505395682648067, 0.6718613566981233, 0.8960971760439522, 0.3278533201653578, 0.9442100823718134, 0.6901262669982268, 0.7856488146629412, 0.8920742720966965, 0.8193805243133054, 0.7830011774921302, 0.9314651191074765, 0.8985609679543393, 0.9202827191608316, 0.548514768046369, 0.8634064000352093, 0.9936419998424288, 0.2975404262271357, 0.7915314290630997, 0.9745770345730578, 0.7407928163996362, 0.5072421348162169, 0.8839836214081072, 0.9146241894400875, 0.591651271701471, 0.2505614906782377, 0.3203803815142294, 0.8121861145626663, 0.22472125686965116, 0.9237914376874523, 0.9311097802383854, 0.9350024698180224, 0.895540655184323, 0.3887427803465273, 0.5747862200752026, 0.8825445333927069, 0.852842712474619, 0.8179617939659143, 0.800083937746499, 0.9666327444353364, 0.8810554316276455, 0.34835062800502736, 0.7229151142178896, 0.7366505750537401, 0.9149306320417432, 0.2649914527014747, 0.9150679539598123, 1.0, 0.45369556474096256, 0.8475849298043776, 0.9133731397684426, 0.8243317050386542, 0.7065961844283755, 0.7538576425176133, 0.7935461504124401, 0.9272271558703311, 0.9016850690943105, 0.8362192242070374, 0.8364278655891917, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.8713421516632174, 0.8983106863329746, 0.3344589207012786, 0.7724155016374759, 0.8698869964564975, 0.904352113990756, 0.9117215889741954, 0.9482791056675344, 0.923792940978809, 0.9275923820999126, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.890490591592302, 0.9449149097946257, 0.9069546174929648, 1.0, 0.9049842403132857, 0.783801630906283, 0.29209666305637233, 0.8965966751963506, 0.8254935742435682, 0.8514674628624775, 0.9302059533563303, 0.32528340979200293, 0.9122540568597661, 0.9142736329880987, 1.0]
Finish training and take 51m
