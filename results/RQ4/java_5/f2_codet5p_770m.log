Namespace(log_name='./RQ5/java_5/f2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_5/f2_codet5p_770m', data_dir='./data/RQ5/java_5_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00179
  global_step = 2
  train_loss = 1.1509
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00179
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 19.36 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.36
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00101
  global_step = 3
  train_loss = 1.0783
  ********************
Previous best ppl:1.00179
Achieve Best ppl:1.00101
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 20.94 	 Previous best codebleu 19.36
  ********************
 Achieve Best bleu:20.94
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0009
  global_step = 4
  train_loss = 0.4077
  ********************
Previous best ppl:1.00101
Achieve Best ppl:1.0009
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 29.61 	 Previous best codebleu 20.94
  ********************
 Achieve Best bleu:29.61
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00084
  global_step = 5
  train_loss = 0.3441
  ********************
Previous best ppl:1.0009
Achieve Best ppl:1.00084
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 32.37 	 Previous best codebleu 29.61
  ********************
 Achieve Best bleu:32.37
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00075
  global_step = 6
  train_loss = 0.2657
  ********************
Previous best ppl:1.00084
Achieve Best ppl:1.00075
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 38.76 	 Previous best codebleu 32.37
  ********************
 Achieve Best bleu:38.76
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00073
  global_step = 7
  train_loss = 0.2298
  ********************
Previous best ppl:1.00075
Achieve Best ppl:1.00073
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 44.04 	 Previous best codebleu 38.76
  ********************
 Achieve Best bleu:44.04
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00072
  global_step = 8
  train_loss = 0.1846
  ********************
Previous best ppl:1.00073
Achieve Best ppl:1.00072
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 48.12 	 Previous best codebleu 44.04
  ********************
 Achieve Best bleu:48.12
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00071
  global_step = 9
  train_loss = 0.179
  ********************
Previous best ppl:1.00072
Achieve Best ppl:1.00071
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 51.48 	 Previous best codebleu 48.12
  ********************
 Achieve Best bleu:51.48
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0007
  global_step = 10
  train_loss = 0.2206
  ********************
Previous best ppl:1.00071
Achieve Best ppl:1.0007
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 54.49 	 Previous best codebleu 51.48
  ********************
 Achieve Best bleu:54.49
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0007
  global_step = 11
  train_loss = 0.1723
  ********************
Previous best ppl:1.0007
Achieve Best ppl:1.0007
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 56.47 	 Previous best codebleu 54.49
  ********************
 Achieve Best bleu:56.47
  ********************
reload model from RQ5/java_5/f2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_2/test.jsonl
  codebleu = 53.81 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 53.81 
[0.23869071389649904, 0.2642172094997072, 0.7244385909782253, 0.10464155786876997, 0.24410471773047732, 0.2948796460845807, 0.20028952928854593, 0.645174063808652, 0.31011824477485056, 0.7457018199401371, 0.398092325164718, 0.7461457066331898, 0.26434454511323896, 0.2848948076167655, 0.6473883252153243, 0.8151988338110974, 0.6249494770026063, 0.8601188370424977, 0.37213916034008, 0.014721466222563236, 0.17973648720626342, 0.3008976735796797, 0.1973450913036684, 0.7195422307059687, 0.4204385779908989, 0.6628038362803049, 0.6817990504880667, 0.11646560126008701, 0.5626815063058519, 0.7984153870943118, 0.285169907522857, 0.2074567327957484, 0.60877122102288, 0.3275607340168888, 0.33569490634556565, 0.21341125928003388, 0.9148265311246491, 0.7286836891678303, 0.36825311623087337, 0.6731590030090238, 0.15381816343112126, 0.1981704993085723, 0.41918840185088607, 0.7418704298025606, 0.4727948490064814, 0.24151821299328607, 0.20168622482466905, 0.38554011646014547, 0.36579541678813393, 0.6564919628282537, 0.7266245133887685, 0.20248755009605446, 0.21341125928003388, 0.8881565719741715, 0.32727673740397745, 0.6989859084333365, 0.10282980389429659, 0.24060036574463645, 0.20028952928854593, 0.3360208380620095, 0.4116264602336949, 0.6307188510165458, 0.7998658507695164, 0.26429383547717245, 0.5557855712253208, 0.8306679051749872, 0.8802718523979132, 0.7228911483719709, 0.33639015055879495, 0.31430331706475045, 0.862422149190751, 0.7156209308844791, 0.8214154016134527, 0.1981704993085723, 0.8030175580931873, 0.4256933862512554, 0.5720924195714442, 0.5963644045915871, 0.2899627141243063, 0.30886348783572165, 0.6466437069569624, 0.667134265678519, 0.849295822235913, 0.8338105714098387, 0.781888623636722, 0.6280485066726383, 0.5733537664492531, 0.3961515888348626, 0.8617334462573462, 0.6183279539137256, 0.3873948197422797, 0.17859920065153279, 0.21846005264357665, 0.3587492206939942, 0.3521384470542897, 0.6588240055068579, 0.3264612711372559, 0.608201670496848, 0.7445266082586584, 0.6638449600070209, 0.9490742122482716, 0.2732832390393036, 0.7829574179238921, 0.35523737012294315, 0.8284020522463414, 0.818604796702507, 0.7310285907669327, 0.790575133765443, 0.6116651566045368, 0.8181224979957988, 0.5806872973075914, 0.2008919110495603, 0.8004855843549328, 0.3968116538880717, 0.7440788399899874, 0.3210308376822612, 0.3791898536025872, 0.8117498885831333, 0.2116146488777832, 0.2292890834795544, 0.9456202538775613, 0.7242920987224982, 0.6464759750524514, 0.9070205459407448, 0.43199608538052786, 0.6787558427106583, 0.6802555707558395, 0.5651942311696617, 0.042857712594154815, 0.20168622482466905, 0.2008919110495603, 0.8387326192522637, 0.37044497946247457, 0.9132024832764702, 0.7276749185776005, 0.3175198834556907, 0.6926585068723075, 0.20173083019628663, 0.31621138061018883, 0.3353288701022899, 0.35086438517516816, 0.8226254192181344, 0.7664023716159446, 0.24372681043005098, 0.7193559387880331, 0.3077356891852089, 0.6098918959986914, 0.33448661666067503, 0.6285616962601648, 0.260058172275396, 0.2328626980828981, 0.319186334679425, 0.657439344826671, 0.22413706052835303, 0.8279924547765871, 0.32727673740397745, 0.8558135649010996, 0.27083357299518, 0.6037956703054032, 0.7756043485098167, 0.8651790920804865, 0.21941246429786135, 0.7519094328197864, 0.794393542184564, 0.7321529777184321, 0.7061673743676599, 0.852565642646234, 0.1017956137102767, 0.6698986596384627, 0.09179947798041273, 0.738590487779091, 0.7569158249722213, 0.7435606041548073, 0.3843046091390142, 0.7662036643795969, 0.327242163389893, 0.6793961328576963, 0.5997478175640913, 0.36756080321168205, 0.0648216557517177, 0.2001482378217965, 0.42446972640012975, 0.9286730262313432, 0.7691833809454508, 0.6365383033267106, 0.3510435644685261, 0.45554737673469164, 0.6507341546899781, 0.33523636347674335, 0.26729538641147194, 0.251735951036083, 0.29559832074054565, 0.381641591289687, 0.38500429226580296, 0.7843891089969268, 0.4818358494581836, 0.4204385779908989, 0.12347711861148852, 0.3522487703575479, 0.8669569460602955, 0.8373887697347597, 0.3311791056653678, 0.641671476933981, 0.6479164016556753, 0.5243968555108266, 0.2818142849343036, 0.8464277836719598, 0.84957403931718, 0.6125773298541886, 0.9471695160234401, 0.3122408992456224, 0.9219870730414177, 0.23963905196336666, 0.8483909315531839, 0.28844221583286656, 0.35527343966398, 0.7522784432283336, 0.8471311821354086, 0.45055043710929, 0.22966215572086612, 0.6116706591348782, 0.2001482378217965, 0.6912491160500718, 0.7869933756745572, 0.7583609709353714, 0.2008919110495603, 0.1814774511537709, 0.21341125928003388, 0.7663246823325051, 0.20168622482466905, 0.34959629402668085, 0.6208716301688832, 0.3184943230303296, 0.707864662347621, 0.6677175969628313, 0.753652946530818, 0.2001482378217965, 0.7703047559758772, 0.30476117118999685, 0.7302132520649481, 0.21341125928003388, 0.45554737673469164, 0.6894914896837585, 0.7005488815491723, 0.6844247461720165, 0.7215632667362883, 0.6160079286053539, 0.2008919110495603, 0.19808589219368875, 0.20299226454152958, 0.5930940973320482, 0.2969449824312744, 0.7577696142127515, 0.819466384813053, 0.6675808446076708, 0.5032931854692144, 0.570308765342594, 0.5434819500316723, 0.33067685025404586, 0.5937137415279337, 0.7675921786664237, 0.8377579876075918, 0.7428177153679325, 0.7629859562209929, 0.8511813664948011, 0.38665324556644304, 0.5894797887373873, 0.7330893881843675, 0.358092899946694, 0.20180525490482165, 0.6788114549584525, 0.7378853667556762, 0.21341125928003388, 0.39563301945234813, 0.6640973001688473, 0.7829386653206072, 0.2738079943920221, 0.2832164825840397, 0.8844585959278155, 0.8805005828240171, 0.7694496510087083, 0.3023912568227982, 0.8766307456380591, 0.7161353506408099, 0.37783303855537054, 0.8844585959278155, 0.7688707613570311, 0.8381189319740585, 0.6689574866712837, 0.7962984695478379, 0.28844221583286656, 0.1948519916187951, 0.24573370884449913, 0.16590916042853185, 0.5735283309621977, 0.3811882467377743, 0.29605088374517596, 0.5197762730101323, 0.7717700585797584, 0.7084827366161301, 0.8712818103255136, 0.31174909490386143, 0.527082373759703, 0.4111800568737842, 0.9471695160234401, 0.6039085649180032, 0.7952451538152271, 0.6997236976075947, 0.7395671307988458, 0.5978755675920615, 0.7875659371823882, 0.6306336683240683, 0.32170960223687345, 0.33150567235697376, 0.21763126446116718, 0.408703562810678, 0.605220265173782, 0.3347880426702654, 0.7942668319530466, 0.7369455784266192, 0.6877773626094266, 0.6995555062483301, 0.8462905315721074, 0.7450148713849064, 0.6272889299506436, 0.6550349773490487, 0.36192676106336225, 0.6312590884673269, 0.20096843400865197, 0.31167715203228497, 0.6082638378466355, 0.36786546761006744, 0.27409422332067795, 0.3169917472384898, 0.7654122113599586, 0.38656545550353316, 0.7354197893197052, 0.30397201393975126, 0.20173083019628663, 0.11513177725506499, 0.7916066576026328, 0.7225044929148478, 0.7167237596778511, 0.7394897373025966, 0.771971362719331, 0.16420904162418065, 0.5567528025673054, 0.2008919110495603, 0.7196849690792362, 0.6190536819386012, 0.7459504764441578, 0.6357409625891831, 0.8663590716880905, 0.36236902088564726, 0.8824573620752161, 0.7014910282390292, 0.8354429870501969, 0.723386181608951, 0.7805351417808857, 0.19463132367257785, 0.8822098793569582, 0.7789857549567092, 0.3641796995745016, 0.7989531841146771, 0.21341125928003388, 0.7931892348677367, 0.6726293393583174, 0.7380569242182775, 0.24574821255531148, 0.3022576743628508, 0.8312518911656201, 0.8282689240087391, 0.3069420066388831, 0.2970704310823876, 0.8183652491683203, 0.3335926025742707, 0.24338189822117606, 0.7159882979479544, 0.7950257247002055, 0.2446002142535679, 0.824482160168883, 0.3359173267478487, 0.7975293719902012, 0.673106131305089, 0.6401135757072111, 0.6417260687505856, 0.38010290200350827, 0.7293532020251807, 0.7703947670703939, 0.21055520280460877, 0.5267390358451736, 0.7703222712043629, 0.2397282396962869, 0.8388749113350329, 0.9336324324973175, 0.6380253857732028, 0.7617421921196805, 0.24451055047002218, 0.6038704051214352, 0.18376787249825327, 0.3514474419544944, 0.33720939096424485, 0.30533354706422233, 0.8251189694211294, 0.35879670835663424, 0.36557312199127234, 0.7272514351510467, 0.7300462638235969, 0.8268323327051484, 0.8448871355655112, 0.46815082712926215, 0.3447000868699527, 0.5592748649951178, 0.6668556867136646, 0.22413147616194493, 0.7353061422230001, 0.24100173204044012, 0.6961646707977531, 0.8171941917074148, 0.3213463959633761, 0.3522487703575479, 0.7017338689597247, 0.7768788610437616, 0.6956973858324682, 0.6441901835803022, 0.3542614549080991, 0.39864510430303113, 0.6045655117031272, 0.2725830664447281, 0.8463835931264172, 0.317980064922753, 0.6049902857272969, 0.6610058582236744, 0.7442948167067518, 0.7302358902439641, 0.3174492441370996, 0.3246727074898911, 0.6235441718214954, 0.22581300539750943, 0.8384654357290597, 0.8678099712026246, 0.3173314023525371, 0.7211648306470044, 0.21769368584720475, 0.21956124707735414, 0.43687363456923717, 0.3216026412312633, 0.7264679440554266, 0.7217298299312938, 0.8169543076043915, 0.6838905656991746, 0.380126796253513, 0.4797599907419905, 0.3329764502897303, 0.32923994943670065, 0.2747274634708004, 0.8374226883180389, 0.20173083019628663, 0.5250144649356459, 0.6739724656666293, 0.8850230971469417, 0.29502345246439604, 0.5556232238490902, 0.30964325103678025, 0.7000938654649012, 0.34052604391719155, 0.6407076198762522, 0.8296936948640998, 0.799068980115264, 0.8389701775936249, 0.3481173556099369, 0.8681824890105203, 0.5667777059882779, 0.8581090612990283, 0.28119927575287545, 0.6676908918326503, 0.8355512057391163, 0.2340780461593803, 0.7937539341675677, 0.5856156793544399, 0.7817538977599741, 0.1976990043848573, 0.7175998270969115, 0.2524568596914068, 0.661565729556063, 0.23813022804046258, 0.7928862649902179, 0.8606603600206091, 0.21341125928003388, 0.7951925101759935, 0.2312030674283777, 0.3126573909443636, 0.8305745044405066, 0.7139260550035793, 0.31859581971619805, 0.8572203816320989, 0.19954501518196285, 0.7969494532893326, 0.7978812937743411, 0.2008919110495603]
Finish training and take 5h46m
