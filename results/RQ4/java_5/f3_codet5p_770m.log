Namespace(log_name='./RQ5/java_5/f3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_5/f3_codet5p_770m', data_dir='./data/RQ5/java_5_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00179
  global_step = 2
  train_loss = 1.3285
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00179
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 19.35 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.35
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00102
  global_step = 3
  train_loss = 0.7899
  ********************
Previous best ppl:1.00179
Achieve Best ppl:1.00102
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 22.28 	 Previous best codebleu 19.35
  ********************
 Achieve Best bleu:22.28
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00087
  global_step = 4
  train_loss = 0.2856
  ********************
Previous best ppl:1.00102
Achieve Best ppl:1.00087
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 29.11 	 Previous best codebleu 22.28
  ********************
 Achieve Best bleu:29.11
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00078
  global_step = 5
  train_loss = 0.3461
  ********************
Previous best ppl:1.00087
Achieve Best ppl:1.00078
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 31.0 	 Previous best codebleu 29.11
  ********************
 Achieve Best bleu:31.0
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00075
  global_step = 6
  train_loss = 0.1448
  ********************
Previous best ppl:1.00078
Achieve Best ppl:1.00075
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 34.7 	 Previous best codebleu 31.0
  ********************
 Achieve Best bleu:34.7
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00073
  global_step = 7
  train_loss = 0.147
  ********************
Previous best ppl:1.00075
Achieve Best ppl:1.00073
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 40.64 	 Previous best codebleu 34.7
  ********************
 Achieve Best bleu:40.64
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00072
  global_step = 8
  train_loss = 0.0932
  ********************
Previous best ppl:1.00073
Achieve Best ppl:1.00072
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 46.94 	 Previous best codebleu 40.64
  ********************
 Achieve Best bleu:46.94
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00071
  global_step = 9
  train_loss = 0.1518
  ********************
Previous best ppl:1.00072
Achieve Best ppl:1.00071
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 51.75 	 Previous best codebleu 46.94
  ********************
 Achieve Best bleu:51.75
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00071
  global_step = 10
  train_loss = 0.1125
  ********************
Previous best ppl:1.00071
Achieve Best ppl:1.00071
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 55.01 	 Previous best codebleu 51.75
  ********************
 Achieve Best bleu:55.01
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0007
  global_step = 11
  train_loss = 0.0558
  ********************
Previous best ppl:1.00071
Achieve Best ppl:1.0007
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 58.16 	 Previous best codebleu 55.01
  ********************
 Achieve Best bleu:58.16
  ********************
reload model from RQ5/java_5/f3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_3/test.jsonl
  codebleu = 58.79 
  Total = 500 
  Exact Fixed = 16 
[23, 36, 112, 131, 181, 222, 228, 241, 248, 273, 305, 315, 339, 348, 489, 500]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 16 
[23, 36, 112, 131, 181, 222, 228, 241, 248, 273, 305, 315, 339, 348, 489, 500]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  codebleu = 58.79 
[0.31533684058711253, 0.26104559225008966, 0.7244385909782253, 0.4145632314515063, 0.8017839755713734, 0.815948851622457, 0.19169260251979953, 0.6510272778451487, 0.31245065054578763, 0.7457018199401371, 0.9217179344125859, 0.747643315953749, 0.14640686124092173, 0.32627888247037007, 0.6658282919309748, 0.1660913096398139, 0.6335226824032332, 0.8601188370424977, 0.38407948583053314, 0.7277313197041092, 0.24974455934032794, 0.3008976735796797, 1.0, 0.7195422307059687, 0.23265638221672902, 0.6925020965521849, 0.3068501341726717, 0.5822661572242728, 0.5740994509755263, 0.7984153870943118, 0.6414813941400557, 0.6873662420545538, 0.6095232043875561, 0.30343171004162167, 0.465715924844207, 1.0, 0.6154417991606029, 0.7311460087612476, 0.36825311623087337, 0.6731590030090238, 0.1819185071276824, 0.2657218228903083, 0.3413971649551465, 0.7582679233225165, 0.2846773343488649, 0.969339732242054, 0.2657218228903083, 0.3875149901599054, 0.9061537357063023, 0.6564919628282537, 0.7953988464436343, 0.18286501007546185, 0.26992032738962607, 0.3863029490696407, 0.26068550632873655, 0.6076666578198298, 0.6160742607444892, 0.8202627647587617, 0.19169260251979953, 0.7680079391194865, 0.4177361154432053, 0.6307188510165458, 0.8233775338229194, 0.2905076151129663, 0.5165326280632435, 0.8272166851138489, 0.9228675103159067, 0.7228911483719709, 0.6941813780840002, 0.41841824219090795, 0.8875485500759485, 0.7547680814955248, 0.8214154016134527, 0.2657218228903083, 0.513828142614124, 0.3060545052802364, 0.6432725542182892, 0.5960942096485482, 0.7180447704136503, 0.6937523721016591, 0.6466437069569624, 0.667134265678519, 0.8692922404836547, 0.8431171619150728, 0.8505699755371832, 0.6280485066726383, 0.7836463999899727, 0.7765776859319277, 0.49446892323831515, 0.8631712921939972, 0.6456110631539317, 0.20552763225274162, 0.26822783896449187, 0.828406709800946, 0.2757795566433626, 0.33040297426504595, 0.32251817118556064, 0.44960565784830414, 0.7155677885532417, 0.6638449600070209, 0.6820516825604106, 0.20270369098519428, 0.8511114916048268, 0.9295084003434915, 0.8284020522463414, 0.818604796702507, 0.7310285907669327, 0.790575133765443, 0.693092311325318, 0.8181224979957988, 0.19874772006150504, 1.0, 0.8004855843549328, 0.24672792394009546, 0.3176822448399781, 0.3210308376822612, 0.7955598954540631, 0.8117498885831333, 0.564955220613006, 0.26830260485541707, 0.376204224944572, 0.7851806793787677, 0.6464759750524514, 0.9101474687962732, 0.31887028763767955, 0.6765601278342184, 0.6969222374225061, 0.37915427759987336, 0.04233018789289574, 0.2657218228903083, 1.0, 0.8387326192522637, 0.3645577458610657, 0.9360251493185239, 0.6736686472528481, 0.19306756628074068, 0.6907319462003152, 0.2688843686424191, 0.3161038016266466, 0.60591624086192, 0.3461438156823666, 0.6877483868598919, 0.7664023716159446, 0.619347995257647, 0.7570780601549043, 0.5445776189071216, 0.842653053572799, 0.5553706305752664, 0.6285616962601648, 0.273043702536169, 0.2328626980828981, 0.31953033773767137, 0.807624185543989, 0.3110894892369726, 0.8279924547765871, 0.26068550632873655, 0.432732422984499, 0.7776475554643918, 0.5974610995176524, 0.3181519283693305, 0.9007978899335911, 0.2222320131700418, 0.3650438453355783, 0.7771353129683136, 0.7321529777184321, 0.7720408278336869, 0.8678657374052126, 0.1588585090389535, 0.5959311746427127, 0.148973070241035, 0.738590487779091, 0.8428147948750389, 0.7435606041548073, 0.6366748184133055, 0.7662036643795969, 0.3259431425651613, 0.8790904185947994, 0.20799290405986584, 0.3702285130210081, 0.30218867653389875, 1.0, 0.37601972490863544, 0.6023785398607646, 0.32716193621164996, 0.6432232678640983, 0.38274515845774787, 0.20282713830471816, 0.29228844093812467, 0.34090508695217475, 0.28415822386429657, 0.251735951036083, 0.28086813209771844, 0.2819782963287243, 0.3851970137812832, 0.34419445174938385, 0.4793497753583987, 0.23265638221672902, 0.12382622520383807, 0.7830011774921302, 0.8669569460602955, 0.8373887697347597, 0.9072725521291267, 0.7616311299749057, 0.6479164016556753, 0.44541381523121215, 0.7647880292188753, 0.9511489077184989, 0.8520881604630502, 0.6125773298541886, 0.9471695160234401, 0.32005508725227894, 0.9419870730414177, 0.4853580745171127, 0.8483909315531839, 0.19919413006252876, 0.26627452484784814, 0.8184691792246914, 0.8471311821354086, 0.5836996421247036, 0.6643219118729957, 0.5796030772727474, 1.0, 0.7103676483086313, 0.8741762120847619, 0.7583609709353714, 0.26128246701727087, 0.3441167952872309, 1.0, 0.7663246823325051, 0.2657218228903083, 0.31334424538125055, 0.6208716301688832, 0.32140694438955286, 0.7397804326929488, 0.6677175969628313, 0.7493723392462364, 0.26339484374474653, 0.7703047559758772, 0.31014758562343, 0.7495693960741996, 1.0, 0.21163960741579183, 0.6894914896837585, 0.7005488815491723, 0.7722773421062352, 0.7560928611949997, 0.6160079286053539, 1.0, 0.19799082312349275, 0.229316228911154, 0.8228893013961295, 0.9142150440420558, 0.7535419624620324, 0.819466384813053, 0.9104948835879048, 0.5032931854692144, 0.6045143452033909, 0.5385006436585963, 0.7124443661463586, 0.4834727614939841, 0.15525678978550927, 0.8568065239230183, 0.7428177153679325, 0.7862600706031022, 0.27260257043057684, 0.3457038805765606, 0.6838773195839767, 0.7579036368724879, 0.35043426110262776, 0.26718446347937613, 0.28763331595380237, 0.33507598111679066, 1.0, 0.2649980138493134, 0.6640973001688473, 0.8338530280205303, 0.29066855074621706, 0.905704145167471, 0.17623651328919782, 0.26255568576728533, 0.7694496510087083, 0.10788829844880107, 0.8462104662406251, 0.7161353506408099, 0.34412178743475147, 0.8905574100475333, 0.7564701306650601, 0.8381189319740585, 0.7124674132929127, 0.7006718914870371, 0.19919413006252876, 0.7415109501172948, 0.23838441806435728, 0.19731207357014596, 0.5735283309621977, 0.27668367586990217, 0.5292941457487407, 0.5755233043849655, 0.3228111738409682, 0.6927442674273416, 0.8712818103255136, 0.6059404980008516, 0.2794396219347856, 0.3427123808766934, 1.0, 0.46686683978209864, 0.742967237535046, 0.7653229744918929, 0.6137897284331812, 0.33076216446219453, 0.45958011381421465, 0.6331832283319798, 0.3218978283435465, 0.33150567235697376, 1.0, 0.8677088799539515, 0.6528936925861237, 0.15202864331451144, 0.7202071754060768, 0.7369455784266192, 0.3395004042419053, 0.7159255999851685, 0.8505095547457877, 0.7380870796051389, 0.23097939314440077, 0.8610447563885792, 0.659854913089954, 0.6312590884673269, 0.27156416449157755, 0.6058662662330025, 0.5953975384906144, 0.32899869674516524, 0.8188151519994624, 0.3088358543959047, 0.8110961111430128, 0.17452169088590025, 0.8260936304650575, 0.26412686887849957, 1.0, 0.39628153425768287, 0.2775014774425143, 0.7225044929148478, 0.28719541356053996, 0.7557992754099112, 0.771971362719331, 0.91970861258227, 0.21609085121936678, 1.0, 0.7196849690792362, 0.738139867805844, 0.7459504764441578, 0.6357409625891831, 0.8663590716880905, 0.2654195484841219, 0.2558117828192494, 0.842292699197474, 0.3428502324051184, 0.7249616363996148, 0.7805351417808857, 0.6632208135509337, 0.8940560390899916, 0.7972206367497516, 0.36229508267533095, 0.7989531841146771, 0.26992032738962607, 0.798985086868838, 0.7552121767449365, 0.2432920646084837, 0.6021203335738069, 0.30343887067298414, 0.8607242266137565, 0.5164615062348044, 0.4404265030912652, 0.533967520717582, 0.8183652491683203, 0.3162846214641445, 0.9421827366463948, 0.24090780921977806, 0.7687860849271059, 0.9468414240145169, 0.22497841114201567, 0.33397802659967346, 0.7592684650585898, 0.673106131305089, 0.8899407620773474, 0.6182557079588125, 0.8200980608690891, 0.7293532020251807, 0.7703947670703939, 0.6794151141339883, 0.5267390358451736, 0.7703222712043629, 0.6961633517954895, 0.8186621220681312, 0.9336324324973175, 0.7626110076927852, 0.2989775854417271, 0.27801044753877824, 0.41793082249288105, 0.7889014571774091, 0.2111124222751568, 0.62166112251739, 0.3001907681306003, 0.8251189694211294, 0.7836654819656461, 0.6071885448403873, 0.7272514351510467, 0.7918578852887754, 0.7891096832635677, 0.8448871355655112, 0.4849466940749762, 0.3015072273605461, 0.5804463678624519, 0.6668556867136646, 0.3273412227974378, 0.31990901236041813, 0.5859059211733765, 0.6961646707977531, 0.8609128930483361, 0.36229822131277906, 0.7830011774921302, 0.7809789214708449, 0.2878380836668859, 0.8150233339421971, 0.6410378892354339, 0.6275898729431233, 0.4014212585225628, 0.6032767242167731, 0.5420424463805805, 0.8463835931264172, 0.48486886901184784, 0.6049902857272969, 0.6456391399059355, 0.7619200542504587, 0.28415822386429657, 0.3186746807893967, 0.6062472723289427, 0.6286302467743045, 0.23243790893663335, 0.25011070450455486, 0.7380332509445473, 0.30698670843371983, 0.7211648306470044, 0.22077382892582087, 0.8381282265344561, 0.7952178398348333, 0.3189758508550123, 0.7527539342948406, 0.7217298299312938, 0.9297574173227554, 0.6723521041607131, 0.32837203235706103, 0.11631556032491128, 0.34172391562542737, 0.8236528441863966, 0.29371401405573144, 0.8374226883180389, 0.26718446347937613, 0.5250144649356459, 0.7157510848420383, 0.8850230971469417, 0.7187435761703644, 0.5049055113666279, 0.2654808144885569, 0.7009592104329774, 0.33885463377124403, 0.6382076198762523, 0.7816113575802897, 0.8016313678472227, 0.8389701775936249, 0.7875223626918182, 0.8565687425019681, 0.6091842339009592, 0.8643563915310817, 0.8607498471534076, 0.6643480585380116, 0.29409552071912043, 0.6764630638499591, 0.7937539341675677, 0.7799754910804182, 0.8052180447809347, 0.9028743010640048, 0.8001554015962489, 0.08590922839630943, 0.649758753550971, 0.33568960334791836, 0.7632043034355118, 0.8143210170502313, 1.0, 0.7951925101759935, 0.2028410431960866, 0.24976250093084418, 0.776397418906814, 0.7403658679909041, 0.8342487100106875, 0.8969572961646821, 0.966410184624177, 0.844952209552059, 0.32324951293216087, 1.0]
Finish training and take 5h57m
