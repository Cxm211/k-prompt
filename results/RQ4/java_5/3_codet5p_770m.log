Namespace(log_name='./RQ5/java_5/3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_5/3_codet5p_770m', data_dir='./data/RQ5/java_5_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '}       private final PassFactory checkGlobalNames =        new PassFactory("Check names", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '}       final PassFactory checkGlobalNames =        new PassFactory("Check names", true) {      @Override      protected CompilerPass createInternal(final AbstractCompiler compiler) {'}]
***** Running training *****
  Num examples = 5
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 3
  train_loss = 36.0685
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 33.56 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:33.56
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 5
  train_loss = 21.5996
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 69.85 	 Previous best codebleu 33.56
  ********************
 Achieve Best bleu:69.85
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 7
  train_loss = 12.0267
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 73.4 	 Previous best codebleu 69.85
  ********************
 Achieve Best bleu:73.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 9
  train_loss = 2.8261
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 78.7 	 Previous best codebleu 73.4
  ********************
 Achieve Best bleu:78.7
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 11
  train_loss = 3.2807
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 80.45 	 Previous best codebleu 78.7
  ********************
 Achieve Best bleu:80.45
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 13
  train_loss = 0.8234
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 82.21 	 Previous best codebleu 80.45
  ********************
 Achieve Best bleu:82.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 15
  train_loss = 1.8749
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 81.8 	 Previous best codebleu 82.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 17
  train_loss = 0.9196
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 81.69 	 Previous best codebleu 82.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 19
  train_loss = 0.7027
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 81.62 	 Previous best codebleu 82.21
  ********************
early stopping!!!
reload model from RQ5/java_5/3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_3/test.jsonl
  codebleu = 81.97 
  Total = 500 
  Exact Fixed = 30 
[23, 36, 42, 46, 47, 53, 74, 93, 112, 130, 131, 138, 181, 222, 226, 228, 230, 237, 241, 248, 270, 273, 315, 329, 339, 348, 365, 458, 489, 500]
  Syntax Fixed = 1 
[103]
  Cleaned Fixed = 6 
[58, 83, 102, 249, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 30 
[23, 36, 42, 46, 47, 53, 74, 93, 112, 130, 131, 138, 181, 222, 226, 228, 230, 237, 241, 248, 270, 273, 315, 329, 339, 348, 365, 458, 489, 500]
  Syntax Fixed = 1 
[103]
  Cleaned Fixed = 6 
[58, 83, 102, 249, 344, 419]
  codebleu = 81.97 
[0.7016265496309229, 0.10818198593880607, 0.859259505063195, 0.5830912642712589, 0.8410886457286899, 0.9000092746228069, 0.9327424974247918, 0.7238925475243609, 0.5368075894502193, 0.8011782298988499, 0.9927616907569681, 0.8627830704763295, 0.6704850073712101, 0.3331617460870835, 0.9816061032688781, 0.831325490226771, 0.7330233520919474, 0.9342846849350264, 0.9403575354525664, 0.8673567485716778, 0.39747027364122034, 0.7747414330116673, 1.0, 0.9000120435131169, 0.6137835332542441, 0.8985819684179537, 0.8702677461904487, 0.7470449835677901, 0.6151952248646311, 0.774571731962115, 0.8961415113886637, 0.8579781671940192, 0.7321748033833095, 0.9267516766457782, 0.792956903878446, 1.0, 0.8332539670791219, 0.8768056751919203, 0.9410056506290296, 0.7467173881179517, 0.6811404084875223, 1.0, 0.7180423708279089, 0.7729994924988572, 0.7746531437200526, 1.0, 1.0, 0.976934640027229, 0.6735996268787028, 0.7366535801418781, 0.9269331830469714, 0.7746531437200526, 1.0, 0.6391051950278103, 0.944274958466798, 0.6708632535879842, 0.6797923102500931, 0.8743509502236928, 0.9327424974247918, 0.8362361439278687, 0.43069384021216905, 0.7942701366539271, 0.9125104348398385, 0.683747344676549, 0.5312731871562226, 0.9471534480091441, 0.9607233175453542, 0.843537340596747, 0.7128828030840891, 0.48144466312678136, 0.1797373256036649, 0.7836690242372425, 0.8963926483875864, 1.0, 0.9363184178206276, 0.8762476125170207, 0.712753268823352, 0.7960147595561631, 0.8970998552848286, 0.8665291872943124, 0.7621674550453241, 0.8693647831726143, 0.9327169633466876, 0.9109429421150292, 0.9852849226532605, 0.7519894248327487, 0.8957659799478881, 0.9197682751991088, 0.9488183079463388, 0.9662763734075034, 0.9745293328019206, 0.5522180470847614, 1.0, 0.9535662529996234, 0.8629423931282016, 0.8545521993874243, 0.8853701119354719, 0.8321624083223698, 0.8576027733277727, 0.8151472726544473, 0.5294563748275725, 0.987026696724848, 0.9870187411951008, 0.9470071513080363, 0.8774842286953382, 0.8853058630953912, 0.8558905963347896, 0.8813815400934406, 0.7674080702465693, 0.8841057642921581, 0.4779736897778699, 1.0, 0.931919905317208, 0.4148942459107765, 0.898122990476288, 0.9179855428506689, 0.8947233410879221, 0.9600094284589669, 0.7177333078378207, 0.23095836239158057, 0.9307233460793374, 0.9418090107609731, 0.8765208719204487, 0.9461408346030404, 0.9596411809238716, 0.7188935477431233, 0.7335237499509486, 0.932758510312542, 0.4008807324531617, 1.0, 1.0, 0.9129875032050465, 0.9498733743967636, 0.9515492621717487, 0.9310402866577385, 0.9027159738051327, 0.8684368528418116, 1.0, 0.8275239936524709, 0.6850661340471761, 0.9140032864002167, 0.7901423077149385, 0.6789308978755005, 0.9583676774082095, 0.8936973938003048, 0.7399046460615835, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.259649632516355, 0.741247170332273, 0.7680952870952458, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.944274958466798, 0.741247170332273, 0.8888435222418036, 0.657410880332295, 0.8951837826828467, 0.47030396322476276, 0.7173692510217399, 0.938879555515129, 0.8623527611587013, 0.8234385994211576, 0.8412239301694939, 0.6973583055150055, 0.2661432662669636, 0.83084241467313, 0.3499041307123179, 0.9041340728227794, 0.8884670476301071, 0.8567354045301243, 0.933286970639994, 0.8285441083529881, 0.8411299909042294, 0.9499046047578625, 0.9214852005355179, 0.6021439638609949, 0.06899015477223336, 1.0, 0.9171481786623457, 0.7676410336466819, 0.8867171706601364, 0.8077560705440479, 1.0, 0.783801630906283, 0.9142346757546067, 0.7506228403443113, 0.8711768984739177, 0.2566418909210896, 0.8820537543669489, 0.8218670339508176, 0.9465491069053984, 0.8756429067894516, 0.7779374023475432, 0.6137835332542441, 0.1809636761504411, 0.5687511804094965, 0.9156873229543501, 0.920847271632746, 0.9072725521291267, 0.8939403219941204, 0.7988848397512625, 0.8153931165528916, 0.9168635615364948, 0.9735542568395896, 0.9055256568213023, 0.6781053145008827, 0.9028547998643877, 0.798272609744273, 0.9734190821154947, 0.6268790890159027, 0.9308545853859829, 0.8357519788197898, 0.5412009653933174, 0.8516665453656083, 0.9603262193723192, 0.7945631480201435, 0.5353056139497385, 0.7170219254969812, 1.0, 0.7703813467645078, 0.9492836546885393, 0.95, 1.0, 0.8130711447701425, 1.0, 0.8801547132641002, 1.0, 0.9288817233176625, 0.7636095864061366, 0.9487145883454327, 0.823130054856027, 0.7360764107540074, 0.7585113536645014, 1.0, 0.8465816263983615, 0.7601552532840905, 0.8498388617714197, 1.0, 0.783801630906283, 0.8651706733544999, 0.7955198695646306, 0.8589960464401944, 0.90642600542328, 0.7730519120573912, 1.0, 0.98612097182042, 0.8093542785781114, 0.8444346457773062, 0.8624153196771271, 0.8761507136068414, 0.9275072085040059, 0.93092593470538, 0.5929022108454984, 0.9447883869073515, 0.8085231843686821, 0.6323785419157972, 0.566033993797615, 0.52948219795271, 0.9303179033192857, 0.9029228697393135, 0.8781617655249798, 0.8674435206185069, 0.9446955185101911, 0.8760135800491213, 0.8344531615285882, 0.8646780713427795, 1.0, 0.8027136845880052, 0.9052981975214478, 1.0, 0.3483032751734815, 0.9131034995733858, 0.9559045711960699, 0.9020987465070704, 0.853964085660508, 0.30793433386770624, 0.7039502994327855, 0.7541722826447841, 0.6130343069338204, 0.9694667888884985, 0.9396180092663717, 0.8688649632416632, 0.30793433386770624, 0.07990281507030435, 0.9035957587334096, 0.8007377532608353, 0.8290832276680398, 0.8357519788197898, 0.9045061327824662, 0.7266707228902942, 0.7814868910272839, 0.5778624281512992, 0.9205458982768264, 0.6992043393714683, 0.647747611811963, 0.9118321120568744, 0.8178334933130291, 0.7983518420066454, 0.9207265089204273, 0.20020948304203823, 0.9790427622042994, 0.9471695160234401, 0.7636060560616008, 0.841875207469964, 0.8619247199392253, 0.948052670635467, 0.7276108755888814, 0.86452394434346, 0.7757388605257256, 0.7932369845589845, 0.7728428990754905, 1.0, 0.957146397188152, 0.7274760312669724, 0.843728854340388, 0.8996252029421135, 0.8712005145728838, 0.9821490926793564, 0.8335028039626476, 0.9023057825877827, 0.7804434274762166, 0.7328516139481585, 0.8118507975002234, 0.8019309772658032, 0.8188865946509148, 1.0, 0.8501853697811779, 0.638311632426873, 0.9606383615912808, 0.8679908487707813, 0.6801949092553191, 0.8617752937444394, 0.7819028822360282, 0.9005739349163713, 0.8411630424921515, 1.0, 0.5443454451372073, 0.8649338414062606, 0.8925235787478019, 0.8377252262881923, 0.9221557993275098, 0.8979311034464716, 0.941246887425869, 0.32011135014340864, 1.0, 0.9288768296987449, 0.7846769542897543, 0.8641269634908533, 0.7571560504922936, 0.9214086041237446, 0.8696537204746508, 0.763380813154787, 0.9809329721714535, 0.942663845472012, 0.785678266679733, 0.7683279276523282, 0.7233456259389972, 0.8989415087276855, 0.934554039638529, 0.9523104352365284, 0.8627830017322995, 1.0, 0.9693877551020409, 0.872223029811398, 0.8142702926091396, 0.852734238473497, 0.8748503542446726, 0.8856179332236309, 0.4516783267135476, 0.5695758149747698, 0.7400913955844939, 0.8561251633304875, 0.8306542844684646, 0.8412123173599384, 0.9170972299520759, 0.9973062440915319, 0.8724693043209948, 0.896457542865793, 0.9120050406373192, 0.9452585311430763, 0.666856131305089, 0.6876788896774265, 0.7223713204162335, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.894141073173163, 0.9598250197069826, 0.9634911520696614, 0.8450356740663163, 0.8323441696598732, 0.8958013658804396, 0.6815374786619629, 0.9642153446749153, 0.9921681817198038, 0.7135449662005422, 0.7280460155792001, 0.9122491485294033, 0.8892976865031101, 0.7708128313590381, 0.7053249317666072, 0.885444769066634, 0.9136051415192505, 0.9060328132136453, 0.792956903878446, 0.8505395682648067, 0.6669257319650184, 0.8960971760439522, 0.9056283213354639, 0.9442100823718134, 0.6901262669982268, 0.8212324536702587, 0.8957718999086999, 0.961965530245021, 0.5687511804094965, 0.8714800909779405, 0.8985609679543393, 0.9202827191608316, 0.6410378892354339, 0.8634064000352093, 0.9936419998424288, 0.7369044591507168, 0.7716201636700017, 0.9745770345730578, 0.74212817232249, 0.7329710871364599, 0.8839836214081072, 0.9146241894400875, 0.8711768984739177, 0.9160192719604185, 0.8821296454239946, 0.7646440952537903, 0.543283237577399, 0.9237914376874523, 0.9343267218430718, 0.9350024698180224, 0.895540655184323, 0.783801630906283, 0.9714587722069028, 0.7856603086282782, 0.7567460534475294, 0.8179617939659143, 0.800083937746499, 0.9666327444353364, 0.8810554316276455, 0.785323400244213, 0.7217595067833324, 0.7366505750537401, 0.9149306320417432, 0.24938030444613957, 0.9150679539598123, 1.0, 0.6873516513487643, 0.850780171352979, 0.7300999858699463, 0.796170256960848, 0.7215961844283755, 0.8316300712381064, 0.7935461504124401, 0.9272271558703311, 0.9016850690943105, 0.8362192242070374, 0.8893509176519561, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.6472464685840424, 0.9139708530670974, 0.7576625406504057, 0.772616988243183, 0.8698869964564975, 0.904352113990756, 0.9152573407816746, 0.9771473873342402, 0.6726552887904861, 0.9275923820999126, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.6705871693225828, 0.9449149097946257, 0.9069546174929648, 1.0, 0.9049842403132857, 0.783801630906283, 0.7526397214496015, 0.9153491465912209, 0.8254935742435682, 0.679392892252542, 0.965572976935686, 0.9327424974247918, 0.9778527235265317, 0.9121508301280472, 1.0]
Finish training and take 1h8m
