Namespace(log_name='./RQ5/java_5/1_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_5/1_codet5p_220m', data_dir='./data/RQ5/java_5_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'void METHOD_1 ( ) { final TYPE_1 < TYPE_2 > VAR_1 = VAR_2 . METHOD_2 ( ) ; while ( VAR_1 . METHOD_3 ( ) ) { final int index = VAR_1 . METHOD_4 ( ) ; final TYPE_2 VAR_3 = VAR_1 . METHOD_5 ( ) ; final int count = VAR_3 . size ( ) ; final TYPE_3 VAR_4 = VAR_5 . get ( index ) ; VAR_4 . setText ( ( STRING_1 + count ) ) ; } METHOD_6 ( ) ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'void METHOD_1 ( ) { final TYPE_1 < TYPE_2 > VAR_1 = VAR_2 . METHOD_2 ( ) ; while ( VAR_1 . METHOD_3 ( ) ) { final int index = VAR_1 . METHOD_4 ( ) ; if ( ( VAR_5 . size ( ) ) > index ) { final TYPE_2 VAR_3 = VAR_1 . METHOD_5 ( ) ; final int count = VAR_3 . size ( ) ; final TYPE_3 VAR_4 = VAR_5 . get ( index ) ; VAR_4 . setText ( ( STRING_1 + count ) ) ; } } METHOD_6 ( ) ; }'}]
***** Running training *****
  Num examples = 5
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 56.875
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 14.1 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:14.1
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 59.3507
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 33.3 	 Previous best codebleu 14.1
  ********************
 Achieve Best bleu:33.3
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 30.4384
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 48.58 	 Previous best codebleu 33.3
  ********************
 Achieve Best bleu:48.58
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 23.9097
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 67.03 	 Previous best codebleu 48.58
  ********************
 Achieve Best bleu:67.03
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 17.9401
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 71.64 	 Previous best codebleu 67.03
  ********************
 Achieve Best bleu:71.64
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 11.3313
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 73.01 	 Previous best codebleu 71.64
  ********************
 Achieve Best bleu:73.01
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 10.0201
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 74.31 	 Previous best codebleu 73.01
  ********************
 Achieve Best bleu:74.31
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 6.7064
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 75.19 	 Previous best codebleu 74.31
  ********************
 Achieve Best bleu:75.19
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 10
  train_loss = 6.5616
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 75.95 	 Previous best codebleu 75.19
  ********************
 Achieve Best bleu:75.95
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 11
  train_loss = 5.0296
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 76.16 	 Previous best codebleu 75.95
  ********************
 Achieve Best bleu:76.16
  ********************
reload model from RQ5/java_5/1_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_1/test.jsonl
  codebleu = 76.28 
  Total = 500 
  Exact Fixed = 1 
[265]
  Syntax Fixed = 1 
[15]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 1 
[265]
  Syntax Fixed = 1 
[15]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  codebleu = 76.28 
[0.7016265496309229, 0.4232240156531161, 0.8344610480137715, 0.5393370720549331, 0.8410886457286899, 0.9000092746228069, 0.20028952928854593, 0.7238925475243609, 0.3068923552159469, 0.8011782298988499, 0.9927616907569681, 0.8593411581323525, 0.6290876190588052, 0.7308269110881658, 0.9720397321741139, 0.8189673446405537, 0.7523277398705996, 0.9342846849350264, 0.949278746782509, 0.8673567485716778, 0.755766537798902, 0.2982968846469775, 0.3213989993739358, 0.9000120435131169, 0.6113472882852959, 0.8985819684179537, 0.7944328442208259, 0.7715641063718415, 0.6151952248646311, 0.7984153870943118, 0.8961415113886637, 0.8579781671940192, 0.7321748033833095, 0.2825835857853979, 0.5118259902024153, 0.3226065051590042, 0.9377704204852682, 0.845119220537944, 0.9410056506290296, 0.7467173881179517, 0.6940078756137626, 0.32457202897753634, 0.9192393156087896, 0.8345641796204135, 0.4955221217337541, 0.24811250672041774, 0.3231806389404378, 0.976934640027229, 0.9061537357063023, 0.7366535801418781, 0.9269331830469714, 0.4338121251386362, 0.20463683557067192, 0.6212060353639447, 0.7535928535782062, 0.6431739117516077, 0.6769389579088759, 0.8855924534744011, 0.20028952928854593, 0.7739991864243869, 0.8309401076758502, 0.7942701366539271, 0.9125104348398385, 0.683747344676549, 0.7484749968253344, 0.9471534480091441, 0.9607233175453542, 0.843537340596747, 0.8475618791154588, 0.8063559408133776, 0.8468791988450306, 0.8286690242372425, 0.8963926483875864, 0.32457202897753634, 0.9442131546627328, 0.8762476125170207, 0.7197300130093984, 0.5422875137889253, 0.9233947978166772, 0.8348149633003471, 0.7621674550453241, 0.8693647831726143, 0.9327169633466876, 0.9192762754483625, 0.9852849226532605, 0.7519894248327487, 0.7421849811897088, 0.9197682751991088, 0.9488183079463388, 0.9660273425375194, 0.961485854541051, 0.20723537075992324, 0.6156159777315479, 0.9535662529996234, 0.8377478491859116, 0.2274483371342389, 0.8853701119354719, 0.9291581580911555, 0.8696882992240296, 0.8151472726544473, 0.26371538839698855, 0.3242812717706271, 0.850716301115219, 0.8962418522850135, 0.8774842286953382, 0.8853058630953912, 0.8558905963347896, 0.8813815400934406, 0.7674080702465693, 0.8619449448829922, 0.20728870208096106, 0.32240081307027274, 0.931919905317208, 0.6750181826457673, 0.898122990476288, 0.8744433206869813, 0.8947233410879221, 0.9600094284589669, 0.8287086343677358, 0.5673759842999113, 0.9068035015596566, 0.9418090107609731, 0.8765208719204487, 0.9461408346030404, 0.9596411809238716, 0.7188935477431233, 0.7335237499509486, 0.932758510312542, 0.48666825572275857, 0.3231806389404378, 0.322996252295271, 0.9129875032050465, 0.8950060624143692, 0.9515492621717487, 0.9310402866577385, 0.9027159738051327, 0.8486719420369903, 0.32392604494770494, 0.8275239936524709, 0.6850661340471761, 0.9140032864002167, 0.7901423077149385, 0.6949186256721644, 0.9492523028091309, 0.8936973938003048, 0.7399046460615835, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.23275643122052037, 0.2604853113508136, 0.9150753119697335, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.7535928535782062, 0.48919313068854076, 0.8888435222418036, 0.6578027058199973, 0.8951837826828467, 0.778090058048052, 0.7173692510217399, 0.938879555515129, 0.8748360029466259, 0.8234385994211576, 0.8412239301694939, 0.8678657374052126, 0.9312991012532417, 0.83084241467313, 0.16866768292575898, 0.9041340728227794, 0.8884670476301071, 0.8189995554735204, 0.9203809441868656, 0.7342587983930369, 0.8721212086381336, 0.9499046047578625, 0.9214852005355179, 0.6121576162732347, 0.07738628201423627, 0.3213989993739358, 0.9171481786623457, 0.6000341797847216, 0.8867171706601364, 0.8502729075207087, 1.0, 0.4955221217337541, 0.809533932004282, 0.7506228403443113, 0.5919675938577496, 0.6243976479578737, 0.8245793719163261, 0.8218670339508176, 0.9465491069053984, 0.8756429067894516, 0.7060089851548503, 0.6113472882852959, 0.06086476296609837, 0.7830011774921302, 0.8685105976184304, 0.920847271632746, 0.936752055107628, 0.8939403219941204, 0.7988848397512625, 0.5870054509531217, 0.9168635615364948, 0.9735542568395896, 0.9055256568213023, 0.6781053145008827, 0.9471695160234401, 0.798272609744273, 0.9734190821154947, 0.6268790890159027, 0.9484513298936408, 0.5394980360463638, 0.5412009653933174, 0.8516665453656083, 0.9603262193723192, 0.7945631480201435, 0.801169789534145, 0.7170219254969812, 0.3213989993739358, 0.7703813467645078, 0.9642836546885394, 0.95, 0.32232518933738535, 0.5118259902024153, 0.32331747838025604, 0.8801547132641002, 0.3231806389404378, 0.9288817233176625, 0.7636095864061366, 0.9487145883454327, 0.8054829960324975, 0.7360764107540074, 0.8017624348471263, 0.3213989993739358, 0.8465816263983615, 0.3073883688139006, 0.8132491132890458, 0.3226065051590042, 0.4419034289589019, 0.8651706733544999, 0.7931351046504085, 0.9257462023534273, 0.8921042672694857, 0.7730519120573912, 0.322996252295271, 0.32499992555402824, 0.8093542785781114, 0.9676227264894173, 0.9142150440420558, 0.8561685420099949, 0.9159167581563037, 0.93092593470538, 0.5929022108454984, 0.7235716998691815, 0.8085231843686821, 0.7527524223620757, 0.5550107088205123, 0.3079976599809352, 0.9227250567758067, 0.9029228697393135, 0.8893504469902027, 1.0, 0.9509455185101909, 0.8760135800491213, 0.8344531615285882, 0.8646780713427795, 0.32392604494770494, 0.8090966633114094, 0.9240481975214478, 0.6718197399549267, 0.800562197652117, 0.9131034995733858, 0.9559045711960699, 0.24417117664266244, 0.32249297810474364, 0.30793433386770624, 0.7036991679288631, 0.9005633971565483, 0.20252939141494047, 0.9694667888884985, 0.9396180092663717, 0.8785395193033665, 0.30793433386770624, 0.7789657345158767, 0.9035957587334096, 0.8210987179039277, 0.9351406123825403, 0.5394980360463638, 0.9045061327824662, 0.6435144749988901, 0.2867874285320503, 0.5778624281512992, 0.9205458982768264, 0.6992043393714683, 0.647747611811963, 0.8843767323807665, 0.8178334933130291, 0.9384313483327684, 0.8591474563888408, 0.2139655337688696, 0.9790427622042994, 0.8357633038269374, 0.7888397714622057, 0.8457765995899758, 0.9234334473517907, 0.948052670635467, 0.7589638512769085, 0.8673254984276466, 0.7526619374488026, 0.7932369845589845, 0.8306850294481685, 0.32331747838025604, 0.957146397188152, 0.8549033669907233, 0.7613674457954455, 0.9198221788234886, 0.8712005145728838, 0.9821490926793564, 0.8335028039626476, 0.8919747869193024, 0.7877605006469484, 0.7107965845401162, 0.9437628092763377, 0.8019309772658032, 0.810416959494236, 0.3251535088891, 0.8501853697811779, 0.6622488093781844, 0.9606383615912808, 0.9204440155861249, 0.651002857872442, 0.848321700198782, 0.5118259902024153, 0.9005739349163713, 0.8411630424921515, 0.32392604494770494, 0.3256360126355407, 0.8861708022314685, 0.8925235787478019, 0.8377252262881923, 0.9221557993275098, 0.8979311034464716, 0.941246887425869, 0.3597554292633871, 0.322996252295271, 0.9288768296987449, 0.7989253595388506, 0.8641269634908533, 0.7463083244055686, 0.9116580305459623, 0.8696537204746508, 0.27746815000223046, 0.9809329721714535, 0.9490468241954162, 0.785678266679733, 0.9423575001497, 0.729714905094921, 0.8989415087276855, 0.8608244486947461, 0.9030791576151858, 0.8627830017322995, 0.20463683557067192, 0.9693877551020409, 0.8492858358166735, 0.8478956043638872, 0.8610675718068304, 0.8859614653557839, 0.9422001488455178, 0.9222725521291268, 0.5695758149747698, 0.7400913955844939, 0.8029545622763895, 0.3361664691432363, 0.3245539117391404, 0.9170972299520759, 0.9973062440915319, 0.9468414240145169, 0.896457542865793, 0.9063324053044334, 0.9452585311430763, 0.6831381239952166, 0.9135566705708424, 0.6723713204162336, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.8802784605138214, 0.9598250197069826, 0.9634911520696614, 0.8712775503716599, 0.8323441696598732, 0.8891624595397802, 0.665961247437309, 0.9455523507964676, 0.9921681817198038, 0.7135449662005422, 0.7220094047517529, 0.9122491485294033, 0.8460725986023474, 0.7708128313590381, 0.756234192312671, 0.8523721266192412, 0.9136051415192505, 0.9060328132136453, 0.5537840321604571, 0.8505395682648067, 0.6669257319650184, 0.8960971760439522, 0.8950515255524043, 0.9442100823718134, 0.6901262669982268, 0.8212324536702587, 0.8957718999086999, 0.961965530245021, 0.7830011774921302, 0.9314651191074765, 0.8917894816720128, 0.9202827191608316, 0.6410378892354339, 0.8919778286066379, 0.9936419998424288, 0.7446841036831817, 0.7915314290630997, 0.9745770345730578, 0.74212817232249, 0.47414181167545866, 0.8839836214081072, 0.9146241894400875, 0.5919675938577496, 0.7537923022577113, 0.8093245396552173, 0.8287194092123789, 0.5522812488118628, 0.8970086390968797, 0.9294814956406305, 0.9350024698180224, 0.895540655184323, 0.4338121251386362, 0.9317654829271134, 0.883136773482629, 0.852842712474619, 0.7745488202426345, 0.800083937746499, 0.9666327444353364, 0.8810554316276455, 0.727440512177434, 0.7229151142178896, 0.7366505750537401, 0.9149306320417432, 0.7177920167829188, 0.9150679539598123, 0.320046641013223, 0.4765881909498342, 0.860780171352979, 0.9133731397684426, 0.8243317050386542, 0.7065961844283755, 0.7538576425176133, 0.7935461504124401, 0.9272271558703311, 0.9016850690943105, 0.8362192242070374, 0.8454786755847934, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.8713421516632174, 0.9139708530670974, 0.3321941761439004, 0.7658255097376656, 0.8613514421762498, 0.904352113990756, 0.9152573407816746, 0.9471414756866046, 0.9265716075901909, 0.3213999891631392, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.890490591592302, 0.9449149097946257, 0.9069546174929648, 0.6718197399549267, 0.9049842403132857, 0.4955221217337541, 0.775230894524044, 0.8950591510527908, 0.8254935742435682, 0.8514674628624775, 0.965572976935686, 0.1967358113275861, 0.9244093879104769, 0.9121508301280472, 0.322996252295271]
Finish training and take 3h44m
