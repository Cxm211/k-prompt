Namespace(log_name='./RQ5/java_5/1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_5/1_codet5p_770m', data_dir='./data/RQ5/java_5_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'void METHOD_1 ( ) { final TYPE_1 < TYPE_2 > VAR_1 = VAR_2 . METHOD_2 ( ) ; while ( VAR_1 . METHOD_3 ( ) ) { final int index = VAR_1 . METHOD_4 ( ) ; final TYPE_2 VAR_3 = VAR_1 . METHOD_5 ( ) ; final int count = VAR_3 . size ( ) ; final TYPE_3 VAR_4 = VAR_5 . get ( index ) ; VAR_4 . setText ( ( STRING_1 + count ) ) ; } METHOD_6 ( ) ; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'void METHOD_1 ( ) { final TYPE_1 < TYPE_2 > VAR_1 = VAR_2 . METHOD_2 ( ) ; while ( VAR_1 . METHOD_3 ( ) ) { final int index = VAR_1 . METHOD_4 ( ) ; if ( ( VAR_5 . size ( ) ) > index ) { final TYPE_2 VAR_3 = VAR_1 . METHOD_5 ( ) ; final int count = VAR_3 . size ( ) ; final TYPE_3 VAR_4 = VAR_5 . get ( index ) ; VAR_4 . setText ( ( STRING_1 + count ) ) ; } } METHOD_6 ( ) ; }'}]
***** Running training *****
  Num examples = 5
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 3
  train_loss = 57.3336
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 33.08 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:33.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 5
  train_loss = 27.9677
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 76.4 	 Previous best codebleu 33.08
  ********************
 Achieve Best bleu:76.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 7
  train_loss = 17.8795
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 80.33 	 Previous best codebleu 76.4
  ********************
 Achieve Best bleu:80.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 9
  train_loss = 8.8743
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 76.35 	 Previous best codebleu 80.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 11
  train_loss = 20.1917
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 74.87 	 Previous best codebleu 80.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 13
  train_loss = 7.6323
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_1/validation.jsonl
  codebleu-4 = 74.9 	 Previous best codebleu 80.33
  ********************
early stopping!!!
reload model from RQ5/java_5/1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_1/test.jsonl
  codebleu = 79.11 
  Total = 500 
  Exact Fixed = 1 
[178]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[58, 344]
  ********************
  Total = 500 
  Exact Fixed = 1 
[178]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[58, 344]
  codebleu = 79.11 
[0.7016265496309229, 0.598956520875984, 0.78947988921951, 0.5830912642712589, 0.8108189699250574, 0.9000092746228069, 0.966410184624177, 0.6966528382511771, 0.7422417992509134, 0.8011782298988499, 0.9765637284540518, 0.8627830704763295, 0.543744840659554, 0.7551554229834665, 0.8815394853084986, 0.831325490226771, 0.6761369386757781, 0.9342846849350264, 0.9125345078691123, 0.8673567485716778, 0.3517432005524611, 0.750544039098089, 0.9438472935309958, 0.9000120435131169, 0.6113472882852959, 0.8956609927523811, 0.8197843289967022, 0.715627616912556, 0.5976962286164679, 0.8028172817807042, 0.8570080513781344, 0.8579781671940192, 0.6722480632530204, 0.9267516766457782, 0.46815082712926215, 0.947403495661826, 0.7303615661254657, 0.8768056751919203, 0.9410056506290296, 0.7467173881179517, 0.6940078756137626, 0.651681541847524, 0.9192393156087896, 0.7805327916332045, 0.45554737673469164, 0.23954761826591986, 0.5747797672895879, 0.976934640027229, 0.9470228248649237, 0.7224244119553702, 0.9269331830469714, 0.45554737673469164, 0.947403495661826, 0.6459966504031366, 0.8198090264095454, 0.9473839944401292, 0.6887538164206723, 0.8855924534744011, 0.966410184624177, 0.7500089745010972, 0.8309401076758502, 0.7942701366539271, 0.9094559456673585, 0.721247344676549, 0.49515413154298216, 0.9134227687864559, 0.9222899096708426, 0.8154345035141883, 0.8215894539265881, 0.8063559408133776, 0.8875485500759485, 0.6344424621148281, 0.8963926483875864, 0.651681541847524, 0.9363184178206276, 0.8762476125170207, 0.7167405472833845, 0.7621872173032606, 0.8562065575600153, 0.8056710335298048, 0.7621674550453241, 0.8693647831726143, 0.9057402942883743, 0.9027337676249234, 0.9852849226532605, 0.7519894248327487, 0.8505327617132828, 0.8877551671045978, 0.899589925779442, 0.9662763734075034, 0.961485854541051, 0.5433053301010067, 0.8901987516303911, 0.9535662529996234, 0.8538020591665474, 0.6305247490329959, 0.8853701119354719, 0.7934323321923333, 0.8576027733277727, 0.8151472726544473, 0.618776212874501, 0.874048446432004, 0.9277295522431621, 0.9147295906449147, 0.8688106732210411, 0.8692331238227045, 0.8447228975598562, 0.8813815400934406, 0.7720861332093774, 0.8841057642921581, 0.6356747935191499, 0.6747584649244471, 0.8896134347474498, 0.680424467207345, 0.5822329705608607, 0.6477570903655119, 0.8947233410879221, 0.9600094284589669, 0.707408833959453, 0.5521363687462473, 0.9456202538775613, 0.7846350604011825, 0.8765208719204487, 0.868520241483107, 0.9596411809238716, 0.6957106509197211, 0.735152910228607, 0.932758510312542, 0.3967261780553253, 0.651681541847524, 0.9438472935309958, 0.9129875032050465, 0.9498733743967636, 0.9515492621717487, 0.7227085362072951, 0.7991442403393788, 0.8684368528418116, 0.5747797672895879, 0.8025837669991328, 0.6850661340471761, 0.8381133298286609, 0.6834717221734773, 0.6975644599335493, 0.9583676774082095, 0.8555613027817166, 0.7330436453324262, 0.9584247004495543, 0.6708135801152272, 0.8388475422356194, 0.2540423330029101, 0.6632276284673109, 0.7022619640013154, 0.892263334227472, 0.7395840494962611, 0.5703803226836399, 0.8198090264095454, 0.7225867052166064, 0.6857526814392924, 0.6614443077211676, 0.834234292378874, 0.78310584232355, 0.7173692510217399, 0.9209893793099855, 0.8661975448314093, 0.7581259198994001, 0.7079054849763758, 0.7011610065299363, 0.42328799015396346, 0.83084241467313, 0.07377237193219775, 0.9041340728227794, 0.8754463556825656, 0.8511540616297135, 0.933286970639994, 0.8285441083529881, 0.8202798291220228, 0.9514950117349144, 1.0, 0.6021439638609949, 0.07823761299435755, 0.9438472935309958, 0.9047050060193129, 0.7042104138501502, 0.8867171706601364, 0.7102690312858229, 1.0, 0.45554737673469164, 0.9349243309270205, 0.49338165704058917, 0.8711768984739177, 0.2476798302272027, 0.8279089615896092, 0.7709273070541807, 0.9465491069053984, 0.8641206218351511, 0.8094493933668323, 0.6113472882852959, 0.1809636761504411, 0.7830011774921302, 0.9156873229543501, 0.920847271632746, 0.9072725521291267, 0.8939403219941204, 0.7227434409927289, 0.4241253179337926, 0.9168635615364948, 0.846250555847087, 0.9055256568213023, 0.6781053145008827, 0.9471695160234401, 0.7574045123158479, 0.9734190821154947, 0.6210624389742789, 0.9484513298936408, 0.7898031455768002, 0.5412009653933174, 0.8231612827455108, 0.9603262193723192, 0.7782344155463395, 0.14222222222222222, 0.7228087949400654, 0.9438472935309958, 0.7703813467645078, 0.9492836546885393, 0.95, 0.5747797672895879, 0.4695859876040583, 0.947403495661826, 0.8801547132641002, 0.5747797672895879, 0.832992463853157, 0.7636095864061366, 0.9487145883454327, 0.8054829960324975, 0.6490358154102875, 0.786536430810245, 0.9438472935309958, 0.7388601329850621, 0.7922775391711969, 0.8299980832506018, 0.947403495661826, 0.45554737673469164, 0.8651706733544999, 0.800525338515579, 0.9002332916734741, 0.8925983506076327, 0.7730519120573912, 0.9438472935309958, 0.9310037632474744, 0.8093542785781114, 0.9676227264894173, 0.8624153196771271, 0.8613273617751571, 0.9275072085040059, 0.93092593470538, 0.5763661264003828, 0.8747736339540747, 0.8157300536317346, 0.8154702514101111, 0.5587417415347213, 0.4949190992043331, 0.9303179033192857, 0.9029228697393135, 0.8649654156925443, 0.5279921590489992, 0.9446955185101911, 0.7380892896849034, 0.7354224022998022, 0.8646780713427795, 0.651681541847524, 0.7903396658006667, 0.5690925961315929, 0.947403495661826, 0.8619963786560572, 0.74373863961227, 0.9559045711960699, 0.9455342747735904, 0.8462032014313217, 0.30793433386770624, 0.7148409689365389, 0.9005633971565483, 0.6229661941006033, 0.961123725345856, 0.9252596868444294, 0.9133094076861077, 0.30793433386770624, 0.7526516494838915, 0.8383573996260072, 0.8007377532608353, 0.888538119547033, 0.7898031455768002, 0.9045061327824662, 0.15151990453533423, 0.23290089542675263, 0.6285578714560316, 0.9205458982768264, 0.6992043393714683, 0.6384487608342887, 0.8422267841020679, 0.7872960011332977, 0.9220636549647714, 0.8591474563888408, 0.11959261785449568, 0.9790427622042994, 0.7375619410974997, 0.7043022870755097, 0.8328707697883151, 0.9234334473517907, 0.8568164816944417, 0.6782596415213834, 0.8884049563995104, 0.7757388605257256, 0.7932369845589845, 0.9012977329022187, 0.6913141824311085, 0.8844722782261099, 0.8342119346139885, 0.8478896852982588, 0.91382009996624, 0.8712005145728838, 0.9821490926793564, 0.7464308431177984, 0.8971060408615139, 0.7727656059816277, 0.7249106544474093, 0.803468777725549, 0.8019309772658032, 0.8042409415109377, 0.9438472935309958, 0.7882392371225049, 0.6497068643706136, 0.9352654750819369, 0.915359269823413, 0.6791382906267537, 0.8926703816314219, 0.46815082712926215, 0.8201644745271046, 0.8286266718738597, 0.9438472935309958, 0.33931181719523845, 0.7637802620604062, 0.8925235787478019, 0.8377252262881923, 0.9221557993275098, 0.8979311034464716, 0.941246887425869, 0.31410012537935006, 0.9438472935309958, 0.9288768296987449, 0.7903013359693134, 0.8429699459955828, 0.7571560504922936, 0.9214086041237446, 0.8531205107407362, 0.7472686199693764, 0.9809329721714535, 0.9006241538941291, 0.7673259834881254, 0.9423575001497, 0.7233456259389972, 0.8989415087276855, 0.9076691773362666, 0.9523104352365284, 0.8627830017322995, 0.947403495661826, 0.9693877551020409, 0.8775169678209718, 0.7569641847555484, 0.7963429588805231, 0.8748503542446726, 0.9347001488455178, 0.936752055107628, 0.5581919876639003, 0.7262415011735316, 0.8561251633304875, 0.7788869621504149, 0.5547987371335956, 0.8870110715942725, 0.9973062440915319, 0.8465575171152397, 0.896457542865793, 0.9120050406373192, 0.9064448569214978, 0.7032463562196102, 0.863994688563068, 0.7023713204162336, 0.9684326167255213, 0.7868707350252664, 0.9973062440915319, 0.8313692681610512, 0.6682225093889983, 0.7373864941132874, 0.8114414573796785, 0.946955540973119, 0.9634911520696614, 0.8712775503716599, 0.8323441696598732, 0.8711249838905324, 0.72775247433702, 0.9174878346674353, 0.9848766537342264, 0.7135449662005422, 0.7280460155792001, 0.8959113683826374, 0.9548431641738979, 0.7708128313590381, 0.7224588281878774, 0.8001181483197561, 0.8966782990287928, 0.890233691528284, 0.46815082712926215, 0.818009014320639, 0.6592579467954058, 0.8960971760439522, 0.8950515255524043, 0.9442100823718134, 0.6675732262170883, 0.7952222473237391, 0.8655583066318011, 0.961965530245021, 0.7830011774921302, 0.8944671595186228, 0.8915664007486455, 0.8306846328444892, 0.6410378892354339, 0.7599022281677876, 0.8518391018422911, 0.7125345052628644, 0.5107005450521716, 0.9487444656008632, 0.7137719024524015, 0.711791739177658, 0.7399794098633764, 0.9146241894400875, 0.8711768984739177, 0.7650437269742527, 0.5995767684283967, 0.8068372263029933, 0.5351130312675151, 0.7781026512782419, 0.897281419648706, 0.9236173657153941, 0.895540655184323, 0.46700301761657115, 0.863356251548834, 0.7736187989994616, 0.882842712474619, 0.8179617939659143, 0.7670599061274495, 0.9666327444353364, 0.8810554316276455, 0.7884440104715674, 0.7229151142178896, 0.7240459224934143, 0.9149306320417432, 0.7340847238599478, 0.9150679539598123, 0.9438472935309958, 0.6208140116111884, 0.7969018169162261, 0.9133731397684426, 0.7604940991209698, 0.6036486334145571, 0.8557322762679598, 0.7935461504124401, 0.8856330191296102, 0.9016850690943105, 0.8277597422498504, 0.8108995328282788, 0.8325509095336996, 0.9210210949858122, 0.8125772005678875, 0.8713421516632174, 0.8750771435700381, 0.5081957994011823, 0.782378936960243, 0.8026247468459797, 0.904352113990756, 0.9152573407816746, 0.8702683725969738, 0.8350452734755971, 0.5643947919062949, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.7601048472501237, 0.9449149097946257, 0.8261182827973954, 0.947403495661826, 0.8208393870096551, 0.45554737673469164, 0.6846881437446781, 0.724781322161187, 0.8254935742435682, 0.8475154492587291, 0.876330339851968, 0.6852000023630597, 0.9175120682578792, 0.9121508301280472, 0.9438472935309958]
Finish training and take 48m
