Namespace(log_name='./RQ5/java_5/2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_5/2_codet5p_770m', data_dir='./data/RQ5/java_5_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'boolean isAssignedOnceInLifetime() {        for (BasicBlock block = ref.getBasicBlock();             block != null; block = block.getParent()) {          if (block.isFunction) {            break;          } else if (block.isLoop) {            return false;', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'boolean isAssignedOnceInLifetime() {        for (BasicBlock block = ref.getBasicBlock();             block != null; block = block.getParent()) {          if (block.isFunction) {           if (ref.getSymbol().getScope() != ref.scope) {             return false;           }            break;          } else if (block.isLoop) {            return false;'}]
***** Running training *****
  Num examples = 5
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 3
  train_loss = 67.19
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 34.45 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:34.45
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 5
  train_loss = 36.7029
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 58.39 	 Previous best codebleu 34.45
  ********************
 Achieve Best bleu:58.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 7
  train_loss = 15.0006
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 81.15 	 Previous best codebleu 58.39
  ********************
 Achieve Best bleu:81.15
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 9
  train_loss = 15.2948
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 82.84 	 Previous best codebleu 81.15
  ********************
 Achieve Best bleu:82.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 11
  train_loss = 9.244
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 84.57 	 Previous best codebleu 82.84
  ********************
 Achieve Best bleu:84.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 13
  train_loss = 2.5677
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 83.69 	 Previous best codebleu 84.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 15
  train_loss = 4.8041
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 84.37 	 Previous best codebleu 84.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 17
  train_loss = 1.4758
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 85.17 	 Previous best codebleu 84.57
  ********************
 Achieve Best bleu:85.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 19
  train_loss = 1.6789
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 85.29 	 Previous best codebleu 85.17
  ********************
 Achieve Best bleu:85.29
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 21
  train_loss = 1.307
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/java_5_2/validation.jsonl
  codebleu-4 = 85.29 	 Previous best codebleu 85.29
  ********************
reload model from RQ5/java_5/2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_2/test.jsonl
  codebleu = 85.35 
  Total = 500 
  Exact Fixed = 3 
[279, 286, 415]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 3 
[279, 286, 415]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[58, 83, 344, 419]
  codebleu = 85.35 
[0.7016265496309229, 0.9263743829012012, 0.8344610480137715, 0.5830912642712589, 0.8410886457286899, 0.9000092746228069, 0.966410184624177, 0.7238925475243609, 0.7165518607362565, 0.8011782298988499, 0.9927616907569681, 0.8593411581323525, 0.6704850073712101, 0.7733456108868908, 0.9816061032688781, 0.8141826330839138, 0.7496850190481392, 0.9342846849350264, 0.9403575354525664, 0.8673567485716778, 0.8535087661549325, 0.865387748772268, 0.9422150756347474, 0.9000120435131169, 0.6577095841418729, 0.8985819684179537, 0.8702677461904487, 0.7366684698333517, 0.6151952248646311, 0.8028172817807042, 0.8961415113886637, 0.8579781671940192, 0.6844774335490393, 0.9267516766457782, 0.9289542494919141, 0.9456377568613374, 0.9490051755223876, 0.8768056751919203, 0.9410056506290296, 0.7467173881179517, 0.7011143192662692, 0.9422150756347474, 0.9449436332701645, 0.8345641796204135, 0.9217146126934004, 0.3189998940170176, 0.9422150756347474, 0.976934640027229, 0.9470228248649237, 0.7366535801418781, 0.9269331830469714, 0.9217146126934004, 0.9500416069012747, 0.8881565719741715, 0.944274958466798, 0.7867458896175421, 0.6683277907155246, 0.8743509502236928, 0.966410184624177, 0.8362361439278687, 0.8309401076758502, 0.7942701366539271, 0.9125104348398385, 0.683747344676549, 0.5312731871562226, 0.9471534480091441, 0.9607233175453542, 0.8154345035141883, 0.8629693959820761, 0.8063559408133776, 0.8875485500759485, 0.7836690242372425, 0.806131729638998, 0.9422150756347474, 0.9363184178206276, 0.8762476125170207, 0.712753268823352, 0.6578077139033633, 0.9233947978166772, 0.8665291872943124, 0.7621674550453241, 0.8693647831726143, 0.9316167713494115, 0.9109429421150292, 0.9852849226532605, 0.7519894248327487, 0.7421849811897088, 0.9197682751991088, 0.9488183079463388, 0.9662763734075034, 0.9745293328019206, 0.5522180470847614, 0.9528697450283947, 0.9535662529996234, 0.8780453492220246, 0.8439975336408694, 0.8853701119354719, 0.9703007228404703, 0.8696882992240296, 0.8151472726544473, 0.9698138929646263, 0.9492321316796177, 0.8710283872232445, 0.9470071513080363, 0.8774842286953382, 0.8853058630953912, 0.8558905963347896, 0.8813815400934406, 0.7374080702465694, 0.8841057642921581, 0.9246060608408084, 0.9422150756347474, 0.931919905317208, 0.8043093371590795, 0.898122990476288, 0.9179855428506689, 0.8947233410879221, 0.9600094284589669, 0.7208757878471993, 0.7733435159102884, 0.9456202538775613, 0.9418090107609731, 0.8765208719204487, 0.9461408346030404, 0.9596411809238716, 0.7145470293539532, 0.7335237499509486, 0.932758510312542, 0.041266631646477876, 0.9422150756347474, 0.9422150756347474, 0.9129875032050465, 0.9498733743967636, 0.9515492621717487, 0.9310402866577385, 0.9027159738051327, 0.8684368528418116, 0.9422150756347474, 0.8275239936524709, 0.6850661340471761, 0.9140032864002167, 0.7901423077149385, 0.775729444252817, 0.9583676774082095, 0.8936973938003048, 0.7399046460615835, 0.9584247004495543, 0.6708135801152272, 0.871607281170603, 0.2651183389029663, 0.9493239030379825, 0.9558104142545454, 0.937188378818399, 0.7583556269641405, 0.95273941015886, 0.944274958466798, 0.9493239030379825, 0.8251092459754528, 0.6509845240018155, 0.8951837826828467, 0.7957091770912168, 0.7173692510217399, 0.938879555515129, 0.8623527611587013, 0.8234385994211576, 0.8412239301694939, 0.852565642646234, 0.9312991012532417, 0.83084241467313, 0.9451488841255142, 0.9041340728227794, 0.8884670476301071, 0.8567354045301243, 0.933286970639994, 0.8285441083529881, 0.9321212086381336, 0.9499046047578625, 0.9214852005355179, 0.6121576162732347, 0.36448005886192997, 0.9422150756347474, 0.9171481786623457, 0.9558104142545454, 0.8867171706601364, 0.8502729075207087, 1.0, 0.9217146126934004, 0.9499861168443409, 0.7506228403443113, 0.8748733743967635, 0.25187630334597605, 0.8820537543669489, 0.8218670339508176, 0.9465491069053984, 0.8756429067894516, 0.7060089851548503, 0.6577095841418729, 0.8503739281133087, 0.6749830550353837, 0.9156873229543501, 0.920847271632746, 0.9072725521291267, 0.8939403219941204, 0.7988848397512625, 0.9535798190943583, 0.9168635615364948, 0.9735542568395896, 0.9055256568213023, 0.6781053145008827, 0.9471695160234401, 0.798272609744273, 0.9534190821154946, 0.6268790890159027, 0.9484513298936408, 0.8640040830519085, 0.3446765455466254, 0.8516665453656083, 0.9603262193723192, 0.7945631480201435, 0.6643219118729957, 0.7170219254969812, 0.9422150756347474, 0.7703813467645078, 0.9492836546885393, 0.95, 0.9422150756347474, 0.9289542494919141, 0.9456377568613374, 0.8801547132641002, 0.9422150756347474, 0.9288817233176625, 0.7636095864061366, 0.9284791869068083, 0.8054829960324975, 0.7239298435826873, 0.8017624348471263, 0.9422150756347474, 0.8465816263983615, 0.9565292378112795, 0.8498388617714197, 0.9456377568613374, 0.9217146126934004, 0.8651706733544999, 0.7957634923159257, 0.9213959176630484, 0.8921042672694857, 0.7730519120573912, 0.9422150756347474, 0.9459070762117685, 0.8093542785781114, 0.9676227264894173, 0.9142150440420558, 0.8761507136068414, 0.9275072085040059, 0.93092593470538, 0.5929022108454984, 0.9447883869073515, 0.8085231843686821, 0.7723865012869936, 0.49732326799104165, 0.7675921786664237, 0.9303179033192857, 0.9029228697393135, 0.8781617655249798, 0.8674435206185069, 0.9446955185101911, 0.7380892896849034, 0.8344531615285882, 0.8646780713427795, 0.9422150756347474, 0.8027136845880052, 0.9052981975214478, 0.9456377568613374, 0.8619963786560572, 0.9131034995733858, 0.9559045711960699, 0.9600948805844232, 0.9621391018662697, 1.0, 0.954646559321255, 0.9005633971565483, 0.6229661941006033, 0.9694667888884985, 0.9396180092663717, 0.8910871854638854, 1.0, 0.7526516494838915, 0.9035957587334096, 0.8007377532608353, 0.8290832276680398, 0.8640040830519085, 0.9045061327824662, 0.6933373895569608, 0.19309796659677195, 0.5778624281512992, 0.9205458982768264, 0.6992043393714683, 0.647747611811963, 0.940888866830786, 0.8178334933130291, 0.9384313483327684, 0.9207265089204273, 0.15519663408218706, 0.9790427622042994, 0.9471695160234401, 0.9685667308405901, 0.841875207469964, 0.9234334473517907, 0.9147286786263675, 0.9612141455329344, 0.9406018863753673, 0.7757388605257256, 0.7932369845589845, 0.8678282088981499, 0.9456377568613374, 0.957146397188152, 0.9141000841539659, 0.843728854340388, 0.9587008191521351, 0.8712005145728838, 0.9821490926793564, 0.8335028039626476, 0.9345537795444867, 0.7804434274762166, 0.7234025659657828, 0.9437628092763377, 0.8019309772658032, 0.8188865946509148, 0.9422150756347474, 0.8816015466501335, 0.626142901900755, 0.9606383615912808, 0.8679908487707813, 0.651002857872442, 0.8926703816314219, 0.9289542494919141, 0.8677987882912936, 0.8411630424921515, 0.9422150756347474, 0.5443454451372073, 0.8793380306954786, 0.8925235787478019, 0.8377252262881923, 0.9221557993275098, 0.8979311034464716, 0.941246887425869, 0.9583676774082095, 0.9422150756347474, 0.9288768296987449, 0.8114803769129251, 0.8641269634908533, 0.7571560504922936, 0.9214086041237446, 0.8696537204746508, 0.2870242105042571, 0.9809329721714535, 0.942663845472012, 0.779144175770642, 0.9423575001497, 0.7297862281282066, 0.8989415087276855, 0.934554039638529, 0.9523104352365284, 0.8627830017322995, 0.9500416069012747, 0.9693877551020409, 0.872223029811398, 0.8251993870186702, 0.852734238473497, 0.8748503542446726, 0.9347001488455178, 0.936752055107628, 0.5838615292604841, 0.7400913955844939, 0.8561251633304875, 0.9325400085710811, 0.9636834829996033, 0.9170972299520759, 0.9973062440915319, 0.9468414240145169, 0.896457542865793, 0.9120050406373192, 0.9452585311430763, 0.7032463562196102, 0.9065499028028898, 0.7023713204162336, 0.9684326167255213, 0.7964581754092617, 0.9973062440915319, 0.8256495113593998, 0.6682225093889983, 0.8125819450296324, 0.894141073173163, 0.9598250197069826, 0.9634911520696614, 0.8712775503716599, 0.8323441696598732, 0.8958013658804396, 0.6624027115825846, 0.9642153446749153, 0.9921681817198038, 0.7135449662005422, 0.7280460155792001, 0.9122491485294033, 0.9548431641738979, 0.7708128313590381, 0.7485418846203633, 0.8978266720737866, 0.9136051415192505, 0.867274641793893, 0.9289542494919141, 0.8505395682648067, 0.6669257319650184, 0.8960971760439522, 1.0, 0.9442100823718134, 0.6901262669982268, 0.8212324536702587, 0.8957718999086999, 0.961965530245021, 0.6749830550353837, 0.9314651191074765, 0.8985609679543393, 0.9202827191608316, 0.6410378892354339, 0.8919778286066379, 0.9936419998424288, 0.8989289901708486, 0.7915314290630997, 0.9745770345730578, 0.74212817232249, 0.9514967635709539, 0.8839836214081072, 0.9146241894400875, 0.8748733743967635, 0.48076943548286705, 0.8821296454239946, 0.8068372263029933, 0.5351130312675151, 0.9797672597865348, 0.9259224226117535, 0.9350024698180224, 0.895540655184323, 0.9217146126934004, 0.9714587722069028, 0.883136773482629, 0.882842712474619, 0.8179617939659143, 0.800083937746499, 0.9666327444353364, 0.8810554316276455, 0.7837568070194644, 0.7217595067833324, 0.7366505750537401, 0.9149306320417432, 0.7698548656934749, 0.9150679539598123, 0.9422150756347474, 0.5250144649356459, 0.850780171352979, 0.9133731397684426, 0.8243317050386542, 0.7065961844283755, 0.9192814728909637, 0.7935461504124401, 0.9272271558703311, 0.9016850690943105, 0.8362192242070374, 0.8893062778978682, 0.8629938783941542, 0.9210210949858122, 0.9015200916626787, 0.9215834056802388, 0.9139708530670974, 0.9077224816006332, 0.772616988243183, 0.8355512057391163, 0.904352113990756, 0.9152573407816746, 0.9771473873342402, 0.9265716075901909, 0.8787531944300546, 0.9206598027093102, 0.7155660357121529, 0.7432816303642229, 0.890490591592302, 0.9449149097946257, 0.9069546174929648, 0.9456377568613374, 0.9049842403132857, 0.9217146126934004, 0.8487134384339086, 0.9058966001121493, 0.8254935742435682, 0.8514674628624775, 0.965572976935686, 0.966410184624177, 0.9778527235265317, 0.9121508301280472, 0.9422150756347474]
Finish training and take 1h11m
