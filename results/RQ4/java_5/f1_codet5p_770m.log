Namespace(log_name='./RQ5/java_5/f1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='java', output_dir='RQ5/java_5/f1_codet5p_770m', data_dir='./data/RQ5/java_5_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00179
  global_step = 2
  train_loss = 1.2848
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00179
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 19.38 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:19.38
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.001
  global_step = 3
  train_loss = 0.9439
  ********************
Previous best ppl:1.00179
Achieve Best ppl:1.001
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 17.02 	 Previous best codebleu 19.38
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00086
  global_step = 4
  train_loss = 0.3517
  ********************
Previous best ppl:1.001
Achieve Best ppl:1.00086
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 29.45 	 Previous best codebleu 19.38
  ********************
 Achieve Best bleu:29.45
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00079
  global_step = 5
  train_loss = 0.2839
  ********************
Previous best ppl:1.00086
Achieve Best ppl:1.00079
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 31.4 	 Previous best codebleu 29.45
  ********************
 Achieve Best bleu:31.4
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00075
  global_step = 6
  train_loss = 0.1629
  ********************
Previous best ppl:1.00079
Achieve Best ppl:1.00075
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 36.0 	 Previous best codebleu 31.4
  ********************
 Achieve Best bleu:36.0
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00073
  global_step = 7
  train_loss = 0.1164
  ********************
Previous best ppl:1.00075
Achieve Best ppl:1.00073
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 44.0 	 Previous best codebleu 36.0
  ********************
 Achieve Best bleu:44.0
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00072
  global_step = 8
  train_loss = 0.1025
  ********************
Previous best ppl:1.00073
Achieve Best ppl:1.00072
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 49.19 	 Previous best codebleu 44.0
  ********************
 Achieve Best bleu:49.19
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00071
  global_step = 9
  train_loss = 0.1162
  ********************
Previous best ppl:1.00072
Achieve Best ppl:1.00071
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 52.71 	 Previous best codebleu 49.19
  ********************
 Achieve Best bleu:52.71
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0007
  global_step = 10
  train_loss = 0.0958
  ********************
Previous best ppl:1.00071
Achieve Best ppl:1.0007
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 53.72 	 Previous best codebleu 52.71
  ********************
 Achieve Best bleu:53.72
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.0007
  global_step = 11
  train_loss = 0.0806
  ********************
Previous best ppl:1.0007
Achieve Best ppl:1.0007
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 54.39 	 Previous best codebleu 53.72
  ********************
 Achieve Best bleu:54.39
  ********************
reload model from RQ5/java_5/f1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_3/test.jsonl
  codebleu = 55.72 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 5 
[58, 83, 110, 344, 419]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 5 
[58, 83, 110, 344, 419]
  codebleu = 55.72 
[0.315233099564373, 0.2618140563616307, 0.7224662648863509, 0.4139013839089223, 0.8017839755713734, 0.7751942659648954, 0.3241218285866614, 0.6482925669271551, 0.31245065054578763, 0.7457018199401371, 0.8993314634488963, 0.7461457066331898, 0.14640686124092173, 0.3227949500287998, 0.7135122240594816, 0.1660913096398139, 0.32147603653661605, 0.8601188370424977, 0.3662914671640273, 0.7248669427451755, 0.3003283852199893, 0.3008976735796797, 0.19066297333550417, 0.7195422307059687, 0.2196642258710724, 0.32562343755927664, 0.7496626570929712, 0.5574212847359036, 0.5740994509755263, 0.33578188076425813, 0.6414813941400557, 0.6873662420545538, 0.6085642807410526, 0.30454671574714953, 0.465715924844207, 0.20646444262663335, 0.5896320160161035, 0.7311460087612476, 0.36825311623087337, 0.6702367035311789, 0.186128622173362, 0.20016447771680176, 0.33151096071474845, 0.7582679233225165, 0.2980490031264402, 0.969339732242054, 0.20016447771680176, 0.41458024701253565, 0.33229186759075535, 0.6564919628282537, 0.7953988464436343, 0.20997319429466865, 0.26992032738962607, 0.3201488303843202, 0.3289340162442905, 0.6076666578198298, 0.6277060810560003, 0.8202627647587617, 0.3241218285866614, 0.7308644506190365, 0.4116264602336949, 0.6307188510165458, 0.8193792029343525, 0.28333953454664973, 0.6122783256183761, 0.8152166851138489, 0.8639821501871383, 0.7228911483719709, 0.7319733117136124, 0.320623985684823, 0.8039215069289513, 0.7506179768038621, 0.7458759899712724, 0.20016447771680176, 0.8009059128228648, 0.6153622819676613, 0.6445124184285407, 0.5960942096485482, 0.32424724620212286, 0.35052708016320144, 0.29754925302661117, 0.667134265678519, 0.8692922404836547, 0.8431171619150728, 0.8505699755371832, 0.6280485066726383, 0.7818910308844198, 0.7724187456276157, 0.3547868912600489, 0.8313892038257795, 0.6222808116662353, 0.20703858459274463, 0.26822783896449187, 0.828406709800946, 0.2791584477835185, 0.21054413723076407, 0.4293258736107629, 0.44960565784830414, 0.7124605737954934, 0.6638449600070209, 0.6820516825604106, 0.2746996781183175, 0.7856161452051413, 0.9289939250572432, 0.8284020522463414, 0.818604796702507, 0.7583492406950604, 0.7993803894154115, 0.5482226027675533, 0.8997211496034032, 0.19850238922117022, 0.19720176343252238, 0.8004855843549328, 0.24379735375963024, 0.7440788399899874, 0.20207869563913475, 0.3703778219425394, 0.8117498885831333, 0.5018938731480711, 0.25912957745882015, 0.37228684602719314, 0.7476806793787676, 0.6464759750524514, 0.898015620382415, 0.8370524077600316, 0.6765601278342184, 0.6969222374225061, 0.5479561631975578, 0.3940118865759852, 0.20016447771680176, 0.1946602548505564, 0.8357568190102038, 0.32302290351501134, 0.927480696886938, 0.7404862801030897, 0.5659635659790536, 0.6926585068723075, 0.19738124531275592, 0.7094783559068917, 0.3353288701022899, 0.3417979082757319, 0.686042829089292, 0.2293446089226352, 0.6194247710454841, 0.7570780601549043, 0.6093567493103687, 0.842653053572799, 0.5395611196036549, 0.6285616962601648, 0.273043702536169, 0.23042586832432663, 0.31953033773767137, 0.8122008768131057, 0.3110894892369726, 0.8279924547765871, 0.3289340162442905, 0.432732422984499, 0.7753337818775026, 0.6149021387093628, 0.3690748626771091, 0.34125715987871474, 0.2222320131700418, 0.8188932995172744, 0.780026977727405, 0.7329790139314107, 0.781050385356299, 0.29865885027151795, 0.1588585090389535, 0.3422866530594614, 0.09412426027544815, 0.738590487779091, 0.8471568278354424, 0.7435606041548073, 0.378316891512126, 0.7662036643795969, 0.3259431425651613, 0.8790904185947994, 0.5972283996791087, 0.3702285130210081, 0.0718081142207486, 0.19066297333550417, 0.8030087262966967, 0.5985517960454418, 0.32724878343382635, 0.7397251267678653, 0.38274515845774787, 0.20282713830471816, 0.31988965413724046, 0.34090508695217475, 0.33417278390578753, 0.251735951036083, 0.7390763177282046, 0.37444714365566395, 0.3725695157723538, 0.3306745936983033, 0.4793497753583987, 0.2196642258710724, 0.03502263361773639, 0.5187014075924715, 0.8669569460602955, 0.8373887697347597, 0.9072725521291267, 0.7596703448022373, 0.644305105272672, 0.5499800790926882, 0.2818142849343036, 0.9511489077184989, 0.8520881604630502, 0.6107313934319497, 0.9471695160234401, 0.6151756185522186, 0.9219870730414177, 0.5239015601562096, 0.8483909315531839, 0.17812455091207519, 0.3410029105745729, 0.8195828071607292, 0.8471311821354086, 0.2139751598263252, 0.6643219118729957, 0.5930698655413258, 0.19066297333550417, 0.7103676483086313, 0.9275038691229298, 0.7583609709353714, 0.19704981620752984, 0.46815082712926215, 0.2044899283896088, 0.7663246823325051, 0.20016447771680176, 0.8115270103245333, 0.6208716301688832, 0.3184943230303296, 0.7595078441336764, 0.6677175969628313, 0.8165963509339259, 0.19183390087752822, 0.7703047559758772, 0.3105840961832109, 0.7495693960741996, 0.20646444262663335, 0.2210208037568644, 0.29126561054639855, 0.7177398667979686, 0.842420466165364, 0.7560928611949997, 0.6160079286053539, 0.1946602548505564, 0.21718562782810436, 0.5923336837055172, 0.7940927628104544, 0.9142150440420558, 0.28655177962789163, 0.8131280406739858, 0.9104948835879048, 0.5032931854692144, 0.5927467743474968, 0.5328342422835319, 0.30218534052969115, 0.4834727614939841, 0.15185584687918416, 0.853140817342724, 0.7428177153679325, 0.36604173041677696, 0.09819530443351764, 0.3457038805765606, 0.6838773195839767, 0.7591895697971243, 0.35043426110262776, 0.19686097753781215, 0.4558216729055823, 0.3344932909281363, 0.20167138921350353, 0.17565799327769385, 0.804363294869249, 0.8338530280205303, 0.29066855074621706, 0.22992221425032433, 0.17623651328919782, 0.24561531820343405, 0.7694496510087083, 0.15103857414564845, 0.8766307456380591, 0.7161353506408099, 0.30085423386196863, 0.3419483034056606, 0.2067417574725372, 0.8381189319740585, 0.7124674132929127, 0.7962984695478379, 0.17812455091207519, 0.3214974158130396, 0.24573370884449913, 0.19457462028156056, 0.5735283309621977, 0.27668367586990217, 0.5168198736262695, 0.5755233043849655, 0.8015678197602771, 0.6891782624879149, 0.8712818103255136, 0.9207265089204273, 0.45947738521823683, 0.3441584061286233, 0.7398893252738135, 0.467541060892978, 0.7952451538152271, 0.6315504007989853, 0.8858492763314334, 0.5009217643395214, 0.46958124932078427, 0.6306336683240683, 0.32170960223687345, 0.3506470064086628, 0.20592938451012854, 0.8677088799539515, 0.05721364262430334, 0.7837753082759602, 0.7905865498731428, 0.7369455784266192, 0.6877773626094266, 0.7159255999851685, 0.8987229835300921, 0.7450148713849064, 0.690412190737297, 0.8610447563885792, 0.659854913089954, 0.6312590884673269, 0.19967888418145718, 0.6058662662330025, 0.5953975384906144, 0.05075490783725169, 0.7176978624234874, 0.5001385173166933, 0.819510223692153, 0.465715924844207, 0.8651318443369054, 0.26412686887849957, 0.20105201456950864, 0.11589375256970387, 0.7912681702693196, 0.7225044929148478, 0.7167237596778511, 0.7578539418647467, 0.771971362719331, 0.91970861258227, 0.22062381107600176, 0.1946602548505564, 0.7196849690792362, 0.7542932135949048, 0.24362477254327386, 0.6357409625891831, 0.8663590716880905, 0.26541954848412186, 0.2558117828192494, 0.8114160811050608, 0.8721876575048557, 0.723386181608951, 0.6176811624770117, 0.6632208135509337, 0.3326058882373878, 0.8384600776366871, 0.37391990073871006, 0.30434807173082856, 0.26992032738962607, 0.762325679357347, 0.7439661637295518, 0.7744813841909455, 0.6600536080160874, 0.7228819421809687, 0.8607242266137565, 0.5566401093247666, 0.4404265030912652, 0.536135426000159, 0.8183652491683203, 0.3162846214641445, 0.24136584377240014, 0.7746089875657862, 0.41972747551469114, 0.33040272351084676, 0.22497841114201567, 0.33186996365929866, 0.7952684098598067, 0.673106131305089, 0.9231744182181167, 0.6317260687505856, 0.8200980608690891, 0.7293532020251807, 0.7703947670703939, 0.14621118260935717, 0.5267390358451736, 0.7703222712043629, 0.7774816465700545, 0.9035797722087997, 0.9336324324973175, 0.7626110076927852, 0.7642418185647498, 0.27801044753877824, 0.39129669758913405, 0.8328468877464585, 0.6267167631279785, 0.62166112251739, 0.2904069795488639, 0.8213601307815548, 0.2590312163758138, 0.38979514221630207, 0.7272514351510467, 0.7918578852887754, 0.8040424242257508, 0.8427374226430995, 0.4849466940749762, 0.7478152221236214, 0.5804463678624519, 0.6668556867136646, 0.32893764199003567, 0.7962751212087769, 0.5516440483947416, 0.6961646707977531, 0.8609128930483361, 0.36208120210786326, 0.5187014075924715, 0.7701699297639746, 0.7319612136357371, 0.8150233339421971, 0.2924833162299562, 0.291818754024824, 0.8160293773371088, 0.6047684680162083, 0.21799673550964727, 0.8463835931264172, 0.6952627622434359, 0.6049902857272969, 0.6610058582236744, 0.7619200542504587, 0.17019719373015108, 0.3186746807893967, 0.8821296454239946, 0.6286302467743045, 0.23243790893663335, 0.2560500397194621, 0.8749250476050281, 0.37374439643908197, 0.7211648306470044, 0.2124114168921116, 0.898871187454698, 0.7933018204510371, 0.6072597003237161, 0.736087267628174, 0.7217298299312938, 0.9297574173227554, 0.6838905656991746, 0.3487982843092213, 0.6068269366055301, 0.5811022701610156, 0.327072363454055, 0.25329899572781717, 0.8374226883180389, 0.19686097753781215, 0.5969370398437226, 0.7845935717860968, 0.8850230971469417, 0.3213713772692359, 0.2111749917219977, 0.2699166461348297, 0.7268501559820937, 0.7722802699686481, 0.6382076198762523, 0.7816113575802897, 0.799068980115264, 0.8389701775936249, 0.7875223626918182, 0.8565687425019681, 0.6064978252468369, 0.8643563915310817, 0.23876898036757793, 0.6643480585380116, 0.8698869964564975, 0.6764630638499591, 0.7937539341675677, 0.8364201925024846, 0.5756265363180908, 0.18768125056823864, 0.8001554015962489, 0.26071247492326616, 0.6596114345858716, 0.3411271774791809, 0.7928862649902179, 0.8262056150784496, 0.20167138921350353, 0.7951925101759935, 0.22357162763817123, 0.30083946632119574, 0.8396343307418863, 0.7403658679909041, 0.8342487100106875, 0.9060544691396217, 0.2617579177082465, 0.844952209552059, 0.32324951293216087, 0.1946602548505564]
Finish training and take 5h23m
