Namespace(log_name='./RQ5/java_5/f3_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='java', output_dir='RQ5/java_5/f3_codet5p_220m', data_dir='./data/RQ5/java_5_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 0
  eval_ppl = 1.00068
  global_step = 2
  train_loss = 0.7331
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00068
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 10.46 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 1
  eval_ppl = 1.00049
  global_step = 3
  train_loss = 0.6392
  ********************
Previous best ppl:1.00068
Achieve Best ppl:1.00049
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 14.07 	 Previous best codebleu 10.46
  ********************
 Achieve Best bleu:14.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 2
  eval_ppl = 1.00043
  global_step = 4
  train_loss = 0.3148
  ********************
Previous best ppl:1.00049
Achieve Best ppl:1.00043
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 26.31 	 Previous best codebleu 14.07
  ********************
 Achieve Best bleu:26.31
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 3
  eval_ppl = 1.00041
  global_step = 5
  train_loss = 0.3173
  ********************
Previous best ppl:1.00043
Achieve Best ppl:1.00041
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 30.84 	 Previous best codebleu 26.31
  ********************
 Achieve Best bleu:30.84
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 4
  eval_ppl = 1.00038
  global_step = 6
  train_loss = 0.2316
  ********************
Previous best ppl:1.00041
Achieve Best ppl:1.00038
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 42.38 	 Previous best codebleu 30.84
  ********************
 Achieve Best bleu:42.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 5
  eval_ppl = 1.00037
  global_step = 7
  train_loss = 0.285
  ********************
Previous best ppl:1.00038
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 50.17 	 Previous best codebleu 42.38
  ********************
 Achieve Best bleu:50.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 6
  eval_ppl = 1.00037
  global_step = 8
  train_loss = 0.0785
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 53.59 	 Previous best codebleu 50.17
  ********************
 Achieve Best bleu:53.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 7
  eval_ppl = 1.00037
  global_step = 9
  train_loss = 0.0894
  ********************
Previous best ppl:1.00037
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 55.55 	 Previous best codebleu 53.59
  ********************
 Achieve Best bleu:55.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 8
  eval_ppl = 1.00037
  global_step = 10
  train_loss = 0.0625
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 56.3 	 Previous best codebleu 55.55
  ********************
 Achieve Best bleu:56.3
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 8
  epoch = 9
  eval_ppl = 1.00037
  global_step = 11
  train_loss = 0.0773
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/java_5_3/validation.jsonl
  codebleu-4 = 57.81 	 Previous best codebleu 56.3
  ********************
 Achieve Best bleu:57.81
  ********************
reload model from RQ5/java_5/f3_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/java_5_3/test.jsonl
  codebleu = 60.21 
  Total = 500 
  Exact Fixed = 2 
[237, 329]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[58, 419]
  ********************
  Total = 500 
  Exact Fixed = 2 
[237, 329]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[58, 419]
  codebleu = 60.21 
[0.7016265496309229, 0.27443698396063176, 0.678206116875189, 0.39112818855606, 0.8017839755713734, 0.8101012526737789, 0.966410184624177, 0.6482925669271551, 0.3068923552159469, 0.7457018199401371, 0.8993314634488963, 0.7297660430753208, 0.488716958756024, 0.3185694564330096, 0.6079100545534813, 0.6671250066340981, 0.4602570476885003, 0.8601188370424977, 0.3947062318531209, 0.7277313197041092, 0.8535087661549325, 0.22155723570567826, 0.26339484374474653, 0.6591638185653054, 0.5382776442994167, 0.6086311949399568, 0.6745161530942612, 0.6163673081460619, 0.5740994509755263, 0.7984153870943118, 0.6414813941400557, 0.6873662420545538, 0.5913752979102076, 0.30870936171343183, 0.4849466940749762, 0.14349066148571044, 0.9490051755223876, 0.7645481686846878, 0.3067093865763636, 0.6731590030090238, 0.17963611355360828, 0.14566801956272532, 0.3413971649551465, 0.8082679233225165, 0.4757701670966781, 0.28059959140853735, 0.14566801956272532, 0.7779120389404082, 0.8511193219345696, 0.6564919628282537, 0.34882047931217625, 0.4020839132815655, 0.14349066148571044, 0.3272282901472101, 0.25570320629243265, 0.7258893392566115, 0.6371825065524551, 0.8294121954691612, 0.966410184624177, 0.7393104356134652, 0.4188929946451023, 0.6307188510165458, 0.5949761746575479, 0.48294284539002985, 0.6881926205839866, 0.34546017974311727, 0.9228675103159067, 0.7518329266471542, 0.7319733117136124, 0.8304561534828454, 0.8875485500759485, 0.7056179768038621, 0.8214154016134527, 0.14566801956272532, 0.36164381583647776, 0.47341248294252425, 0.6779455495961997, 0.43900881747375853, 0.7592456306928997, 0.6360260155438118, 0.6344404544815491, 0.667134265678519, 0.2677805607544489, 0.8431171619150728, 0.8505699755371832, 0.6280485066726383, 0.6752355601043107, 0.7724187456276157, 0.35983624369735645, 0.8601343123148637, 0.46612744634239256, 0.2076254204373124, 0.14369770497801376, 0.828406709800946, 0.8497690400336877, 0.26029509453828886, 0.3395826039290692, 0.6027009825614271, 0.7155677885532417, 0.6638449600070209, 0.7666867901738155, 0.2716732813618859, 0.8052873181774567, 0.91317627538844, 0.8284020522463414, 0.818604796702507, 0.3104963197539697, 0.7854061653794722, 0.663092311325318, 0.8286256581082037, 0.4673784287592213, 0.14499651689173385, 0.8087684100925288, 0.3012748706720946, 0.30036370986402344, 0.24134476685534437, 0.7955598954540631, 0.8117498885831333, 0.6331438731480711, 0.8331411098199868, 0.332217254853598, 0.7851806793787677, 0.6464759750524514, 0.8750763697264827, 0.825167321220964, 0.6787558427106583, 0.6802555707558395, 0.46931872730164104, 0.48666825572275857, 0.14541536122076537, 0.14573997615913467, 0.8357568190102038, 0.3195506575207539, 0.927480696886938, 0.598957138165816, 0.26319677344405495, 0.6907319462003152, 0.14720555311498307, 0.7094783559068917, 0.5985460132137562, 0.7353020545729524, 0.6767483924650631, 0.7664023716159446, 0.8117523028091309, 0.7570780601549043, 0.6246792166404407, 0.8523136740825921, 0.5553706305752664, 0.6285616962601648, 0.2621236128143856, 0.2594915868685824, 0.27300902272050304, 0.8044385000874581, 0.6556746711014457, 0.6862855927644096, 0.25570320629243265, 0.49025574515829035, 0.7753337818775026, 0.5835824961718392, 0.39750127803986546, 0.36173831928766775, 0.4077623218455863, 0.8258882813622048, 0.7466338052646712, 0.7329790139314107, 0.7720408278336869, 0.8678657374052126, 0.9312991012532417, 0.5836520604981593, 0.126513577501894, 0.7548340382627486, 0.8561505784124128, 0.7373410951492374, 0.6236313401524359, 0.7662036643795969, 0.284012874300824, 0.8089552429754516, 0.34554382898280517, 0.37794425135743503, 0.07562712638538245, 0.26339484374474653, 0.3774732807488886, 0.6000341797847216, 0.34764349362588354, 0.6942035322886506, 0.6678967767415399, 0.4955221217337541, 0.26050288104682284, 0.29918171495468043, 0.24639409188156727, 0.23994804977779344, 0.7895448214766185, 0.29520957781669477, 0.3709656863550345, 0.7958913835066506, 0.4794267326955328, 0.5382776442994167, 0.09820353246016708, 0.7830011774921302, 0.8669569460602955, 0.8373887697347597, 0.802665482083851, 0.7969684130597641, 0.6479164016556753, 0.5799705167001106, 0.7828712498372785, 0.9511489077184989, 0.36512235851737007, 0.6541584027535726, 0.9471695160234401, 0.6151756185522186, 0.9219870730414177, 0.5239015601562096, 0.8483909315531839, 0.24005095379975158, 0.13743385843588427, 0.8195828071607292, 0.8471311821354086, 0.5880270323250234, 0.3256898103243061, 0.5930698655413258, 0.26339484374474653, 0.7103676483086313, 0.8741762120847619, 0.3651510145100101, 0.9468301376782615, 0.44634077476229317, 0.14499683217725734, 0.7494395276683508, 0.14566801956272532, 0.7842249806081718, 0.6052018797716385, 0.3184943230303296, 0.6741299683448158, 0.6690874155175593, 0.8165963509339259, 1.0, 0.6745429475720148, 0.3131659166837761, 0.7050969433793484, 0.14349066148571044, 0.4757701670966781, 0.6894914896837585, 0.7005488815491723, 0.8446086634415404, 0.7560928611949997, 0.6160079286053539, 0.14573997615913467, 0.2663391779465325, 0.5072712935510809, 0.7940927628104544, 0.9142150440420558, 0.7530691613978637, 0.819466384813053, 0.9104948835879048, 0.4951113672873962, 0.7725040894073211, 0.5385006436585963, 0.3046646793726664, 0.4834727614939841, 0.15598397903221362, 0.7332693402146129, 0.7428177153679325, 0.7852938733209828, 0.27260257043057684, 0.3457038805765606, 0.7882502594900578, 0.7591895697971243, 0.3546381841865703, 0.14805687450774188, 0.687429850954064, 0.3068926840104059, 0.14349066148571044, 0.7700784271528478, 0.804363294869249, 0.8338530280205303, 0.8232176813523622, 0.34660111413670824, 0.9439975336408695, 0.2605541538756195, 0.7694496510087083, 0.2130287600536875, 0.3782132409338611, 0.7161353506408099, 0.9133094076861077, 0.9439975336408695, 0.6879557781873078, 0.8381189319740585, 0.7124674132929127, 0.7962984695478379, 0.24005095379975158, 0.7415109501172948, 0.13921751652060338, 0.20499983354352266, 0.5735283309621977, 0.3248082296136914, 0.5308235666472325, 0.5755233043849655, 0.8015678197602771, 0.6660806926819249, 0.8712818103255136, 0.4820640446385215, 0.8132148025904986, 0.7681525464234976, 0.33605692405869025, 0.4961333772820462, 0.7952451538152271, 0.32805142690151046, 0.6511746593297703, 0.4788293436623317, 0.37444034333058024, 0.6331832283319798, 0.702947721035756, 0.34683461789558667, 0.14499683217725734, 0.82309105548606, 0.23574706114868, 0.1265210632086266, 0.7884672721218537, 0.7369455784266192, 0.6877773626094266, 0.6846204124290926, 0.8311901863097908, 0.5566526520653059, 0.6902104411450565, 0.8610447563885792, 0.6604856852777883, 0.7260210457102022, 1.0, 0.8816015466501335, 0.5912143005982238, 0.33614716791192095, 0.8269596834833635, 0.5001385173166933, 0.8637506510554597, 0.46815082712926215, 0.8511366582900484, 0.2564086913149391, 0.14805687450774188, 0.18318911859768797, 0.8257451932547546, 0.7225044929148478, 0.7167237596778511, 0.6141213895851205, 0.771971362719331, 0.91970861258227, 0.9583676774082095, 0.14573997615913467, 0.7196849690792362, 0.6353088531190157, 0.7384587839515977, 0.5650588589860007, 0.8663590716880905, 0.327177684688137, 0.27559450056550083, 0.8114160811050608, 0.8669986592629282, 0.7249616363996148, 0.7805351417808857, 0.6632208135509337, 0.8940560390899916, 0.7852738292659969, 0.327821193426125, 0.7800014062778508, 0.14349066148571044, 0.7484033038991893, 0.7517334539095855, 0.7496029721183599, 0.7320257182091405, 0.7228819421809687, 0.8607242266137565, 0.9222725521291268, 0.45400208721526436, 0.2960531231532469, 0.8805049277089647, 0.303978016524615, 0.9327026660812721, 0.7746089875657862, 0.8080097251870451, 0.9468414240145169, 0.22497841114201567, 0.6527254693532221, 0.7189934530906017, 0.673106131305089, 0.9231744182181167, 0.2739145518387767, 0.8200980608690891, 0.7209812531394335, 0.7855396578232392, 0.6794151141339883, 0.5267390358451736, 0.7703222712043629, 0.7774816465700545, 0.9035797722087997, 0.906578804126535, 0.7293311258571498, 0.7562937394396303, 0.23308984401571764, 0.42655112785220906, 0.8328468877464585, 0.35923832409171297, 0.62166112251739, 0.40556079212383545, 0.8213601307815548, 0.8762638509930809, 0.48100842274362, 0.7272514351510467, 0.8131793069362292, 0.8268323327051484, 0.8427374226430995, 0.3309425339074759, 0.35048535285470694, 0.5804463678624519, 0.6668556867136646, 0.32554383166405926, 0.7962751212087769, 0.6087825592068604, 0.6916082981499294, 0.8609128930483361, 0.4194373299123148, 0.7830011774921302, 0.7480362763120801, 0.7776681442394777, 0.8150233339421971, 0.2097944092384392, 0.23134237975021565, 0.38052434430616033, 0.8989289901708486, 0.5275636356582715, 0.8463835931264172, 0.6952627622434359, 0.4767041020593347, 0.7483831811253446, 0.32684843138587977, 0.6401389819910749, 0.2530489236659783, 0.8136624181690861, 0.6292578836055056, 0.10393583327837619, 0.9064912363271289, 0.8749250476050281, 0.35132931334884127, 0.7211648306470044, 0.4020839132815655, 0.8290654837536902, 0.7933018204510371, 0.17373093729523104, 0.7527539342948406, 0.7558054094868673, 0.9297574173227554, 0.7164457650558593, 0.37798263206299826, 0.6393076886071098, 0.33688715579667433, 0.7196685513661517, 0.27419425503392303, 0.8374226883180389, 0.14805687450774188, 0.49476448093991265, 0.5680536975978081, 0.8850230971469417, 0.3471088903009367, 0.5548209269393796, 0.33070747050350274, 0.7268501559820937, 0.7548015882539172, 0.6382076198762523, 0.7816113575802897, 0.7237371049053192, 0.7070009166825795, 0.7875223626918182, 0.8565687425019681, 0.8713421516632174, 0.6164422111009924, 0.3546009054782717, 0.6626827994664675, 0.6375363995347938, 0.6764630638499591, 0.7937539341675677, 0.85818607897772, 0.8069749328508724, 0.33193716201825973, 0.8001554015962489, 0.24552187300860284, 0.661565729556063, 0.9495565333799167, 0.7928862649902179, 0.8303674988265835, 0.14349066148571044, 0.7951925101759935, 0.5677210446832675, 0.4757907088436371, 0.8270038397188577, 0.7163097185467847, 0.8342487100106875, 0.9353225362371966, 0.3233013178841037, 0.844952209552059, 0.7978812937743411, 0.14573997615913467]
Finish training and take 33m
