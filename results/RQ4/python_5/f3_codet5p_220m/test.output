class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)
class HiveCommandClient(HiveClient):              if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:
          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):          if len(
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read_pyproject_toml      ),      is_eager=True,      callback=read
class Sanic:                  if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x
class XportReader(abc.Iterator):          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()
class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value)
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop)              return task      def __await__(self):          task = asyncio.ensure_future(self.serve_coro)
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):          if dim_size is None:          return None      if output_padding is None:          if padding == 'valid':          return None
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:
class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path:          else:              path, _, _, _ = get_filepath_or_buffer(path)
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
class HTTPRequest(HTTPMessage):          )          headers = dict(self._orig.headers)         if 'Host' not in in              headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['Host']          headers['
class TestProcessProtocol(protocol.ProcessProtocol):          self.deferred = defer.Deferred()         self.out = ''         self.err = ''         self.exitcode = None      def outReceived(self, data):
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __eq__")
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)          for task in tasks:             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING and in_workers:
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',', '')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)"
def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False
def run_script(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd, context):
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider}))          return d      def spider_is_idle(self, spider):
_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name
def validate_baseindexer_support(func_name: Optional[str] -> None:          "median",          "std",          "var",          "kurt",          "quantile",     
class BarPlot(MPLPlot):          def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))         ax.set_xticks(self.tick_pos)         ax.set_xticklabels(ticklabels)          if name is not None and self.use_index:              ax.set_xlabel(name)
def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],                                      self.tasks)          else:             return state.get_pending_tasks()      def is_trivial_worker(self, state):
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_downcast_to_dtype(agged[v] = maybe_down
class create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values().values():             use_type.__fields__[f.name]          use_type.__validators__ = original_type.__validators__      if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = ModelField(              original_type.__name__, __config__=original_type.__config          )          if PYDANT_1:          new_field = Model
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) if head != "0" else id_                      dep = "ROOT" if dep == "root" else id_                      tag = pos if tag == "_" else tag                      tag = tag + "__" if use_morphology else tag
class BaseReshapingTests(BaseExtensionTests):          result[0]          assert data[0] == data[1]          assert data[1] == data[2]          assert data[2] == data[3]          assert data[3] == data[4]          assert data[4] == data[5]          assert data[5] == data[6]          assert data[6] == data[7]          assert data[7] == data[8]          assert data[8] == data[9]          assert data[9] == data[10]          assert data[10] == data[11]          assert data[11] == data[12]          assert data[12] == data[13]          assert data[13] == data[14]          assert data[14] == data[15]          assert data[15] == data[16]          assert data[16] == data[17]          assert data[17] == data[18]          assert data[18] == data[19]          assert data[19] == data[20]          assert data[20] == data[21]          assert data[21] == data[22]          assert data[22] == data[23]          assert data[23] == data[24]          assert data[24] == data[25]          assert data[25] == data[26]          assert data[26] == data[27]          assert data[27] == data[28]          assert data[28] == data[29]          assert data[29] == data[30]          assert data[30] == data[31]          assert data[31] == data[32]          assert data[32] == data[33]          assert data[33] == data[34]          assert data[34] == data[35]          assert data[35] == data[36]          assert data[36] == data[37]          assert data[37] == data[38]          assert data[38] == data[39]          assert data[39] == data[40]          assert data[40] == data[41]          assert data[41] == data[42]          assert data[42] == data[43]          assert data[43] == data[44]          assert data[44] == data[45
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script)
def add_codes(err_cls):             class ErrorsWithCodes(object):             def __getattribute(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences)
def _isna_ndarraylike_old(obj):  return result  def notna(obj):      Detect non-missing values for an array_like object.
class _LocIndexer(_LocationIndexer):             if isinstance(labels, MultiIndex):             if isinstance(key, str): and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels - 1))
class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON              self.depth -= 1             self._lambda_arguments              return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):          if isinstance(maybe_slice, slice):              return self[maybe_slice]         taken = ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )         freq = self.freq if is_period_dtype(self) else None         assert taken.freq == freq, (taken.freq, taken)
def _isna_new(obj):          elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti) assert result.name == names[2]
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):              return v         if v.startswith(''"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',                  '\\\\': '\\\\',
class Model(BaseModel):      class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)
default 'raise'              )          new_dates = new_dates.view(DT64NS_DTYPE          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=self.freq)
 = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort) def _get_distinct_objs(objs: List[Index]) -> List[Index]:
class QuarterOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterEnd(QuarterOffset):
class ImageDataGenerator(object):              The inputs, normalized.         if self.preprocessing_function:             x = self.preprocessing_function          if self.rescale:              x *= self.rescale          if self.samplewise_center:
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask]              return ret          if is_scalar(other):
def match(command, settings):      return _search(command.stderr) or _search(command.stdout)     def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, "same" or "valid".          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'same'.
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)      def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override start_requests method instead."              )              for url in self.start_urls:                  yield self.make_requests_from
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)     executor = ProcessPoolExecutor(max_workers=worker_count)      try:          loop.run_until_complete(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_formatting(              schedule_format
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)
class base_url(url):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not re.match(r'^(?:https?:)?//', path):          return compat_urlparse.urljoin(base, path)
class _LocIndexer(_LocationIndexer):              return self._getbool_axis(key, axis=axis)          elif is_list_like_indexer(key):             if isinstance(labels, ABCMultiIndex):                 if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:                     key = list(key)                 elif isinstance(key, ABCDataFrame):                     raise NotImplementedError(                         "Indexing a MultiIndex with a DataFrame key is not "                         "implemented"                     )                 elif hasattr(key, "ndim") and key.ndim > 1:
def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)
class _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset )      elif isinstance(res, list):          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [             _prepare_response_content(item, exclude_unset )      else:          return [
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True          if self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0]
class CategoricalBlock(ExtensionBlock):          def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                             ' and multiple workers may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if is_sequence:             steps_per_epoch = len(
def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)
class YoutubeIE(YoutubeBaseInfoExtractor):          video_id = mobj.group(2)          return video_id      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters(description, duration):          if not description              return None          chapter_lines = re.findall(re.escape(description), duration)          return chapter_lines      @staticdef     def _extract_chapters
class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return the normalized (hashable/immutable) value.
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]
def _urlencode(seq, enc):      def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()      root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)
class Request(object_ref):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))
class EmptyLineTracker:                  return 0              newlines = 2              if current_line.depth                  newlines -= 1              if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth ==                  if current_line.depth
class DataFrame(NDFrame):          return new_data     def _combine_match_index(self, other, func):          if ops.should_series_dispatch(self, other, func):
def disp_trim(data, length):          if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):          return data + "\033[0m"      return data
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result
def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']              version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version."             
class LinuxHardware(Hardware):              mtab_entries.append(fields)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)          return
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")          good = self._codes != -1          if not good.all():              if skipna:
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_LOADER_CLASS',                             settings.get('SPIDER_MANAGER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())
logger = logging.getLogger(__name__)  title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text>)")  info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)
class WebSocketHandler(tornado.web.RequestHandler):          .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value)          def on_connection_close(self) -> None          if self.ws_connection:
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.path.expanduser(os.
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:
["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task
class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:
class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan
class RedirectMiddleware(BaseRedirectMiddleware):          def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) or                response.status in getattr(spider, 'handle_httpstatus_list', []):              return response          if request.method == HEAD:              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD':              if request.method == 'HEAD
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps is None:         if is_sequence:              steps = len(generator)
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return super().rename(index=index, inplace=kwargs.get("inplace"))      @Substitution(**_shared_doc_kwargs)
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return          return
class Categorical(ExtensionArray, PandasObject): Only ordered `Categoricals` have a maximum!          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises          ------          TypeError          Raises
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],)
def next_palindrome(digit_list):     high_mid = len(digit_list) // 2     low_mid = (len(digit_list) - 1) // 2     while high_mid < len(digit_list) and low_mid >= 0:         if digit_list[high_mid] == 9:             digit_list[high_mid] = 0             digit_list[low_mid] = 0             high_mid += 1             low_mid -= 1         else:             digit_list[high_mid] += 1             if low_mid
def bucketsort(arr, k):     counts = [0] * k     for x in arr     counts += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] * count)     return sorted_arr
def to_base(num, b):     result = ''     alphabet = string.digits + string.ascii_uppercase     while num > 0:         i = num % b         num = num // b         result = result + alphabet[i]     return result
def shunting_yard(tokens):     precedence = {         '+': 1,         '-': 1,         '*': 2,         '/': 2     }     rpntokens = []     opstack = []     for token in tokens:         if isinstance(token, int):         else:         while opstack and precedence[token] <= precedence[opstack[-1]]:         rpntokens.append(opstack.pop())     while opstack:         rpntokens.append(opstack.pop())     return rpntokens
