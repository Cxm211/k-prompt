class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label) 
class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len(self.outputs)] class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)         return fetched[:len
class TimedeltaIndex(                      result._set_freq("infer") return result     def _fast_union(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_intersection(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_intersection(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_difference(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_remainder(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_divmod(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_floordiv(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_mod(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_lshift(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_rshift(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_and(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_or(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_xor(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_or_(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_xor_(self, other)          if len(other) == 0:              return self.view(type(self))     def _fast_or_(self, other)          if len(other) == 0:              return self.view(type(self))     def _
def _unstack_multiple(data, clocs, fill_value=None):              result = data              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val)              clocs = [v if i > v else v - 1 for v in clocs]              return result
class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._write(s)          def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame) class Tracer:          self._
class CollectionRequirement:                  requirement = req                  op = operator.and_                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LoVersion(version), LoVersion(requirement)):                     break                 if parent and version == '*' and requirement != '*'
def dispatch_to_series(left, right, func, str_rep=None, axis=None):          assert right.index.equals(left.columns) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}         elif isinstance(right, ABCSeries): assert right.index.equals(left.index) def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in
class Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) def Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key) 
def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x) def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x) def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def flatten(arr):     for x in arr:         if isinstance(x, list):             for y in flatten(x):                 yield y         else:             yield flatten(x)def
def in_top_k(predictions, targets, k):  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                       padding='valid', data_format=None):      data_format = normalize_data_format(data_format)       x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x, data_format)      x = _preprocess_conv2d_input(x,
class Categorical(ExtensionArray, PandasObject):              if dtype == self.dtype:                  return self              return self._set_dtype(dtype) if is_integer_dtype(dtype) and self.isna().any():              msg = "Cannot convert float NaN to integer"              raise ValueError(msg)
def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) def update_add(x, increment):          The variable `x` updated.  return tf_state_ops.assign_add(x, increment) 
class Task(object):          params_str = {}          params = dict(self.get_params())         for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)         return params_str class Task(object):          params_str = {}          params = dict(self.get_params())         for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)         return params_str class Task(object):          params_str = {}          params = dict(self.get_params())         for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)         return params_str class Task(object):          params_str = {}          params = dict(self.get_params())         for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)         return params_str class Task(object):          params_str = {}          params = dict(self.get_params())         for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)         return params_str class Task(object):          params_str = {}          params = dict(self.get_params())         for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)         return params_str class Task(object):          params_str = {}          params = dict(self.
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         
def count_leading_spaces(s):  def process_list_block(docstring, starting_point, section_end,                         leading_spaces, marker):      ending_point = docstring.find('\n\n', starting_point)      block = docstring[starting_point:(None if ending_point == -1 else                                        ending_point - 1)]       docstring_slice = docstring[starting_point:section_end].replace(block, marker)      docstring = (docstring[:starting_point]).replace(docstring_slice, marker)
def whitespace(leaf: Leaf) -> str:          )              return NO      elif prev.type in OPENING_BRACKETS:             return NO      elif prev.type in CLOSING_BRACKETS:             return NO      elif prev.type in OPENING_BRACES:             return NO      elif prev.type in CLOSING_BRACES:             return NO      elif prev.type in OPENING_DOUBLE_BRACKETS:             return NO      elif prev.type in CLOSING_DOUBLE_BRACKETS:             return NO      elif prev.type in OPENING_SINGLE_BRACKETS:             return NO      elif prev.type in CLOSING_SINGLE_BRACKETS:             return NO      elif prev.type in OPENING_BRACKETS:             return NO      elif prev.type in CLOSING_BRACKETS:             return NO      elif prev.type in OPENING_DOUBLE_BRACKETS:             return NO      elif prev.type in CLOSING_DOUBLE_BRACKETS:             return NO      elif prev.type in OPENING_SINGLE_BRACKETS:             return NO      elif prev.type in CLOSING_SINGLE_BRACKETS:             return NO      elif prev.type in OPENING_BRACKETS:             return NO      elif prev.type in CLOSING_BRACKETS:             return NO      elif prev.type in OPENING_DOUBLE_BRACKETS:             return NO      elif prev.type in CLOSING_DOUBLE_BRACKETS:             return NO      elif prev.type in OPENING_SINGLE_BRACKETS:             return NO      elif prev.type in CLOSING_SINGLE_BRACKETS:             return NO      elif prev.type in OPENING_BRACKETS:             return NO      elif prev.type in CLOSING_BRACKETS:             return NO      elif prev.type in OPENING_DOUBLE_BRACKETS:             return NO      elif prev.type in CLOSING_DOUBLE_BRACKETS:             return NO      elif prev.type in OPENING_SINGLE_BRACKETS:             return NO      elif prev.type in CLOSING_SINGLE_BRACKETS:             return NO      elif prev.type in OPENING_BRACKETS:             return NO      elif prev.type in CLOSING_BRACKETS:             return NO      elif prev.type in OPENING_DOUBLE_BRACKETS:             return NO      elif prev.type in CLOSING_DOUBLE_BRACKETS:             return NO      elif prev.type in OPENING_
def reverse_linked_list(node):     prevnode = None     while node:         nextnode = node.successor          node.successor = prevnode         node = nextnode
class ExecutionEngine(object):          d = self._download(request, spider)          d.addBoth(self._handle_downloader_output, request, spider)          d.addErrback(lambda f: logger.info('Error while handling downloader output',                                             extra={'spider': spider, 'failure': f}))          d.addBoth(lambda _: slot.remove_request(request))          d.addErrback(lambda f: logger.info('Error while removing request from slot',                                             extra={'spider': spider, 'failure': f}))
class Language(object):              kwargs = component_cfg.get(name, {})              kwargs.setdefault("batch_size", batch_size)              if not hasattr(pipe, "pipe"):                 docs = _pipe(pipe, docs, kwargs)              else:                  docs = pipe.pipe(docs, **kwargs)
class CSVLogger(Callback):          if not self.writer:              class CustomDialect(csv.excel):                  delimiter = self.sep              self.writer = csv.DictWriter(self.csv_file,                                          fieldnames=['epoch'], dialect=CustomDialect)          if self.append_header:                  self.writer.writeheader()
class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))             self._viewLim.intervalx = (left, right)         if auto is not None:             self._autoscaleXon = bool(auto) class _AxesBase(m
def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,             custom_encoder=encoder,             sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(              )             return jsonable_encoder(              obj_dict,             include_none=include_none,            
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')
def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs] def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs
def uppercase_escape(s):      return re.sub(          r'\\U([0-9a-fA-F]{8})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def lowercase_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def lowercase_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-fA-F]{4})',          lambda m: compat_chr(int(m.group(1), base=16)), s)def unicode_escape(s):      return re.sub(          r'\\u([0-9a-
class Sequential(Model):                                               use_multiprocessing=use_multiprocessing) @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0):         return self.predict_generator(generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0) class Sequential(Model):                                               use_multiprocessing=use_multiprocessing) @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0):         return self.predict_generator(generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0) class Sequential(Model):                                               use_multiprocessing=use_multiprocessing) @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0):         return self.predict_generator(generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0) class Sequential(Model):                                               use_multiprocessing=use_multiprocessing) @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0):         return self.predict_generator(generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0) class Sequential(Model):                                               use_multiprocessing=use_multiprocessing) @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                             use_multiprocessing=False, verbose=0):
class RetcodesTest(LuigiTestCase):             with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):              self.run_and_expect('RequiringTask', 0)             self.run_and_expect('RequiringTask --retcode-not-run 5', 5)             self.run_and_expect('RequiringTask --retcode-not-run 6', 6)             self.run_and_expect('RequiringTask --retcode-not-run 7', 7)             self.run_and_expect('RequiringTask --retcode-not-run 8', 8)             self.run_and_expect('RequiringTask --retcode-not-run 9', 9)             self.run_and_expect('RequiringTask --retcode-not-run 10', 10)             self.run_and_expect('RequiringTask --retcode-not-run 11', 11)             self.run_and_expect('RequiringTask --retcode-not-run 12', 12)             self.run_and_expect('RequiringTask --retcode-not-run 13', 13)             self.run_and_expect('RequiringTask --retcode-not-run 14', 14)             self.run_and_expect('RequiringTask --retcode-not-run 15', 15)             self.run_and_expect('RequiringTask --retcode-not-run 16', 16)             self.run_and_expect('RequiringTask --retcode-not-run 17', 17)             self.run_and_expect('RequiringTask --retcode-not-run 18', 18)             self.run_and_expect('RequiringTask --retcode-not-run 19', 19)             self.run_and_expect('RequiringTask --retcode-not-run 20', 20)             self.run_and_expect('RequiringTask --retcode-not-run 21', 21)             self.run_and_expect('RequiringTask --retcode-not-run 22', 22)             self.run_and_expect('RequiringTask --retcode-not-run 23', 23)             self.run_and_expect('RequiringTask --retcode-not-run 24', 24)
def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0 def match_
def create_instance(objcls, settings, crawler, *args, **kwargs):      ``*args`` and ``**kwargs`` are forwarded to the constructors.      Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.      if settings is None:          if crawler is None:              raise ValueError("Specify at least one of settings and crawler.")         settings = crawler.settings      if crawler and hasattr(objcls, 'from_crawler'):         return objcls.from_crawler(crawler, *args, **kwargs)      elif hasattr(objcls, 'from_instance'):         return objcls.from_instance(objcls, *args, **kwargs)
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()          mat = numeric_df.values          if method == "pearson":             correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)          elif method == "spearman":             correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)          elif method == "kendall":             correl = libalgos.nancorr_kendall(ensure_float64(mat), minp=min_periods)
def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement) def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement) def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement) def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement) def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement) def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement) def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)] if
class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl) class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(
class Tracer:          self.target_codes = set()          self.target_frames = set()          self.thread_local = threading.local()def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__frame__)def __call__(self, function):          self.target_codes.add(function.__code__)          self.target_frames.add(function.__
class BusinessHourMixin(BusinessMixin):              if bd != 0:                 skip_bd = BusinessDay(n=bd) if not self.next_bday.is_on_offset(other):                      prev_open = self._prev_opening_time(other)
def conv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size, padding, output_padding) def deconv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size, padding, output_padding) def deconv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size, padding, output_padding) def deconv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size, padding, output_padding) def deconv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size, padding, output_padding) def deconv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size, padding, output_padding) def deconv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      return (dim_size, stride_size, kernel_size
class GalaxyCLI(CLI):          obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]         inject_data = dict(             description='your description',             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          if galaxy_type == 'role':         obj_name = context.CLIARGS['{0}_name'.format(galaxy
class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph')) class scheduler(Config):     
def possible_change(coins, total):     if total == 0:         return 1     if total < 0:         return 0     first, *rest = coins     return possible_change(coins, total - first)
class _AxesBase(martist.Artist):              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log':              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left is None:                  left = old_left         if self.get_xscale() == 'log
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              response_model_exclude_defaults=response_model_exclude_defaults,              response_model_exclude_none=response_model_exclude_none,              response_model_exclude_none_defaults=response_model_exclude_none_defaults,              response_model_exclude_none_none=response_model_exclude_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_
class BlockManager(PandasObject):                          convert=convert,                          regex=regex,                      )                     if m.any():                          new_rb = _extend_blocks(result, new_rb) else:                          new_rb.append(b)
single_quoted = (  tabsize = 8  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError(Exception): pass  class TokenError
def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator)             else:                  output_generator = generator def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator)             else:                  output_generator = generator def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator) def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator) def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator) def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator) def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get() else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator) def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_
def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (2,), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (4, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP) def test_print_tensor(self):         
TEST_MODULES = [      'tornado.test.httpclient_test',      'tornado.test.httpserver_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_test',      'tornado.test.httputil_
def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00\x00\x00') def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00\x00\x00') def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00\x00\x00') def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00\x00\x00') def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00\x00\x00') def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00\x00\x00') def write_flv_header(stream, metadata):      stream.write(b'\x12') stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x
class BaseGrouper:              if mask.any():                  result = result.astype("float64")              result[mask] = np.nan          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2          if kind == "aggregate" and self._filter_empty_groups
class StackedRNNCells(Layer):                                   '`state_size` attribute. '                                   'received cells:', cells)          self.cells = cells          super(StackedRNNCells, self).__init__(**kwargs) @property      def state_size(self):          state_size = []         for cell in self.cells[::-1]:              if hasattr(cell.state_size, '__len__'):                  state_size += list(cell.state_size) 
class HTTP1Connection(httputil.HTTPConnection):              return connection_header != "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "keep-alive"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked" or start_line.method in ("HEAD", "GET")):              return connection_header == "close"             elif ("Content-Length" in headers                or headers.get("Transfer-Encoding",
patterns = (          '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',         '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}: ',         '^lol: {file}:{line}:',         '^{file} \(line {
class OffsiteMiddleware(object):          if not allowed_domains: return re.compile('')          url_pattern = re.compile(r'^https?://.*$')          for domain in allowed_domains:             if url_pattern.match(domain):                  message = ("allowed_domains accepts only domains, not URLs. "                             "Ignoring URL entry %s in allowed_domains." % domain)                  warnings.warn(message, URLWarning)
def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new def _isna_ndarraylike_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(
def assert_series_equal(          Compare datetime-like which is comparable ignoring dtype.         check_categorical : bool, default True          Whether to compare internal Categorical exactly.         obj : str, default 'Series'          Specify object name being compared, internally used to show appropriate assertion message.
def na_value_for_dtype(dtype, compat: bool = True):      if is_extension_array_dtype(dtype):          return dtype.na_value     if (         is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype) or is_timedelta64_dtype(dtype) or is_period_dtype(dtype)  ):          return NaT      elif is_float_dtype(dtype):          return np.nan      elif is_integer_dtype(dtype):          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else:          return np.nan      else
def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required_arguments(argument_spec, module_parameters):              missing.append(k) def check_required
setup(              "options_test.cfg",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",              "static/robots.txt
class Scraper(object):                      spider=spider, exception=output.value)              else:                  logger.error('Error processing %(item)s', {'item': item},                              extra={'spider': spider, 'failure': output})
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      @cache_readonly      def _engine(self):         period = weakref.ref(self)         return self._engine_type(period, len(self))
class tqdm(object):                      l_bar_user, r_bar_user = bar_format.split('{bar}')                     l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)                     else:                      return bar_format.format(**bar_args)
def js_to_json(code):              ([{,]\s*)              ("[^"]*"|\'[^\']*\'|[a-z0-9A-Z]+)             (:\s*)             ([0-9.]+|true|false|"[^"]*"|\'[^\']*\'|\[|\{) res = re.sub(r',(\s*\])', lambda m: m.group(1), res) return res
def standardize_weights(y,      Everything gets normalized to a single sample-wise (or timestep-wise) weight array.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y: Numpy array of the targets to be weighted.          y
def unescapeHTML(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)def get_subprocess_encoding(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m
default: :rc:`scatter.edgecolors`              - 'none': No patch boundary will be drawn.              - A color or sequence of colors.             For non-filled markers, the *edgecolors* kwarg is ignored and             forced to 'face' internally.             plotnonfinite : bool, default: False             Set to plot points with nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *nonfinite *
class S3CopyToTable(rdbms.CopyToTable):          if '.' in self.table:              query = ("select 1 as table_exists "                       "from information_schema.tables "                      "where table_schema = %s and table_name = %s limit 1") else:              query = ("select 1 as table_exists "                       "from pg_table_def "                      "where tablename = %s limit 1")
def shortest_path_lengths(n, length_by_edge):     length_by_path = defaultdict(lambda: float('inf'))     length_by_path.update({(i, i): 0 for i in range(n)})     length_by_path.update(length_by_edge)     for k in range(n):         for i in range(n):             for j in range(n):                 length_by_path[i, j] = min(                     length_by_path[i, j],                     length_by_path[i, j])
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              response_model_exclude_defaults=response_model_exclude_defaults,              response_model_exclude_none=response_model_exclude_none,              response_model_exclude_none_defaults=response_model_exclude_none_defaults,              response_model_exclude_none_none=response_model_exclude_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_
def format_stdin_to_stdout(      `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to      :func:`format_file_contents`.     src = sys.stdin.read()      dst = src      try:          dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)      except:          dst = src
def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1) expected["Group"] = expected["Group_obj"].astype("category") tm.assert_frame_equal(result, expected)  def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1) expected["Group"] = expected["Group_obj"].astype("category") tm.assert_frame_equal(result, expected)  def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1) expected["Group"] = expected["Group_obj"].astype("category") tm.assert_frame_equal(result, expected)  def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1) expected["Group"] = expected["Group_obj"].astype("category") tm.assert_frame_equal(result, expected)  def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1) expected["Group"] = expected["Group_obj"].astype("category") tm.assert_frame_equal(result, expected)  def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj
class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_scalar_indexer(key, kind=kind) return super()._convert_scalar_indexer(key, kind=kind) class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_list_indexer"])      def _convert_list_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_list_indexer(key, kind=kind) return super()._convert_list_indexer(key, kind=kind) class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_scalar_indexer(key, kind=kind) return super()._convert_scalar_indexer(key, kind=kind) class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_list_indexer"])      def _convert_list_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_list_indexer(key, kind=kind) return super()._convert_list_indexer(key, kind=kind) class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_scalar_indexer(key, kind=kind) return super()._convert_scalar_indexer(key, kind=kind) class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_list_indexer"
def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              ):response = response_class( def get_request_handler
class PagedList(object):  def uppercase_escape(s):      return re.sub(          r'\\U[0-9a-fA-F]{8}',         lambda m: m.group(0).decode('unicode_escape'), s)
class Model(Container):              validation_steps: Only relevant if `validation_data`                  is a generator. Total number of steps (batches of samples) to yield from `generator` before stopping.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Integer. Maximum size for the generator queue.
array(obj, dtype=object))      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) def _isna_ndarraylike_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) def _isna_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) def _isna_ndarraylike_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):
class APIRoute(routing.Route):          self.response_model_exclude = response_model_exclude          self.response_model_by_alias = response_model_by_alias          self.response_model_exclude_unset = response_model_exclude_unset          self.include_in_schema = include_in_schema          self.response_class = response_class
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later. if isinstance(line, UnformattedLines) and line.is_comment:      yield line 
class WebSocketHandler(tornado.web.RequestHandler):          if not self._on_close_called:              self._on_close_called = True              self.on_close()
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')
class TestInsertIndexCoercion(CoercionBase):          )         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd
default: :rc:`scatter.edgecolors``collection = mcoll.Collection(                  (path,), scales,                 facecolors=colors,                 edgecolors=edgecolors,                  linewidths=linewidths,                  offsets=offsets,                  transOffset=kwargs.pop('transOffset', self.transData),
class MailSender(object):              msg = MIMEMultipart()          else:              msg = MIMENonMultipart(*mimetype.split('/', 1))          msg['From'] = self.mailfrom          msg['To'] = COMMASPACE.join(to)          msg['Date'] = formatdate(localtime(localtime=True))
class LSTMCell(Layer):                  inputs_f = inputs                  inputs_c = inputs                  inputs_o = inputs             x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i             x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f             x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c             x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o
def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script, 'push', push_upstream) def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[3].strip().partition('git ')[2] return replace_argument(command.script
class DictParameter(Parameter):      tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related      values (like a database connection config).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~FrozenOrderedDict` JSON serializable).      JSON encoder for :py:class
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or np.isnan(right_value):                      return False              else:                 if left_value != right_value:                      return False
def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing) def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                         
class MultiIndex(Index):                      indexer = self._get_level_indexer(key, level=level)             new_index = maybe_mi_droplevels(indexer, [0], drop_level)             return indexer, new_index             except TypeError:                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and isinstance(k, slice)):                  pass             if not any(isinstance(k, slice) and
9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s
class SimpleRNNCell(Layer):          self.dropout = min(1., max(0., dropout))          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.state_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:      srcbuf = io.BytesIO(src)      encoding, lines = tokenize.detect_encoding(srcbuf.readline)      newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"      srcbuf.seek(0)      with io.TextIOWrapper(srcbuf, encoding) as tiow:     return tiow.read(), encoding, newline
def srt_subtitles_timecode(seconds):  def dfxp2srt(dfxp_data):      LEGACY_NAMESPACES = (         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf1',          ]),         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf1',          ]),         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf1',          ]),         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf1',          ]),         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf1',          ]),         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf
class CannotSplit(Exception):     It holds the number of bytes of the prefix consumed before the format     control comment appeared.         unformatted_prefix = leaf.prefix[: self.consumed] return Leaf(token.NEWLINE, unformatted_prefix) class FormatOn(FormatError):  class WriteBack(Enum):      NO = 0      NO = 1      NO = 2      NO = 3      NO = 4      NO = 5      NO = 6      NO = 7      NO = 8      NO = 9      NO = 10      NO = 11      NO = 12      NO = 13      NO = 14      NO = 15      NO = 16      NO = 17      NO = 18      NO = 19      NO = 20      NO = 21      NO = 22      NO = 23      NO = 24      NO = 25      NO = 26      NO = 27      NO = 28      NO = 29      NO = 30      NO = 31      NO = 32      NO = 33      NO = 34      NO = 35      NO = 36      NO = 37      NO = 38      NO = 39      NO = 40      NO = 41      NO = 42      NO = 43      NO = 44      NO = 45      NO = 46      NO = 47      NO = 48      NO = 49      NO = 50      NO = 51      NO = 52      NO = 53      NO = 54      NO = 55      NO = 56      NO = 57      NO = 58      NO = 59      NO = 60      NO = 61      NO = 62      NO = 63      NO = 64      NO = 65      NO = 66      NO = 67      NO = 68      NO = 69      NO = 70      NO = 71      NO = 72      NO = 73      NO = 74      NO = 75      NO = 76      NO = 77      NO = 78      NO = 79      NO = 80      NO = 81      NO = 82      NO = 83      NO = 84      NO = 85      NO = 86      NO = 87      NO = 88      NO = 89      NO = 90      NO = 91      NO = 92      NO = 93      NO = 94      NO = 95      NO = 96      NO = 97      NO = 98      NO = 99      NO = 100      NO = 101      NO = 102      NO = 103      NO = 104      NO = 105      NO = 106      NO = 107      NO = 108      NO =
class TestSeriesAnalytics:          assert s.is_monotonic is False          assert s.is_monotonic_decreasing is True     def test_unstack(self):         index = MultiIndex(             levels=[["bar", "foo"], ["one", "three", "two"]],             codes=[[1, 1, 0, 0], [0, 1, 0, 2]],         )         s = Series(np.arange(4.0), index=index)         unstacked = s.unstack()         expected = DataFrame(             [[2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0,
class LinuxHardware(Hardware):          pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))          maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT          for fields in mtab_entries:              device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]
def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms): with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None) assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error) def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms): with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None) assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error) def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms): with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None) assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error) def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms): with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None) assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error) def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms): with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None) assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error) def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms): with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None) assert "TypeError: 'NoneType' object is not iterable" in to_native
class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish Shell {}'.format(version) class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish Shell {}'.format(version) class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish Shell {}'.format(version) class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish Shell {}'.format(version) class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish Shell {}'.format(version) class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish Shell {}'.format(version) class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)             version = proc.stdout.read().decode('utf-8').strip()             return u'Fish
def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype) def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype,
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max
class Conv2DTranspose(Conv2D):              output_shape,              self.strides,              padding=self.padding,             data_format=self.data_format)
class FeedExporter(object):          d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                               extra={'spider': spider}))         d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                              extra={'spider': spider, 'failure': f}))         return d      def item_scraped(self, item, spider):         d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                               extra={'spider': spider}))         d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                              extra={'spider': spider, 'failure': f}))         return d      def item_scraped(self, item, spider):         d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                               extra={'spider': spider}))         d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                              extra={'spider': spider, 'failure': f}))         return d      def item_scraped(self, item, spider):         d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                               extra={'spider': spider}))         d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                              extra={'spider': spider, 'failure': f}))         return d      def item_scraped(self, item, spider):         d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                               extra={'spider': spider}))         d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                              extra={'spider': spider, 'failure': f}))         return d      def item_scraped(self, item, spider):         d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                               extra={'spider': spider}))         d.addErrback(lambda f:
class Conv2DTranspose(Conv2D):          out_height = conv_utils.deconv_length(height,                                                stride_h, kernel_h,                                                self.padding,                                               out_pad_h)          out_width = conv_utils.deconv_length(width,                                               stride_w, kernel_w,                                               self.padding,                                               out_pad_w) if self.data_format == 'channels_first':              output_shape = (batch_size, self.filters, out_height, out_width)
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              return self         arr = operator.inv(com.values_from_object(self)) return self.__array_wrap__(arr)
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         
def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024]      src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)      try:         drv.parse(src_txt)      except ValueError as ve:         drv = driver.Driver(grammar, pytree.convert)      return drv      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024]      src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)      try:         drv.parse(src_txt)      except ValueError as ve:         drv = driver.Driver(grammar, pytree.convert)      return drv      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024]      src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)      try:         drv.parse(src_txt)      except ValueError as ve:         drv = driver.Driver(grammar, pytree.convert)      return drv      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024]      src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)      try:         drv.parse(src_txt)      except ValueError as ve:         drv = driver.Driver(grammar, pytree.convert)      return drv      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024]      src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)      try:         drv
