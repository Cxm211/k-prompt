class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False} return output.reindex(**d) class GroupBy(_GroupBy):          ).sortlevel() if self.as_index:             d = {self
class HiveCommandClient(HiveClient):          if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table)) return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0};
class BinGrouper(BaseGrouper):              ngroups,          )      @cache_readonly      def result_index(self):          if len(self.binlabels) == 0 and isna(self.binlabels[0]):          return 0
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict()) def DataFrame(ND
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=False, readable=True, allow_dash=False  ),      is_eager=True,      callback=read_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False  ),      is_eager=True,      callback=write_pyproject_toml,      is_safe=True,      allow_dash=False 
class Sanic:                  if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)def middleware(self, middleware_or_request):                     if _rn not in self.
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size) def Model(Container):                  enqueuer.start(workers=workers, max
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1)     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)     if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))     return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1)     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)     if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))     return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1)     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)     if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))     return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1)     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)     if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))     return x def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1)    
class XportReader(abc.Iterator):          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb") else:             contents = filepath_or_buffer.read() try:                 contents = contents.encode(self._encoding) except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)
class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)         super(LocalCache, self).__setitem__(key, value)
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop) return task      def __await__(self):          task = asyncio.ensure_future(self.serve_coro, loop=self.loop) return task
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None      if output_padding is None:          if padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding == padding
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator) else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = val_data
class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive" if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")          else:              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")          kwargs["open_with"] = lambda path, _: path
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())             for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
class HTTPRequest(HTTPMessage):          )          headers = dict(self._orig.headers) if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]
class TestProcessProtocol(protocol.ProcessProtocol):      def __init__(self):          self.deferred = defer.Deferred()         self.out = ''         self.err = ''          self.exitcode = None      def outReceived(self, data):          self.deferred.callback(data)      def errReceived(self, data):          self.deferred.callback(data)
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)]         return np.NaN class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __ne__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __lt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __le__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __gt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __ge__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __ne__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __lt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __le__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __gt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)             for task in tasks:             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING' and in_workers:
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)     timezone, date_str = extract_timezone(date_str)     if day_first:      date_str = date_str.replace(',', ' ')     return date_str     else:      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)     timezone, date_str = extract_timezone(date_str)     if day_first:      date_str = date_str.replace(',', ' ')     return date_str     else:      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)     timezone, date_str = extract_timezone(date_str)     if day_first:      date_str = date_str.replace(',', ' ')     return date_str     else:      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)     timezone, date_str = extract_timezone(date_str)     if day_first:      date_str = date_str.replace(',', ' ')     return date_str     else:      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)     timezone, date_str = extract_timezone(date_str)     if day_first:      date_str = date_str.replace(',', ' ')     return date_str     else:      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()             results.addError(case, exc_info)             request.callback = cb_wrapper class ContractsManager(object):          def eb_wrapper(failure):              case = _create_
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              response_model_exclude_defaults=response_model_exclude_defaults,              response_model_exclude_none=response_model_exclude_none,              response_model_exclude_none_defaults=response_model_exclude_none_defaults,              response_model_exclude_none_none=response_model_exclude_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_none_none,              response_model_exclude_none_none_defaults=response_model_exclude_none_none_defaults,              response_model_exclude_none_none_none=response_model_exclude_none_
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src} (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src} (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src} (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src} (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src} (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src} (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src
def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(
def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script(script_path, cwd='.'):          shell=run_thru_shell,         cwd=cwd      )     return proc.wait()def run_script_with_context(script_path, cwd, context):         shell=run_thru_shell,         cwd=cwd
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)         d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                              extra={'spider': spider, 'failure': f}))         return d      def spider_is_idle(self, spider):         return d      def spider_process_response(self, response, request, spider):         d = self.scraper.enqueue_scrape(response, request, spider)         d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                              extra={'spider': spider, 'failure': f}))         return d      def spider_process_request(self, request, spider):         d = self.scraper.enqueue_scrape(response, request, spider)         d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                              extra={'spider': spider, 'failure': f}))         return d
class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         try                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id) except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)
class Router(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,
def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",
class BarPlot(MPLPlot):      def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))          ax.set_xticks(self.tick_pos)          ax.set_xticklabels(ticklabels)          if name is not None and self.use_index:              ax.set_xlabel(name)
def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]  return shells.and_("vagrant up " +  machine, command.script) def match(command, settings):  def get_new_command(command, settings):     
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],                                      self.tasks) else:             return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_trivial_worker(self, state):         return state.get_pending_tasks()def is_
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype) def pivot_table(                  agged[v] = maybe_down
def create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(original_type.__name__, __config__=original_type.__config__          )for f in original_type.__fields__.values():             use_type.__fields__[f.name]
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head != "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head != "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head != "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head != "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head != "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag def read_conll
class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1] class BaseResh
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self
def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg) def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)             return result class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)             if post_processing:                  result = post_processing(result, obj.values)            
def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _isna_ndarraylike_old(obj):     return result  def notna(obj):      Detect non-ndarray-like values for an ndarray-like object. def _is
class _LocIndexer(_LocationIndexer):          if isinstance(labels, MultiIndex):                  if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] + (len(labels.levels) - 1))
class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1             return True             return False class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:             self.depth -= 1             self._lambda_arguments -= 1            
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):          if isinstance(maybe_slice, slice):              return self[maybe_slice]         taken = ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )         freq = self.freq if is_period_dtype(self)         assert taken.freq == freq, (taken.freq, freq, taken)
def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarray
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2] class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):              return v         if v.startswith(''"):              v = v[1:-1]          v = re.sub(r"\\\\|\\'|\"", lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\': '\\\\', lambda m: '\\\\':
class ModelSubclass(BaseModel):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True) def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)
def 'raise'              )          new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz) return self._simple_new(new_dates, dtype=dtype, freq=self.freq)
class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon) class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon) class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon) class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon) class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon) class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon) class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs] def get_objs_combined
class BQuarterStart(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterStart(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterStart(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )class BQuarterStart(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted,
class ImageDataGenerator(object):              The inputs, normalized.         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale          if self.samplewise_center:         x = x.reshape(x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1)         return x         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale          if self.samplewise_center:         x = x.reshape(x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1)         return x         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale          if self.samplewise_center:         x = x.reshape(x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1)         return x         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale          if self.samplewise_center:         x = x.reshape(x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1)         return x         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale          if self.samplewise_center:         x = x.reshape(x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1)         return x         if self.preprocessing_function:             x = self.preprocessing_function(x)         if self.rescale:              x *= self.rescale          if self.samplewise_center:         x = x.reshape(x.shape[0], x.shape[1], x.shape[2], x.shape[3], 1)         return x         if self.preprocessing_function:             x
def _cat_compare_op(op):              mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(self):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(self):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(self):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(self):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(self):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):             mask = (self._codes == -1) | other_codes == -1 if mask.any():                 ret[mask] = False              return ret          if is_scalar(self):             mask = (self._codes ==
def match(command, settings):      return _search(command.stderr) or _search(command.stdout) def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout) editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line')) return shells.and_(editor_call, command.script)def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout) editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line')) return shells.and_(editor_call, command.script)def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout) editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line')) return shells.and_(editor_call, command.script)def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout) editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line')) return shells.and_(editor_call, command.script)def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout) editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line')) return shells.and_(editor_call, command.script)def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout) editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line')) return shells.and_(editor_call, command.script)def get_new_command(command, settings):      m = _search(command.stderr) or _search
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, 'same' or 'valid'.             data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs         ValueError: if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an even kernel size with padding 'same' or 'valid': if using an
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it's deprecated in future Scrapy releases. "                 "Please override start_requests method instead."              )         for url in self.start_urls:                  yield self.make_requests_from  class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it's deprecated in future Scrapy releases. "                 "Please override start_requests method instead."              )         for url in self.start_urls:                  yield self.make_requests_from  class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it's deprecated in future Scrapy releases. "                 "Please override start_requests method instead."              )         for url in self.start_urls:                  yield self.make_requests_from  class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it's deprecated in future Scrapy releases. "                 "Please override start_requests method instead."              )         for url in self.start_urls:                  yield self.make_requests_from
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61):          executor = ProcessPoolExecutor(max_workers=worker_count)
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key) return series[loc] except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)
def base_url(url):  def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None     if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None
class _LocIndexer(_LocationIndexer):              return self._getbool_axis(key, axis=axis)             elif is_list_like_indexer(key):             if isinstance(labels, ABCMultiIndex):                 if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:                     key = list(key)             elif isinstance(key, ABCDataFrame):                     raise NotImplementedError(                         "Indexing a MultiIndex with a "                         "DataFrame key is not "                         "implemented"                         )
def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''):          The same tensor `x`, unchanged def print_tensor(x, message=''
class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(PandasObject):          check_setitem_lengths(indexer, value, values) class Block(P
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:         return response         location = to_native_str(response.headers['location'].decode('latin1'))         redirected_url = urljoin(request.url, location)         return response
def _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool  ) -> Any      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset) else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset )
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))             return codes_list def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))             return codes_list def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))             return codes_list def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n = initial              return          if kwargs:             self.iterable = iterable             self.disable = disable             self.pos = self._get_free_pos(self)             self._instances.remove(self)             self.n =
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels() 
class CategoricalBlock(ExtensionBlock): def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values =
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                             ' and multiple workers may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if is_sequence:             steps_per_epoch = len(generator)
def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols) if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"                  )def melt(          else
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)         self.updates = [K.update_add(self.iterations, 1)]         opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)
class YoutubeIE(YoutubeBaseInfoExtractor):          video_id = mobj.group(2)          return video_id      @staticmethod     def _extract_chapters(description, duration):          if not description:              return None
class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] =
class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.          :rtype: str class ListParameter(Parameter):         Ensure that list parameter is converted to a
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]              if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name) class NumericIndex(Index):
def _urlencode(seq, enc):  def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()      root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)
class Response(object_ref):          s = safe_url_string(url, self.encoding)             self._url = escape_ajax(s)  if ':' not in self._url:              raise ValueError('Missing scheme in response url: %s' % self._url)
class EmptyLineTracker:                  return 0, 0              newlines = 2              if current_line.depth == 0:                  newlines -= 1              if current_line.depth == 1:                  newlines -= 1             if current_line.depth == 2:                  newlines -= 1             if current_line.depth == 3:                  newlines -= 1             if current_line.depth == 4:                  newlines -= 1             if current_line.depth == 5:                  newlines -= 1             if current_line.depth == 6:                  newlines -= 1             if current_line.depth == 7:                  newlines -= 1             if current_line.depth == 8:                  newlines -= 1             if current_line.depth == 9:                  newlines -= 1             if current_line.depth == 10:                  newlines -= 1             if current_line.depth == 11:                  newlines -= 1             if current_line.depth == 12:                  newlines -= 1             if current_line.depth == 13:                  newlines -= 1             if current_line.depth == 14:                  newlines -= 1             if current_line.depth == 15:                  newlines -= 1             if current_line.depth == 16:                  newlines -= 1             if current_line.depth == 17:                  newlines -= 1             if current_line.depth == 18:                  newlines -= 1             if current_line.depth == 19:                  newlines -= 1             if current_line.depth == 20:                  newlines -= 1             if current_line.depth == 21:                  newlines -= 1             if current_line.depth == 22:                  newlines -= 1             if current_line.depth == 23:                  newlines -= 1             if current_line.depth == 24:                  newlines -= 1             if current_line.depth == 25:                  newlines -= 1             if current_line.depth == 26:                  newlines -= 1             if current_line.depth == 27:                  newlines -= 1             if current_line.depth == 28:                  newlines -= 1             if current_line.depth == 29:                  newlines -= 1             if current_line.depth == 30:                  newlines -= 1             if current_line.depth == 31:                  newlines -= 1             if current_line.depth == 32:                  newlines -= 1             if current_line.depth == 33:                  newlines -= 1             if current_line.depth == 34:                  newlines -= 1             if current_line.
class DataFrame(NDFrame):         return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(self, other, func):             return new_data     def _combine_match_index(self, other, func):          if ops.series_dispatch(
def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] if RE_ANSI.search(data):         return data + "\033[0m" def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1] def disp_trim(data, length):      if len(data) == disp_len(
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val, fill_value=fill_value)             clocs = [v if i > v else v - 1 for v in clocs]             return result def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val, fill_value=fill_value)             clocs = [v if i > v else v - 1 for v in clocs]             return result def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val, fill_value=fill_value)             clocs = [v if i > v else v - 1 for v in clocs]             return result def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val, fill_value=fill_value)             clocs = [v if i > v else v - 1 for v in clocs]             return result def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val, fill_value=fill_value)             clocs = [v if i > v else v - 1 for v in clocs]             return result def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]              result = result.unstack(val, fill_value=fill_value)             clocs = [v if i > v else v - 1 for v in clocs]             return result def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):
def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe):                      return jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe):                      return jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe):                      return jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe):                      return jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe) def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']             namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']
class LinuxHardware(Hardware):              mtab_entries.append(fields)return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)         return mt
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical` self.check_for_ordered("min") class Categorical(ExtensionArray, PandasObject):          min : the minimum
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_LOADER_CLASS',                             settings.get('SPIDER_MANAGER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)
logger = logging.getLogger(__name__)  title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text)") info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;[^-]*--&gt;") ref_regex = re.compile(r"&lt;[^-]*--&gt;")
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          ) else:          return jsonable_encoder(response_content)
class WebSocketHandler(tornado.web.RequestHandler):          .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value)
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5' def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5' def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5' def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5' def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5' def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5' def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body) def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_sequence_field(field: Field) -> bool     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body) def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body) def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_sequence_field(field: Field) -> bool     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body) def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body) def get_flat_dependant(dependant: Dependant) -> Depend
def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks} def _partition_tasks(worker):      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]} def _partition_tasks(worker):      set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'} def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks} def _partition_tasks(worker):      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]} def _partition_tasks(worker):      set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'} def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks} def _partition_tasks(worker):      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]} def _partition_tasks(worker):      set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'} def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks} def _partition_tasks(worker):      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not
class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified. class Parameter(object):          :raises Missing
class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array() def should_store(self
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if not good.all():             if skipna:                  pointer = self._codes[good].min() else:                  return np.nan class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1 if
class RedirectMiddleware(BaseRedirectMiddleware):      def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) and                 response.status in getattr(spider, 'handle_httpstatus_list', [])):              return response          if request.method == 'GET':              return response          if request.method == 'HEAD':              return response          if request.method == 'POST':              return response          if request.method == 'PUT':              return response          if request.method == 'DELETE':              return response          if request.method == 'TRACE':              return response          if request.method == 'CONNECT':              return response          if request.method == 'OPTIONS':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if request.method == 'CONNECT':              return response          if
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index) command.script_parts.pop(upstream_option_index) push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2] return replace_argument(" ".join(command.script_parts), 'push', push_upstream) def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index) command.script_parts.pop(upstream_option_index) push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2] return replace_argument(" ".join(command.script_parts), 'push', push_upstream) def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index) command.script_parts.pop(upstream_option_index) push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2] return replace_argument(" ".join(command.script_parts), 'push', push_upstream) def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index) command.script_parts.pop(upstream_option_index) push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2] return replace_argument(" ".join(command.script_parts), 'push', push_upstream) def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index) command.script_parts.pop(upstream_option_index) push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2] return replace_
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)          result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)  class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)          result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)  class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)          result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)  class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)          result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)  class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)          result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)  class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)          result = ts
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     is_sequence = isinstance(generator, Sequence) if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)  )         if non_mapping:              return self._set_name(index, inplace=kwargs.get("inplace"))         return super().rename(index=index, **kwargs)      @Substitution(**_shared_doc_kwargs)
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)             self.asyncio_loop.close()def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.add_handler(fd, handler, events)def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)             self.asyncio_loop.
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block          if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]          return block
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!Raises          ------          ValueError 
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition         ]def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition         ]def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition         ]def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition         ]def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition         ]def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape) def conv2d_transpose(x,
def next_palindrome(digit_list):     high_mid = len(digit_list) // 2     low_mid = (len(digit_list) - 1) // 2     while high_mid < len(digit_list) and low_mid >= 0:         if digit_list[high_mid] == 9:             digit_list[high_mid] = 0             digit_list[low_mid] = 0             high_mid += 1             low_mid -= 1         else:             digit_list[high_mid] += 1
def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i] + count)     return sorted_arr     def bucketsort(arr, k):     counts = [0]     for x in arr:         counts
def to_base(num, b):     result = ''     alphabet = string.digits + string.ascii_uppercase while num > 0:         i = num % b         num = num // b         result = result + alphabet[i]
def shunting_yard(tokens):     precedence = {         '+': 1,         '-': 1,         '*': 2,         '/': 2     }     rpntokens = []     opstack = []     for token in tokens:         if isinstance(token, int):             rpntokens.append(token)  else:             while opstack and precedence[token] <= precedence[opstack[-1]]:                 rpntokens.append(opstack.pop())  while opstack:         rpntokens.append(opstack.pop())
