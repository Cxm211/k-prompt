class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)             class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d
class HiveCommandClient(HiveClient):          if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:
class BinGrouper(BaseGrouper):              ngroups,          )      @cache_readonly      def result_index(self):          if len(self.binlabels)!= 0 and isna(self.binlabels[0]):          if len(self.binlabels)!= 1 and isna(self.binlabels[1]):          if len(self.binlabels)!= 2 and isna(self.binlabels[2]):          if len(self.binlabels)!= 3 and isna(self.binlabels[3]):          if len(self.binlabels)!= 4 and isna(self.binlabels[4]):          if len(self.binlabels)!= 5 and isna(self.binlabels[5]):          if len(self.binlabels)!= 6 and isna(self.binlabels[6]):          if len(self.binlabels)!= 7 and isna(self.binlabels[7]):          if len(self.binlabels)!= 8 and isna(self.binlabels[8]):          if len(self.binlabels)!= 9 and isna(self.binlabels[9]):          if len(self.binlabels)!= 10 and isna(self.binlabels[10]):          if len(self.binlabels)!= 11 and isna(self.binlabels[11]):          if len(self.binlabels)!= 12 and isna(self.binlabels[12]):          if len(self.binlabels)!= 13 and isna(self.binlabels[13]):          if len(self.binlabels)!= 14 and isna(self.binlabels[14]):          if len(self.binlabels)!= 15 and isna(self.binlabels[15]):          if len(self.binlabels)!= 16 and isna(self.binlabels[16]):          if len(self.binlabels)!= 17 and isna(self.binlabels[17]):          if len(self.binlabels)!= 18 and isna(self.binlabels[18]):          if len(self.binlabels)!= 19 and isna(self.binlabels[19]):          if len(self.binlabels)!= 20 and isna(self.binlabels[20]
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)         class DataFrame(ND
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml, @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=True, readable=True
class Sanic:                  if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):                  if middleware_or_request not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(target=steps) Progbar(
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x     def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x     def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x     def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x     def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
class XportReader(abc.Iterator):          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:             contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._read_header()          self._
class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value) class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop)              return task      def __await__(self):          task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(coro, loop=self.loop)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(self):              task = asyncio.ensure_future(self.serve_coro)              return task      def serve_coro(
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None      if output_padding is None:          return None      if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if padding == 'valid':          if
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:                      val_enqueuer_gen = generator              else:
class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):              path, _, _, _ = get_filepath_or_buffer(path, mode="wb")              kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)             kwargs["open_with"] = lambda path, _: path
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x   class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x   class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x   class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x   class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x   class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.float
class HTTPRequest(HTTPMessage):          )          headers = dict(self._orig.headers)         if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value) for (name, value) in headers.items()]          headers = ['%s: %s' % (name, value)
class TestProcessProtocol(protocol.ProcessProtocol):          def __init__(self):          self.deferred = defer.Deferred()         self.out = ''         self.err = ''          self.exitcode = None      def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def errReceived(self, data):          self.deferred.addCallback(self.errReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def outReceived(self, data):          self.deferred.addCallback(self.outReceived)         def
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))         values[np.isinf(values)] = np.NaN          return values     class _Window(Pandas
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __eq__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __ne__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __ne__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __lt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __lt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __le__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __le__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __gt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __gt__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __ge__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __ge__") class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name} does not implement __le__") class BaseComparisonOpsTests(
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)          for task in tasks:             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING':             in_workers = assistant or worker in task.workers              if task.status ==
def jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',','')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day_first=day_first)      date_str = unified_timestamp(date_str, day
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper          def cb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model_exclude_unset,              response_model_exclude_unset=response_model
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)" def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src,
def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):          return True      return False     def should_series
def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):          d = self.scraper
class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         try:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format.update(f)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats     class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         try:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format.update(f)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats     class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         try:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format.update(f)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats     class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response
def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      } def validate_baseindexer_support(func_name: Optional[str
class BarPlot(MPLPlot):     def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))         ax.set_xticks(self.tick_pos)         ax.set_xticklabels(ticklabels)          if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel(name)             if name is not None and self.use_index:              ax.set_xlabel(name)             if name is not None and self.use_index:              ax.set_ylabel
def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)     def match(command, settings):      def get_new_
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING], self.tasks)          else:             return state.get_pending_tasks()      def is_trivial_worker(self, state):             return state.get_running_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state):             return state.get_trivial_tasks()      def is_trivial_worker(self, state
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1: pivot_table(agged[v])     if table.index.nlevels > 2: pivot_table(agged[v])     if table.index.nlevels > 3: pivot_table(agged[v])     if table.index.nlevels > 4: pivot_table(agged[v])     if table.index.nlevels > 5: pivot_table(agged[v])     if table.index.nlevels > 6: pivot_table(agged[v])     if table.index.nlevels > 7: pivot_table(agged[v])     if table.index.nlevels > 8: pivot_table(agged[v])     if table.index.nlevels > 9: pivot_table(agged[v])     if table.index.nlevels > 10: pivot_table(agged[v])     if table.index.nlevels > 11: pivot_table(agged[v])     if table.index.nlevels > 12: pivot_table(agged[v])     if table.index.nlevels > 13: pivot_table(agged[v])     if table.index.nlevels > 14: pivot_table(agged[v])     if table.index.nlevels > 15: pivot_table(agged[v])     if table.index.nlevels > 16: pivot_table(agged[v])     if table.index.nlevels > 17: pivot_table(agged[v])     if table.index.nlevels > 18: pivot_table(agged[v])     if table.index.nlevels > 19: pivot_table(agged[v])     if table.index.nlevels > 20: pivot_table(agged[v])     if table.index.nlevels > 21: pivot_table(agged[v])     if table.index.nlevels > 22: pivot_table(agged[v])     if table.index.nlevels > 23: pivot_table(agged[v])     if table.index.nlevels > 24: pivot_
:     def create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField( original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField( original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField( original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField( original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField( original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField( original_type.__name__, __config__=original_type.__config          )          for f in original_type.__fields__.values():             use_type.__fields__[f
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head!= "0" else id_                      dep = "ROOT" if dep == "root"                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag + "__" + morph if use_morphology else tag                      tag = tag
class BaseExtensionTests(BaseExtensionTests):          result[0]          result[1]          assert data[0] == data[1]          assert data[1] == data[2]          assert data[2] == data[3]          assert data[3] == data[4]          assert data[4] == data[5]          assert data[5] == data[6]          assert data[6] == data[7]          assert data[7] == data[8]          assert data[8] == data[9]          assert data[9] == data[10]          assert data[10] == data[11]          assert data[11] == data[12]          assert data[12] == data[13]          assert data[13] == data[14]          assert data[14] == data[15]          assert data[15] == data[16]          assert data[16] == data[17]          assert data[17] == data[18]          assert data[18] == data[19]          assert data[19] == data[20]          assert data[20] == data[21]          assert data[21] == data[22]          assert data[22] == data[23]          assert data[23] == data[24]          assert data[24] == data[25]          assert data[25] == data[26]          assert data[26] == data[27]          assert data[27] == data[28]          assert data[28] == data[29]          assert data[29] == data[30]          assert data[30] == data[31]          assert data[31] == data[32]          assert data[32] == data[33]          assert data[33] == data[34]          assert data[34] == data[35]          assert data[35] == data[36]          assert data[36] == data[37]          assert data[37] == data[38]          assert data[38] == data[39]          assert data[39] == data[40]          assert data[40] == data[41]          assert data[41] == data[42]          assert data[42] == data[43]          assert data[43] == data[44]          assert data[44] ==
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script) class CorrectedCommand(object):              compatibility_call(self
class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()      def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences) 	         class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.
def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_old(obj): return result  def notna(obj):      Detect non-missing values for an array-like object. def _isna_ndarraylike_new(obj): return result  def notna(obj
class _LocIndexer(_LocationIndexer):          if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[1], MultiIndex):             if isinstance(key, str) and labels.levels[1].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[2], MultiIndex):             if isinstance(key, str) and labels.levels[2].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[3], MultiIndex):             if isinstance(key, str) and labels.levels[3].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[4], MultiIndex):             if isinstance(key, str) and labels.levels[4].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[5], MultiIndex):             if isinstance(key, str) and labels.levels[5].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[6], MultiIndex):             if isinstance(key, str) and labels.levels[6].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[7], MultiIndex):             if isinstance(key, str) and labels.levels[7].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))             if isinstance(labels.levels[8], MultiIndex):             if isinstance(key, str) and labels.levels[8].is_all_dates:                  key = tuple([key] +
class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:              self.depth -= 1             self._lambda_arguments = False              return False          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True          return True
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):          if isinstance(maybe_slice, slice):              return self[maybe_slice]         taken = ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )         freq = self.freq if is_period_dtype(self) else None         assert taken.freq == freq, (taken.freq, freq, taken)         return self._shallow_copy(taken, freq=freq)      _can_hold_na = True      _na_value = NaTNetworkingSpec         _can_hold_fill = True      _can_hold_fill_na = True      _can_hold_fill_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can_hold_na = True      _can
def _isna_ndarraylike(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):      if hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_new(obj):      if hasattr(obj, "__array__"):          return _isna_
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]   class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):             return v         if v.startswith("'"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"': '"',                  '"
class Model(BaseModel):     class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)   class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() ->
:     def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):         new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=freq)      def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):         new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=freq)      def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):         new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=freq)      def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):         new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=freq)      def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):         new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=freq)      def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):         new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=freq)      def _simple_new(self, new_dates, dtype=DT64NS_DTYPE, freq=self.freq):
class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode =='min' or             (self.mode == 'auto and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0          self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0          self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.monitor_op = lambda a, b: np.less(a, b + self.epsilon)              self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.monitor_op = lambda a, b: np.less(a, b
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)  def _get_combined_index(obs_idxes: List[Index], intersect: bool, sort: bool)  def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)  def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)  def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)  def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)  def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj
class QuarterOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class QuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class QuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class QuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class QuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class QuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class QuarterEnd(QuarterOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(             shifted, freq=
class ImageDataGenerator(object):              The inputs, normalized.         if self.preprocessing_function:             x = self.preprocessing_function(x)          if self.rescale:              x *= self.rescale          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)          if self.samplewise_center:             x = self.samplewise_center(x)
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = True              return ret          if is_scalar(other):             ret[other] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] = True              return ret          if is_scalar(op):             ret[op] =
def match(command, settings):      return _search(command.stderr) or _search(command.stdout) def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)   def match(command, settings):      return _search(command.stderr) or _search(command.stdout) def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)   def match(command, settings):      return _search(command.stderr) or _search(command.stdout) def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)   def match(command, settings):      return _search(command.stderr) or _search(command.stdout) def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)   def match(command, settings):      return _search(command.stderr) or _search(command.stdout) def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)     editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shell
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, "same" or "valid".          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          data_format: "channels_
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)      def make_requests_from_url(self, url):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override make_requests_from_url method instead."              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)          def make_requests_from_url(self, url):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override make_requests_from_url method instead."              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)          def make_requests_from_url(self, url):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override make_requests_from_url method instead."              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)          def make_requests_from_url(self, url):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override make_requests_from_url method instead."              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)          def make_requests_from_url(self, url):         if self.make_requests_from_
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)     executor = ProcessPoolExecutor(max_workers=worker_count)      try:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting( Executor(max_workers=worker_count), executor=executor))      except:          loop.run_until_complete(              schedule_formatting(
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)              return asdt, reso             except (TypeError, ValueError):                  pass              asdt, reso =
def base_url(url):      def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path) def base_url(url):      def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path) def base_url(url):      def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path) def base_url(url):      def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path) def base_url(url):      def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path) def base_url(url):      def urljoin(base, path):
class _LocIndexer(_LocationIndexer):              return self._getbool_axis(key, axis=axis)          elif is_list_like_indexer(key):             if isinstance(labels, ABCMultiIndex):                 if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:                     key = list(key)                 elif isinstance(key, ABCDataFrame):                     raise NotImplementedError(                         "Indexing a MultiIndex with a DataFrame key is not "                         "implemented"                     )                 elif hasattr(key, "ndim") and key.ndim > 1:                     raise NotImplementedError(                         "Indexing a MultiIndex with a multidimensional key is not "                         "implemented"                     )                 if (                     not isinstance(key, tuple)                     and len(key)                     and not isinstance(key[0], tuple)                 ):                     key = tuple([key])              if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):             raise NotImplementedError(                         "Indexing a MultiIndex with a MultiIndex key is not "                         "implemented"                     )             if (                     not isinstance(key, tuple)                     and len(key)                     and not isinstance(key[0], tuple)                 ):                     key = tuple([key])              if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):             raise NotImplementedError(                         "Indexing a MultiIndex with a MultiIndex key is not "                         "implemented"                     )             if is_list_like_indexer(key):             if isinstance(labels, ABCMultiIndex):                 if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:                     key = list(key)                 elif isinstance(key, ABCDataFrame):                     raise NotImplementedError(                         "Indexing a MultiIndex with a DataFrame key is not "                         "implemented"                     )                 elif hasattr(key, "ndim") and key.ndim > 1:                     raise NotImplementedError(                         "Indexing a MultiIndex with a multidimensional key is not "                         "implemented"                     )                 if (                     not isinstance(key, tuple)                     and len(key)                     and not isinstance(key[0], tuple)                 ):                     key = tuple([key])              if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):             raise NotImplementedError(                         "Indexing a
def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.     return tf.Print(x, [x], message)        def print_tensor(x, message=''):          The same tensor `x`, unchanged.
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer, arr_value):              pass VpnObject class Block(PandasObject):          check_setitem_lengths(indexer, value, values)          if is_empty_indexer(indexer
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('https://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('https://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('https://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('http://', '')          redirected_url = redirected_url.replace('
:     def _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset )      elif isinstance(res, list):          return [             _prepare_response_content(item, exclude_unset=exclude_unset) for item in res          ]      elif isinstance(res, dict):          return {             k: _prepare_response_content(v, exclude_unset=exclude_unset)              for k, v in res.items()          }      return res         def _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset )      elif isinstance(res, list):          return [             _prepare_response_content(item, exclude_unset=exclude_unset) for item in res          ]      elif isinstance(res, dict):          return {             k: _prepare_response_content(v, exclude_unset=exclude_unset)              for k, v in res.items()          }      return res         def _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset )      elif isinstance(res, list):          return [             _prepare_response_content(item, exclude_unset=exclude_unset) for item in res          ]      elif isinstance(res, dict
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))  def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))  def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))  def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len
(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:             self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n =
class IndexOpsMixin(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[1] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[2] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[3] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[4] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[5] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[6] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[7] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[8] class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return
class CategoricalBlock(ExtensionBlock):          def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs):          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(slicer=slicer, na_rep="", quoting=quoting, **kwargs)          values = values PidBlock.to_native_types(sl
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             steps_per_epoch = len(generator)          enqueuer = None          try: Enqueuer(steps_per_epoch=steps_per_epoch, enqueuer=queuer)          except Exception as e:             warnings.warn(                 UserWarning(e))         if is_sequence:             val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                            'class.'))         if is_sequence:             val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                            'and multiple workers may duplicate your data.'                            'Please consider using the`keras.utils.Sequence'                             '
def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in" Arnold) def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 2)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 3)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 4)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 5)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 6)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 7)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 8)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 9)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 10)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)          self.updates = [K.update_add(self.iterations, 11)]          opt_
class YoutubeIE(YoutubeBaseInfoExtractor):          video_id = mobj.group(2)          return video_id      @staticmethod     def _extract_chapters(description, duration):          if not description:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', description)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-Z0-9]+$/i', chapter_lines)          if not chapter_lines:              return None          chapter_lines = re.findall( '/^[a-zA-
class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs) class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class']          return super(ItemMeta, mcs).__new__(mcs, class
class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value. class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)             if left_drop:              self.left = self.left._drop_labels_or_levels(left_
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]          return cls._simple_new(subarr, name=name)
def _urlencode(seq, enc):      def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)      forms = forms.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)      def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)      def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)      def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)      def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)      def _get_form(response, formname, formid
class Request(object_ref):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))      url = property(_get_url, obsolete_setter(_set_url, 'url'))      def __init__(self, url, encoding=None):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))      def __init__(self, url, encoding=None):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))      def __init__(self, url, encoding=None):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))      def __init__(self, url, encoding=None):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))      def __init__(self, url, encoding=None):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s'
class EmptyLineTracker:                  return 0, 0              newlines = 2              if current_line.depth:                  newlines -= 1              if current_line.depth == 1:                  newlines -= 1              if current_line.depth == 2:                  newlines -= 1              if current_line.depth == 3:                  newlines -= 1              if current_line.depth == 4:                  newlines -= 1              if current_line.depth == 5:                  newlines -= 1              if current_line.depth == 6:                  newlines -= 1              if current_line.depth == 7:                  newlines -= 1              if current_line.depth == 8:                  newlines -= 1              if current_line.depth == 9:                  newlines -= 1              if current_line.depth == 10:                  newlines -= 1              if current_line.depth == 11:                  newlines -= 1              if current_line.depth == 12:                  newlines -= 1              if current_line.depth == 13:                  newlines -= 1              if current_line.depth == 14:                  newlines -= 1              if current_line.depth == 15:                  newlines -= 1              if current_line.depth == 16:                  newlines -= 1              if current_line.depth == 17:                  newlines -= 1              if current_line.depth == 18:                  newlines -= 1              if current_line.depth == 19:                  newlines -= 1              if current_line.depth == 20:                  newlines -= 1              if current_line.depth == 21:                  newlines -= 1              if current_line.depth == 22:                  newlines -= 1              if current_line.depth == 23:                  newlines -= 1              if current_line.depth == 24:                  newlines -= 1              if current_line.depth == 25:                  newlines -= 1              if current_line.depth == 26:                  newlines -= 1              if current_line.depth == 27:                  newlines -= 1              if current_line.depth == 28:                  newlines -= 1              if current_line.depth == 29:                  newlines -= 1              if current_line.depth == 30:                  newlines -= 1              if current_line.depth == 31:                  newlines -= 1              if current_line.depth == 32:                  newlines -= 1              if current_line.depth == 33:                  newlines -= 1              if current_line.depth == 34:                  newlines -= 1              if current_line.depth ==
class DataFrame(NDFrame):          return new_data     def _combine_match_index(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.should_series_dispatch(self, other, func):          if ops.
def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data + "\033[0m" def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data + "\033[0m" def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data + "\033[0m" def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data + "\033[0m" def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data + "\033[0m" def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data + "\033[0m" def disp_trim(data, length):      if len(
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result._unstack_multiple(data, clocs, fill_value=fill_value) def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result._unstack_multiple(data, clocs, fill_value=fill_value) def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result._unstack_multiple(data, clocs, fill_value=fill_value) def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result._unstack_multiple(data, clocs, fill_value=fill_value) def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result._unstack_multiple(data, clocs, fill_value=fill_value) def _unstack_multiple(data, clocs, fill_value
def jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_safe,                  ) jsonable_decoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_decoder=custom_decoder,                      sqlalchemy_safe=sqlalchemy_
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (info['manifest_file']['collection_info']['name'])) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (info['manifest_file']['collection_info']['name'])) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (info['manifest_file']['collection_info']['name'])) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (info['manifest_file']['collection_info']['name'])) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version." % (info['manifest_file']['collection_info']['name'])) class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect
class LinuxHardware(Hardware):              mtab_entries.append(fields)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)      def get_mount_info(self, mount, device, uuids):          mount_size = get
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")          good = self._codes!= -1          if not good.all():              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skipna: PandasObject.check_for_ordered("min"):              if skip
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_LOADER_CLASS',                             settings.get('SPIDER_MANAGER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())     def _get_spider_manager(settings):              'Please use SPIDER_MANAGER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_MANAGER_CLASS',                             settings.get('SPIDER_LOADER_CLASS'))      manager_cls = load_object(cls_path)      verifyClass(ISpiderManager, manager_cls)      return manager_cls.from_settings(settings.frozencopy())     def _get_spider_manager_loader(settings):              'Please use SPIDER_MANAGER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_MANAGER_CLASS',                             settings.get('SPIDER_LOADER_CLASS'))      manager_cls = load_object(cls_path)      verifyClass(ISpiderManager, manager_cls)      return manager_cls.from_settings(settings.frozencopy())     def _get_spider_manager(settings):              'Please use SPIDER_MANAGER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_MANAGER_CLASS',                             settings.get('SPIDER_LOADER_CLASS'))      manager_cls = load_object(cls_path)      verifyClass(ISpiderManager, manager_cls)      return manager_cls.from_settings(settings.frozencopy())     def _get_spider_manager_loader(settings):              'Please use SPIDER_MANAGER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls
:     def __init__(self):         self.title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text>)")  info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text>)")         self.info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")         self.html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")
!"#%&'()*+,-./ async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)             async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=
class WebSocketHandler(tornado.web.RequestHandler):         .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value)      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.set_nodelay(value)      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()      def on_connection_open(self) -> None:          if self.ws_connection: self.ws_connection.open()      def on_connection_close(self) -> None:          if self.ws_connection: self.ws_connection.close()
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm ='md5'          if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def get_flat_depend
def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history                                   if status == 'FAILED'}      set_tasks["scheduling_error"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}      set_tasks["still_pending_ext"] = {task for (task, status, ext) in task_history                                       if status == 'PENDING' and task not in set_tasks["failed"] and task not in set_tasks["completed"] and not ext}      set_tasks["still_pending_not_ext"] = {task for (task, status, ext) in task_history                                           if status == 'PENDING' and task not in set_tasks["failed"] and task not in set_tasks["completed"] and ext}      set_tasks["run_by_other_worker"] = set()      set_tasks["upstream_failure"] = set()      set_tasks["upstream_missing_dependency"] = set()      set_tasks["still_pending_ext"] = {task for (task, status, ext) in task_history                                       if status == 'PENDING' and task not in set_tasks["failed"] and task not in set_tasks["completed"] and not ext}      set_tasks["still_pending_not_ext"] = {task for (task, status, ext) in task_history                                           if status == 'PENDING' and task not in set_tasks["failed"] and task not in set_tasks["completed"] and ext}      set_tasks["run_by_other_worker"] = set()      set_tasks["upstream_failure"] = set()      set_tasks["upstream_missing_dependency"] = set()      set_tasks["still_pending_ext"] = {task for (task, status, ext) in task_history
class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:                  return False              elif self.is_dict:                  return False              elif self.is_list:                  return False              elif self.is_dict:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_list:                  return False              elif self.is_
class FloatOrComplexBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtypeVille class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as
class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan WebACL class Categorical(ExtensionArray, PandasObject):          good = self._codes!= -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()
Middleware(BaseRedirectMiddleware):     def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in getattr(spider,
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)     def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)     def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)     def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)     def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)             class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)             class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)             class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)             class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)             class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                         'and multiple workers may duplicate your data.'                         'Please consider using the`keras.utils.Sequence'                         'class.'))      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator' Arnold):      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return self._set_name(index, inplace=kwargs.get("inplace"))         return super().rename(index=index, **kwargs)      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.reindex.__doc__)          @Appender(generic.NDFrame.re
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)          self.asyncio_loop.add_handler(fd, handler, events)      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return selfAddrs class ObjectBlock
(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!          Raises          ------          TypeError ArgumentError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError          Raises          ------          TypeError TypeError
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition             security_definition
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0], kernel)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data
def next_palindrome(digit_list):     high_mid = len(digit_list) // 2     low_mid = (len(digit_list) - 1) // 2     while high_mid < len(digit_list) and low_mid >= 0:         if digit_list[high_mid] == 9:             digit_list[high_mid] = 0             digit_list[low_mid] = 0             high_mid += 1             low_mid -= 1         else:             digit_list[high_mid] += 1             if low_mid!= high_mid:                 digit_list[low_mid] += 1             return digit_list     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_list)) * [0] + [1] + next_palindrome(digit_list)     return [1] + (len(digit_
class Solution(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)     return sorted_arr     class Solution(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)     return sorted_arr     class Solution2(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)     return sorted_arr     class Solution3(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)     return sorted_arr     class Solution4(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)     return sorted_arr     class Solution5(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)     return sorted_arr     class Solution6(object):     def bucketsort(arr, k):     counts = [0] * k     for x in arr:         counts[x] += 1     sorted_arr = []     for i, count in enumerate(arr):         sorted_arr.extend([i * count] * count)
import string def to_base(num, b):     result = ''     alphabet = string.digits + string.ascii_uppercase     while num > 0:         i = num % b         num = num // b         result = result + alphabet[i]     return result     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(num, b)     return string def to_base(
def shunting_yard(tokens):     precedence = {         '+': 1,         '-': 1,         '*': 2,         '/': 2     }     rpntokens = []     opstack = []     for token in tokens:         if isinstance(token, int):             rpntokens.append(token)         else:             while opstack and precedence[token] <= precedence[opstack[-1]]:                 rpntokens.append(opstack.pop())     while opstack:         rpntokens.append(opstack.pop())     return rpntokens     def shunting_yard(tokens):     precedence = {         '+': 1,         '-': 1,         '*': 2,         '/': 2     }     rpntokens = []     opstack = []     for token in tokens:         if isinstance(token, int):             rpntokens.append(token)         else:             while opstack and precedence[token] <= precedence[opstack[-1]]:                 rpntokens.append(opstack.pop())     while opstack:         rpntokens.append(opstack.pop())     return rpntokens     def shunting_yard(tokens):     precedence = {         '+': 1,         '-': 1,         '*': 2,         '/': 2     }     rpntokens = []     opstack = []     for token in tokens:         if isinstance(token, int):             rpntokens.append(token)         else:             while opstack and precedence[token] <= precedence[opstack[-1]]:                 rpntokens.append(opstack.pop())     while opstack:         rpntokens.append(opstack.pop())     return rpntokens     def shunting_yard(tokens):     precedence = {         '+': 1,         '-': 1,         '*': 2,         '/': 2     }     rpntokens = []     opstack = []     for token in tokens:         if isinstance(token, int):             rpntokens.append(token)         else:             while opstack and precedence[token] <= precedence[opstack[-1]]:                 rpntokens.append(opstack.pop())     while opstack:         rpntokens.append(opstack.pop())     return rpntokens     def shunting_y
