Namespace(log_name='./RQ5/tfix_500_3/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_500_3/codet5p_220m', data_dir='./data/RQ5/tfix_500_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'var { Observable } = require("canal-js-utils/rx"); var { empty, fromEvent, fromPromise, merge, just, throwError } = Observable; var {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'var { Observable } = require("canal-js-utils/rx"); var { empty, fromPromise, merge, just } = Observable; var {'}]
***** Running training *****
  Num examples = 500
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = inf
  global_step = 64
  train_loss = 21.2822
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 59.55 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:59.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = inf
  global_step = 127
  train_loss = 10.1015
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 62.03 	 Previous best codebleu 59.55
  ********************
 Achieve Best bleu:62.03
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = inf
  global_step = 190
  train_loss = 5.6557
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.1 	 Previous best codebleu 62.03
  ********************
 Achieve Best bleu:63.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = inf
  global_step = 253
  train_loss = 3.6354
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 62.25 	 Previous best codebleu 63.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = inf
  global_step = 316
  train_loss = 2.4167
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.19 	 Previous best codebleu 63.1
  ********************
 Achieve Best bleu:63.19
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = inf
  global_step = 379
  train_loss = 1.6186
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 64.14 	 Previous best codebleu 63.19
  ********************
 Achieve Best bleu:64.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 442
  train_loss = 1.0316
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 64.4 	 Previous best codebleu 64.14
  ********************
 Achieve Best bleu:64.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 505
  train_loss = 0.8046
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.48 	 Previous best codebleu 64.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 568
  train_loss = 0.6124
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.9 	 Previous best codebleu 64.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 631
  train_loss = 0.5396
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.24 	 Previous best codebleu 64.4
  ********************
early stopping!!!
reload model from RQ5/tfix_500_3/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_500_3/test.jsonl
  codebleu = 61.05 
  Total = 500 
  Exact Fixed = 82 
[13, 19, 39, 47, 62, 65, 67, 69, 77, 79, 81, 93, 97, 109, 111, 117, 129, 130, 131, 134, 139, 147, 151, 152, 153, 162, 170, 176, 187, 188, 191, 197, 198, 199, 205, 211, 213, 217, 226, 229, 241, 252, 253, 258, 261, 263, 265, 269, 284, 295, 298, 299, 308, 310, 316, 317, 324, 331, 333, 336, 337, 340, 344, 345, 351, 387, 391, 396, 403, 407, 426, 434, 437, 439, 441, 446, 449, 453, 475, 487, 488, 493]
  Syntax Fixed = 7 
[41, 49, 60, 175, 238, 411, 425]
  Cleaned Fixed = 9 
[9, 61, 85, 120, 159, 268, 280, 417, 425]
  ********************
  Total = 500 
  Exact Fixed = 82 
[13, 19, 39, 47, 62, 65, 67, 69, 77, 79, 81, 93, 97, 109, 111, 117, 129, 130, 131, 134, 139, 147, 151, 152, 153, 162, 170, 176, 187, 188, 191, 197, 198, 199, 205, 211, 213, 217, 226, 229, 241, 252, 253, 258, 261, 263, 265, 269, 284, 295, 298, 299, 308, 310, 316, 317, 324, 331, 333, 336, 337, 340, 344, 345, 351, 387, 391, 396, 403, 407, 426, 434, 437, 439, 441, 446, 449, 453, 475, 487, 488, 493]
  Syntax Fixed = 7 
[41, 49, 60, 175, 238, 411, 425]
  Cleaned Fixed = 9 
[9, 61, 85, 120, 159, 268, 280, 417, 425]
  codebleu = 61.05 
[0.3852352687934718, 0.8170015046833874, 0.7456907908606256, 0.4787139878886564, 0.5642010748634618, 0.8697471734123385, 0.7252232811270083, 0.11582292438020222, 0.6119613551150405, 0.6779253872262, 0.14726550033812452, 0.7067366858513033, 0.6640033451354017, 0.41459221401144064, 0.39323313167877133, 0.6127572978503959, 0.45189927805897867, 0.44412484527547247, 1.0, 0.9207265089204273, 0.832900393434221, 0.3743172882353815, 0.15, 0.5515809931907855, 0.5826136760976885, 0.2504033576162784, 0.7949680697811121, 0.15747669278081078, 0.5276850589244717, 0.110722640539825, 0.535133493667276, 0.6142482969348242, 0.7134655991716751, 0.4198769757196277, 0.32651208993424297, 0.8149139863647084, 0.6371296454239948, 0.8504403342456683, 1.0, 0.41673766744658963, 0.9377649116482698, 0.22267396568917747, 0.6390982360654051, 0.5229326815734496, 0.5481721142074378, 0.5952887122558812, 1.0, 0.602585911416486, 0.6888724788208753, 0.8951959058230954, 0.5524581952101042, 0.5266057619745405, 0.8223873319410013, 0.5135741820077107, 0.677190822574687, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 0.7802740295913121, 0.8325722947872878, 0.9426294298277487, 1.0, 0.5610771314034941, 0.4803494323607689, 0.8229808431453671, 0.48289849585465594, 1.0, 0.6170182644937875, 1.0, 0.7539630945946609, 0.1976850589244717, 0.20369111792447886, 0.08959041984600925, 0.8917740712291482, 0.5007846217557572, 0.20613487118166404, 1.0, 0.548663900608423, 1.0, 0.6724062327149246, 0.8249365300761395, 0.6387289085944345, 0.30365138677226156, 0.6953856477238599, 0.6372358081500218, 0.9179350389391119, 0.0, 0.2758146988607844, 0.0485646584311115, 0.5566895982149789, 0.4570583972426556, 0.7299948711471688, 1.0, 0.814064525684886, 0.4134404736596011, 0.4484520389345378, 1.0, 0.016782765293716924, 0.686863907300452, 0.3248878349419664, 0.5553082743436026, 0.4965144687034782, 0.793158309009629, 0.0, 0.07719082257468701, 0.30480616432215846, 0.48167268419835185, 0.467913762972279, 1.0, 0.5024101654079205, 1.0, 0.6785036503543816, 0.35473913473843294, 0.17344240248645176, 0.2924537101693822, 0.549512081770263, 0.9474466761527047, 0.2610380151684039, 0.19999999999999998, 0.08953276868214524, 0.33036017439379955, 0.9068864709786153, 0.6553513243806304, 0.10906101778358473, 0.8840188084091536, 0.8150666162955742, 0.4118819918812901, 0.726624382466148, 1.0, 0.9891483218006352, 1.0, 0.7386991709927695, 0.5777131912598308, 1.0, 0.22619887519287302, 0.44196276172452564, 0.4831321633398339, 0.5860678628763278, 1.0, 0.6937464703296872, 0.45590150397397095, 0.5960707868709735, 0.27528728007290226, 0.5440521407974296, 0.7396601242701673, 0.7530045026259591, 1.0, 0.523507649142383, 0.6060694567816493, 0.5063942770541421, 1.0, 1.0, 1.0, 0.40135160729534664, 0.7728222909971192, 0.5632387151836264, 0.734603718873536, 0.8245957586820076, 0.7290594020931371, 0.5266057619745405, 0.5348435312891645, 0.7863340021370451, 0.5461243465154161, 0.5301818996067954, 0.7262862588837676, 0.3396347706525644, 0.44478640618236376, 0.35511870560120445, 0.6957231649810731, 0.7135428903906851, 0.6336131842675646, 0.7620962140028005, 0.4831458551130241, 0.6239994222745141, 0.8149139863647084, 1.0, 0.5882210413806648, 0.39211329027742914, 0.8273136413238535, 0.510409269308891, 0.7269320077439674, 0.4274182217806679, 0.7302007404183433, 0.8426568563575418, 0.7546391146496889, 0.3812999423756144, 1.0, 1.0, 0.41925376624580646, 0.5609260915878896, 1.0, 0.5728633222115624, 0.7359097981643703, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 1.0, 0.8114529051148651, 1.0, 0.39873667339437135, 0.15429293535427072, 0.8191441569283882, 0.7296508600957852, 0.6867787885259955, 1.0, 0.6558149041754464, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.31888879608349985, 0.8114529051148651, 0.3972934177879658, 1.0, 0.35715141059137, 0.8249783514952764, 0.1615772774872922, 1.0, 0.6663998078842303, 0.6931204477227866, 0.6530836285702823, 0.59820323220824, 0.48897284678550934, 0.5960626227910577, 0.3819511489967859, 0.6387851138365109, 1.0, 0.6271806220068676, 0.4766759794022652, 1.0, 0.24940393718519505, 0.3678831758068888, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.5274151448167971, 0.6726527033303111, 0.7970182644937875, 0.3938266805419558, 0.6029055732198362, 1.0, 0.87889664547395, 0.0, 0.3538685674110905, 0.5944836924880659, 0.45483270617506144, 0.7450769337104594, 0.15553373183372082, 0.4435027248795795, 0.8113118214155393, 0.04101911673989318, 1.0, 1.0, 0.4858071622471934, 0.4089903936086955, 0.4223835094667513, 0.0, 1.0, 0.6924012535180504, 0.16826111698077859, 0.8114529051148651, 0.5721360784673309, 1.0, 0.604436274930823, 0.8249365300761395, 0.6124900507484021, 0.0, 0.6993751196103626, 1.0, 0.367641751923923, 0.9272945586059962, 0.7582416075216148, 0.6463338290119179, 0.177190822574687, 0.06666666666666667, 0.5058144125160369, 0.7439654153934541, 0.8020701463285549, 0.13846153846153847, 0.873209263429523, 0.06423223587777374, 0.6506923930808135, 0.6041077305454495, 1.0, 0.7695636757535107, 0.4141117485802601, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.612487962443162, 0.8244788648855748, 0.29286074313859833, 0.8398078253515011, 0.7835249822766466, 0.7826555560719357, 0.027660853979899265, 0.5550049310474542, 0.9155641924881595, 1.0, 0.560293349578547, 0.43701826449378744, 0.7318065132585239, 0.19587703500732306, 0.3814205333654446, 0.7839520803542337, 0.3844698211793555, 0.5824615640430578, 1.0, 0.40660822240095185, 0.8936825710002676, 0.8095727631302025, 0.3227886222566654, 0.8086110153378172, 0.8251411716151402, 0.46526859680566335, 1.0, 0.9318657024016066, 0.5123425659061871, 0.6227241494026018, 0.5678911585054794, 0.5537998353931795, 0.8255980153481091, 0.6969247006410559, 1.0, 0.32202961630539056, 0.6148191280817265, 0.6910080849676471, 0.5189589454524783, 0.5382922926202521, 0.4343002319965344, 1.0, 0.6742738870938194, 1.0, 0.72455988025806, 0.4707909983708979, 0.8114529051148651, 0.9096188406411201, 0.7144948346694379, 0.4709261928229648, 1.0, 0.6352900380997268, 0.7329853689231081, 0.7668758886842345, 1.0, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.46247879873840503, 1.0, 0.6467214894577404, 0.6781982935333077, 0.629920929034602, 0.5924295517447509, 0.3426136760976885, 0.26822487472667034, 0.8016991175434878, 0.7886327123579241, 0.5140913145175611, 0.4147896756607047, 0.7907748714315581, 0.5190854538169563, 0.370321911798854, 0.5226136760976885, 0.7391886750930661, 0.3639660544781372, 0.5530325714154424, 0.14283693344527487, 0.24887760375821233, 0.6530550965302279, 0.4557833072319293, 0.4366371358026476, 0.8701746619282504, 0.5099766481408452, 0.11453360085481801, 0.5926173418488229, 0.6892870539097846, 0.8102736261199102, 0.49202520055807764, 0.8042470965743207, 0.6530367547828486, 0.6804874069228855, 0.05127041479686025, 0.3154745710868131, 0.6742613142686853, 1.0, 0.4387244306491825, 0.4713750451807391, 0.6193453441662242, 1.0, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 1.0, 0.6941813926327731, 0.7308835793300246, 0.35437970233785665, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 1.0, 0.7313559408133775, 0.7017410285928236, 0.7787918207406566, 1.0, 0.4054183564853334, 0.34575990398983625, 0.8433369370338967, 0.9365276486140073, 0.702985368923108, 0.5084373166195929, 0.8214029683767206, 0.40209781994840155, 0.40400364537321126, 0.6816015466501335, 0.7972388343569652, 0.602487595382981, 0.24658767523161879, 0.23683637774429345, 0.6428938087244226, 0.3372632052291401, 0.940598945729278, 0.8931507219600794, 0.8114529051148651, 0.19272438732935, 0.6418894161880858, 0.5149139863647084, 0.5399727492914578, 0.4085332823846201, 0.8661547359011423, 0.15472876794829787, 0.8686013831393846, 0.39999999999999997, 0.7023594991536818, 1.0, 0.6847340670900772, 1.0, 0.48861662650065896, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 0.7674961219905688, 1.0, 0.5316562756993463, 0.7191441569283882, 1.0, 0.6733027629890207, 0.37509948078494726, 0.8542712839031905, 1.0, 0.39206686043974615, 0.28890659929471596, 0.6578607431385983, 0.5353082743436026, 0.7988542022748455, 0.8334272760895116, 0.6945758964627062, 0.003996009641464269, 0.839616326453424, 0.47102574599540464, 0.6943509330652489, 0.3842198496059124, 0.18133585795716756, 0.41118547774125425, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.7412242091620882, 0.355067633384039, 0.7398815472813818, 0.8114529051148651, 0.643309301212212, 0.30215269230653224, 0.12, 0.8010669062882427, 0.7016265496309229, 0.766682914692771, 0.46843920200122524, 0.39164729797306413, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 1.0, 1.0, 0.8180779662080888, 0.5580125488038365, 0.566221018567181, 0.5957444644618661, 1.0, 0.6840624465693822, 0.7396773803089467, 0.5909395885250581, 0.7279095254199106, 0.8691371695837178, 0.20034796016239415, 0.23857157802053697]
Finish training and take 10m
