Namespace(log_name='./RQ5/tfix_500_3/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_500_3/codet5p_770m_f', data_dir='./data/RQ5/tfix_500_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 500 training instances 
***** Running training *****
  Num examples = 500
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00408
  global_step = 126
  train_loss = 0.7889
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00408
  ********************
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 60.39 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:60.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00421
  global_step = 251
  train_loss = 0.3791
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 60.67 	 Previous best codebleu 60.39
  ********************
 Achieve Best bleu:60.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0043
  global_step = 376
  train_loss = 0.157
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.39 	 Previous best codebleu 60.67
  ********************
 Achieve Best bleu:63.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00478
  global_step = 501
  train_loss = 0.0762
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.1 	 Previous best codebleu 63.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00475
  global_step = 626
  train_loss = 0.0457
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 64.12 	 Previous best codebleu 63.39
  ********************
 Achieve Best bleu:64.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00495
  global_step = 751
  train_loss = 0.0385
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.51 	 Previous best codebleu 64.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00515
  global_step = 876
  train_loss = 0.018
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 64.27 	 Previous best codebleu 64.12
  ********************
 Achieve Best bleu:64.27
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00544
  global_step = 1001
  train_loss = 0.0219
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 64.59 	 Previous best codebleu 64.27
  ********************
 Achieve Best bleu:64.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00556
  global_step = 1126
  train_loss = 0.009
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.68 	 Previous best codebleu 64.59
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00568
  global_step = 1251
  train_loss = 0.007
  ********************
Previous best ppl:1.00408
BLEU file: ./data/RQ5/tfix_500_3/validation.jsonl
  codebleu-4 = 63.49 	 Previous best codebleu 64.59
  ********************
reload model from RQ5/tfix_500_3/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_500_3/test.jsonl
  codebleu = 62.51 
  Total = 500 
  Exact Fixed = 91 
[13, 14, 18, 19, 44, 47, 59, 62, 65, 69, 77, 81, 85, 93, 99, 109, 111, 117, 129, 130, 131, 133, 134, 139, 147, 151, 152, 153, 162, 170, 172, 187, 188, 191, 197, 198, 199, 205, 211, 213, 217, 226, 229, 242, 252, 253, 255, 258, 261, 263, 265, 269, 284, 295, 298, 308, 310, 316, 317, 320, 331, 336, 337, 340, 344, 345, 357, 359, 365, 371, 374, 387, 391, 396, 403, 417, 418, 424, 434, 437, 439, 441, 445, 446, 449, 453, 475, 483, 487, 488, 493]
  Syntax Fixed = 6 
[41, 49, 60, 236, 238, 291]
  Cleaned Fixed = 6 
[61, 67, 120, 138, 268, 285]
  ********************
  Total = 500 
  Exact Fixed = 91 
[13, 14, 18, 19, 44, 47, 59, 62, 65, 69, 77, 81, 85, 93, 99, 109, 111, 117, 129, 130, 131, 133, 134, 139, 147, 151, 152, 153, 162, 170, 172, 187, 188, 191, 197, 198, 199, 205, 211, 213, 217, 226, 229, 242, 252, 253, 255, 258, 261, 263, 265, 269, 284, 295, 298, 308, 310, 316, 317, 320, 331, 336, 337, 340, 344, 345, 357, 359, 365, 371, 374, 387, 391, 396, 403, 417, 418, 424, 434, 437, 439, 441, 445, 446, 449, 453, 475, 483, 487, 488, 493]
  Syntax Fixed = 6 
[41, 49, 60, 236, 238, 291]
  Cleaned Fixed = 6 
[61, 67, 120, 138, 268, 285]
  codebleu = 62.51 
[0.33190193546013846, 0.8170015046833874, 0.4625592077246987, 0.4787139878886564, 0.6827566080910634, 0.940598945729278, 0.6267077432616137, 0.11582292438020222, 0.5981716191316424, 0.824344816363836, 0.3499927730653973, 0.7067366858513033, 1.0, 1.0, 0.3976608387094363, 0.6127572978503959, 0.45189927805897867, 0.6826968889437066, 1.0, 0.9207265089204273, 0.8214203428813993, 0.3743172882353815, 0.15, 0.7680343754298893, 0.5826136760976885, 0.2772943579237247, 0.7949680697811121, 0.15747669278081078, 0.44766085397989924, 0.110722640539825, 0.535133493667276, 0.6142482969348242, 0.5137245744633643, 0.43212619375090827, 0.24508959038955502, 0.8149139863647084, 0.6371296454239948, 0.6954704545438943, 0.6934531647858191, 0.49269306161613136, 0.9377649116482698, 0.22267396568917747, 0.6550835185056715, 0.9891483218006352, 0.5481721142074378, 0.6450835185056716, 1.0, 0.5961851509772491, 0.6888724788208753, 0.8951959058230954, 0.5524581952101042, 0.5266057619745405, 0.8223873319410013, 0.6886211851856385, 0.677190822574687, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 1.0, 0.8325722947872878, 0.9426294298277487, 1.0, 0.5610771314034941, 0.4803494323607689, 1.0, 0.7487325657010794, 0.8665243973805226, 0.6170182644937875, 1.0, 0.7539630945946609, 0.1976850589244717, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 0.5007846217557572, 0.20613487118166404, 1.0, 0.548663900608423, 0.643291057814614, 0.7328492172088266, 0.8249365300761395, 0.7265018183047776, 0.30365138677226156, 0.803950001904677, 1.0, 0.9179350389391119, 0.0, 0.2758146988607844, 0.0485646584311115, 0.5675986891240699, 0.4570583972426556, 0.7299948711471688, 1.0, 0.9190411344533542, 0.3915187569135013, 0.4484520389345378, 0.7459627599846212, 0.016782765293716924, 1.0, 0.2224886975190719, 0.4326263272143769, 0.5150788862839089, 0.8199440232953434, 0.0, 0.07719082257468701, 0.30480616432215846, 0.48167268419835185, 0.467913762972279, 1.0, 0.3240227257474313, 1.0, 0.6785036503543816, 0.35473913473843294, 0.24487097391502316, 0.2924537101693822, 0.7634863571404549, 1.0, 0.16950123401194037, 0.19999999999999998, 0.08953276868214524, 0.33036017439379955, 0.9068864709786153, 0.7233058773873271, 0.10906101778358473, 0.8840188084091536, 0.8150666162955742, 0.4118819918812901, 0.726624382466148, 1.0, 0.9891483218006352, 1.0, 0.7386991709927695, 1.0, 1.0, 0.22619887519287302, 0.4544627617245256, 0.4831321633398339, 0.8382851365602284, 1.0, 0.4441507205891465, 0.5187838131148098, 0.5960707868709735, 0.27320579161919023, 0.5440521407974296, 0.7396601242701673, 0.7530045026259591, 1.0, 0.523507649142383, 0.5760694567816493, 0.5063942770541421, 1.0, 1.0, 1.0, 0.35436935077271325, 0.7728222909971192, 0.5632387151836264, 0.6241730049753154, 0.8245957586820076, 0.7053538799174164, 0.48124999999999996, 0.5348435312891645, 1.0, 0.6211243465154161, 0.5301818996067954, 0.7424282823720908, 0.6745836923955046, 0.44478640618236376, 0.03606856837889304, 0.6957231649810731, 0.7135428903906851, 0.6336131842675646, 1.0, 0.4831458551130241, 0.5297071420499005, 0.6477517173276517, 0.7202172177276029, 0.5882210413806648, 0.39211329027742914, 0.8273136413238535, 0.6190738228228467, 0.7269320077439674, 0.3040107499200273, 0.7302007404183433, 0.8426568563575418, 0.7546391146496889, 0.3149468650780947, 1.0, 1.0, 0.4231498701419104, 0.5609260915878896, 1.0, 0.5728633222115624, 0.7359097981643703, 0.3086952587081496, 0.3376480475289295, 0.4718765181257531, 1.0, 0.8114529051148651, 1.0, 0.3187366733943714, 0.15429293535427072, 0.6530550965302279, 0.8201517014631392, 0.6867787885259955, 1.0, 0.6459247942853364, 0.9226212781081518, 0.0731716191316424, 0.4307250470563342, 0.31888879608349985, 0.8114529051148651, 0.3972934177879658, 1.0, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 1.0, 0.6663998078842303, 0.3771457608737994, 0.619750295236949, 0.6487021300892772, 0.48897284678550934, 0.5960626227910577, 0.3819511489967859, 0.6387851138365109, 1.0, 0.6271806220068676, 0.4971121850651573, 1.0, 0.43016109313909, 0.39359500830217886, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.8821296454239946, 0.6801527033303111, 0.6352205093306242, 0.5389213653390948, 0.6029055732198362, 0.714297617187952, 1.0, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6952637111045539, 0.8407059481807464, 0.1588669502253369, 0.4435027248795795, 0.8113118214155393, 0.04030522491224135, 1.0, 1.0, 0.6179144359460748, 1.0, 0.5582924787290234, 0.0, 1.0, 0.8046451547488727, 0.16826111698077859, 0.8114529051148651, 0.7563915229324041, 1.0, 0.604436274930823, 0.8249365300761395, 0.7855975357012536, 0.0, 0.6993751196103626, 1.0, 0.367641751923923, 0.7272945586059962, 0.5159638752036539, 0.5970071773399841, 0.177190822574687, 0.13636363636363635, 0.4650973805314187, 0.5663655696579224, 0.8020701463285549, 0.13846153846153847, 0.8212928138736779, 0.03969884422068212, 0.6260995190602316, 0.6041077305454495, 1.0, 0.8516684096158422, 0.4141117485802601, 0.29123935575300347, 0.6899766481408451, 0.6768986255548952, 0.8148035587128197, 0.8821296454239946, 0.29286074313859833, 0.8398078253515011, 0.8144831258437577, 0.7826555560719357, 0.027660853979899265, 0.5550049310474542, 1.0, 0.5843539719797703, 0.560293349578547, 0.43701826449378744, 0.5782512480545838, 0.2778574746187954, 0.2630461137211068, 0.7839520803542337, 0.3844698211793555, 0.6107000692160132, 1.0, 0.40660822240095185, 1.0, 0.8095727631302025, 0.3227886222566654, 0.8274001021378707, 0.8997846296433376, 0.5644004745782011, 1.0, 0.9318657024016066, 0.5290092325728539, 0.7041809935823391, 1.0, 0.5537998353931795, 0.8255980153481091, 0.6392333266349789, 0.5019100417936252, 0.37549356386576216, 0.630608601765937, 0.6910080849676471, 0.5189589454524783, 0.7431003732421251, 0.4343002319965344, 1.0, 0.6742738870938194, 0.38050945743445547, 0.4420538042073847, 0.4707909983708979, 0.8114529051148651, 1.0, 0.6658559747984352, 0.4709261928229648, 1.0, 0.6352900380997268, 0.7329853689231081, 0.8591190003999107, 1.0, 1.0, 0.6209070226208282, 0.4776873053131037, 0.5909402501865353, 0.38304611372110686, 0.5761290178648047, 0.7502495415001358, 0.6999214919756145, 0.7889213653390947, 0.629920929034602, 0.5924295517447509, 0.3426136760976885, 1.0, 0.9608461673226849, 1.0, 0.5140913145175611, 0.4147896756607047, 0.5640790326515688, 0.5190854538169563, 0.370321911798854, 1.0, 0.7391886750930661, 0.4407283286877167, 0.6909998157436037, 0.34003623200632915, 0.3022109370915457, 1.0, 0.5864709810898098, 0.4998348846944547, 1.0, 0.5099766481408452, 0.05632993161855453, 0.536560588211956, 0.6892870539097846, 0.8103529851697227, 0.545545649326698, 0.8042470965743207, 0.6164734436957756, 0.7795599284362567, 0.05966044812932477, 0.4141001550304735, 0.6400373216188153, 1.0, 0.4387244306491825, 0.2644534467602079, 0.6193453441662242, 1.0, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 1.0, 0.6941813926327731, 0.746924700641056, 0.35437970233785665, 0.46226804990029324, 0.11841238567827725, 0.5519698034749387, 1.0, 0.8007265089204272, 0.7017410285928236, 0.6937918207406566, 0.8604352575955447, 0.4054183564853334, 0.34575990398983625, 0.8433369370338967, 0.8710592763633311, 0.691735368923108, 0.5084373166195929, 0.8214029683767206, 0.40209781994840155, 0.40400364537321126, 1.0, 1.0, 0.602487595382981, 0.21948160223692786, 0.23683637774429345, 0.6428938087244226, 0.3372632052291401, 1.0, 0.7510051655425185, 0.4830718224801436, 0.19272438732935, 0.6585173023913214, 0.5149139863647084, 0.6443786426429735, 0.4085332823846201, 0.8661547359011423, 0.35029733718731737, 0.8686013831393846, 0.39999999999999997, 0.7023594991536818, 1.0, 0.6847340670900772, 1.0, 0.6937628353106088, 0.9891483218006352, 0.7744515423179406, 0.8519938276459833, 0.48492549020055464, 1.0, 1.0, 0.6151628493773373, 0.6901940524102714, 1.0, 0.6905247640638313, 0.42038175275163125, 0.8542712839031905, 1.0, 0.24895420055876946, 0.28890659929471596, 0.6578607431385983, 0.41080814539619515, 0.7322642870443095, 0.4598713891666677, 0.8004787053442896, 0.003996009641464269, 0.8253119306693903, 0.47102574599540464, 0.8034443090225696, 0.3842198496059124, 0.46628336334230014, 0.41118547774125425, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.7412242091620882, 0.6835302798798335, 0.7398815472813818, 0.8114529051148651, 0.643309301212212, 0.30215269230653224, 0.12, 0.8560255464372679, 0.7016265496309229, 0.766682914692771, 0.46843920200122524, 1.0, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 1.0, 1.0, 0.8180779662080888, 0.621238225076177, 0.7494203690984217, 0.5957444644618661, 1.0, 0.6988219320042083, 0.7396773803089467, 0.5909395885250581, 0.570217217727603, 0.8691371695837178, 0.20034796016239415, 0.7158229243802021]
Finish training and take 40m
