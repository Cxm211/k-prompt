Namespace(log_name='./RQ5/xcodeeval_16_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_16_2/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_16_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00045
  global_step = 3
  train_loss = 0.6514
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00045
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 41.34 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:41.34
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00042
  global_step = 5
  train_loss = 0.4816
  ********************
Previous best ppl:1.00045
Achieve Best ppl:1.00042
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 60.17 	 Previous best codebleu 41.34
  ********************
 Achieve Best bleu:60.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00038
  global_step = 7
  train_loss = 0.4161
  ********************
Previous best ppl:1.00042
Achieve Best ppl:1.00038
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 70.6 	 Previous best codebleu 60.17
  ********************
 Achieve Best bleu:70.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00037
  global_step = 9
  train_loss = 0.3277
  ********************
Previous best ppl:1.00038
Achieve Best ppl:1.00037
  ********************
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 73.5 	 Previous best codebleu 70.6
  ********************
 Achieve Best bleu:73.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00037
  global_step = 11
  train_loss = 0.2939
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 74.48 	 Previous best codebleu 73.5
  ********************
 Achieve Best bleu:74.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00038
  global_step = 13
  train_loss = 0.2654
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 74.98 	 Previous best codebleu 74.48
  ********************
 Achieve Best bleu:74.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00038
  global_step = 15
  train_loss = 0.2157
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 75.55 	 Previous best codebleu 74.98
  ********************
 Achieve Best bleu:75.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00038
  global_step = 17
  train_loss = 0.1907
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 75.05 	 Previous best codebleu 75.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00039
  global_step = 19
  train_loss = 0.1618
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 75.02 	 Previous best codebleu 75.55
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00039
  global_step = 21
  train_loss = 0.2064
  ********************
Previous best ppl:1.00037
BLEU file: ./data/RQ5/xcodeeval_16_2/validation.jsonl
  codebleu-4 = 75.59 	 Previous best codebleu 75.55
  ********************
 Achieve Best bleu:75.59
  ********************
reload model from RQ5/xcodeeval_16_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_16_2/test.jsonl
  codebleu = 72.98 
  Total = 500 
  Exact Fixed = 1 
[73]
  Syntax Fixed = 1 
[425]
  Cleaned Fixed = 2 
[254, 355]
  ********************
  Total = 500 
  Exact Fixed = 1 
[73]
  Syntax Fixed = 1 
[425]
  Cleaned Fixed = 2 
[254, 355]
  codebleu = 72.98 
[0.9249386102976953, 0.9684186537117094, 0.3597028007181292, 0.8285503011428544, 0.5240259477923144, 0.21604519322083335, 0.49007773220454587, 0.23811850501424137, 0.5760442553563221, 0.6717364058179986, 0.9486021234366251, 0.8127989580285089, 0.7050736143053937, 0.7840664381594029, 0.8738583170021379, 0.4693955864510486, 0.5313357348360503, 0.9785699270907009, 0.7996636861006762, 0.8635205655237428, 0.8687207744628378, 0.589751403979808, 0.959945180208001, 0.9344367229312802, 0.8596055360028632, 0.45607321335150974, 0.5452287811589471, 0.901175714210833, 0.9194633420202082, 0.11061604063943534, 0.6563492736647024, 0.8441372338928508, 0.8234174544615629, 0.22467714258428573, 0.5393913750037733, 0.8059663450023036, 0.6527764179246671, 0.9426787386209374, 0.9847813251545576, 0.6214028319131882, 0.38055604522674474, 0.9802578447391919, 0.9321918895830998, 0.21659623863784408, 0.806490922254441, 0.9386210590467857, 0.8574290303367051, 0.7113199842059187, 0.6194178615287717, 0.8803828210815028, 0.42728670898141874, 0.9382702025626704, 0.7513610022490866, 0.5438775510204081, 0.8850357193836327, 0.9036267549024084, 0.8675700280536951, 0.7796474387592713, 0.7829914767019434, 0.5445425808684521, 0.6984245131809161, 0.32950943279889294, 0.9174356609699263, 0.663952114335559, 0.8927899452472845, 0.9299687695441708, 0.7924048855268875, 0.5338617744031935, 0.8995377367121644, 0.21013864355629927, 0.9285424148564965, 0.2690529321329123, 0.9674900184215338, 0.7302671755916025, 0.871143401314145, 0.5931843588460451, 0.8384056697171074, 0.9837244566208403, 0.9321212086381336, 0.3895038116127578, 0.696318134186358, 0.9505095058348281, 0.683426352936423, 0.8441882987189739, 0.2858574379633786, 0.39017622745904734, 0.8014139035130625, 0.8085386086430999, 0.40657051476123585, 0.2459864585205677, 0.9352829048891074, 0.8001529489753312, 0.9529223256974573, 0.5985680617000989, 0.6326608997800717, 0.9486052495001633, 0.9348200839674985, 0.3325566319740153, 0.5505450693049353, 0.8156688509011429, 0.9060561003826615, 0.948343184028229, 0.46470075473436884, 0.7763592102744433, 0.9857368328793006, 0.8256562521445959, 0.6783309780452145, 0.4810010686280327, 0.8987877877981749, 0.9085405865863987, 0.6249954610889428, 0.6181106606584272, 0.8077465948099589, 0.6442954909515588, 0.672952360812484, 0.8077897114293389, 0.6377297703424358, 0.9515948565292411, 0.40981188778552935, 0.854906905704294, 0.890827574259109, 0.7970123315164127, 0.8257815238937638, 0.8551429730510784, 0.78946048013366, 0.6359084461047401, 0.6690731553562745, 0.4472915191655734, 0.4657333819352129, 0.8443840073818547, 0.3560328527895647, 0.6120229206037862, 0.3110090555266167, 0.956301548538977, 0.9246772982250628, 0.3085356854689776, 0.9545958335617004, 0.553376063289682, 0.9257967173408735, 0.7920617845792342, 0.9242338290598, 0.8741647989058143, 0.9251375134812259, 0.9183268771187578, 0.8979282931477564, 0.8274771270568544, 0.4974787561896511, 0.5720677290587932, 0.9521775318277603, 0.8944384288649418, 0.975255869034251, 0.8320476098374262, 0.9813483172696138, 0.9762098677705813, 0.855857715023951, 0.7350520091789042, 0.7202507328873297, 0.5488586128738006, 0.679420971784908, 0.7242120100978349, 0.2648126845281553, 0.6777438912696802, 0.962102708701077, 0.9121866600841926, 0.9021902095080698, 0.46124203184813023, 0.6114198900860407, 0.8392067093595759, 0.968639364262265, 0.8297713512550178, 0.6258706049241418, 0.7494592296476774, 0.8975978993714945, 0.8807627027973213, 0.7280873261576656, 0.8199251872608782, 0.8300820971250142, 0.8139937284064771, 0.2217088286454293, 0.9410067825194739, 0.5399191018302276, 0.5184046294885256, 0.25613546330440956, 0.7425959612860655, 0.8694952470533828, 0.6639452340766057, 0.35342583816828116, 0.9191335221980989, 0.8658945472295025, 0.49086107377017946, 0.5896707088335948, 0.8743040827904878, 0.6274915173149378, 0.5928108277067345, 0.5603169493698431, 0.19834047868927998, 0.8373737221839024, 0.911135440721319, 0.6872874943212983, 0.9642836546885394, 0.1427616762031343, 0.9508336769881993, 0.9441311059579389, 0.06701038259138103, 0.9617458190493637, 0.39013478691113623, 0.6622159150456424, 0.9447313137718458, 0.5317588322123294, 0.9087123429574342, 0.48964031787592843, 0.9498961575933424, 0.8006221940463284, 0.953707845659838, 0.8090043314450341, 0.8916198680457144, 0.9136463643811175, 0.7627672284250658, 0.11420513410843788, 0.19832929234074576, 0.8055864673667326, 0.9477526452574314, 0.9324333440186039, 0.9079970942932454, 0.3330584663396393, 0.4822120322474115, 0.8044136830940034, 0.8942668643714475, 0.9232567135322138, 0.5869861009994992, 0.6234595852577036, 0.6586811579191418, 0.9559272188646564, 0.7013845075598505, 0.5065430957977182, 0.8553678887608935, 0.7819620806451755, 0.7826741503670085, 0.9634485800803125, 0.5520925204619396, 0.8695075881277006, 0.7901580927216791, 0.9861254102280717, 0.24979512120906117, 0.947573383205105, 0.9013388166447551, 0.6351531795376738, 0.9583676774082095, 0.7562246195563819, 0.7223585526424052, 0.9221927719632479, 0.9604527042523284, 0.5845276904503154, 0.943907690508595, 0.8640291460643674, 0.6041408550694012, 0.6918292471870391, 0.55371209196799, 0.24205445563494446, 0.9029521565089935, 0.19752609894716355, 0.3986890909032312, 0.9284243634590652, 0.637582838704339, 0.755835248897915, 0.9176543852669206, 0.3933186311989352, 0.9342402682492861, 0.8002173829016934, 0.6304354583069092, 0.9692036989337549, 0.6365891247781525, 0.9425986064448988, 0.9617839620555828, 0.9266465778740411, 0.9474658767322486, 0.7481412556512403, 0.9498690706862425, 0.8277424644338935, 0.7565711923511311, 0.0968369622508172, 0.7325511439136385, 0.9334215487306798, 0.9751679833306697, 0.7416781597721337, 0.7757425155088926, 0.9100431637124131, 0.7698422759711345, 0.9758763186213713, 0.5547416116865838, 0.9407613352737074, 0.7232043610871355, 0.7493577391671854, 0.4948768409876656, 0.8912891763677813, 0.8207174296242723, 0.46158825189644004, 0.9928116843198462, 0.718322036158023, 0.9147509231273736, 0.465091875967109, 0.847976013670539, 0.6345917614751148, 0.9732456940628658, 0.9668495847081082, 0.46804311875093774, 0.937831257401287, 0.9601642855670185, 0.8857658481998287, 0.19040347702585775, 0.7455769257725797, 0.8206082150573881, 0.940663678095734, 0.8113688567236648, 0.8943767670150264, 0.5475247932541882, 0.9291578006474959, 0.25592663683341055, 0.637455335351989, 0.3454641283088835, 0.9655267507815484, 0.22091638149421566, 0.79908393484228, 0.9058299447549807, 0.6924354337443256, 0.6548454072481232, 0.9823448177520746, 0.7501255059004492, 0.7698459366315475, 0.9666856331752072, 0.9296230645706806, 0.9074482656355349, 0.9295976632675957, 0.6495267219526154, 0.9866981257943701, 0.570989795360499, 0.8489768594294123, 0.7311090731099734, 0.8785117532819908, 0.8934978877717091, 0.6830631050329368, 0.802792334029212, 0.6946257494460855, 0.9582488725526279, 0.8788846428096109, 0.9094304925332901, 0.928413279757291, 0.6216058374935409, 0.8202499593885415, 0.8268222710184233, 0.7233190966578671, 0.9629364535821479, 0.48965579428890615, 0.9285198204347194, 0.9393720392848304, 0.7463795010367267, 0.29119834125570104, 0.9243493537919436, 0.9539904817368636, 0.940720679223622, 0.9498733743967636, 0.9262386824642832, 0.2535810718077191, 0.9042111220423195, 0.642313952795702, 0.5810734406270385, 0.4328828981880556, 0.9731496374915098, 0.30758102701304096, 0.24322148069118696, 0.8212828564690575, 0.3742924995543897, 0.9446842919965326, 0.42428373858678303, 0.4661937691043915, 0.8121424710065667, 0.28685396953518916, 0.8259108946799769, 0.9646055689051855, 0.8302506632849143, 0.867980815381816, 0.7552847912459166, 0.13579710600674177, 0.853484704976984, 0.900000684067662, 0.8725901158068361, 0.30044722639222426, 0.9212100020533327, 0.5898743279997497, 0.31764171070174957, 0.9136846145795283, 0.7806363178563275, 0.8078579828142751, 0.8562515048247652, 0.8045201592161606, 0.9544248082184021, 0.9127782544487824, 0.8699110799666838, 0.3325195752446274, 0.9465224177706513, 0.8128239022460542, 0.962229839777732, 0.633509288830505, 0.8284512541634445, 0.41836453231796233, 0.6562955803712851, 0.5643672366251316, 0.43808453932073843, 0.5864458370254797, 0.9800602346600462, 0.321597484442518, 0.8050090290148326, 0.9624306767024622, 0.9246652084929193, 0.967150116841063, 0.9736311442344765, 0.6826635463996102, 0.8229777078946539, 0.6414197836322189, 0.8093645661462701, 0.5291262812864778, 0.944817116312316, 0.945897725871365, 0.6113887462384089, 0.9497338034253828, 0.7914330306940703, 0.31443578868236866, 0.9277810530903238, 0.9049287137173128, 0.9161103307638829, 0.8601706721604159, 0.7628541847804483, 0.4680849519025553, 0.9388131407190206, 0.9675589330971408, 0.6458485015843256, 0.6024557974505752, 0.5591321187210834, 0.6742207105645364, 0.6362439585829495, 0.852471803082816, 0.8601402886946303, 0.7701904603659033, 0.37194891290568377, 0.6592136870095452, 0.973146212599268, 0.5978112872134451, 0.23077016795629074, 0.8038306806820452, 0.8836863746511332, 0.7964726901713943, 0.7351718888828274, 0.8151275754024493, 0.5309920984684203, 0.8696331987786137, 0.8118756794025181, 0.49718591281011865, 0.7699457110604023, 0.9528960563859521, 0.8746778191063311, 0.852277159412532, 0.36665611193413983, 0.9715471473605286, 0.031930394824990466, 0.779609604768952, 0.78022613983968, 0.8897241732458334, 0.5978882651004173, 0.9469679666767363, 0.9355918017706362, 0.9405088388791978, 0.9823448177520746, 0.4777354426610022, 0.7488636949660352, 0.3990229047985451, 0.9262112558154699, 0.8241594897707057, 0.8333506758305262, 0.25489934536355485, 0.8564822037598797, 0.511501959683854, 0.6810842044592471, 0.3004090125035224, 0.23845222619115825, 0.9207225522863047, 0.9087427909672332, 0.44949524080022113, 0.6219943883989547, 0.9290885216076363, 0.8389497416642955, 0.9407613352737074, 1.0, 0.9292619245050848, 0.8227600561608516, 0.959095521027233, 0.4927053152538463, 0.4129771429438506, 0.8020816921073726, 0.9271328028414589, 0.8457781931186075]
Finish training and take 55m
