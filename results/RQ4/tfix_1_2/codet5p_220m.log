Namespace(log_name='./RQ5/tfix_1_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_1_2/codet5p_220m', data_dir='./data/RQ5/tfix_1_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'var fSuccess = true;           while (fSuccess && nSigsCount > 0) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'fSuccess = true;           while (fSuccess && nSigsCount > 0) {'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.4058221012890009e+293
  global_step = 2
  train_loss = 29.6595
  ********************
Previous best ppl:inf
Achieve Best ppl:1.4058221012890009e+293
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 10.6 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:10.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 4.385358648630702e+296
  global_step = 3
  train_loss = 33.374
  ********************
Previous best ppl:1.4058221012890009e+293
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 17.08 	 Previous best codebleu 10.6
  ********************
 Achieve Best bleu:17.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 4.317703207881324e+294
  global_step = 4
  train_loss = 16.1848
  ********************
Previous best ppl:1.4058221012890009e+293
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 20.05 	 Previous best codebleu 17.08
  ********************
 Achieve Best bleu:20.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 6.8840229657037865e+289
  global_step = 5
  train_loss = 16.8339
  ********************
Previous best ppl:1.4058221012890009e+293
Achieve Best ppl:6.8840229657037865e+289
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 20.42 	 Previous best codebleu 20.05
  ********************
 Achieve Best bleu:20.42
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 5.867847168199355e+282
  global_step = 6
  train_loss = 5.1728
  ********************
Previous best ppl:6.8840229657037865e+289
Achieve Best ppl:5.867847168199355e+282
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 20.7 	 Previous best codebleu 20.42
  ********************
 Achieve Best bleu:20.7
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.6939384544865267e+276
  global_step = 7
  train_loss = 4.1192
  ********************
Previous best ppl:5.867847168199355e+282
Achieve Best ppl:2.6939384544865267e+276
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 24.92 	 Previous best codebleu 20.7
  ********************
 Achieve Best bleu:24.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.775699648760871e+271
  global_step = 8
  train_loss = 2.296
  ********************
Previous best ppl:2.6939384544865267e+276
Achieve Best ppl:1.775699648760871e+271
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 28.57 	 Previous best codebleu 24.92
  ********************
 Achieve Best bleu:28.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 5.484812611750434e+267
  global_step = 9
  train_loss = 2.3575
  ********************
Previous best ppl:1.775699648760871e+271
Achieve Best ppl:5.484812611750434e+267
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 31.54 	 Previous best codebleu 28.57
  ********************
 Achieve Best bleu:31.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 4.3835667757046005e+265
  global_step = 10
  train_loss = 0.9574
  ********************
Previous best ppl:5.484812611750434e+267
Achieve Best ppl:4.3835667757046005e+265
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 32.63 	 Previous best codebleu 31.54
  ********************
 Achieve Best bleu:32.63
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 4.694525617475053e+264
  global_step = 11
  train_loss = 1.3008
  ********************
Previous best ppl:4.3835667757046005e+265
Achieve Best ppl:4.694525617475053e+264
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 33.28 	 Previous best codebleu 32.63
  ********************
 Achieve Best bleu:33.28
  ********************
reload model from RQ5/tfix_1_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_2/test.jsonl
  codebleu = 31.46 
  Total = 500 
  Exact Fixed = 2 
[250, 371]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[439, 469]
  ********************
  Total = 500 
  Exact Fixed = 2 
[250, 371]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[439, 469]
  codebleu = 31.46 
[0.294193123345844, 0.003529411764705882, 0.0, 0.003389830508474576, 0.6017044953842279, 0.8320581942140033, 0.0, 0.5230337173743334, 0.001176470588235294, 0.1263157894736842, 0.48892217889857903, 0.40351010348410404, 0.0, 0.7081901974921907, 0.6179484048244397, 0.0, 0.5007520143098162, 0.44537042738686844, 0.6072829381957063, 0.0, 0.32500621671119223, 0.3810839201460291, 0.12024412733257556, 0.4695215451415645, 0.5947004743508131, 0.007318543317782915, 0.2168912243557935, 0.20098573217566892, 0.5876704051777762, 0.2585854775543299, 0.2701592287180146, 0.0, 0.3041184861110963, 0.002352941176470588, 0.33593196041831674, 0.6068905779332499, 0.0, 0.10556510016615601, 0.1313088648086764, 0.3172502790488379, 0.5511412946838178, 0.6617159068122367, 0.10202520055807765, 0.16884893628390868, 0.30638080817174057, 0.0, 0.5853252928385881, 0.25030218563680073, 0.23118228907369376, 0.08941705499276116, 0.03197607455155234, 0.06868871052596268, 0.0, 0.30547859799333926, 0.14272438732934997, 0.3037943265112842, 0.1540411422937383, 0.28743604624264807, 0.2809224343436397, 0.09321329541140749, 0.8694537065658159, 0.714297617187952, 0.30400707665831134, 0.004705882352941176, 0.5531486686075724, 0.6065660011644513, 0.3030235636121487, 0.3514204039027595, 0.001176470588235294, 0.18982883380417964, 0.3249374989928413, 0.12524853777356526, 0.5495870268935772, 0.16009196214281068, 0.002352941176470588, 0.33035107882031767, 0.14651625746516259, 0.21624791656496387, 0.3707382893374178, 0.2089288547269641, 0.259901330321429, 0.3734424024864518, 0.671354105093982, 0.6142325450376052, 0.26171603076506994, 0.0, 0.16798247283121842, 0.6193215043761717, 0.19928064558318875, 0.0634760408840261, 0.2325411694815258, 0.27374575900475356, 0.34422563116929883, 0.0, 0.013029282330504833, 0.2817058924642449, 0.4650122347466785, 0.0, 0.334613477749145, 0.5688724788208753, 0.3064349380623741, 0.001176470588235294, 0.36378698407319565, 0.5865501458907969, 0.7744515423179406, 0.03876390056280174, 0.32768505892447164, 0.6046805983404462, 0.6853285594711894, 0.05270446328111167, 0.0, 0.5876935279546878, 0.28813818961193893, 0.3079510025390523, 0.001176470588235294, 0.0, 0.013156973412695786, 0.5394384445658513, 0.10978757137963861, 0.6870008904965429, 0.5991006755277279, 0.15792031937901393, 0.19965776880677807, 0.07258513757691838, 0.0, 0.0, 0.18987101675515033, 0.35774435710276153, 0.14063318583784112, 0.5959201143573529, 0.39707312871002565, 0.6333222681500437, 0.0, 0.29176871792267156, 0.5445712219962231, 0.4460952454685818, 0.5104049742151137, 0.001764705882352941, 0.05107366833322386, 0.0, 0.6368551910912801, 0.4964988656582785, 0.2553806629235062, 0.41389347677795546, 0.0, 0.4320939278284144, 0.3245966590765959, 0.19721551143918004, 0.001176470588235294, 0.0, 0.001176470588235294, 0.6612091961731477, 0.0, 0.0, 0.23852669171465737, 0.7810169160146574, 0.5456083510375599, 0.5869940359456796, 0.0, 0.5781699704154728, 0.16387247882087536, 0.5968569446448142, 0.3962063391124521, 0.716112366099465, 0.4289622792485105, 0.0, 0.001176470588235294, 0.17104328647540679, 0.1309488510007413, 0.38577998647726336, 0.30548353255042543, 0.0, 0.3509513834079392, 0.004117647058823529, 0.1867721495706093, 0.6937610625069692, 0.19811311270360624, 0.0, 0.21648010094722356, 0.05454545454545454, 0.001176470588235294, 0.19683536503236188, 0.40045633055488905, 0.21613854980565517, 0.5120840309198935, 0.0, 0.0, 0.0, 0.0, 0.20249538690248395, 0.7562262989907678, 0.3175534792958457, 0.0, 0.599954433216811, 0.0763969765448633, 0.003529411764705882, 0.0, 0.3041184861110967, 0.6441764742916929, 0.0, 0.5351672642347434, 0.5225513859617287, 0.40485075092622447, 0.7120481432211276, 0.08632896902254418, 0.6271806220068676, 0.2515899102808434, 0.5057016122268714, 0.41555396853593685, 0.20544130682897116, 0.3670360873164629, 0.3291080008520348, 0.2074841111688946, 0.30789007812502395, 9.204946623287262e-05, 0.0, 0.5096575323755069, 0.44619887519287305, 0.6014355612114213, 0.7586059507468779, 0.23881670491218876, 0.6740103526642764, 0.0, 0.17649475021297867, 0.13999305755157954, 0.5151167061659059, 0.3383981883736556, 0.7049292663393218, 0.8711014238325483, 0.0007636634189037896, 0.5864902989394949, 0.0, 0.001176470588235294, 0.23501280275786365, 0.15444633579600947, 0.002352941176470588, 0.5952507734471753, 0.6276141042587143, 0.19999999999999998, 0.24554880580590016, 0.6934634181317687, 0.16445081466346223, 0.002352941176470588, 0.6744957521914574, 0.33349400983462363, 0.5783426377247933, 0.001176470588235294, 0.7066764917309769, 0.0, 1.0, 0.16739236471601618, 0.8081432239997528, 0.3983170314086985, 0.001764705882352941, 0.0, 0.08341906490542277, 0.0, 0.31571709605517556, 0.0, 0.38537461062620737, 0.6137830788325551, 0.39984818242981623, 0.46058639652925126, 0.5658229243802022, 0.36168740955916506, 0.6624678525854575, 0.04288633403141762, 0.0, 0.002352941176470588, 0.3224763972171874, 0.6390840461779514, 0.3086793245130883, 0.16404587542424348, 0.20547931557570304, 0.31221243482615924, 0.004117647058823529, 0.0, 0.29772988250804316, 0.3025026244190036, 0.0, 0.02509257616554318, 0.3899037221692271, 0.29730870951168364, 0.24515163944586357, 0.7462148574687999, 0.0, 0.5966289756845221, 0.0029411764705882353, 0.240165525932854, 0.4924026003452121, 0.19208433116832097, 0.2926440451121681, 0.21167688369919976, 0.5908527155778469, 0.6752168053670519, 0.5945461805321692, 0.0, 0.43092220702775774, 0.22563905418142233, 0.0, 0.3161536118593882, 0.1616361737803548, 0.2604036277698142, 0.15167147511043055, 0.6729687842179634, 0.275806923890949, 0.9332374077890744, 0.002352941176470588, 0.5512806130163017, 0.3094129188248411, 0.05434085966643876, 0.0023715415019762843, 0.001176470588235294, 0.30817812844912906, 0.001176470588235294, 0.23437619784260316, 0.0, 0.587302552820651, 0.29118482953445396, 0.0, 0.8378414230005442, 0.13886455556987948, 0.9279295654418451, 0.1249790236206589, 0.3084137759911653, 0.8263746416410218, 0.2571934617905235, 0.05129879162633799, 0.0, 0.21665383671431127, 0.24113622206392205, 0.5972918031262187, 0.0, 0.6828632333952069, 0.2932175409781875, 0.0, 0.7869809565486254, 0.690431182325474, 0.5141093065454635, 0.6802269979836307, 0.5482702090228039, 0.0029411764705882353, 0.0, 0.6991152153229385, 0.11846759342348784, 0.1623497956803782, 0.009097321330253455, 0.32974530567275967, 0.4796559689210862, 0.6625167201921691, 0.2920007155807635, 0.30911278243440216, 0.4447037792542853, 0.5350656746186406, 0.03451009443553219, 0.31125710805847795, 0.42611431926626925, 0.003529411764705882, 0.5939638212098222, 0.7244538835978105, 0.6431847411727684, 0.002352941176470588, 0.49509521539880075, 0.5962686948344054, 0.002352941176470588, 0.12372369087900142, 0.5307403851472986, 0.6898018405233148, 0.008140532858308255, 0.001176470588235294, 1.0, 0.5821510125623002, 0.001764705882352941, 0.5954580416706734, 0.30146363447564245, 0.8250515340301889, 0.365545649326698, 0.12619887519287304, 0.3053372331674803, 0.2632088866091689, 0.000588235294117647, 0.3755980153481091, 0.6167627952584476, 0.001176470588235294, 0.2570176621196889, 0.5758081453961951, 0.7225812256666353, 0.04668633182469073, 0.005294117647058823, 0.5953312173245887, 0.3027335840242695, 0.5964188584437852, 0.0, 0.06305872131083244, 0.5989739517110069, 0.315626396611076, 0.6468219027944806, 0.1897656759158195, 0.004705882352941176, 0.4871274292728927, 0.32300250443075146, 0.7179453925554036, 0.6447093051280361, 0.14904400199501283, 0.39969884422068214, 0.12863345901812176, 0.001176470588235294, 0.7192415097097951, 0.6754386750930661, 0.0, 0.0, 0.38053094249542213, 0.003529411764705882, 0.6714771286623475, 0.6662022025756482, 0.18305414547576998, 0.5931028959125855, 0.3570581685065034, 0.19999999999999998, 0.15529892970465703, 0.16654119641819093, 0.4778533416343903, 0.40509947643523875, 0.27699577248297835, 0.8761070565997585, 0.5278000584279399, 0.5922870716373501, 0.3039874013638151, 0.21056545775751523, 0.6183689771600134, 0.3184856323680557, 0.0, 0.09923884320773292, 0.5722818111225448, 0.001176470588235294, 0.6163404365389471, 0.6017607272210093, 0.5870409412798016, 0.6319730195988179, 0.09, 0.40739727760582384, 0.038139497655491794, 0.7260484492137269, 0.24115726433327778, 0.8292906179772745, 0.03126528209192489, 0.6071161915374703, 0.6633817131448447, 0.11994980970743098, 0.5157934678740115, 0.4362905012599202, 0.5764817322245884, 0.31979211542077673, 0.19944598337950137, 0.2633867379816763, 0.0, 0.0, 0.8326781496971694, 0.5180708900575993, 0.5351213133439364, 0.5960773901726633, 0.002352941176470588, 0.22914971209235602, 0.4311786804290618, 0.001176470588235294, 0.11958452021462818, 0.3583507427572924, 0.2007875138142567, 0.6142756196415784, 0.4267511289804571, 0.7539341600372499, 0.3119767832960183, 0.0, 0.7710658705890923, 0.32303938735359705, 0.0, 0.4951618230098882, 0.003529411764705882, 0.002352941176470588, 0.6617190381976104, 0.42580814539619516, 0.001176470588235294, 0.5955384178961776, 0.0, 0.48336583921273063, 0.5095323210175184, 0.6197413161741244, 0.0, 0.0062887001189286754, 0.17251988127521883, 0.6721193723777923, 0.4695633649758072, 0.6986613814884177, 0.642418735455043, 0.16161528055738653, 0.5831600240538137, 0.47047922050080593, 0.594425881153537, 0.001176470588235294, 0.001764705882352941]
Finish training and take 49m
