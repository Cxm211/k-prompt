Namespace(log_name='./RQ5/tfix_1_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_1_2/codet5p_770m', data_dir='./data/RQ5/tfix_1_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': 'var fSuccess = true;           while (fSuccess && nSigsCount > 0) {', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'fSuccess = true;           while (fSuccess && nSigsCount > 0) {'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.972028155874038e+271
  global_step = 2
  train_loss = 31.9807
  ********************
Previous best ppl:inf
Achieve Best ppl:3.972028155874038e+271
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 17.54 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:17.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 6.718840131615326e+260
  global_step = 3
  train_loss = 26.3273
  ********************
Previous best ppl:3.972028155874038e+271
Achieve Best ppl:6.718840131615326e+260
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 20.86 	 Previous best codebleu 17.54
  ********************
 Achieve Best bleu:20.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 7.038824006400027e+248
  global_step = 4
  train_loss = 7.4087
  ********************
Previous best ppl:6.718840131615326e+260
Achieve Best ppl:7.038824006400027e+248
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 25.38 	 Previous best codebleu 20.86
  ********************
 Achieve Best bleu:25.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 4.120047153241395e+243
  global_step = 5
  train_loss = 3.6495
  ********************
Previous best ppl:7.038824006400027e+248
Achieve Best ppl:4.120047153241395e+243
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 35.94 	 Previous best codebleu 25.38
  ********************
 Achieve Best bleu:35.94
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 2.889257642588306e+245
  global_step = 6
  train_loss = 2.3759
  ********************
Previous best ppl:4.120047153241395e+243
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 39.17 	 Previous best codebleu 35.94
  ********************
 Achieve Best bleu:39.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.2268573344602687e+249
  global_step = 7
  train_loss = 0.3941
  ********************
Previous best ppl:4.120047153241395e+243
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 41.69 	 Previous best codebleu 39.17
  ********************
 Achieve Best bleu:41.69
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 5.60258156053588e+253
  global_step = 8
  train_loss = 0.1694
  ********************
Previous best ppl:4.120047153241395e+243
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 46.01 	 Previous best codebleu 41.69
  ********************
 Achieve Best bleu:46.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 2.757517932805119e+257
  global_step = 9
  train_loss = 0.0589
  ********************
Previous best ppl:4.120047153241395e+243
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 45.98 	 Previous best codebleu 46.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.716424459002836e+259
  global_step = 10
  train_loss = 0.066
  ********************
Previous best ppl:4.120047153241395e+243
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 46.48 	 Previous best codebleu 46.01
  ********************
 Achieve Best bleu:46.48
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.5706614062071932e+260
  global_step = 11
  train_loss = 0.1858
  ********************
Previous best ppl:4.120047153241395e+243
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 46.77 	 Previous best codebleu 46.48
  ********************
 Achieve Best bleu:46.77
  ********************
reload model from RQ5/tfix_1_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_2/test.jsonl
  codebleu = 44.9 
  Total = 500 
  Exact Fixed = 3 
[20, 371, 435]
  Syntax Fixed = 2 
[151, 250]
  Cleaned Fixed = 4 
[179, 183, 439, 469]
  ********************
  Total = 500 
  Exact Fixed = 3 
[20, 371, 435]
  Syntax Fixed = 2 
[151, 250]
  Cleaned Fixed = 4 
[179, 183, 439, 469]
  codebleu = 44.9 
[0.2969229739025319, 0.4923184035468318, 0.3822611684518117, 0.13314480047865082, 0.6058511595444083, 0.8089812711370801, 0.031479448748117415, 0.4324155868831505, 0.4838532730110269, 0.1263157894736842, 0.4868474083243507, 0.0, 0.19694955531861266, 0.2432629996892332, 0.6196949179201965, 0.7487851138365109, 0.3664788113796552, 0.6822267233801039, 0.6072829381957063, 0.2650192378669725, 0.7145082485446812, 0.3810839201460291, 0.18071425679443714, 0.4769755715894127, 0.6316808931901063, 0.007318543317782915, 0.9247064373218827, 0.26164945162585174, 0.5876704051777762, 0.26571258603602055, 0.7527713968307794, 0.0, 0.40797050720149675, 0.9113854927948712, 0.6701882867722436, 0.5593491945324636, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6398412446099295, 0.5709813808622628, 0.7398428843643332, 0.08243441225905121, 0.6179786396129474, 0.5762213228133293, 0.0, 0.377688124735147, 0.025332943843621038, 0.000588235294117647, 0.26463432232066675, 0.4897956518467331, 0.6261644484790369, 0.3428571428571428, 0.3076743686758359, 0.14272438732934997, 0.5699752229822191, 0.3070782538395746, 0.8320872291303743, 0.621669267950019, 0.5806555300037702, 0.2797939725273053, 0.32590164790385, 0.4403046408286988, 0.7425037474433094, 0.5531486686075724, 0.5486995342090832, 0.7583139646397308, 0.42051948730583255, 0.39531233064601895, 0.24993129827696675, 0.9488930040605423, 0.05150111485929067, 0.5426744349179697, 0.5590215237734176, 0.702664818575847, 0.29035107882031763, 0.3, 0.8362094899535875, 0.4096090687678984, 0.5937823617478168, 0.4718994912491358, 0.23587195517880344, 0.5489947351756637, 0.15402766256134798, 0.6291132729759822, 0.10046757545996973, 0.48115588878443294, 0.4288084969234561, 0.18386051839175482, 0.38221344923103673, 0.7307832770028482, 0.7328635972785995, 0.38869711590386236, 0.2955414762032743, 0.3100552189599008, 0.6432003763581217, 0.8349004181959196, 0.8078417066089706, 0.7203032087588108, 0.5688724788208753, 0.6114366051014047, 0.35773256899516204, 0.4499814192409278, 0.6445931574752105, 0.7744515423179406, 0.0, 0.32768505892447164, 0.517680392217046, 0.7089189741040722, 0.05270446328111167, 0.03985271067894747, 0.5876935279546878, 0.3776850589244717, 0.29986724447220714, 0.509901499184051, 0.0, 0.47937957396762365, 0.5394384445658513, 0.43963148731349166, 0.6620008904965429, 0.6054648889134016, 0.5599220696579421, 0.6389870100570282, 0.15, 0.080430901690111, 0.5914684833112177, 0.005911738669324414, 0.38928655869457884, 0.5246680028432865, 0.4581666787126958, 0.39707312871002565, 0.9572022249328025, 0.0, 0.7356545700963575, 0.6667528334695118, 0.4460952454685818, 0.5104049742151137, 0.4981021054383593, 0.075349882027966, 0.0, 0.7368551910912801, 0.4964988656582785, 0.5815717636244672, 0.41389347677795546, 0.0, 0.3196164854641599, 0.3152548038096513, 0.30384756258295087, 0.44341793336632984, 0.17619887519287303, 0.7547391347384329, 0.6365638961412512, 0.39999999999999997, 0.32937243030954233, 0.24558090566512325, 0.6432204007219688, 0.5456083510375599, 0.5869940359456796, 0.075, 0.3974665348343659, 0.39347209291868157, 0.35359847771707387, 0.35112474680063555, 0.725598015348109, 0.32766085397989925, 0.39606856837889304, 0.27811123504621826, 0.3013490547001119, 0.316275735434565, 0.38577998647726336, 0.6868474083243508, 0.2571428571428571, 0.24229545276671333, 0.744172077582127, 0.11769995376933454, 0.0, 0.3521020340200895, 0.4976851784475747, 0.9545937759301073, 0.11453360085481801, 0.36958023439972043, 0.6304241997444744, 0.8480337752809987, 0.39271107513751286, 0.5865433553637871, 0.693164185232684, 0.6582801045768549, 0.42869111792447884, 0.0, 0.627433210392216, 0.7562262989907678, 0.668303287269264, 0.015545917150946366, 0.6637923553099535, 0.3724522513548441, 0.5263595999090642, 0.0, 0.2516696835818385, 0.21553759164314246, 0.03606856837889304, 0.49155776520961103, 0.6119897554126184, 0.40485075092622447, 0.7577915078725159, 0.3650204762337497, 0.6177818063286673, 0.2515899102808434, 0.813555039320798, 0.4697300318922557, 0.604268896394741, 0.5938525099362653, 0.9365276486140073, 0.2074841111688946, 0.610749695042806, 0.8731821665766971, 0.289188727096548, 0.5856658580524812, 0.5536911179244788, 0.6515539773644267, 0.44042413256505974, 0.2188168287832668, 0.6562426784907449, 0.2882517404063692, 0.18697910980105636, 0.5824960113234149, 0.8212015472955612, 0.5490415390782131, 0.16699790063982922, 0.9414428605659417, 0.01147923782919177, 0.46696280030444565, 0.6778946593651672, 0.02594241303126727, 0.3228616462719601, 0.6318245278332744, 0.6513340021370451, 0.4062652820919249, 0.6276141042587143, 0.24649139863647082, 0.7126993893916607, 0.5565282465341846, 0.3703816369492127, 0.6028436898434268, 0.6744957521914574, 0.777720847005166, 0.5783426377247933, 0.007942529874534395, 0.7066764917309769, 0.4453380192160682, 0.8839519845504367, 0.4997519941450126, 0.677539604764779, 0.5870313226770849, 0.4742971510014376, 0.22499999999999998, 0.304739134738433, 0.24971846964396216, 0.741008357228776, 0.4769157224074937, 0.0, 0.499703190454726, 0.39450059105565455, 0.3949591699549406, 0.32056997169235896, 0.29996157080519553, 0.6353451348753147, 0.2918856692576617, 0.4412889248912303, 0.6512701852171989, 0.3022406529370334, 0.6390840461779514, 0.5291289441434743, 0.6318245278332744, 0.12285168104633384, 0.3159503405396788, 0.6463729364766281, 0.31497974289458347, 0.4106853453725031, 0.3054149464302111, 0.0, 0.6308145099011879, 0.41450751442680045, 0.0, 0.293808526265082, 0.7462148574687999, 0.031479448748117415, 0.5966289756845221, 0.6253143105742339, 0.23685590915022586, 0.4924026003452121, 0.16661547895217288, 0.0, 0.48984480713799083, 0.6215013444171207, 0.7510484411519667, 0.0, 0.8072615927227478, 0.4395150618969229, 0.6213540763089959, 0.001176470588235294, 0.7907486083314552, 0.3581126260069283, 0.7923024832485084, 0.26397069234540055, 0.23909950518348563, 0.4614904538788478, 0.6376971907620531, 0.6791769534168779, 0.7017258711057603, 0.7641554939163866, 0.05349072297901379, 0.002514246921675957, 0.031479448748117415, 0.586931012770975, 0.2947014396684688, 0.4843669201329633, 0.714297617187952, 0.3428571428571428, 0.29118482953445396, 0.0, 0.7340445690739947, 0.3576952594275509, 0.9279295654418451, 0.3050292265061689, 0.8052765029811182, 0.8181408218180712, 0.5184828839829643, 0.024257700866305763, 0.4372429839446083, 0.7914194502725269, 0.4081352907293705, 0.5899406213929941, 0.03242112268665885, 0.48709421299671357, 0.5361548119357558, 0.32283693344527487, 0.787412077515603, 0.7124914457448025, 0.4006114548496291, 0.6602365607318305, 0.3134806011985177, 0.46979131982331335, 0.0, 0.7029599683356129, 0.6287796716807859, 0.23586155825833904, 0.5398855599662549, 0.46858990773037834, 0.4652571156156813, 0.6625167201921691, 0.6244056216972086, 0.8016344953010826, 0.4447037792542853, 0.5972227336966478, 0.0, 0.4890714508161259, 0.42611431926626925, 0.5869015785622078, 0.5964858645175034, 0.2020565829424824, 0.5921208682859209, 0.4435544498128231, 0.6002115932468047, 0.29787272911546214, 0.4480504106866168, 0.43795872266628, 0.5254301437088552, 0.6898018405233148, 0.23267827069448496, 0.5429044721287437, 1.0, 0.7385103536314397, 0.272850465727977, 0.5642595261562982, 0.0331649402678469, 0.8250515340301889, 0.3196524780899271, 0.12619887519287304, 0.26283445449679005, 0.6155386493684107, 0.011543851366559629, 0.16509689218126905, 0.6167627952584476, 0.6877452504188784, 0.6363088006476244, 0.4809442817534614, 0.7456581487435584, 0.7932158795845483, 0.7475241598897441, 0.46272438732934995, 0.0, 0.5039351943352545, 0.6203534857578763, 0.002868727513479586, 0.47413612899439683, 0.7210821183929019, 0.49795575207241255, 0.12675934632139163, 0.3880269143763632, 0.5037027495271645, 0.5318506489774957, 0.5035171748800702, 0.5191613415044368, 0.18585881606736232, 0.020172282812417154, 0.4371694316558574, 0.42301816063606423, 0.8122861149630978, 0.5520581748828688, 0.0857142857142857, 0.0, 0.46778116746533216, 0.30576894725027826, 0.6714771286623475, 0.5720417360015888, 0.09792390161025119, 0.5833105633098615, 0.5554183564853334, 0.026198875192873054, 0.00783625026922906, 0.2815400648057302, 0.48207521287632626, 0.09469840876719007, 0.3725222457573894, 0.5621609692548745, 0.42549277925119, 0.6589537383040167, 0.12647880211200663, 0.19466186396013, 0.6183689771600134, 0.5539185089279641, 0.21246114118447335, 0.5793212151712596, 0.405777902774542, 1.0, 0.6829777303051304, 0.6017607272210093, 0.5720177963464216, 0.6319730195988179, 0.09, 0.35574612212598433, 0.05369111792447885, 0.7260484492137269, 0.43920818779035364, 0.6603973421033571, 0.03126528209192489, 0.6405523211133647, 0.535485999712475, 0.11411203966917517, 0.5157934678740115, 0.6378769644385496, 0.8379283989138028, 0.7068303583618756, 0.305545649326698, 0.644875685538236, 0.0, 0.5835925173963659, 0.001176470588235294, 0.3274711907798726, 0.5351213133439364, 0.6099380655621088, 0.5737553834911108, 0.29721228898372315, 0.42678850394002743, 0.5738129322772348, 0.13349772190665637, 0.3242843622325243, 0.718168796139468, 0.6142756196415784, 0.47665507080591274, 0.6748563659602985, 0.33263541811429875, 0.7802568700480011, 0.5358346408073879, 0.4818836696845309, 0.14702442372871036, 0.6761160600334989, 0.8362495782685979, 0.7648917708614881, 0.47794899677670044, 0.5953125, 0.6056908407550751, 0.6511894706429886, 0.08877215101753995, 0.1934331262209042, 0.5227731119358705, 0.5867497392560301, 0.46699403594567956, 0.06611802165472852, 0.5282087277319494, 0.3000638651712572, 0.3609941873132326, 0.6475965652338334, 0.5915367632043977, 0.5501419845789082, 0.646516427917569, 0.30073786544503595, 0.6159550306659207, 0.23899023360129432, 0.42123081514027727]
Finish training and take 1h3m
