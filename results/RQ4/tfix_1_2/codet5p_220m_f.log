Namespace(log_name='./RQ5/tfix_1_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_1_2/codet5p_220m_f', data_dir='./data/RQ5/tfix_1_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0143
  global_step = 1
  train_loss = 2.0186
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0143
  ********************
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 9.8 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.0143
  global_step = 1
  train_loss = 1.3919
  ********************
Previous best ppl:1.0143
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 9.8 	 Previous best codebleu 9.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0143
  global_step = 1
  train_loss = 2.4899
  ********************
Previous best ppl:1.0143
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 9.8 	 Previous best codebleu 9.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0143
  global_step = 1
  train_loss = 1.9667
  ********************
Previous best ppl:1.0143
BLEU file: ./data/RQ5/tfix_1_2/validation.jsonl
  codebleu-4 = 9.8 	 Previous best codebleu 9.8
  ********************
reload model from RQ5/tfix_1_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_1_2/test.jsonl
  codebleu = 9.26 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 9.26 
[0.0375, 0.0957015162326195, 0.0020720323417875233, 0.0, 0.0784313725490196, 0.0524454191823572, 0.0, 0.06698156682027649, 0.0, 0.0003649122351582114, 0.5876689247493568, 0.00015664774213823774, 0.0, 0.14290732140712145, 0.3377713811955922, 0.0, 0.011901891714229174, 0.0, 0.0, 0.2236875030313623, 0.1136493541988628, 0.09641967621419675, 0.0, 0.0, 0.3001402995428433, 0.0, 0.07236220029508797, 0.04, 0.05657894736842105, 0.04858248485953959, 0.27082327710428916, 0.00016231901715213512, 0.1607617492613618, 0.08, 0.0224625035476168, 0.02566332090073921, 0.0, 0.0014806229484508693, 0.03, 0.3451783492255866, 0.04, 0.0, 0.0038699208427482837, 0.0, 0.07515625000000001, 0.00022820272129332725, 0.10666666666666667, 0.0, 0.1540668097418569, 0.04071349355543404, 0.3003010781600992, 0.0001280235917704036, 0.0854385461752231, 0.0, 0.004125894871098143, 0.014394221717442174, 0.0, 0.2846563835880485, 0.08994851825029315, 0.06, 0.10481251484308554, 0.04444444444444444, 0.0, 0.1392351044471204, 0.07527799030059983, 0.0001873845929680468, 0.12744845377081582, 0.061538461538461535, 0.000269643481665621, 0.12567538933923886, 0.036439367521603004, 0.03155910729659695, 0.1411764705882353, 0.5739130434782609, 0.10145412493051008, 0.2390835008764976, 0.025240641711229944, 0.24356778948506072, 0.18, 0.11154453865028004, 0.04, 0.0, 0.00039332173972560317, 0.3042754044490551, 0.11230194688465112, 0.0, 0.1056689342403628, 0.00019960223845056353, 0.0, 0.29, 0.2711708487006213, 0.17139126650224867, 0.021428571428571425, 0.2050632911392405, 0.0, 0.03776433420500826, 0.08781519435646867, 0.00035812860006013107, 0.06, 0.04, 0.0006649510059370934, 0.0, 0.03214285714285714, 0.0, 0.06, 0.3834934853054017, 0.07847222222222222, 0.30022820347454077, 0.2228250727649394, 0.0, 0.020841275608273646, 0.04285714285714285, 0.009436675482516614, 0.053931827113058294, 0.022499897737465215, 0.46469541369228395, 0.12718054524020211, 0.11964017991004497, 0.0002697341407666418, 0.0, 0.06496815286624204, 0.2844081060630673, 0.3244155399451255, 0.028452079384019606, 0.0, 0.516664472495804, 0.08344671201814058, 0.06450903052439266, 0.1345339550664513, 0.07374706302170445, 0.015982729192420518, 0.19, 0.0669098108237609, 0.03310638183800771, 0.08333333333333333, 0.35, 0.048, 0.06914639289077079, 0.0423148634485412, 0.0, 0.057391304347826085, 0.11057680518954137, 0.3092648419890851, 0.06017108930258991, 0.0, 0.00029834991544331604, 0.0003387944687087028, 0.07338129496402877, 0.05623226408943903, 0.041379310344827586, 0.0, 0.04784580498866213, 0.00022820272129332725, 0.00023726946334646026, 0.17191011235955053, 0.04, 0.055156250000000004, 0.022465366428571335, 0.25116279069767444, 0.040333805598255126, 0.04184302552318947, 0.0, 0.35717944334954355, 0.0904470086958443, 0.0, 0.15689998940336564, 0.11492906927316573, 0.06701998763449753, 0.0002267422294395796, 0.02018402297627912, 0.0002533676256881555, 0.12726694176232528, 0.0, 0.13999999999999999, 0.00023736840213587975, 0.015384615384615384, 0.09777777777777777, 0.11273349611622603, 0.22501743737715457, 0.0, 0.03, 0.1575, 0.00023349611539724187, 0.10023349634537156, 0.19219498838813603, 0.1482580750294521, 0.0003219227835699228, 0.1479932418445123, 0.004651162790697674, 0.10040797008113775, 0.09246723137617321, 0.06032493046242515, 0.00016829407349945946, 0.05839762103369989, 0.13658819063432798, 0.08209788302346585, 0.0, 0.0, 0.3497649197393852, 0.003821656050955414, 0.11277799030059982, 0.043062108859356796, 0.10666666666666667, 0.05579427546833263, 0.0, 0.06551369134469681, 0.00015817604714341398, 0.5103958510284663, 0.049999999999999996, 0.08974347577170216, 0.01111111111111111, 0.13070258672756224, 0.0, 0.0856871251355588, 0.00039001327133322757, 0.00016227537425179234, 0.24, 0.23934426229508196, 0.00014371651532206318, 0.21080051794905755, 0.00017236094458242195, 0.14533000139090807, 0.019572567933919562, 0.0, 0.08926054223561214, 0.09279270332421134, 0.00038150055352924875, 0.199047819577795, 0.0, 0.0, 0.07325581395348836, 0.00028034413651807546, 0.049999999999999996, 0.03, 0.13908326687886058, 0.09515625, 0.5862595419847327, 0.10615384615384615, 0.0, 0.24844148058339854, 0.03, 0.11218487394957982, 0.00034763682452306437, 0.06, 0.04579582949761718, 0.00024662639742040155, 0.02, 0.04444444444444444, 0.00034159333645095564, 0.06, 0.0034767882749332044, 0.06428571428571428, 0.29846153846153844, 0.2333558424904378, 0.0, 0.0879545030766362, 0.10096153846153846, 0.06747443306929747, 0.01678806749727917, 0.05737704918032786, 0.0, 0.00040292385744761456, 0.04, 0.021428571428571425, 0.0001756352113293001, 0.18041817076057717, 0.014754595447990506, 0.4250076260181407, 0.045173152342516204, 0.06752759395139687, 0.0945578231292517, 0.030329772894231492, 0.03, 0.0, 0.0, 0.23535593075760844, 0.0, 0.04838709677419355, 0.2997224103395028, 0.0, 0.2917198927956374, 0.04285714285714285, 0.0, 0.04, 0.026383631919666934, 0.0, 0.014734478217923109, 0.07283343480786046, 0.11678289425301636, 0.022602763393261175, 0.01334015365879737, 0.0001756352113293001, 0.11845995562765571, 0.013636363636363636, 0.07525781909342692, 0.0, 0.0, 0.359249138959132, 0.11668202764976957, 0.0, 0.05002154291761589, 0.0922080770422607, 0.04046813737961993, 0.0, 0.04031262320318013, 0.24846879181394668, 0.00044935372568402713, 0.016666666666666666, 0.02, 0.3106592105134916, 0.0, 0.00023349611538816682, 0.04, 0.23200318257019878, 0.00021393641401449957, 0.16455784589250055, 0.1733333333333333, 0.0, 0.10302631358854009, 0.0, 0.0, 0.05217391304347826, 0.02608695652173913, 0.04, 0.10440203739117856, 0.3377574147260342, 0.2488475287195966, 0.0004713204749758039, 0.005133094275219697, 0.05387792241934146, 0.0002698345653761312, 0.46805898881822694, 0.07142857142857142, 0.05578910856757906, 0.04, 0.14285714285714285, 0.060355159355954285, 0.0005418575590246188, 0.0, 0.00035479984220565145, 0.0802049927315964, 0.3002282149800296, 0.015840746856205266, 0.04285714285714285, 0.0, 0.04539064939016163, 0.10402327436714291, 0.007199256519284204, 0.5795754804485487, 0.0631578947368421, 0.13471049197912677, 0.038709677419354833, 0.33749999999999997, 0.026135337751586853, 0.0, 0.06666666666666667, 0.02882174111514274, 0.06706217459045882, 0.041675597828538816, 0.07777777777777777, 0.0, 0.13428571428571429, 0.14222222222222222, 0.5984497000739815, 0.09009426904084991, 0.03529411764705882, 0.00031287015930521017, 0.04, 0.0, 0.04285714285714285, 0.04, 0.022872354982299344, 0.014867701188502802, 0.006487890174150185, 0.0763757346111085, 0.3378746032389689, 0.005562668524666001, 0.0, 0.2209949566687243, 0.075, 0.058035477756549776, 0.015, 0.08916890697238157, 0.04, 0.054044117647058826, 0.2206896551724138, 0.04, 0.33749999999999997, 0.08925367498915472, 0.3004572267421954, 0.07847222222222222, 0.17271647578613825, 0.08673780292029748, 0.0, 0.4055432993716387, 0.04, 0.0, 0.006562362297352842, 0.0857142857142857, 0.3, 0.049999999999999996, 0.06, 0.021428571428571425, 0.13222681757761595, 0.15164778230292306, 0.11608041269637456, 0.024999999999999998, 0.1422222222222222, 0.05217391304347826, 0.09938650306748466, 0.0, 0.0004431893811284148, 0.06428571428571428, 0.3061643835616438, 0.1581214404071053, 0.056249999999999994, 0.0416112879477674, 0.0, 0.024, 0.0001323068336182539, 0.0, 0.07762258317465437, 0.04, 0.11516531220831963, 0.06666666666666667, 0.2063607509657136, 0.1516720502920991, 0.19298844218929195, 0.3003712053311756, 0.06, 0.3260905684208671, 0.041379310344827586, 0.04480988588764336, 0.21728769882745425, 0.09111237273291893, 0.045, 0.30036198188651975, 0.00022727507290873925, 0.04184302552318947, 0.08344671201814058, 0.0, 0.0, 0.09, 0.25509195351283487, 0.0, 0.004294899027910078, 0.36829086744540485, 0.1663070569241623, 0.0, 0.014634146341463414, 0.3084604578903368, 0.0, 0.05741854561564276, 0.1614035087719298, 0.0029126213592233006, 0.08786781897963336, 0.00019239981563718193, 0.03529411764705882, 0.47777806634648173, 0.3063412040108749, 0.0, 0.02, 9.993952993397488e-05, 0.06968637353427132, 0.026264501727443223, 0.05787935737172772, 0.3275542061046039, 0.09, 0.19615186346232694, 0.11858819722155639, 0.30418953167796753, 0.03660807001417802, 0.4562914879300718, 0.0, 0.16122887645567827, 0.10302868161700171, 0.11599393878354465, 0.3182082757472285, 0.006474256294885302, 0.021428571428571425, 0.30152137242893023, 0.0, 0.5975715443011376, 0.05553372759663572, 0.00035479984220565145, 0.04, 0.0, 0.0, 0.02222222222222222, 0.17251988127521883, 0.05217391304347826, 0.11033879430716918, 0.0, 0.0, 0.16132556311264845, 0.06, 0.5758131627804778, 0.04876864915088334, 0.08415362731152204, 0.09355322338830585]
Finish training and take 51m
