Namespace(log_name='./RQ5/tfix_100_2/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_100_2/codet5p_770m_f', data_dir='./data/RQ5/tfix_100_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 100 training instances 
***** Running training *****
  Num examples = 100
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00459
  global_step = 26
  train_loss = 1.0764
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00459
  ********************
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 55.81 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:55.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00428
  global_step = 51
  train_loss = 0.3099
  ********************
Previous best ppl:1.00459
Achieve Best ppl:1.00428
  ********************
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 59.82 	 Previous best codebleu 55.81
  ********************
 Achieve Best bleu:59.82
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00442
  global_step = 76
  train_loss = 0.1329
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.47 	 Previous best codebleu 59.82
  ********************
 Achieve Best bleu:60.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0048
  global_step = 101
  train_loss = 0.0597
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.71 	 Previous best codebleu 60.47
  ********************
 Achieve Best bleu:60.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00512
  global_step = 126
  train_loss = 0.0387
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 61.12 	 Previous best codebleu 60.71
  ********************
 Achieve Best bleu:61.12
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00509
  global_step = 151
  train_loss = 0.0288
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 62.22 	 Previous best codebleu 61.12
  ********************
 Achieve Best bleu:62.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00522
  global_step = 176
  train_loss = 0.0269
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 62.6 	 Previous best codebleu 62.22
  ********************
 Achieve Best bleu:62.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00536
  global_step = 201
  train_loss = 0.0178
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.43 	 Previous best codebleu 62.6
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00545
  global_step = 226
  train_loss = 0.0086
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.97 	 Previous best codebleu 62.6
  ********************
reload model from RQ5/tfix_100_2/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_100_2/test.jsonl
  codebleu = 59.92 
  Total = 500 
  Exact Fixed = 53 
[3, 17, 50, 57, 98, 109, 118, 132, 135, 140, 145, 153, 157, 166, 176, 188, 191, 193, 198, 201, 202, 205, 226, 252, 263, 272, 290, 294, 296, 303, 305, 318, 327, 328, 330, 336, 348, 350, 359, 360, 367, 368, 374, 390, 393, 415, 417, 426, 435, 438, 439, 442, 457]
  Syntax Fixed = 2 
[71, 482]
  Cleaned Fixed = 4 
[35, 88, 172, 475]
  ********************
  Total = 500 
  Exact Fixed = 53 
[3, 17, 50, 57, 98, 109, 118, 132, 135, 140, 145, 153, 157, 166, 176, 188, 191, 193, 198, 201, 202, 205, 226, 252, 263, 272, 290, 294, 296, 303, 305, 318, 327, 328, 330, 336, 348, 350, 359, 360, 367, 368, 374, 390, 393, 415, 417, 426, 435, 438, 439, 442, 457]
  Syntax Fixed = 2 
[71, 482]
  Cleaned Fixed = 4 
[35, 88, 172, 475]
  codebleu = 59.92 
[0.5670445241047504, 0.6932410335616551, 1.0, 0.31331096368700495, 0.6058511595444083, 0.8821894623768516, 0.0, 0.5417785626903434, 0.47091526118050575, 0.12, 0.5368474083243506, 0.5638063820069366, 0.17189979897705582, 0.7569706667451968, 0.612478798738405, 0.811117687292167, 1.0, 0.7099766481408452, 0.6885240359080308, 0.6974942116108808, 0.6716511056875383, 0.39323313167877133, 0.24915625771353556, 0.6232430066758361, 0.5970182644937875, 0.006758238160278511, 0.9194711239895874, 0.32184467070784006, 0.6624101654079205, 0.2855072113105671, 0.7059323615142865, 0.0, 0.4507183101653729, 0.9113854927948712, 0.8790355083665293, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6160122780155695, 0.5511412946838178, 0.35997270548760957, 0.10202520055807765, 0.40366685053304824, 0.7183643415906251, 0.3403395101580171, 0.572116493710585, 0.274670867776058, 0.8208732553989724, 1.0, 0.350381564995214, 0.6433348104458009, 0.6558003592103332, 0.506868741276913, 0.1297947365831559, 0.5194637253863024, 0.7135428903906851, 0.7068248908900194, 0.8460094492769958, 0.4923575041086071, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.7618233428889547, 0.7091417634296828, 0.6197233100748608, 0.6877745357134093, 0.49918120383843156, 0.4199748401004021, 0.28269688894370654, 0.9822263373938755, 0.714297617187952, 0.8508222909971193, 0.2172673004339611, 0.643557482735935, 0.29035107882031763, 0.15, 0.8965241752682729, 0.3762263314774751, 0.621238225076177, 0.591516286342532, 0.3734424024864518, 0.6895579051414615, 0.7190865275431211, 0.6359585563603126, 0.5451417837590342, 0.48115588878443294, 0.9143690661375261, 0.8053935031810717, 0.44058280681232764, 0.8550899856418173, 0.7402739997668998, 0.3633344444942923, 0.45579909520741635, 0.3056299173484432, 0.7470738524243741, 0.8349004181959196, 1.0, 0.7962637890346522, 0.5688724788208753, 0.7942011322130667, 0.8505137985106188, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.0, 0.4593867118293433, 0.8399114554115357, 1.0, 0.05270446328111167, 0.2878852119582799, 0.5876935279546878, 0.39272004516267667, 0.6553082743436025, 0.4819989645916939, 0.5638669995902533, 0.6903510788203177, 1.0, 0.5143209096122012, 0.7231896021778675, 0.7302547136373172, 0.7807350977435055, 0.7421110988498729, 0.5145298416182899, 0.2176694832919011, 0.8424789818939842, 0.2011770485941022, 0.8227990230461388, 0.5246680028432865, 0.4976113387647193, 0.3378661976937814, 1.0, 0.12869111792447885, 0.39089314466834374, 1.0, 0.4460952454685818, 0.553492655520654, 0.523099034776898, 0.14876752225153467, 0.7135428903906851, 0.6368551910912801, 0.7245741928928766, 0.5290863131818948, 0.47180509653022784, 0.8249365300761395, 0.6387218496729328, 0.3607039998273758, 0.37668959821497894, 0.5862322739141566, 0.26493187938965956, 0.4854183564853334, 0.4051798613404909, 0.8114529051148651, 0.8045937759301073, 0.29306704220365265, 0.64935519109128, 1.0, 0.5869940359456796, 0.22499999999999998, 0.6684064493975892, 0.39347209291868157, 0.5370116053397941, 0.7504004004704901, 0.7954279937030855, 0.42418872709654803, 1.0, 0.5908527155778469, 0.49215257204284135, 0.316689598214979, 0.40621281109105456, 0.8149139863647084, 0.6806552176459515, 0.45449078148809957, 0.9500330416345475, 0.40693291435805307, 0.8249365300761395, 0.41996134394226886, 0.5745997086539989, 0.6278910313966588, 0.11453360085481801, 0.36686807121046905, 0.6780323751412478, 0.5164927712445991, 0.6046379048692274, 0.629072265262878, 0.7107915119421391, 0.669924448317195, 0.7135428903906851, 0.3, 0.7725146668513112, 1.0, 0.94333927756915, 0.8249365300761395, 0.40638849227697993, 0.43648868589003154, 0.5758089748131177, 0.007044218708112843, 1.0, 0.4969652718549196, 0.5919953379147749, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 1.0, 0.6271806220068676, 0.4139240818757265, 0.6613122421311423, 0.4479321127343209, 0.6599223799332465, 0.6116850869767493, 0.9365276486140073, 0.32627945612776854, 0.6192538700356924, 0.8981777535482824, 0.174436274930823, 0.5796605122596801, 0.5536911179244788, 0.9320894171538912, 0.7755969629986152, 0.42684513628385, 0.6902373932796317, 0.731117687292167, 0.15997084006767517, 0.691318859382493, 1.0, 0.677288551000834, 0.8547211363207963, 0.9414428605659417, 0.11558530246943605, 0.48074569931823535, 0.7583496612315528, 0.0, 0.7416865196891184, 0.6844125934903402, 0.60638849227698, 0.5981716191316424, 0.7139368889915563, 0.3683458566180241, 0.7875054975857689, 0.7471765364160405, 0.3711234393139395, 0.6564629613776192, 0.7263487966630597, 0.9056583090096291, 0.6080451238953586, 0.43111016267734115, 0.6448087107793964, 0.4349579637879685, 0.6350178817306947, 0.5073028085492076, 1.0, 0.6088783494196637, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.2772943579237247, 0.836942030771336, 0.5082953743097742, 0.7299948711471688, 0.5017814354051087, 0.5585140118159326, 1.0, 0.7503305249945151, 0.4766895982149789, 0.6624678525854575, 0.33372428676905674, 0.6558003592103332, 0.6876346520405138, 0.37379038309331053, 0.6848698374809952, 1.0, 0.6844125934903402, 0.43829433999099154, 0.6, 0.6736456637493553, 0.3866534478992738, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.4452105198475923, 0.40732381855793504, 0.5642680213004776, 0.8254585981793112, 0.0, 0.5966289756845221, 0.8420812716756092, 0.40630808744877833, 1.0, 0.24698819231451424, 0.22719082257468703, 0.6366015466501336, 1.0, 0.8232059780715621, 0.8249365300761395, 0.7911679345786041, 0.4594370462111316, 0.7212489793637704, 0.2758114916555782, 0.8374634481857526, 0.4550767418333481, 1.0, 0.44245571892563784, 1.0, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.769581697257735, 0.8642470965743208, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 0.6276136760976885, 0.714297617187952, 0.6443152851788645, 0.5698516999312941, 0.075, 0.7340445690739947, 0.5174693084765555, 0.5702518310230941, 0.6240048954556174, 0.5308812792845414, 0.7442122249364763, 1.0, 1.0, 0.24, 1.0, 0.5025228342396566, 0.6273488306532815, 0.546640835574196, 0.7189894209825303, 0.656468030647374, 1.0, 0.3669033918159327, 0.551305343775713, 0.5015372100944879, 0.7591116846422887, 0.8358211452160682, 0.624178976103728, 0.3381394976554918, 0.6968952972483801, 0.6287796716807859, 0.44507381648898203, 0.5398855599662549, 1.0, 0.5193052677964398, 1.0, 0.8069007178395178, 0.9176675865658077, 0.4147896756607047, 0.5652887122558813, 0.0, 0.4890714508161259, 0.5305242027189159, 0.6745228788727515, 0.9891483218006352, 1.0, 0.7451384298635613, 0.6245836923955046, 0.6980083797903092, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 1.0, 1.0, 0.2696847134058581, 0.8263522613816869, 0.444006594916457, 0.7536060548993002, 0.32898762131831805, 1.0, 0.7700435930643703, 0.5632985040139638, 0.45101776545562844, 0.4129740706026147, 0.3053372331674803, 0.7905056700329363, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.7916479842275448, 0.5884119940010644, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.820504309245877, 0.8114529051148651, 0.5654473178899494, 0.7756211334750283, 1.0, 0.17731538118686083, 0.4592493207167594, 0.7254641862621534, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.15720396139730028, 0.5566984938224081, 0.7108049881272258, 0.752380452288719, 0.18585881606736232, 0.39969884422068214, 0.5033554133637828, 0.6354739770278475, 0.4378145748707828, 0.6754386750930661, 0.0857142857142857, 0.0, 0.823863323049132, 0.8655056700329364, 0.8, 1.0, 0.12142173779679513, 0.9891483218006352, 0.5554183564853334, 0.0, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.7054819616225767, 0.5383089748131177, 0.5621609692548745, 0.8249365300761395, 0.6453246818220744, 0.5092272948800609, 0.06808859345352361, 0.6183689771600134, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.7288312733021203, 1.0, 0.6829777303051304, 0.7511655122445988, 1.0, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.361488561314608, 0.8292906179772745, 0.6, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.5518505299283467, 0.6592906179772744, 0.8379283989138028, 0.8917028689549173, 0.2670447530210518, 0.8648103351245982, 0.0828714707479615, 1.0, 0.8326781496971694, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.7357332854212001, 0.5149441763089433, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.3583507427572924, 0.8626491724949568, 0.33743558442762467, 0.6809070226208283, 0.7316423857228858, 0.7195565989965169, 0.7802568700480011, 0.7710658705890923, 0.9248007629373642, 0.3, 0.6812000145029719, 0.6893720612567699, 0.8038961967467184, 0.22369320116988367, 0.5220398231634966, 0.8955980328281301, 0.8115900311996962, 0.0817743687432679, 0.6785036503543816, 0.562520658360374, 0.6901940524102714, 0.5089439475762831, 0.2686834861779044, 0.5282087277319494, 0.7135160729534664, 0.47099945841825197, 0.6986613814884177, 0.6580098924657095, 0.4803606316930303, 0.8632148025904984, 0.47073389088669926, 0.7533655306263078, 0.36791104518876483, 0.6101458438731493]
Finish training and take 16m
Namespace(log_name='./RQ5/tfix_100_2/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_100_2/codet5p_770m_f', data_dir='./data/RQ5/tfix_100_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 100 training instances 
***** Running training *****
  Num examples = 100
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00459
  global_step = 26
  train_loss = 1.0764
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00459
  ********************
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 55.86 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:55.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00428
  global_step = 51
  train_loss = 0.3099
  ********************
Previous best ppl:1.00459
Achieve Best ppl:1.00428
  ********************
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 59.87 	 Previous best codebleu 55.86
  ********************
 Achieve Best bleu:59.87
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00442
  global_step = 76
  train_loss = 0.1329
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.49 	 Previous best codebleu 59.87
  ********************
 Achieve Best bleu:60.49
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.0048
  global_step = 101
  train_loss = 0.0597
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.76 	 Previous best codebleu 60.49
  ********************
 Achieve Best bleu:60.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00512
  global_step = 126
  train_loss = 0.0387
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 61.17 	 Previous best codebleu 60.76
  ********************
 Achieve Best bleu:61.17
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00509
  global_step = 151
  train_loss = 0.0288
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 62.3 	 Previous best codebleu 61.17
  ********************
 Achieve Best bleu:62.3
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00522
  global_step = 176
  train_loss = 0.0269
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 62.68 	 Previous best codebleu 62.3
  ********************
 Achieve Best bleu:62.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00536
  global_step = 201
  train_loss = 0.0178
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.49 	 Previous best codebleu 62.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00545
  global_step = 226
  train_loss = 0.0086
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 61.02 	 Previous best codebleu 62.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00546
  global_step = 251
  train_loss = 0.0099
  ********************
Previous best ppl:1.00428
BLEU file: ./data/RQ5/tfix_100_2/validation.jsonl
  codebleu-4 = 60.95 	 Previous best codebleu 62.68
  ********************
reload model from RQ5/tfix_100_2/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_100_2/test.jsonl
  codebleu = 59.98 
  Total = 500 
  Exact Fixed = 53 
[3, 17, 50, 57, 98, 109, 118, 132, 135, 140, 145, 153, 157, 166, 176, 188, 191, 193, 198, 201, 202, 205, 226, 252, 263, 272, 290, 294, 296, 303, 305, 318, 327, 328, 330, 336, 348, 350, 359, 360, 367, 368, 374, 390, 393, 415, 417, 426, 435, 438, 439, 442, 457]
  Syntax Fixed = 1 
[71]
  Cleaned Fixed = 4 
[35, 88, 172, 475]
  ********************
  Total = 500 
  Exact Fixed = 53 
[3, 17, 50, 57, 98, 109, 118, 132, 135, 140, 145, 153, 157, 166, 176, 188, 191, 193, 198, 201, 202, 205, 226, 252, 263, 272, 290, 294, 296, 303, 305, 318, 327, 328, 330, 336, 348, 350, 359, 360, 367, 368, 374, 390, 393, 415, 417, 426, 435, 438, 439, 442, 457]
  Syntax Fixed = 1 
[71]
  Cleaned Fixed = 4 
[35, 88, 172, 475]
  codebleu = 59.98 
[0.5670445241047504, 0.6932410335616551, 1.0, 0.31331096368700495, 0.6058511595444083, 0.8821894623768516, 0.0, 0.5417785626903434, 0.47091526118050575, 0.12, 0.5368474083243506, 0.5638063820069366, 0.17189979897705582, 0.7569706667451968, 0.612478798738405, 0.811117687292167, 1.0, 0.7099766481408452, 0.6885240359080308, 0.6974942116108808, 0.7145082485446812, 0.39323313167877133, 0.24915625771353556, 0.6232430066758361, 0.5970182644937875, 0.006758238160278511, 0.9194711239895874, 0.32184467070784006, 0.6624101654079205, 0.2855072113105671, 0.7059323615142865, 0.0, 0.4507183101653729, 0.9113854927948712, 0.8790355083665293, 0.6068905779332499, 0.0, 0.10556510016615601, 0.7155056700329363, 0.6160122780155695, 0.5511412946838178, 0.35997270548760957, 0.10202520055807765, 0.40366685053304824, 0.7183643415906251, 0.3403395101580171, 0.572116493710585, 0.274670867776058, 0.8208732553989724, 1.0, 0.350381564995214, 0.6433348104458009, 0.6558003592103332, 0.506868741276913, 0.1297947365831559, 0.5194637253863024, 0.7135428903906851, 0.7068248908900194, 0.8460094492769958, 0.4923575041086071, 0.9315380507550484, 0.714297617187952, 0.5348435312891645, 0.7618233428889547, 0.7091417634296828, 0.6197233100748608, 0.6877745357134093, 0.49918120383843156, 0.4199748401004021, 0.28269688894370654, 0.9822263373938755, 0.714297617187952, 0.8508222909971193, 0.2172673004339611, 0.643557482735935, 0.29035107882031763, 0.15, 0.8965241752682729, 0.3762263314774751, 0.6587382250761771, 0.591516286342532, 0.3734424024864518, 0.6895579051414615, 0.7190865275431211, 0.6359585563603126, 0.5451417837590342, 0.48115588878443294, 0.9143690661375261, 0.8053935031810717, 0.44058280681232764, 0.8550899856418173, 0.7402739997668998, 0.3633344444942923, 0.45579909520741635, 0.3056299173484432, 0.7470738524243741, 0.8349004181959196, 1.0, 0.7962637890346522, 0.5688724788208753, 0.7942011322130667, 0.8505137985106188, 0.7275723927836586, 0.6445931574752105, 0.7744515423179406, 0.0, 0.4593867118293433, 0.8399114554115357, 1.0, 0.05270446328111167, 0.2878852119582799, 0.5876935279546878, 0.39272004516267667, 0.6553082743436025, 0.4819989645916939, 0.5638669995902533, 0.6903510788203177, 1.0, 0.5143209096122012, 0.7231896021778675, 0.7302547136373172, 0.7807350977435055, 0.7421110988498729, 0.5145298416182899, 0.2176694832919011, 0.8424789818939842, 0.2011770485941022, 0.8227990230461388, 0.5246680028432865, 0.4976113387647193, 0.3753661976937814, 1.0, 0.12869111792447885, 0.39089314466834374, 1.0, 0.4460952454685818, 0.553492655520654, 0.523099034776898, 0.14876752225153467, 0.7135428903906851, 0.78685519109128, 0.7245741928928766, 0.5290863131818948, 0.47180509653022784, 0.8249365300761395, 0.6387218496729328, 0.3607039998273758, 0.37668959821497894, 0.5862322739141566, 0.26493187938965956, 0.4854183564853334, 0.4051798613404909, 0.8114529051148651, 0.8045937759301073, 0.29306704220365265, 0.64935519109128, 1.0, 0.5869940359456796, 0.22499999999999998, 0.6684064493975892, 0.39347209291868157, 0.5370116053397941, 0.7504004004704901, 0.7954279937030855, 0.42418872709654803, 1.0, 0.5908527155778469, 0.49215257204284135, 0.316689598214979, 0.40621281109105456, 0.8149139863647084, 0.6806552176459515, 0.45449078148809957, 0.9500330416345475, 0.40693291435805307, 0.8249365300761395, 0.41996134394226886, 0.5745997086539989, 0.6278910313966588, 0.11453360085481801, 0.36686807121046905, 0.6780323751412478, 0.5164927712445991, 0.6046379048692274, 0.629072265262878, 0.7107915119421391, 0.669924448317195, 0.7135428903906851, 0.3, 0.7725146668513112, 1.0, 0.94333927756915, 0.8249365300761395, 0.40638849227697993, 0.43648868589003154, 0.5758089748131177, 0.007044218708112843, 1.0, 0.4969652718549196, 0.5919953379147749, 1.0, 1.0, 0.40485075092622447, 0.7825255142177938, 1.0, 0.6271806220068676, 0.4139240818757265, 0.6613122421311423, 0.4479321127343209, 0.6599223799332465, 0.6116850869767493, 0.9365276486140073, 0.32627945612776854, 0.6192538700356924, 0.8981777535482824, 0.174436274930823, 0.5796605122596801, 0.5536911179244788, 0.9320894171538912, 0.7755969629986152, 0.42684513628385, 0.6902373932796317, 0.731117687292167, 0.15997084006767517, 0.691318859382493, 1.0, 0.677288551000834, 0.8547211363207963, 0.9414428605659417, 0.11558530246943605, 0.48074569931823535, 0.7583496612315528, 0.0, 0.7416865196891184, 0.6844125934903402, 0.60638849227698, 0.5981716191316424, 0.7139368889915563, 0.3683458566180241, 0.7875054975857689, 0.7471765364160405, 0.3711234393139395, 0.6564629613776192, 0.7263487966630597, 0.9056583090096291, 0.6080451238953586, 0.43111016267734115, 0.6448087107793964, 0.4349579637879685, 0.6350178817306947, 0.5073028085492076, 1.0, 0.6088783494196637, 0.6636363636363636, 0.22499999999999998, 0.7430779662080889, 0.2772943579237247, 0.836942030771336, 0.5082953743097742, 0.7299948711471688, 0.5017814354051087, 0.5585140118159326, 1.0, 0.7503305249945151, 0.4766895982149789, 0.6624678525854575, 0.33372428676905674, 0.6558003592103332, 0.7209679853738471, 0.37379038309331053, 0.6848698374809952, 1.0, 0.6844125934903402, 0.43829433999099154, 0.6, 0.6736456637493553, 0.3866534478992738, 0.6174969094123448, 0.7142845202249919, 0.5551668790236555, 0.5600468038501167, 0.4452105198475923, 0.40732381855793504, 0.5642680213004776, 0.8254585981793112, 0.0, 0.5966289756845221, 0.8420812716756092, 0.40630808744877833, 1.0, 0.24698819231451424, 0.22719082257468703, 0.6366015466501336, 1.0, 0.8232059780715621, 0.8249365300761395, 0.7911679345786041, 0.4594370462111316, 0.7212489793637704, 0.2758114916555782, 0.8374634481857526, 0.4550767418333481, 1.0, 0.44245571892563784, 1.0, 0.5885667747214658, 0.9238877689380496, 0.8356658099532572, 0.769581697257735, 0.8642470965743208, 0.3220998891127413, 0.1344057918034266, 0.22368441215365098, 0.6682556320550647, 0.5680054404199835, 0.6276136760976885, 0.714297617187952, 0.6443152851788645, 0.5698516999312941, 0.075, 0.7340445690739947, 0.5174693084765555, 0.5702518310230941, 0.6240048954556174, 0.5308812792845414, 0.7442122249364763, 1.0, 1.0, 0.24, 1.0, 0.5025228342396566, 0.6273488306532815, 0.546640835574196, 0.7189894209825303, 0.656468030647374, 1.0, 0.3669033918159327, 0.551305343775713, 0.5015372100944879, 0.7591116846422887, 0.8358211452160682, 0.624178976103728, 0.3381394976554918, 0.6968952972483801, 0.6287796716807859, 0.44507381648898203, 0.5398855599662549, 1.0, 0.5193052677964398, 1.0, 0.8069007178395178, 0.9176675865658077, 0.4147896756607047, 0.5952887122558812, 0.0, 0.4890714508161259, 0.5305242027189159, 0.6745228788727515, 0.9891483218006352, 1.0, 0.7451384298635613, 0.6245836923955046, 0.6980083797903092, 0.4614470406899055, 0.5027054768195429, 0.5932305781811854, 1.0, 1.0, 0.2696847134058581, 0.8263522613816869, 0.444006594916457, 0.7536060548993002, 0.32898762131831805, 1.0, 0.7700435930643703, 0.5632985040139638, 0.45101776545562844, 0.4129740706026147, 0.3053372331674803, 0.7905056700329363, 0.4621787547146431, 0.6398103351245982, 0.8184276974159628, 0.7916479842275448, 0.5884119940010644, 0.5758081453961951, 0.8233303578084277, 0.7530301180787049, 0.820504309245877, 0.8114529051148651, 0.5654473178899494, 0.7756211334750283, 1.0, 0.17731538118686083, 0.4592493207167594, 0.7254641862621534, 0.7230706259860853, 0.12296711450648444, 0.5019774510104837, 0.15720396139730028, 0.5566984938224081, 0.7108049881272258, 0.752380452288719, 0.18585881606736232, 0.39969884422068214, 0.5033554133637828, 0.6354739770278475, 0.4378145748707828, 0.6754386750930661, 0.0857142857142857, 0.0, 0.823863323049132, 0.8655056700329364, 0.8, 1.0, 0.12142173779679513, 0.9891483218006352, 0.5554183564853334, 0.0, 0.15529892970465703, 0.2815400648057302, 0.4778533416343903, 0.7054819616225767, 0.5383089748131177, 0.5621609692548745, 0.8249365300761395, 0.6453246818220744, 0.5092272948800609, 0.06808859345352361, 0.6183689771600134, 0.5646170798740607, 0.21151332797896238, 0.685303136808272, 0.7288312733021203, 1.0, 0.6829777303051304, 0.7511655122445988, 1.0, 1.0, 0.08928571428571427, 0.40739727760582384, 0.7135428903906851, 0.7260484492137269, 0.361488561314608, 0.8292906179772745, 0.6, 0.6883329804487951, 0.7148321218614053, 0.19404871089985237, 0.5518505299283467, 0.6592906179772744, 0.8379283989138028, 0.8917028689549173, 0.2670447530210518, 0.8648103351245982, 0.0828714707479615, 1.0, 0.8326781496971694, 0.7387037834978107, 0.7845578251721461, 0.6099380655621088, 0.7357332854212001, 0.5149441763089433, 0.4714698407062744, 0.8677459225855626, 0.6745410470195684, 0.3583507427572924, 0.8626491724949568, 0.33743558442762467, 0.6809070226208283, 0.7316423857228858, 0.7195565989965169, 0.7802568700480011, 0.7710658705890923, 0.9248007629373642, 0.3, 0.6812000145029719, 0.6893720612567699, 0.8038961967467184, 0.22369320116988367, 0.5220398231634966, 0.8955980328281301, 0.8115900311996962, 0.0817743687432679, 0.6785036503543816, 0.562520658360374, 0.6901940524102714, 0.5089439475762831, 0.2686834861779044, 0.5282087277319494, 0.7135160729534664, 0.47099945841825197, 0.6986613814884177, 0.6580098924657095, 0.4803606316930303, 0.8632148025904984, 0.47073389088669926, 0.7533655306263078, 0.36791104518876483, 0.6101458438731493]
Finish training and take 17m
