Namespace(log_name='./RQ5/tfix_8_1/codet5p_770m_f.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_8_1/codet5p_770m_f', data_dir='./data/RQ5/tfix_8_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 8 training instances 
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01468
  global_step = 3
  train_loss = 2.8465
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01468
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 15.71 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:15.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00893
  global_step = 5
  train_loss = 1.4696
  ********************
Previous best ppl:1.01468
Achieve Best ppl:1.00893
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 25.79 	 Previous best codebleu 15.71
  ********************
 Achieve Best bleu:25.79
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00735
  global_step = 7
  train_loss = 0.5639
  ********************
Previous best ppl:1.00893
Achieve Best ppl:1.00735
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 32.1 	 Previous best codebleu 25.79
  ********************
 Achieve Best bleu:32.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00688
  global_step = 9
  train_loss = 0.4723
  ********************
Previous best ppl:1.00735
Achieve Best ppl:1.00688
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 47.22 	 Previous best codebleu 32.1
  ********************
 Achieve Best bleu:47.22
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00681
  global_step = 11
  train_loss = 0.3145
  ********************
Previous best ppl:1.00688
Achieve Best ppl:1.00681
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 51.61 	 Previous best codebleu 47.22
  ********************
 Achieve Best bleu:51.61
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00676
  global_step = 13
  train_loss = 0.1681
  ********************
Previous best ppl:1.00681
Achieve Best ppl:1.00676
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 52.58 	 Previous best codebleu 51.61
  ********************
 Achieve Best bleu:52.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00672
  global_step = 15
  train_loss = 0.0327
  ********************
Previous best ppl:1.00676
Achieve Best ppl:1.00672
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 52.94 	 Previous best codebleu 52.58
  ********************
 Achieve Best bleu:52.94
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00677
  global_step = 17
  train_loss = 0.018
  ********************
Previous best ppl:1.00672
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 52.98 	 Previous best codebleu 52.94
  ********************
 Achieve Best bleu:52.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0068
  global_step = 19
  train_loss = 0.0613
  ********************
Previous best ppl:1.00672
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 53.41 	 Previous best codebleu 52.98
  ********************
 Achieve Best bleu:53.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00681
  global_step = 21
  train_loss = 0.0441
  ********************
Previous best ppl:1.00672
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 53.59 	 Previous best codebleu 53.41
  ********************
 Achieve Best bleu:53.59
  ********************
reload model from RQ5/tfix_8_1/codet5p_770m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_1/test.jsonl
  codebleu = 53.74 
  Total = 500 
  Exact Fixed = 11 
[16, 49, 69, 78, 119, 121, 184, 201, 205, 235, 263]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[300, 432, 482, 485]
  ********************
  Total = 500 
  Exact Fixed = 11 
[16, 49, 69, 78, 119, 121, 184, 201, 205, 235, 263]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[300, 432, 482, 485]
  codebleu = 53.74 
[0.36593621655465947, 0.7852532841267479, 0.8415580218210685, 0.415309394211987, 0.3519980391978243, 0.8216460780407606, 0.26789299814414175, 0.6981063591001481, 0.6870691732474511, 0.5505831472572245, 0.824425175651095, 0.1283950208311778, 0.271898412993546, 0.6983535654700125, 0.49987619040560277, 1.0, 0.792947845270083, 0.5404095184045611, 0.307406932087109, 0.522583151455642, 0.8633530795828024, 0.00011949851130014561, 0.6903510788203177, 0.5930968391167379, 0.3540134104327906, 0.7716085560414976, 0.1294211266479198, 0.6180730700716379, 0.3333333333333333, 0.6459857482140212, 0.08154518978633504, 0.6606391824574767, 0.46467930495716736, 0.5400294185210878, 0.5471580496396822, 0.45040497426175063, 0.5537244306491824, 0.20342150380611967, 0.5638730135768784, 0.5013961837363989, 0.4427200451626767, 0.27666535735640196, 0.7122853668557851, 0.5184345861528767, 0.8356274376774735, 0.9431571237072573, 0.29500390830455314, 0.46015703048046497, 1.0, 0.5205584248799071, 0.41494193057548645, 0.4266408688157862, 0.8715580647670746, 0.8696847134058581, 0.8081779991158766, 0.3329974507665251, 0.6530550965302279, 0.5873074388023817, 0.35487283135827297, 0.6273520828335467, 0.4905880158658711, 0.5494978414909922, 0.5426010687314248, 0.22242802984682342, 0.7517070403538422, 0.9260406910477721, 0.2376473938080234, 0.6894119210985092, 1.0, 0.3208206201870649, 0.5129834344293507, 0.6914188211062342, 0.21063336828616458, 0.5843871292056362, 0.5248713413132184, 0.4779922774375015, 0.42074846483844186, 1.0, 0.7158229243802021, 0.6197269433299534, 0.6132366516010306, 0.6578602200025533, 0.6901940524102714, 0.651535222322672, 0.7148321218614053, 0.5351506255180523, 0.5757481451784977, 0.5844322860768962, 0.5295971223679855, 0.8630487585927691, 0.06611802165472852, 0.6870857528793353, 0.41126883251465846, 0.7759778505057264, 0.44375943570639337, 0.18093469809571647, 0.7887437726770301, 0.39873667339437135, 0.6266865967254232, 0.8053927133422001, 0.3514506678404373, 0.5768949713773843, 0.7275723927836586, 0.7318075644896502, 0.7405912085706381, 0.6265958401383632, 0.7903356967388899, 0.8165273869902545, 0.04535576197454044, 0.7575469685760496, 0.8491530757973555, 0.7477990230461387, 0.5631391755664542, 0.6245487261605229, 0.6316585996401112, 0.6357741351064675, 0.8642470965743208, 0.6221584174428315, 0.8114529051148651, 0.9141000841539659, 1.0, 0.000652855132677805, 0.619380623260443, 0.6967165256510812, 0.8520172592821194, 0.506187385602821, 0.4531855447309179, 0.556487662226795, 0.47283723761204166, 0.02357226111775114, 0.34272438732934996, 0.5811409219639153, 0.7245741928928766, 0.7410762804239461, 0.42174156708230004, 0.05236101266527961, 0.6201281281538, 0.4425387668374365, 0.4106098317062211, 0.8348436581869974, 0.5342366061507683, 0.598856437651094, 0.5142557134172622, 0.4210366752511142, 0.3096321480873838, 0.30859469411985707, 0.6554289700851836, 0.25775847535210283, 0.5157990952074163, 0.5365573165566148, 0.8079072311547144, 0.10209781994840159, 0.6002569359025927, 0.6536911179244789, 0.20167918995135747, 0.8330693404648544, 0.737896877118873, 0.3586627568222359, 0.3253313725311576, 0.5529615399519516, 0.542720104512177, 0.11367377306961232, 0.8807251792644784, 0.4471047149903593, 0.5142742735155721, 0.003157006751173676, 0.42292029976321466, 0.48074569931823535, 0.5264886858900315, 0.6625508357540224, 0.378235966508134, 0.49350233883087624, 0.027660853979899265, 0.28168179022383655, 0.6157132877459149, 0.8695475294213804, 0.8642531454411047, 0.2690474829925025, 0.9039342742606371, 0.5852685371188103, 0.21056545775751523, 0.35260703155848694, 0.571700750841684, 1.0, 0.684518199459927, 0.6006394976554918, 0.5910631333323872, 0.6545676223066557, 0.3419264503833429, 0.6011910399083685, 0.5297376135294585, 0.3850581958489265, 0.4363399509030277, 0.597415144816797, 0.0, 0.11523607763797766, 0.8821296454239946, 0.5334126180997758, 0.5960182409639856, 0.4736728434913504, 1.0, 0.2935973670960437, 0.7763469234319054, 0.6030410974533609, 1.0, 0.7417094915222344, 0.7220196246987096, 0.8301971233899816, 0.7623676872921671, 0.26213469909221376, 0.28925454700617426, 0.5946615235495416, 0.8034443090225696, 0.5190098186509483, 0.727697190762053, 0.6012653840740645, 0.7423958127221755, 0.5616798194467534, 0.16241527933295796, 0.8291884689534064, 0.21100892280323053, 0.21407800067854044, 0.0, 0.5321161339294644, 0.45066219168465615, 0.6587003940912031, 0.6209147654899613, 0.0, 0.5005929540204035, 0.5418455338416813, 0.0, 0.5235173156441624, 0.8232148025904986, 0.5131501387427619, 1.0, 0.5777830323781311, 0.3032212220646428, 0.7145087250676192, 0.5227886222566653, 0.09138854364080462, 0.7954935638657621, 0.7242967432293177, 0.6047568636054375, 0.8845572237610086, 0.7302007404183433, 0.3604281563086096, 0.3449411645032098, 0.7834298712141025, 0.32061687616798873, 0.43396716741543306, 0.08152573352530099, 0.7170369693946632, 0.7357300536317345, 0.42668545161773713, 0.4808854637922756, 0.40054306507297777, 0.4326263272143769, 0.6451645648968785, 0.3703988534694904, 0.5625034521772871, 0.5178989075058493, 0.8751893231491812, 1.0, 0.47457319057647784, 0.5400113278670842, 0.25456463735979495, 0.37189178247919374, 0.503404575411008, 0.7762047810201861, 0.6661120443406857, 0.40748362263541327, 0.6273977784387645, 0.3847960593523654, 0.676664818073909, 0.8736081095465102, 0.8052186899151903, 0.59486889942327, 0.7477579031356868, 0.07153421598018944, 0.22239930173190853, 0.5424335679710613, 0.4967181302788028, 0.8546109197500102, 0.3338184360111758, 0.5334126180997758, 0.3961983248070703, 0.7970182644937875, 0.2636668331849268, 0.38690016126589566, 0.6366159932299253, 0.5567052722262924, 0.9130169160146575, 0.20036562068264174, 0.39969884422068214, 0.31407610853279483, 0.3284266552141134, 0.5610561569234591, 0.8291884689534064, 0.4032606213616311, 0.39887834941966377, 0.48568383374204216, 0.0, 0.6395594029973759, 0.7358502882945588, 0.3076974924282943, 0.6994853675077588, 0.6111824626395996, 0.7202460953544985, 0.5606891562047243, 0.6355230163254099, 0.3075403667724301, 0.6049522285347575, 0.44022455039920705, 0.4637360866549208, 0.6887796716807859, 0.5092272948800609, 0.30508195771998325, 0.5765188209577117, 0.5342084433274925, 0.3164973788712128, 0.25447186640984776, 0.18499423211916005, 0.24136613624760855, 0.4964902913435728, 0.8475948805844231, 0.7149373135417085, 0.5747247951349734, 0.3735119256631941, 0.5365573165566148, 0.4582096230882099, 0.38876250635767995, 0.6892870539097846, 0.5385955305547254, 0.714297617187952, 0.6588643904221269, 0.6490264366907621, 0.692426316104594, 0.1342605543861164, 0.33556025372044823, 0.586176316104594, 0.4048126665159984, 0.3567897820436159, 0.3877668064270981, 0.41499118775675903, 0.7686443097434874, 0.522244255914633, 0.33034222781287603, 0.06430709861041217, 0.8340277308647588, 0.12179140591536411, 0.6066855853913183, 0.7866780561193292, 0.3882203984945388, 0.47665344789927383, 0.6519266365055134, 0.760152703330311, 0.20034796016239415, 0.6125559617261662, 0.11058267573293427, 0.6214002582757183, 0.3231065632068434, 0.9092813183436577, 0.9237479530474457, 0.819985574584918, 0.3154745710868131, 0.15421129818718932, 0.37535820701102107, 0.7731012556269232, 0.6863884298635612, 0.6836465025036174, 0.02913215362273379, 0.3805123536307766, 0.5774963429007336, 0.6208332549596509, 0.5519698034749387, 0.8975222218133634, 0.10003169217221897, 0.7414411521586157, 0.6471395484126266, 0.3760202619925232, 0.18613616175361308, 0.3101351894089418, 0.732236802097879, 0.075, 0.5871052192777595, 0.40660822240095185, 0.6685733388407632, 0.47180509653022784, 0.9039342742606371, 0.3102167775596418, 0.6540674839146357, 0.8357967156935899, 0.5705213788589844, 0.5845720003763292, 0.39999999999999997, 0.3714764154580026, 0.11772151898734176, 0.2465085296294044, 0.6731093971662766, 0.7591116846422887, 0.5732466007964828, 0.3285885556764443, 0.593328204480502, 0.27557102703119074, 0.6049888916030399, 0.0, 0.26107577186759023, 0.48882701787106714, 0.6129938455930134, 0.64272438732935, 0.7253305249945152, 0.8403751771302204, 0.5927200451626766, 0.06596858638743455, 0.6280140656881399, 0.30092201845272026, 0.4852392939116568, 0.5454443536750092, 0.2705664592842756, 0.6346254211318283, 0.49935498108571197, 0.12152819075380272, 0.7148321218614053, 0.689001498804813, 0.6987190209890508, 0.7740868998413366, 0.6607530176165711, 0.7823941821523834, 0.23687288125023864, 0.00022581894814892355, 0.6208776725666132, 0.7919953379147748, 0.38183354569032213, 0.9718352575347056, 0.8661547359011423, 0.28129969793939424, 0.5334147323445907, 0.7075905528115117, 0.6376288368886729, 0.9320894171538912, 0.5798418629627378, 0.7722344586267278, 0.5931785266046519, 0.6870606775436042, 0.6995228788727517, 0.48535576197454045, 0.7730817392743747, 0.2457519517810554, 0.6983761581889011, 0.5898186976657341, 0.8102736261199102, 0.8386800272334847, 0.3137948765704863, 0.682489182612035, 0.5470783037679993, 0.5149139863647084, 0.566344928301069, 0.859655200961138, 0.519681564481641, 0.6780323751412478, 0.2157314592227594, 0.388645918070455, 0.5549192578530084, 0.47222973989608136, 0.617098062347059, 0.23981732530607297, 0.6985540637369606, 0.3166508010405151, 0.04362372606317965, 0.8753481549016396, 0.12444994767545024, 0.33433367971754413, 0.2564794487481174, 0.7569706667451968, 0.7451384298635613, 0.13337173484667555, 0.7235277716721366, 0.30345899159460493, 0.8857441887419302, 0.6399520850864666, 0.8164562066675629, 0.9189641994569014, 0.623431183841569, 0.4952887122558812, 0.5007265995447144, 0.6162109760811862, 0.34556562059599494, 0.8586654659334061, 0.5943893444913173, 0.6, 0.02381136515041503, 0.7798152948467496, 0.2475891977778692, 0.39696520560566795, 0.6290960494103124, 0.8150666162955742, 0.5930360606692744, 0.6190128819697975, 0.4161379676803153, 0.6607637451794129]
Finish training and take 1h2m
