Namespace(log_name='./RQ5/tfix_8_1/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_8_1/codet5p_770m', data_dir='./data/RQ5/tfix_8_1', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '_initialize(callback) {   console.log("init")', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '_initialize(callback) {'}]
***** Running training *****
  Num examples = 8
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.6109279472597214e+266
  global_step = 3
  train_loss = 34.6616
  ********************
Previous best ppl:inf
Achieve Best ppl:1.6109279472597214e+266
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 17.5 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:17.5
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.3302020677698986e+239
  global_step = 5
  train_loss = 21.4649
  ********************
Previous best ppl:1.6109279472597214e+266
Achieve Best ppl:3.3302020677698986e+239
  ********************
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 37.54 	 Previous best codebleu 17.5
  ********************
 Achieve Best bleu:37.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 8.608882901403547e+245
  global_step = 7
  train_loss = 7.4926
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 39.77 	 Previous best codebleu 37.54
  ********************
 Achieve Best bleu:39.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.631656434846378e+263
  global_step = 9
  train_loss = 3.8276
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 47.58 	 Previous best codebleu 39.77
  ********************
 Achieve Best bleu:47.58
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 5.3133432652012053e+281
  global_step = 11
  train_loss = 2.1519
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 50.57 	 Previous best codebleu 47.58
  ********************
 Achieve Best bleu:50.57
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 2.3281347187618045e+295
  global_step = 13
  train_loss = 0.2504
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 51.47 	 Previous best codebleu 50.57
  ********************
 Achieve Best bleu:51.47
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = inf
  global_step = 15
  train_loss = 0.247
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 51.68 	 Previous best codebleu 51.47
  ********************
 Achieve Best bleu:51.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = inf
  global_step = 17
  train_loss = 0.1203
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 51.81 	 Previous best codebleu 51.68
  ********************
 Achieve Best bleu:51.81
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = inf
  global_step = 19
  train_loss = 0.0668
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 51.89 	 Previous best codebleu 51.81
  ********************
 Achieve Best bleu:51.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = inf
  global_step = 21
  train_loss = 0.1191
  ********************
Previous best ppl:3.3302020677698986e+239
BLEU file: ./data/RQ5/tfix_8_1/validation.jsonl
  codebleu-4 = 51.76 	 Previous best codebleu 51.89
  ********************
reload model from RQ5/tfix_8_1/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_8_1/test.jsonl
  codebleu = 53.11 
  Total = 500 
  Exact Fixed = 20 
[16, 22, 24, 31, 36, 49, 53, 119, 121, 145, 149, 157, 177, 184, 235, 263, 356, 407, 419, 421]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[23, 300, 411, 482]
  ********************
  Total = 500 
  Exact Fixed = 20 
[16, 22, 24, 31, 36, 49, 53, 119, 121, 145, 149, 157, 177, 184, 235, 263, 356, 407, 419, 421]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 4 
[23, 300, 411, 482]
  codebleu = 53.11 
[0.3488152698595746, 0.8789433149074164, 0.8185961732176825, 0.5451417837590342, 0.3519980391978243, 0.8216460780407606, 0.24770430686313644, 0.828218881300387, 0.7489935400021297, 0.44533781015729623, 0.8474682488744534, 0.15482473196980417, 0.3121594417706924, 0.0, 0.49987619040560277, 1.0, 0.792947845270083, 0.49952193216699714, 0.5701122864094044, 0.5525831514556421, 0.8099908471008581, 0.8249365300761395, 0.6126392797557267, 0.7135428903906851, 0.32799787354643783, 0.8465532719298088, 0.7349314779921117, 0.6287206814660993, 0.5336911179244788, 0.7348892033125801, 1.0, 0.3496110097993006, 0.6908005346740018, 0.621862347508016, 0.36697868604090833, 0.6443152851788645, 0.7349975480234779, 0.22499999999999998, 0.8478031368082721, 0.22499999999999998, 0.2831343999027389, 0.3738723994987426, 0.7122853668557851, 0.6785036503543816, 0.8356274376774735, 0.9431571237072573, 0.1965085296294044, 0.47578985205417273, 1.0, 0.5205584248799071, 0.41494193057548645, 0.4266408688157862, 0.8840113838519741, 0.6701687939127736, 0.5753891248592926, 0.396349572956634, 0.6530550965302279, 0.5873074388023817, 0.4016494526714358, 0.5834426521087845, 0.4905880158658711, 0.4229681039858978, 0.8052607518236046, 0.22242802984682342, 0.7517070403538422, 0.9260406910477721, 0.18, 0.6147920245401823, 0.6584460213186272, 0.5189589454524783, 0.5129834344293507, 0.275462082749631, 0.28890659929471596, 0.5843871292056362, 0.5459147667190589, 0.493447734789995, 0.42074846483844186, 0.22727148307139622, 0.20025890746608005, 0.6884734483714399, 0.6132366516010306, 0.6953602200025533, 0.6901940524102714, 0.49775691732305655, 0.7148321218614053, 0.631327274473851, 0.3267913765525572, 0.3944424098543815, 0.2240264366907621, 0.6487002210737599, 0.20356651918683533, 0.0, 0.4965085296294044, 0.7553359752999136, 0.44375943570639337, 0.6988432397208316, 0.7887437726770301, 0.39873667339437135, 0.6266865967254232, 0.8459894670707158, 0.0, 0.6416532852733938, 0.7275723927836586, 0.5916626897233772, 0.7464240007681606, 0.6265958401383632, 0.7903356967388899, 0.8165273869902545, 0.0, 0.5845976172210523, 0.8974634481857526, 0.7347169240849791, 0.5590480071842594, 0.3708863418440276, 0.6316585996401112, 0.6357741351064675, 0.8642470965743208, 0.3330387647191816, 0.8114529051148651, 0.7506656240001803, 1.0, 0.0731716191316424, 0.28613047846117523, 0.5557485571882866, 0.8990265449613299, 0.7135683378672271, 0.46768505892447165, 0.38644780929554395, 0.5523753896742678, 0.02357226111775114, 0.34272438732934996, 0.5811409219639153, 0.7245741928928766, 0.884411967378405, 0.22615023964596825, 0.06781788320735163, 0.6201281281538, 0.8540113838519741, 0.41605741671770546, 0.7668758886842345, 0.6597329991291957, 0.4901365436405146, 0.6383263299771573, 0.5340106749112018, 1.0, 0.762178754714643, 0.6331637502182879, 0.25775847535210283, 0.8114529051148651, 0.5365573165566148, 0.6224713793398138, 0.0004112690245826183, 0.6623525476789132, 0.12, 0.07380938629771232, 0.6607896535602065, 0.9891483218006352, 0.6148573601579692, 0.3253313725311576, 0.4222827177663117, 0.5289052690315463, 0.10640344428918075, 0.8397852044976344, 0.27605760761234577, 0.5142742735155721, 0.0, 0.2464217459109938, 0.6307456993182354, 0.5264886858900315, 0.6625508357540224, 0.34045596268696576, 0.5009596020209375, 0.027660853979899265, 0.13670507808851706, 0.6157132877459149, 0.5603706290417659, 1.0, 0.26660679600042475, 0.9519671371303184, 0.5852685371188103, 0.21410743157431567, 0.35260703155848694, 0.571700750841684, 0.7450395226130992, 0.42517784461691976, 0.6006394976554918, 0.625801768826227, 0.6578356986646514, 0.3549155125612238, 0.6105309741258526, 0.6339262873537368, 0.3903021600145299, 0.4363399509030277, 0.4829570445323632, 0.020944281753461413, 0.08201771471520561, 0.8821296454239946, 0.5334126180997758, 0.5960182409639856, 0.3314794487481174, 0.25662859879012523, 0.30462367113656974, 0.7944964819871267, 0.6498414563513699, 0.7821296454239948, 0.6262890508561036, 0.46886758067119544, 0.6276261338928446, 0.7623676872921671, 0.0, 0.5550049310474542, 0.38887247882087533, 0.8034443090225696, 0.5297241043652341, 0.727697190762053, 0.6583332549596508, 0.7423958127221755, 0.4496604481293247, 0.3002221934183693, 0.8291884689534064, 0.13357796393359542, 0.5924424048742483, 0.0, 0.29251884118473825, 0.4152413986364708, 0.6751390428831902, 0.7301961448188755, 0.0, 0.5011965082849565, 0.5407306205581414, 0.0, 0.5566984938224081, 0.8232148025904986, 0.5124567555628179, 1.0, 0.5987433254597354, 0.314114717309419, 0.3753212309992703, 0.5001583443992349, 0.09138854364080462, 0.7954935638657621, 0.980720400721969, 0.3782269592796987, 0.8845572237610086, 0.7302007404183433, 0.20248759730923885, 0.3449411645032098, 0.7834298712141025, 0.3583507427572924, 0.37257593934497335, 0.1126640645534398, 0.7327777279454752, 0.7357300536317345, 0.521841879407444, 0.43389678890316624, 0.6903510788203177, 0.4326263272143769, 0.6451645648968785, 0.48468456775520463, 0.671225780303527, 0.5178989075058493, 0.49648342523741423, 1.0, 0.8300792809207385, 0.39039047093608203, 0.29514178375903416, 0.38412587020318456, 0.503404575411008, 0.8513726633741598, 0.6661120443406857, 0.24993338842731957, 0.8134557045901138, 0.3847960593523654, 0.7338660149773069, 0.7916795920020299, 0.6828613511382454, 0.6995445329739118, 0.8942213309763742, 0.07153421598018944, 0.2923083608934159, 0.5726252770810941, 0.6043938999758249, 0.794074191877338, 0.5289167766291551, 0.5334126180997758, 0.3961983248070703, 0.7970182644937875, 0.2636668331849268, 0.0, 0.54359187342109, 0.6558003592103332, 0.9130169160146575, 0.2261166763951413, 0.39969884422068214, 0.31407610853279483, 0.5381429054415754, 0.6158725934593983, 0.8291884689534064, 0.41389347677795546, 0.7816265496309229, 0.4567932655066944, 0.0, 0.49812901135123744, 0.7358502882945588, 0.5707142567944372, 0.6197326548150837, 0.4921783165925231, 0.8178607431385985, 0.49458515552807747, 0.6355230163254099, 0.45222870310832236, 0.6049522285347575, 0.0, 0.4637360866549208, 0.15827751196172246, 0.5092272948800609, 0.4544467383889777, 0.5606065632068434, 0.3182071016610548, 0.8642470965743208, 0.26347186640984777, 0.8038961967467184, 0.4714698407062744, 0.4964902913435728, 0.8975948805844232, 0.3433513281205126, 0.3788887960834998, 0.362758161505804, 0.0, 0.4582096230882099, 0.8434613529595205, 0.6892870539097846, 0.5879792831098578, 0.714297617187952, 0.6265515982184771, 0.6490264366907621, 0.692426316104594, 0.00014274278804485472, 0.17399832950726074, 0.586176316104594, 0.0141550492747272, 0.07851580591138811, 0.4677820396003337, 0.5650218404915538, 0.7686443097434874, 0.5445795536136034, 0.8017397827309225, 0.06318453573472939, 0.5826652659811986, 0.008597490904973595, 0.6066855853913183, 0.7866780561193292, 0.10035199430589256, 0.5609260915878896, 0.586064317562083, 1.0, 0.20034796016239415, 0.6302733611924758, 0.053439818590077126, 0.6327003062366663, 0.2938379285853673, 0.9092813183436577, 0.9053229040324895, 0.6696215157128935, 0.3154745710868131, 0.15421129818718932, 0.3949005844802781, 0.7392647227570666, 0.6530550965302279, 0.776687390116769, 0.19234326037190141, 0.518486191548339, 0.5774963429007336, 0.6583332549596508, 0.5519698034749387, 0.8244901833316776, 0.8525366466180964, 0.6455554135282277, 0.5249883996044001, 0.3760202619925232, 0.6392333266349789, 0.3275887968144088, 0.732236802097879, 0.075, 0.591516286342532, 0.3979319487585916, 0.2935665191868353, 0.24850801306021933, 0.9039342742606371, 0.6903510788203177, 0.6214284196739419, 0.8357967156935899, 0.6289313661614635, 0.5845720003763292, 0.676275735434565, 0.3714764154580026, 0.09999999999999999, 0.2465085296294044, 0.591817701714571, 0.7591116846422887, 0.6320615067866382, 0.4162135382912301, 0.593328204480502, 0.3798512599079524, 0.7418205419054216, 0.0, 1.0, 0.5053527754134511, 0.5796605122596801, 0.64272438732935, 0.8587480609952844, 0.7946898908258391, 0.49605468697269356, 0.15, 0.7304885888713295, 0.5200859424960422, 0.628162097767208, 0.6793826064669832, 0.6922382617563554, 0.8202499593885415, 1.0, 0.06321846471364724, 0.7148321218614053, 0.6273749825292059, 0.7543841330490832, 0.49634974605925863, 0.4468450130404956, 0.7750881277610704, 0.23687288125023864, 0.03115588878443292, 0.4984571392302903, 0.16722832123730585, 0.4528832509666797, 0.5906192662145219, 0.8661547359011423, 0.3259403648918202, 0.5334147323445907, 0.610821244538978, 0.6804859797458157, 0.9320894171538912, 0.525, 0.7722344586267278, 0.6368650958534154, 0.3274795143841589, 0.6995228788727517, 0.5234369219530679, 0.560765580121347, 0.2457519517810554, 0.6983761581889011, 0.6894939194751454, 0.27930843784758125, 0.8386800272334847, 0.27967444404486574, 0.6838528189756714, 0.5470783037679993, 0.5149139863647084, 0.7065605978001186, 0.7994319352648402, 0.5408401405239117, 0.6304241997444744, 0.5801154532856382, 0.388645918070455, 0.5549192578530084, 0.4940378154875154, 0.6545980623470591, 0.7076923076923076, 0.8642470965743208, 0.3166508010405151, 0.04603121926729405, 0.13512279940401256, 0.07944994767545024, 0.0, 0.2141529587579214, 0.5062907188047069, 0.7451384298635613, 0.05467743614169578, 0.38515770440128066, 0.565671952352334, 0.8857441887419302, 0.6399520850864666, 0.7161478911751015, 0.6032506780138034, 0.499973975204846, 0.5252887122558813, 0.5764788021120066, 0.6162109760811862, 0.37336167952230914, 0.673520179686609, 0.5943893444913173, 0.0, 0.25648766222679503, 0.791455477731289, 0.3683458566180241, 0.12975774235270576, 0.5127882863610167, 0.8150666162955742, 0.41504311652514486, 0.6113235238734058, 0.6573831527026088, 0.6109308002123074]
Finish training and take 40m
