Namespace(log_name='./RQ5/tfix_16_3/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/tfix_16_3/codet5p_220m_f', data_dir='./data/RQ5/tfix_16_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 16 training instances 
***** Running training *****
  Num examples = 16
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01096
  global_step = 3
  train_loss = 1.5233
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01096
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 9.8 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.8
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00749
  global_step = 5
  train_loss = 0.9982
  ********************
Previous best ppl:1.01096
Achieve Best ppl:1.00749
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 25.08 	 Previous best codebleu 9.8
  ********************
 Achieve Best bleu:25.08
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00657
  global_step = 7
  train_loss = 0.5637
  ********************
Previous best ppl:1.00749
Achieve Best ppl:1.00657
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 37.13 	 Previous best codebleu 25.08
  ********************
 Achieve Best bleu:37.13
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00628
  global_step = 9
  train_loss = 0.4376
  ********************
Previous best ppl:1.00657
Achieve Best ppl:1.00628
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 49.21 	 Previous best codebleu 37.13
  ********************
 Achieve Best bleu:49.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00626
  global_step = 11
  train_loss = 0.275
  ********************
Previous best ppl:1.00628
Achieve Best ppl:1.00626
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 52.33 	 Previous best codebleu 49.21
  ********************
 Achieve Best bleu:52.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00621
  global_step = 13
  train_loss = 0.2377
  ********************
Previous best ppl:1.00626
Achieve Best ppl:1.00621
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 52.93 	 Previous best codebleu 52.33
  ********************
 Achieve Best bleu:52.93
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00614
  global_step = 15
  train_loss = 0.192
  ********************
Previous best ppl:1.00621
Achieve Best ppl:1.00614
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 53.36 	 Previous best codebleu 52.93
  ********************
 Achieve Best bleu:53.36
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.0061
  global_step = 17
  train_loss = 0.1316
  ********************
Previous best ppl:1.00614
Achieve Best ppl:1.0061
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 54.01 	 Previous best codebleu 53.36
  ********************
 Achieve Best bleu:54.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00611
  global_step = 19
  train_loss = 0.1454
  ********************
Previous best ppl:1.0061
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 53.72 	 Previous best codebleu 54.01
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00611
  global_step = 21
  train_loss = 0.1392
  ********************
Previous best ppl:1.0061
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 53.4 	 Previous best codebleu 54.01
  ********************
reload model from RQ5/tfix_16_3/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_16_3/test.jsonl
  codebleu = 52.44 
  Total = 500 
  Exact Fixed = 5 
[60, 80, 111, 182, 198]
  Syntax Fixed = 2 
[123, 411]
  Cleaned Fixed = 5 
[3, 53, 199, 345, 441]
  ********************
  Total = 500 
  Exact Fixed = 5 
[60, 80, 111, 182, 198]
  Syntax Fixed = 2 
[123, 411]
  Cleaned Fixed = 5 
[3, 53, 199, 345, 441]
  codebleu = 52.44 
[0.4068864709786152, 0.8170015046833874, 0.8607870392543291, 0.4787139878886564, 0.7561984520017521, 0.5196070804402777, 0.7130804239841513, 0.10993069851559968, 0.5541384249646617, 0.6779253872262, 0.3499927730653973, 0.7067366858513033, 0.7986647134866534, 0.49718705940327845, 0.39323313167877133, 0.5179832112495028, 0.3513926103474601, 0.3951417837590342, 0.5930248247228662, 0.9207265089204273, 0.832900393434221, 0.3743172882353815, 0.08069897885804689, 0.6793670579146192, 0.5826136760976885, 0.2772943579237247, 0.7949680697811121, 0.15747669278081078, 0.39985271067894745, 0.14820673147239685, 0.535133493667276, 0.6142482969348242, 0.7134655991716751, 0.35445033440961526, 0.3326555560719357, 0.38537461062620737, 0.0983633086619942, 0.8504403342456683, 0.6934531647858191, 0.47504600279260195, 0.6389870100570282, 0.22267396568917747, 0.6724315693987384, 0.5229326815734496, 0.5481721142074378, 0.5652887122558813, 0.49930531945812784, 0.602585911416486, 0.46877089663917104, 0.2968222263370741, 0.5123984208296868, 0.5266057619745405, 0.8919671371303186, 0.622877007215058, 0.00041948941474664794, 0.5649628059954859, 0.8968939355581875, 0.7801421850012838, 0.6170408500468526, 1.0, 0.5543393803680781, 0.4585617401550842, 0.1767032844714636, 0.5897692121354159, 0.3141503804981154, 0.4760919216151017, 0.7879897739458661, 0.46240136695573764, 0.6168173810866084, 0.5416581477675209, 0.1621787547146431, 0.1526586440216252, 0.08978100888954238, 0.8917740712291482, 0.3507846217557572, 0.11301169937893027, 0.4567932655066944, 0.548663900608423, 0.643291057814614, 1.0, 0.6530747837026651, 0.7265018183047776, 0.26090620551116817, 0.803950001904677, 0.5766433148075119, 0.9179350389391119, 0.0, 0.33408795799742014, 0.004644195706906341, 0.5675986891240699, 0.47132649074660393, 0.7299948711471688, 0.15102685321885573, 0.9190411344533542, 0.3915187569135013, 0.4484520389345378, 0.7459627599846212, 0.016782765293716924, 0.6488884298635612, 0.4261742857516409, 0.4065085296294044, 0.3326111746047917, 0.20420431542563985, 0.0, 0.07719082257468701, 0.30480616432215846, 0.5996203926131513, 0.467913762972279, 0.7764804587657133, 0.5163011429973444, 1.0, 0.6785036503543816, 0.35473913473843294, 0.414297617187952, 0.2924537101693822, 0.525828873805819, 0.3755170666459996, 0.16950123401194037, 0.19999999999999998, 0.007212488274723348, 0.10648766222679504, 0.30382645947812886, 0.9778171304985477, 0.10906101778358473, 0.852173641641244, 0.8150666162955742, 0.4118819918812901, 0.5551676892768422, 0.08105542962504894, 0.5737030996295007, 0.7923024832485084, 0.7386991709927695, 0.30248080109292785, 0.7768898072067754, 0.22619887519287302, 0.0792910267555298, 0.7853313927871439, 0.4812527431200818, 0.6052067080970429, 0.1408351627063215, 0.7239139638913099, 0.5960707868709735, 0.33560958904485566, 0.5440521407974296, 0.6420343616573666, 0.6530662700819763, 0.19229545276671334, 0.48948121825674606, 0.6060694567816493, 0.5063942770541421, 0.7059117582516798, 0.7767788352354288, 0.921263693253253, 0.3717045733209613, 0.7728222909971192, 0.0, 0.7028425048476072, 0.8245957586820076, 0.9321253696376492, 0.5266057619745405, 0.0004441760249550684, 0.8741625849783967, 0.5461243465154161, 0.5301818996067954, 0.7750198530224517, 0.6745836923955046, 0.44478640618236376, 0.47511870560120445, 0.6957231649810731, 0.19999999999999998, 0.5432390392078216, 0.4858412195420705, 0.4831458551130241, 0.5532531256091296, 0.6477517173276517, 0.7202172177276029, 0.5386504401342904, 0.6706847188488577, 0.6577191681808412, 0.6190738228228467, 0.7269320077439674, 0.795720901409837, 0.7302007404183433, 0.8426568563575418, 0.6814650334361707, 0.0, 0.6583603643329422, 0.836150319044902, 0.4231498701419104, 0.49716968796987465, 0.050923035847610806, 0.5728633222115624, 0.7359097981643703, 0.49393424193861524, 0.29818583914268965, 0.6243518451142414, 0.5266551101233702, 0.8114529051148651, 0.6319730195988179, 0.39873667339437135, 0.15429293535427072, 0.6863884298635612, 0.5975331770417378, 0.1898355357735218, 0.4739790984471416, 0.6459247942853364, 0.8348655886940333, 0.000652855132677805, 0.4307250470563342, 0.2984943145963362, 0.1552248764707024, 0.3972934177879658, 0.9080331470954286, 0.322412788792651, 0.8249783514952764, 0.2067414939499072, 0.8956377568613374, 0.6663998078842303, 0.42493877026999405, 0.6530836285702823, 0.330161739005039, 0.11674655903247219, 0.5960626227910577, 0.3819511489967859, 0.6387851138365109, 0.6093189247482212, 0.6271806220068676, 0.45443178762703906, 0.8541030238941549, 0.43016109313909, 0.3566743097500106, 0.7862551025473414, 0.5914362471075907, 0.47035576197454043, 0.7521271372862586, 0.5274151448167971, 0.6726527033303111, 0.2087918430297149, 0.48442063021559306, 0.6029055732198362, 0.3965630043849799, 0.7816396051000123, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6700451085394062, 0.8407059481807464, 0.1616084432544398, 0.4435027248795795, 0.565629122056696, 0.0, 0.7796468115916302, 0.8599305805820563, 0.6179144359460748, 0.47535257334060044, 0.5223835094667514, 0.0, 0.3112087107959515, 0.7355980328281302, 0.09418181350952061, 0.5376172770255981, 0.5721360784673309, 0.8016000841539659, 0.611572567109274, 0.565895296523178, 0.6875131192847503, 0.0, 0.24210967177642617, 0.6499702347088693, 0.367641751923923, 0.6568173810866085, 0.7582416075216148, 0.6463338290119179, 0.13497366551702938, 0.1636363636363636, 0.5058144125160369, 0.6139713807029061, 0.7570701463285548, 0.11349072297901378, 0.830159378073015, 0.6519830178464932, 0.6506923930808135, 0.6041077305454495, 0.47205977696298684, 0.8710525186025484, 0.6959913353679734, 0.2992410381348145, 0.6899766481408451, 0.3128876281187488, 0.6339165338717334, 0.8244788648855748, 0.03821771168817468, 0.8398078253515011, 0.9103556223308578, 0.5782030614679, 0.00020245709315312648, 0.5550049310474542, 0.7208697179699937, 0.5843539719797703, 0.6197856242749173, 0.43701826449378744, 0.7601650731090275, 0.09477552840328918, 0.5960707868709735, 0.7839520803542337, 0.3844698211793555, 0.5903958808688652, 0.48793250423030166, 0.40660822240095185, 0.8794984978839551, 0.6948195962314024, 0.1728369334452749, 0.25718473747965853, 0.8997846296433376, 0.47497783724203585, 0.6255312872878935, 0.5853377757733382, 0.3854194889831103, 0.5768908160692685, 0.32291232918053253, 0.46013112345018237, 0.8255980153481091, 0.6392333266349789, 0.45882322282895327, 0.3008285762477364, 0.5459123879120212, 0.6901359472484083, 0.6009071204548626, 0.23979699636679497, 0.4343002319965344, 0.6865490061448141, 0.6742738870938194, 0.4429570445323633, 0.8341971802528994, 0.4707909983708979, 0.6465085296294044, 0.9195522364276514, 0.7144948346694379, 0.4709261928229648, 0.6540660710707717, 0.23698351772910467, 0.7329853689231081, 0.8591190003999107, 0.5942976171879519, 0.7188342657129119, 0.6209070226208282, 0.45149530211935385, 0.5909402501865353, 0.38304611372110686, 0.47786341412302047, 0.7135160729534664, 0.6999214919756145, 0.6781982935333077, 0.619380623260443, 0.41425300780066243, 0.18707206776696358, 0.5761020526714384, 0.9608461673226849, 0.7886327123579241, 0.5140913145175611, 0.4147896756607047, 0.6213411107115325, 0.5190854538169563, 0.370321911798854, 0.43554564932669804, 0.7391886750930661, 0.4729254579408031, 0.6655012758056489, 0.34003623200632915, 0.3022109370915457, 0.6530550965302279, 0.6149139863647084, 0.4366371358026476, 0.32606388765886823, 0.3105028374306678, 0.11453360085481801, 0.43018775411435767, 0.6892870539097846, 0.7965481359238318, 0.49202520055807764, 0.14128159428409093, 0.7201963381119434, 0.7802383125555377, 0.04448415542647708, 0.3154745710868131, 0.6400373216188153, 0.5217365165784443, 0.23485079460385788, 0.29490446731397885, 0.6193453441662242, 0.8174230148919821, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 0.8165273869902545, 0.651478562336719, 0.7308835793300246, 0.35437970233785665, 0.5763240578218234, 0.11841238567827725, 0.5519698034749387, 0.5509782586537075, 0.8007265089204272, 0.7017410285928236, 0.7787918207406566, 0.8604352575955447, 0.4054183564853334, 0.34575990398983625, 0.8433369370338967, 0.9365276486140073, 0.6555507559481846, 0.5084373166195929, 0.8214029683767206, 0.3334120966880527, 0.3006391743081693, 0.4352237115020325, 0.35236123004653963, 0.4007535003838721, 0.06381981325167131, 0.23683637774429345, 0.4261332479851998, 0.3122632052291401, 0.940598945729278, 0.4514966211671801, 0.4830718224801436, 0.5943495293944743, 0.6418894161880858, 0.4353746106262073, 0.7233004498085515, 0.424004582733079, 0.8661547359011423, 0.15472876794829787, 0.6685993728980426, 0.39999999999999997, 0.2819635355836436, 0.7098872906896011, 0.6847340670900772, 0.32664086881578624, 0.6937628353106088, 0.6285117129535003, 0.7834843745068932, 0.8519938276459833, 0.2220581256939062, 0.40787638072411536, 0.6375167201921692, 0.6151628493773373, 0.5612238344291625, 0.5925167201921692, 0.6424778561738973, 0.3734198009532672, 0.8542712839031905, 0.7548295193530756, 0.3691289879974663, 0.28890659929471596, 0.6578607431385983, 0.41080814539619515, 0.7322642870443095, 0.8334272760895116, 0.6745758964627063, 0.003996009641464269, 0.8253119306693903, 0.5227854886870906, 0.6268288199455332, 0.11580535393740449, 0.34509049865171215, 0.5017413598251923, 0.2900371026939149, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.7412242091620882, 0.355067633384039, 0.7020875907084383, 0.4326263272143769, 0.6825587717608834, 0.29063831791603756, 0.12, 0.8924270154069307, 0.7016265496309229, 0.6997109227752119, 0.6028785751316023, 0.36393172467891854, 0.5677242879412823, 0.6137392159029667, 0.42880120128837773, 0.5907820443144609, 0.5030897366221532, 0.8180779662080888, 0.621238225076177, 0.24412331445752453, 0.6186389706521688, 0.7803858638186933, 0.6988219320042083, 0.7396773803089467, 0.6492633713864637, 0.7279095254199106, 0.6056671842902857, 0.08093484546425737, 0.7158229243802021]
Finish training and take 25m
