Namespace(log_name='./RQ5/tfix_16_3/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/tfix_16_3/codet5p_770m', data_dir='./data/RQ5/tfix_16_3', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "},  createIcon: function () {   var div = document.createElement('div'),       options = this.options;", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "},  createIcon: function (oldIcon) {   var div = (oldIcon && oldIcon.tagName === 'DIV') ? oldIcon : document.createElement('div'),       options = this.options;"}]
***** Running training *****
  Num examples = 16
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 6.887590891413863e+254
  global_step = 5
  train_loss = 35.5547
  ********************
Previous best ppl:inf
Achieve Best ppl:6.887590891413863e+254
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 30.33 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:30.33
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 8.036996933059851e+226
  global_step = 9
  train_loss = 13.4124
  ********************
Previous best ppl:6.887590891413863e+254
Achieve Best ppl:8.036996933059851e+226
  ********************
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 55.91 	 Previous best codebleu 30.33
  ********************
 Achieve Best bleu:55.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 2.352206262110356e+265
  global_step = 13
  train_loss = 4.538
  ********************
Previous best ppl:8.036996933059851e+226
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 57.28 	 Previous best codebleu 55.91
  ********************
 Achieve Best bleu:57.28
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.7623651969717523e+290
  global_step = 17
  train_loss = 1.7926
  ********************
Previous best ppl:8.036996933059851e+226
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 57.38 	 Previous best codebleu 57.28
  ********************
 Achieve Best bleu:57.38
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.899992530066979e+297
  global_step = 21
  train_loss = 1.5176
  ********************
Previous best ppl:8.036996933059851e+226
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 57.71 	 Previous best codebleu 57.38
  ********************
 Achieve Best bleu:57.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.8424362498699705e+298
  global_step = 25
  train_loss = 0.9221
  ********************
Previous best ppl:8.036996933059851e+226
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 58.41 	 Previous best codebleu 57.71
  ********************
 Achieve Best bleu:58.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.1558433501426452e+299
  global_step = 29
  train_loss = 0.8617
  ********************
Previous best ppl:8.036996933059851e+226
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 57.95 	 Previous best codebleu 58.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 5.513345236830266e+302
  global_step = 33
  train_loss = 0.2425
  ********************
Previous best ppl:8.036996933059851e+226
BLEU file: ./data/RQ5/tfix_16_3/validation.jsonl
  codebleu-4 = 57.39 	 Previous best codebleu 58.41
  ********************
early stopping!!!
reload model from RQ5/tfix_16_3/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/tfix_16_3/test.jsonl
  codebleu = 55.2 
  Total = 500 
  Exact Fixed = 18 
[19, 62, 79, 105, 123, 130, 149, 162, 188, 226, 252, 284, 417, 465, 474, 487, 488, 489]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[9, 120, 138, 159, 441, 445]
  ********************
  Total = 500 
  Exact Fixed = 18 
[19, 62, 79, 105, 123, 130, 149, 162, 188, 226, 252, 284, 417, 465, 474, 487, 488, 489]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 6 
[9, 120, 138, 159, 441, 445]
  codebleu = 55.2 
[0.13187633450068909, 0.8170015046833874, 0.861672684198352, 0.3668284850162606, 0.5377168767164375, 0.8697471734123385, 0.7130804239841513, 0.10993069851559968, 0.6119613551150405, 0.6779253872262, 0.3499927730653973, 0.7067366858513033, 0.5274043937673059, 0.49718705940327845, 0.39323313167877133, 0.6298602502474342, 0.566580541559268, 0.3951417837590342, 1.0, 0.9207265089204273, 0.5967187034833847, 0.3743172882353815, 0.1714285714285714, 0.4462869840731956, 0.5826136760976885, 0.22789573685021236, 0.7949680697811121, 0.15747669278081078, 0.5252284068217509, 0.0214411564631476, 0.535133493667276, 0.6142482969348242, 0.7134655991716751, 0.4359623196486092, 0.3326555560719357, 0.8149139863647084, 0.33369892294571574, 0.8504403342456683, 0.6934531647858191, 0.41019633162889435, 0.6873166917950988, 0.22267396568917747, 0.47388842986356117, 0.5229326815734496, 0.5481721142074378, 0.5616090180988553, 0.4852646477679837, 0.5961851509772491, 0.46877089663917104, 0.2968222263370741, 0.5123984208296868, 0.5266057619745405, 0.8223873319410013, 0.32435175818106565, 0.677190822574687, 0.861482988138697, 0.8968939355581875, 0.8581179629766709, 0.6303518920101568, 0.6607530176165711, 0.608455162308236, 1.0, 0.5610771314034941, 0.5897692121354159, 0.5558895153674697, 0.48289849585465594, 0.8303363603804119, 0.5970182644937875, 0.6168173810866084, 0.7539630945946609, 0.2752284068217509, 0.20369111792447886, 0.08978100888954238, 0.8917740712291482, 0.5007846217557572, 0.20613487118166404, 0.4887096364908238, 0.48719425841404007, 1.0, 0.6724062327149246, 0.6530747837026651, 0.6387289085944345, 0.26090620551116817, 0.6953856477238599, 0.5766433148075119, 0.9179350389391119, 0.0, 0.2966597993138361, 0.034776848365803435, 0.5675986891240699, 0.4570583972426556, 0.6715950349550133, 0.582226723380104, 0.814064525684886, 0.4134404736596011, 0.4484520389345378, 0.7459627599846212, 0.016782765293716924, 0.6488884298635612, 0.4261742857516409, 0.15, 0.4690750900518629, 0.8199440232953434, 0.0, 0.8249365300761395, 0.30480616432215846, 0.48167268419835185, 0.467913762972279, 0.7764804587657133, 0.5024101654079205, 0.5799982506601298, 0.6785036503543816, 0.35473913473843294, 0.5563659179388998, 0.2924537101693822, 0.5566638701334943, 0.638161672266765, 0.16950123401194037, 0.19999999999999998, 0.08953276868214524, 0.10648766222679504, 0.9068864709786153, 0.9472083239018387, 0.10398409089266691, 0.8840188084091536, 0.8150666162955742, 0.196275735434565, 0.5790749086635656, 0.7562262989907678, 0.9891483218006352, 0.7977695539611169, 0.7386991709927695, 0.5777131912598308, 0.7768898072067754, 0.18, 0.2118371591503378, 0.7760858398620529, 0.6290977791076333, 0.2991202346041056, 0.6937464703296872, 0.7239139638913099, 0.5960707868709735, 0.33993335366224897, 0.5440521407974296, 0.8036686678144145, 0.7530045026259591, 0.23624690106638713, 0.523507649142383, 1.0, 0.5063942770541421, 0.7059117582516798, 0.7834741354873337, 0.921263693253253, 0.3571446984837483, 0.7728222909971192, 0.5632387151836264, 0.734603718873536, 0.7264591035982078, 0.7290594020931371, 0.5266057619745405, 0.5348435312891645, 0.7863340021370451, 0.5461243465154161, 0.5301818996067954, 0.5392238094630241, 0.6745836923955046, 0.43638309498695516, 0.41511870560120445, 0.6957231649810731, 0.3536911179244788, 0.5823857657872067, 0.4858412195420705, 0.5351384298635612, 0.5532531256091296, 0.6477517173276517, 0.7202172177276029, 0.8043648769426102, 0.6706847188488577, 0.03134183179736939, 0.510409269308891, 0.7269320077439674, 0.484474208553701, 0.7302007404183433, 0.8426568563575418, 0.6814650334361707, 0.3812999423756144, 0.6956009081078438, 1.0, 0.4231498701419104, 0.49716968796987465, 0.050923035847610806, 0.5728633222115624, 0.644008445950933, 0.49393424193861524, 0.3376480475289295, 0.4718765181257531, 0.5266551101233702, 0.5236728434913505, 0.5692803427643551, 0.39873667339437135, 0.15429293535427072, 0.6863884298635612, 0.7296508600957852, 0.1898355357735218, 0.4739790984471416, 0.32342195343189895, 0.9226212781081518, 0.0731716191316424, 0.46418189365627316, 0.37050628892829995, 0.21245371016938225, 0.37412242494906767, 0.9080331470954286, 0.35715141059137, 0.8249783514952764, 0.2067414939499072, 0.8614568392030433, 0.661561098206811, 0.41398045346368395, 0.619750295236949, 0.6487021300892772, 0.8573835536643046, 0.73633134627943, 0.3819511489967859, 0.6387851138365109, 1.0, 0.6271806220068676, 0.46132529083669765, 0.8541030238941549, 0.5660528353107435, 0.39359500830217886, 0.7862551025473414, 0.4821379477582639, 0.47035576197454043, 0.7521271372862586, 0.5274151448167971, 0.6726527033303111, 0.4755831472572245, 0.6056583090096291, 0.6029055732198362, 0.3307456993182354, 0.7816396051000123, 0.0, 0.3538685674110905, 0.5944836924880659, 0.6208906511898491, 0.7080707346422974, 0.1717728294285068, 0.4435027248795795, 0.4136914204540241, 0.0375, 1.0, 0.8599305805820563, 0.6179144359460748, 0.4089903936086955, 0.4223835094667513, 0.0, 0.6613564407173638, 0.7355980328281302, 0.09418181350952061, 0.5376172770255981, 0.5721360784673309, 0.8016000841539659, 0.5568810347888558, 0.565895296523178, 0.5589682282930881, 0.0, 0.5764788021120066, 0.3285867151222953, 0.367641751923923, 0.6568173810866085, 0.7582416075216148, 0.35928389427319457, 0.1405109696242391, 0.13636363636363635, 0.5058144125160369, 0.7439654153934541, 0.5516841554878408, 0.11349072297901378, 0.8212928138736779, 0.6519830178464932, 0.6506923930808135, 0.6041077305454495, 1.0, 0.837527253566337, 0.390330239830439, 0.24302916939320657, 0.6899766481408451, 0.6589933075722675, 0.48147307154173735, 0.8244788648855748, 0.12327907825685894, 0.8398078253515011, 0.7721214735047168, 0.5782030614679, 0.027660853979899265, 0.5550049310474542, 0.820109647033614, 0.5843539719797703, 0.6197856242749173, 0.43701826449378744, 0.7215433553637871, 0.15837703500732306, 0.4203815723264836, 0.7839520803542337, 0.3844698211793555, 0.6107000692160132, 0.49930531945812784, 0.40660822240095185, 0.816225472769031, 0.8312424689833122, 0.3227886222566654, 0.7461473435553267, 0.8251411716151402, 0.46526859680566335, 0.7531697895341449, 0.5853377757733382, 0.5290092325728539, 0.6060574827359351, 0.5678911585054794, 0.5537998353931795, 0.7442518335455897, 0.6392333266349789, 0.5110656166644442, 0.37549356386576216, 0.2713904431698031, 0.3001402995437201, 0.5189589454524783, 0.44076500003648533, 0.4343002319965344, 0.6007538980470817, 0.6742738870938194, 0.4429570445323633, 0.3682936672819867, 0.4707909983708979, 0.5841414787295285, 0.8140165767392193, 0.7144948346694379, 0.4709261928229648, 0.6540660710707717, 0.6352900380997268, 0.7329853689231081, 0.7277728941065077, 0.5942976171879519, 0.6371351487081305, 0.6209070226208282, 0.4776873053131037, 0.5188036285789263, 0.38304611372110686, 0.47786341412302047, 0.6721193723777923, 0.6999214919756145, 0.5363372239048205, 0.5840403648839952, 0.6737569575259978, 0.3426136760976885, 0.5883089748131176, 0.8193461763670171, 0.7886327123579241, 0.5140913145175611, 0.4147896756607047, 0.8155450160863305, 0.5190854538169563, 0.370321911798854, 0.43554564932669804, 0.7391886750930661, 0.4729254579408031, 0.5669580702376444, 0.34003623200632915, 0.075, 0.6530550965302279, 0.37336167952230914, 0.4366371358026476, 0.6616064237622327, 0.3105028374306678, 0.11453360085481801, 0.536560588211956, 0.6130037692121442, 0.7965481359238318, 0.545545649326698, 0.5902838873166409, 0.6286282482830732, 0.7802383125555377, 0.04448415542647708, 0.3651230568257158, 0.6355353074494696, 0.5217365165784443, 0.8623473261888206, 0.8685222066525069, 0.6193453441662242, 0.8174230148919821, 0.4444597200395934, 0.5781687723560363, 0.3508081453961951, 0.3807256007926331, 0.7879154447011512, 0.7009116410097869, 0.7308835793300246, 0.35437970233785665, 0.5674643022117409, 0.11841238567827725, 0.5519698034749387, 0.4302797163570308, 0.6863559408133775, 0.7017410285928236, 0.7787918207406566, 0.8604352575955447, 0.4054183564853334, 0.34575990398983625, 0.5682029006685881, 0.7292234646171288, 0.6590926274900687, 0.5084373166195929, 0.8214029683767206, 0.40209781994840155, 0.3006391743081693, 1.0, 0.7972388343569652, 0.34363804004496623, 0.24658767523161879, 0.23683637774429345, 0.6428938087244226, 0.3226773198519566, 0.940598945729278, 0.7923043045126105, 0.43819881886061685, 0.19272438732935, 0.6204878154434781, 0.2666932800422169, 0.5335009416319088, 0.4085332823846201, 0.8661547359011423, 0.14140966929422433, 0.5545448329873377, 0.39999999999999997, 0.7023594991536818, 0.6646919686683406, 0.49671403360290034, 0.8742687840283396, 0.48861662650065896, 0.6285117129535003, 0.7744515423179406, 0.8519938276459833, 0.45318582487514386, 0.6522551430667551, 0.6375167201921692, 0.5316562756993463, 0.6901940524102714, 0.6019640702012092, 0.6733027629890207, 0.34644440865423376, 0.8312408824177573, 0.7548295193530756, 0.31599709189457237, 0.28890659929471596, 0.6578607431385983, 0.42454256914753175, 0.6085471970529851, 0.8334272760895116, 0.46080177396663735, 0.003996009641464269, 0.8253119306693903, 0.45013958763906625, 0.5811145153819444, 1.0, 0.34509049865171215, 0.5075783901825247, 0.22719082257468703, 0.4016265496309229, 0.45473913473843297, 0.3008557137029968, 0.7412242091620882, 0.355067633384039, 1.0, 0.4326263272143769, 0.643309301212212, 0.30215269230653224, 0.12, 0.8294519950448036, 0.7016265496309229, 0.766682914692771, 0.4421458507062197, 0.22333115374121348, 0.7451384298635613, 0.6137392159029667, 0.42880120128837773, 1.0, 1.0, 1.0, 0.6254581948739375, 0.7494203690984217, 0.6186389706521688, 0.7803858638186933, 0.6834373166195928, 0.7396773803089467, 0.6173015055893455, 0.7279095254199106, 0.8691371695837178, 0.20034796016239415, 0.3158229243802022]
Finish training and take 21m
