Namespace(log_name='./RQ5/javascript_5/f2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_5/f2_codet5p_770m', data_dir='./data/RQ5/javascript_5_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0157
  global_step = 2
  train_loss = 4.7322
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0157
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 12.05 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00982
  global_step = 3
  train_loss = 2.1456
  ********************
Previous best ppl:1.0157
Achieve Best ppl:1.00982
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 16.86 	 Previous best codebleu 12.05
  ********************
 Achieve Best bleu:16.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00799
  global_step = 4
  train_loss = 0.7356
  ********************
Previous best ppl:1.00982
Achieve Best ppl:1.00799
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 25.21 	 Previous best codebleu 16.86
  ********************
 Achieve Best bleu:25.21
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00737
  global_step = 5
  train_loss = 1.1202
  ********************
Previous best ppl:1.00799
Achieve Best ppl:1.00737
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 28.1 	 Previous best codebleu 25.21
  ********************
 Achieve Best bleu:28.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00684
  global_step = 6
  train_loss = 0.4841
  ********************
Previous best ppl:1.00737
Achieve Best ppl:1.00684
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 27.07 	 Previous best codebleu 28.1
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00645
  global_step = 7
  train_loss = 0.5361
  ********************
Previous best ppl:1.00684
Achieve Best ppl:1.00645
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 28.16 	 Previous best codebleu 28.1
  ********************
 Achieve Best bleu:28.16
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00615
  global_step = 8
  train_loss = 0.1932
  ********************
Previous best ppl:1.00645
Achieve Best ppl:1.00615
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 29.14 	 Previous best codebleu 28.16
  ********************
 Achieve Best bleu:29.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00595
  global_step = 9
  train_loss = 0.2242
  ********************
Previous best ppl:1.00615
Achieve Best ppl:1.00595
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 30.04 	 Previous best codebleu 29.14
  ********************
 Achieve Best bleu:30.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00584
  global_step = 10
  train_loss = 0.7259
  ********************
Previous best ppl:1.00595
Achieve Best ppl:1.00584
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 31.03 	 Previous best codebleu 30.04
  ********************
 Achieve Best bleu:31.03
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00578
  global_step = 11
  train_loss = 0.2154
  ********************
Previous best ppl:1.00584
Achieve Best ppl:1.00578
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 31.51 	 Previous best codebleu 31.03
  ********************
 Achieve Best bleu:31.51
  ********************
reload model from RQ5/javascript_5/f2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_5_2/test.jsonl
  codebleu = 30.91 
  Total = 500 
  Exact Fixed = 1 
[434]
  Syntax Fixed = 1 
[109]
  Cleaned Fixed = 2 
[163, 456]
  ********************
  Total = 500 
  Exact Fixed = 1 
[434]
  Syntax Fixed = 1 
[109]
  Cleaned Fixed = 2 
[163, 456]
  codebleu = 30.91 
[0.4771778024826042, 0.20697538878103366, 0.30652477859476746, 0.16008107721714762, 0.0004934439758772523, 0.04619278750949188, 0.2688634419713267, 0.5184663923404347, 0.5303543300556994, 0.23841741623626564, 0.3048854584238466, 0.47122437807166834, 0.23823643376179188, 0.04487037789654178, 0.31702929645110545, 0.29819443354900677, 0.20385618023880805, 0.22113672857645916, 0.0, 0.2792460911230246, 0.2598788763961276, 0.0004019031272141792, 0.3174085084172368, 0.27697938979461884, 0.41712511755845916, 0.5974397427090397, 0.1362189419254087, 0.396349572956634, 0.0, 0.3637731464862741, 0.2985001073608541, 0.00692920503002769, 0.10475106880255679, 0.30823981225993086, 0.5955975001342526, 0.3082747617951199, 0.5974483778767543, 0.3057242540571371, 0.3340047425620204, 0.39492113149627195, 0.5712947220128751, 0.6260377305921012, 0.26451605429732816, 0.32081083228311624, 0.6576785356046926, 0.3975507380413851, 0.9068382385286933, 0.3092101804236478, 0.2987684908491319, 0.0035135376255636553, 0.2505247414983807, 0.2885776668336304, 0.20803554143182829, 0.17518465037669909, 0.15440338985315158, 0.3985239852398524, 0.14845360824742268, 0.30078498163232603, 0.3002840366788788, 0.4639495093129433, 0.16926857194500194, 0.20080403541629063, 0.2255486965121209, 0.18153877287404874, 0.16284104078001574, 0.24702596328288945, 0.5997940926982217, 0.3306768355922871, 0.00014029954284333122, 0.8656047064362231, 0.24291832346919023, 0.5552657079890537, 0.5858043294263362, 0.19999999999999998, 0.47940134229312364, 0.15942028985507248, 0.3183160553832456, 0.377251685355458, 0.15222268786707382, 0.216445081922306, 0.40300530652200584, 0.5601105358504508, 0.17648147134115005, 0.33676595545103494, 0.043422220157354044, 0.2680471167162233, 0.5021465005608821, 0.11019436185060572, 0.4652882527972455, 0.3706847188488578, 0.05945054945054945, 0.17178864247888095, 0.6791146311515831, 0.3841905923603004, 0.5965373900797808, 0.49849892797089584, 0.6980472624402649, 0.4458633146181764, 0.35364130131482985, 0.3111069937307952, 0.2920558245096652, 0.7819827494849558, 0.3027018281893235, 0.4056324670646792, 0.26106089424811746, 0.4727842874251903, 0.5349414664820301, 0.20192389276716746, 0.9270606961251804, 0.21894453414150325, 0.5023615236140815, 0.308947437538266, 0.2551181102362205, 0.39896103896103896, 0.8433864730341443, 0.324491522329245, 0.14820888023299356, 0.36447045923974036, 0.2530544887057472, 0.19762515218474974, 0.6471683402839902, 0.2843904298071134, 0.20033897120699162, 0.006165589590102966, 0.1655265754885696, 0.3151255401185577, 0.19830056153517203, 0.259800404987126, 0.40040796709778675, 0.293517512788959, 0.07398545043576837, 0.4608711707105444, 0.5643931042805187, 0.42107493389138084, 0.19868852459016392, 0.3, 0.12250642292957327, 0.27958726519058685, 0.3168970229738709, 0.40879607792222944, 0.18710611984876885, 0.2916805188983068, 0.16688101446517814, 0.1468302757070934, 0.039153099343976844, 0.26316807474076404, 0.19803437055129958, 0.503191349857689, 0.2404247382142077, 0.32476443266952143, 0.036397041774275427, 0.49748647789767486, 0.06321813073503603, 0.4409678446410348, 0.6033759728689037, 0.20959632222018837, 0.31585537598306757, 0.13051876966841675, 0.0005518978962284056, 0.09341855218006818, 0.2569559885900208, 0.3091474411279012, 0.8662528514160328, 0.10947133983396526, 0.2295359574097633, 0.324957439257179, 0.36955946780348625, 0.2072022430897242, 0.07501048117154568, 0.42976656372499167, 0.26051886245192774, 0.35616317411222653, 0.24104416545954144, 0.33358937803092387, 0.25960138194547633, 0.7318997749990142, 0.1729386096602994, 0.13949438172674558, 0.45508310949075587, 0.3652869957028732, 0.2501431546905579, 0.0002480689730419594, 0.236263286598525, 0.2717716609374966, 0.20497028852258245, 0.31535601841036437, 0.4019599237903219, 0.30213011750180885, 0.5967982498663289, 0.25257148036699323, 0.052999059986409094, 0.3122644814108053, 0.23266356539873778, 0.44517081803587816, 0.15110456844353817, 0.28979525077418367, 0.7504004004704901, 0.278497921456518, 0.20036454398233594, 0.24406179934592148, 0.3130115231682857, 0.3831723356041721, 0.2512490321741657, 0.21220292783898265, 0.3142240277900291, 0.7803034446230322, 0.3982006353640182, 0.15894059762028215, 0.48015525477821586, 0.3731888205553385, 0.8824709043482357, 0.853251435829042, 0.06222748143347623, 0.19707845671549284, 0.5973080883457038, 0.3944886609406727, 0.20598495228263797, 0.5977086764133333, 0.31044338867822946, 0.11894747013989151, 0.13473123602004036, 0.3405947017714371, 0.06057831219177891, 0.45068544015424605, 0.3092681112369559, 0.07647229880397044, 0.6001735159553974, 0.1636149312958769, 0.40711571496601096, 0.3009864121173136, 0.7184213680934424, 0.33293070226901617, 0.20246944319223514, 0.3385933519550263, 0.2984219553073336, 0.09348414661693424, 0.3098742860447756, 0.14856247653918703, 0.2590025021969955, 0.26707511545802043, 0.17809424010514693, 0.21274792894912425, 0.20877707681772878, 0.2896504412206137, 0.3884793392274075, 0.5833975252239264, 0.09327792302663393, 0.16600219430288085, 0.6496441836400741, 0.28556940475960796, 0.746924700641056, 0.19329414510767018, 0.08988768399388929, 0.1604275157606544, 0.30492088474539664, 0.2401304630561214, 0.0, 0.5973900501811698, 0.42691070620000726, 0.05118661588276318, 0.19925402158022876, 0.2983107329408001, 0.16207704952636096, 0.3094384753660158, 0.871149014981726, 0.7127774792622994, 0.3786418760239946, 0.3094838374186901, 0.3570853335266103, 0.0010190055525727858, 0.2580725419163422, 0.17985897733379794, 0.2414661462062447, 0.21278246174257043, 0.2538672487499068, 0.004724409448818898, 0.6012781844913317, 0.19840368175458767, 0.15396617193015968, 0.2932907317357314, 0.0007514041975054871, 0.3218581180158807, 0.11515151515151514, 0.4505658052470366, 0.001307759009422, 0.31013902146204086, 0.24164357233327183, 0.4199386077723073, 0.15959241975142183, 0.31550596130531494, 0.11134020618556702, 0.028007507048196104, 0.20472598942351297, 0.32138542359237243, 0.0, 0.29862737195037004, 0.3, 0.5995748648104571, 0.15416687581582247, 0.30218632643626797, 0.30559649717340487, 0.36082829487437973, 0.5374087292326695, 0.3288137353224801, 0.0, 0.7678525946438988, 0.6802578735020394, 0.4022005483418911, 0.32867568547715664, 0.38696364929470595, 0.4248659595689752, 0.09753427589258061, 0.39795995890341895, 0.22876023329393483, 0.0, 0.23590404436681747, 0.31478098417109396, 0.3012673398550546, 0.04310271587410342, 0.2270022549732466, 0.41897123001359804, 0.5988258691034984, 0.2794117647058823, 0.33185498349520004, 0.18471434701767236, 0.7538024285875055, 0.23554642367309295, 0.36023761442056734, 0.790681636742723, 0.2769846459024924, 0.28044505898228356, 0.006, 0.44055982738507116, 0.44768375827824286, 0.1368386947680252, 0.19404527641964925, 0.20542646798574893, 0.2631319747913516, 0.13567100288207218, 0.14896098673832114, 0.1859388219645906, 0.11035495549992339, 0.31082339996230457, 0.9225934589608529, 0.4475033938094736, 0.27738166413010207, 0.4060040488638472, 0.13850177303199818, 0.5566478602423253, 0.8176593989567298, 0.16206722388142897, 0.7309443592247309, 0.2925527475673238, 0.597519452891939, 0.0, 0.4769725409613548, 0.05486152408713606, 0.20866950642830145, 0.44609398100435566, 0.31966915752998515, 0.013204170376746613, 0.3117418028300822, 0.06258329962630418, 0.5223015946997971, 0.30994371484899497, 0.5910526331567303, 0.29867841409691626, 0.273964046766856, 0.026962189872870586, 0.22188761942002208, 0.11085093632332138, 0.15053707467227775, 0.17372979695135576, 0.008076686212172047, 0.20708690359029758, 0.08069897885804689, 0.6096534277348536, 0.47052180077640926, 0.037411251985788256, 0.2682602489801395, 0.18021402841295575, 0.2222444404194521, 0.0005388425630151585, 0.14637411241067838, 0.17276255563556425, 0.056506564781066325, 0.5441764742916928, 0.2561358299411751, 0.3032196220884641, 0.21684639194797048, 0.2006650246059476, 0.5982918264865769, 0.19186454089242821, 0.30217027683977926, 0.30086280876890936, 0.5913567014511316, 0.13973893580783803, 0.3971872172581696, 0.30490675276796403, 0.5989661522564682, 0.00035634915161000233, 0.2087918430297149, 0.12834106144266935, 0.11289998365615328, 0.11526717357506512, 0.2990253790127648, 0.12454411908378282, 0.30450956098665194, 0.42396048369867045, 0.29828746254349864, 0.0196602986307288, 0.42866391297799367, 0.0002347627506537334, 0.3084532680162354, 0.24466286107817448, 0.44090640231253164, 0.30280565353753275, 0.24589755061325091, 0.5921002985847819, 0.9437985500759485, 0.08943514341254502, 0.0006347291962821538, 0.006303275389754221, 0.22874821726540914, 0.6013453641951644, 0.24137496309074893, 0.20665662211317046, 0.23248330682908164, 0.4803335831386436, 0.11496607762457872, 0.2132667771348017, 0.15346719074799528, 0.43246862225748745, 0.9891483218006352, 0.5967448891703986, 0.838479936444588, 0.3054746484981245, 0.18570909651170167, 0.8348591893331347, 0.2174842089305361, 0.24138431576170952, 0.20929047064039416, 0.06661215914229623, 0.17027835007110195, 0.21577509138148893, 0.5977913424415755, 0.009046406611840153, 0.276308287201011, 0.26511313229965383, 0.11813117441708759, 0.23026169263263785, 0.6030960677776888, 0.2281123614225309, 0.280275470473721, 0.003015075376884422, 0.8954450935705085, 0.4100179901618679, 0.41955183060916035, 0.527543250577078, 0.22270101327295627, 0.03478736263068646, 0.23404053995640697, 0.4730477743796533, 0.12510238418124375, 0.597518863278535, 0.29909577551470723, 0.11484145722129302, 0.12610404767576638, 0.35677824031348704, 0.17379769773248088, 0.3994485031285021, 0.18296893351902704, 0.16734522043152897, 0.45022167535332314, 0.18590397988008078, 0.08160859311251459, 0.30487249483148654, 0.22454538800499335, 0.1946081829887542, 0.35073686782719293, 0.19999999999999998, 0.2754797784565356, 0.5981498631955867, 0.3122212368763607, 0.18125339569187757, 0.3867475425030824, 0.2362936605600503, 0.00279542200548812, 0.3615313711563134, 0.4386361328970143, 0.15332420538110214, 0.47874942008500154, 0.02029042662351188, 0.5744100321686132, 0.39447887985298, 0.24185858486350784, 0.5982043428994597, 0.46222186754673855, 0.6777557818070205, 0.49295227033953065]
Finish training and take 1h36m
