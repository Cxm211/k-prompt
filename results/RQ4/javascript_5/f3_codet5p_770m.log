Namespace(log_name='./RQ5/javascript_5/f3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_5/f3_codet5p_770m', data_dir='./data/RQ5/javascript_5_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0157
  global_step = 2
  train_loss = 1.9095
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0157
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 12.04 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00967
  global_step = 3
  train_loss = 2.2124
  ********************
Previous best ppl:1.0157
Achieve Best ppl:1.00967
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 15.07 	 Previous best codebleu 12.04
  ********************
 Achieve Best bleu:15.07
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0081
  global_step = 4
  train_loss = 2.5992
  ********************
Previous best ppl:1.00967
Achieve Best ppl:1.0081
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 25.74 	 Previous best codebleu 15.07
  ********************
 Achieve Best bleu:25.74
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00761
  global_step = 5
  train_loss = 0.6836
  ********************
Previous best ppl:1.0081
Achieve Best ppl:1.00761
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 28.25 	 Previous best codebleu 25.74
  ********************
 Achieve Best bleu:28.25
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00721
  global_step = 6
  train_loss = 0.7982
  ********************
Previous best ppl:1.00761
Achieve Best ppl:1.00721
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 28.52 	 Previous best codebleu 28.25
  ********************
 Achieve Best bleu:28.52
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00694
  global_step = 7
  train_loss = 0.3621
  ********************
Previous best ppl:1.00721
Achieve Best ppl:1.00694
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 31.24 	 Previous best codebleu 28.52
  ********************
 Achieve Best bleu:31.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00667
  global_step = 8
  train_loss = 0.3256
  ********************
Previous best ppl:1.00694
Achieve Best ppl:1.00667
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 33.06 	 Previous best codebleu 31.24
  ********************
 Achieve Best bleu:33.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00652
  global_step = 9
  train_loss = 0.3113
  ********************
Previous best ppl:1.00667
Achieve Best ppl:1.00652
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 35.3 	 Previous best codebleu 33.06
  ********************
 Achieve Best bleu:35.3
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00642
  global_step = 10
  train_loss = 0.1575
  ********************
Previous best ppl:1.00652
Achieve Best ppl:1.00642
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 35.89 	 Previous best codebleu 35.3
  ********************
 Achieve Best bleu:35.89
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00638
  global_step = 11
  train_loss = 0.1544
  ********************
Previous best ppl:1.00642
Achieve Best ppl:1.00638
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 36.96 	 Previous best codebleu 35.89
  ********************
 Achieve Best bleu:36.96
  ********************
reload model from RQ5/javascript_5/f3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_5_3/test.jsonl
  codebleu = 33.27 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[35, 163]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[35, 163]
  codebleu = 33.27 
[0.5036976697528142, 0.2963832176595419, 0.27181569759925406, 0.18677183855630736, 0.0008203569738912913, 0.04985890361001963, 0.31530803180504496, 0.4904014727971656, 0.5303543300556994, 0.591516286342532, 0.20047920978773853, 0.4795980509202729, 0.25255157651202054, 0.434126496248902, 0.31536140969114906, 0.6452148813321528, 0.24205933666093812, 0.28878041289055917, 0.0, 0.26338217149240173, 0.6643842626707708, 0.0007062554630404529, 0.8708250323025346, 0.27697938979461884, 0.42993371798728885, 0.5974397427090397, 0.396689508773651, 0.408694754573109, 0.0, 0.3262977027743438, 0.28588071427786643, 0.3356972262297304, 0.07318125726610578, 0.09395622033056893, 0.6126392797557267, 0.3082747617951199, 0.5974483778767543, 0.4252562381374042, 0.3340047425620204, 0.5962645893235128, 0.5712947220128751, 0.5255831472572245, 0.11982951103654793, 0.822526074431996, 0.03057041701051613, 0.3975507380413851, 0.7314771093546691, 0.6423991042001886, 0.21445862147356637, 0.0069393642438447325, 0.3093872707802237, 0.7358863528261614, 0.03901893506565684, 0.38058318513755585, 0.3533755248262469, 0.42, 0.1489655172413793, 0.297587385460517, 0.22814730589024507, 0.5940706777749343, 0.13689984571749664, 0.15592325048097017, 0.10979159231222624, 0.18153877287404874, 0.4220616356144443, 0.24702596328288945, 0.5997940926982217, 0.3350954520148929, 0.00013974825696549807, 0.8972124779923019, 0.8301377689380496, 0.5652195562887228, 0.48839502083117775, 0.19999999999999998, 0.5925669042178512, 0.1485981308411215, 0.31561677940065624, 0.44694573249540037, 0.048941295226070866, 0.2598788763961276, 0.5198150749155045, 0.196183197925473, 0.3025916318015571, 0.4371471773375026, 0.12331564596487743, 0.2680471167162233, 0.4281259356919006, 0.10775444557031037, 0.4652882527972455, 0.444744599226491, 0.06, 0.7054819616225767, 0.5259851851878158, 0.39605565346252447, 0.5886491809897834, 0.6772308404485861, 0.21916867799839113, 0.5960230253923471, 0.35364130131482985, 0.3111069937307952, 0.3001756352113333, 0.10410170143203386, 0.3027018281893235, 0.7550038426404158, 0.2083752687427499, 0.5550573374070904, 0.42119622547442515, 0.20310577684765452, 0.3095230031504157, 0.2694375177034555, 0.3034218584269058, 0.30997832153072835, 0.25749999999999995, 0.5942382114856641, 0.6111593097845102, 0.5647825125422088, 0.44562446590318683, 0.3098819790424698, 0.7244412894832517, 0.1963436734310725, 0.39141007113640175, 0.31206083645536303, 0.19961155448600582, 0.20083015390585474, 0.1631883998146876, 0.3178362052799467, 0.19830056153517203, 0.259800404987126, 0.13714129756083454, 0.6309545231356429, 0.14224981392389122, 0.45728811503051014, 0.754704902378399, 0.41395985591738116, 0.3079510025390523, 0.0, 0.12250642292957327, 0.5229126769874679, 0.3154467212900657, 0.4079047882186483, 0.25747877177823175, 0.2022980573715535, 0.14375394361011168, 0.1504521628105704, 0.024966181492070413, 0.26316807474076404, 0.1483750322706796, 0.3922183578105481, 0.2282185514691919, 0.020806680115941265, 0.6429326815734496, 0.6507190856640116, 0.24478323105711808, 0.526797617187952, 0.5054129525542546, 0.40943044298353226, 0.17326315125063624, 0.13051876966841675, 0.12408049003549648, 0.09369945852975234, 0.19689924056263813, 0.2567468320892382, 0.8662528514160328, 0.11128827084589896, 0.23543371140530736, 0.24283000482798167, 0.36962660970678474, 0.1711052005003255, 0.07501048117154568, 0.8020311540826717, 0.7071565338034236, 0.37619722674951217, 0.24104416545954144, 0.20199669296593317, 0.5302136214930269, 0.5293023724016116, 0.1729386096602994, 0.5620541673204225, 0.30670957394059634, 0.25560636255574853, 0.2501431546905579, 0.00012589687340237512, 0.26804304158317016, 0.3986029789539759, 0.22203219531473256, 0.6087150812881095, 0.4019599237903219, 0.30505839368447957, 0.5957975076263056, 0.25755291473829967, 0.4885926413190037, 0.6668764807545298, 0.31266595097230326, 0.44517081803587816, 0.15894059762028215, 0.2865465064216934, 0.3461524229887739, 0.3569795678062106, 0.2340415292212659, 0.24406179934592148, 0.287845907098695, 0.5454125193552363, 0.16725239060200897, 0.0029716973539856703, 0.3142240277900291, 0.15276392449573864, 0.159623097024921, 0.6887796716807859, 0.48063070593260065, 0.4870682522333919, 0.5625477657920791, 0.853251435829042, 0.06333408341592456, 0.22098412709324805, 0.3001402995428433, 0.34066725256963193, 0.2035648146566414, 0.5977086764133333, 0.31390074651501687, 0.18871531270419303, 0.2517571862485346, 0.5119808925419361, 0.20795485677154812, 0.05757571746296134, 0.2597338085963471, 0.1525623237259383, 0.19926305350941798, 0.22655701324700084, 0.40317510959765, 0.39257011400453834, 0.05409475827214062, 0.0994565099048211, 0.13598242085029544, 0.3385933519550263, 0.3229801869588078, 0.400020819191936, 0.3098742860447756, 0.14706409894241673, 0.2542574141857458, 0.2656298774775104, 0.1753301341893729, 0.2132934394937433, 0.23120373106826186, 0.2959745369673469, 0.37802445481042224, 0.30872233124987236, 0.4208397894246906, 0.42538091223228613, 0.24720033732584, 0.28556940475960796, 0.746924700641056, 0.2502680141513933, 0.20866354767478065, 0.11714395384525675, 0.30492088474539664, 0.14680851063829786, 0.4118819918812901, 0.003399613210516978, 0.39658067051930984, 0.05118661588276318, 0.23669321919655517, 0.25473922636836926, 0.7511200104643803, 0.02465040921962507, 0.8570807887257177, 0.7127774792622994, 0.42927687247119717, 0.2622132000702716, 0.5347428106038496, 0.2015372100944879, 0.6643842626707708, 0.18140830465683985, 0.2414661462062447, 0.2110765079237599, 0.2538672487499068, 0.11592765706326556, 0.6012781844913317, 0.1601974224782819, 0.14421038448099657, 0.10607499275177751, 0.0009616851121187304, 0.7233656687527403, 0.3748917273757959, 0.3428571428571428, 0.001307759009422, 0.3138828510496251, 0.772149973326848, 0.43931371540666964, 0.7965063467921383, 0.6340753780940847, 0.1111111111111111, 0.45032732738616654, 0.15756984558991255, 0.28581484134381474, 0.0, 0.29862737195037004, 0.21299714723889376, 0.6028301304189128, 0.15505458194769425, 0.30218632643626797, 0.2672947856921336, 0.36082829487437973, 0.15537359900203998, 0.3305989610406744, 0.0, 0.6585173378598056, 0.5979604899723578, 0.3934165940607258, 0.2899202167464079, 0.38683567216420955, 0.25149657107997125, 0.3021698992526036, 0.5991455099604635, 0.22876023329393483, 0.0, 0.4969617815012778, 0.22288904386671957, 0.3012673398550546, 0.04310271587410342, 0.2235294117647059, 0.35370466016800844, 0.3427686800892532, 0.27981580691052804, 0.6201953009108648, 0.18852050605724455, 0.34250024542886154, 0.6829859387503355, 0.33985271067894746, 0.594850045020346, 0.582694686729035, 0.35597589829431087, 0.006, 0.4741201013290658, 0.43963300340781625, 0.09912046594255859, 0.19404527641964925, 0.20520117023371384, 0.2621444206198803, 0.17782274876295467, 0.051835069023604255, 0.37791806659375793, 0.3266648180739089, 0.29433224646258316, 0.0889235333304541, 0.4464397245478966, 0.27738166413010207, 0.4674560214224379, 0.13850177303199818, 0.5566478602423253, 0.5754737332784626, 0.16219766264407745, 0.7309443592247309, 0.29596164483796544, 0.5973096088583119, 0.0, 0.49488213533881786, 0.23538311929256434, 0.5085697288931634, 0.5946143087645109, 0.29786495284947845, 0.009533674028472949, 0.49749197696421477, 0.13566522181965215, 0.5223015946997971, 0.32455881064158265, 0.12, 0.733444202560376, 0.4057903543461506, 0.023098322720512134, 0.36587342440580645, 0.07357140028863911, 0.2627166511815171, 0.17568945345577994, 0.13157378345307527, 0.6723104602956884, 0.08069897885804689, 0.5641993557027227, 0.3461524229887739, 0.006555054873123389, 0.15539188438021123, 0.25538756825509223, 0.8288120051119818, 0.0005602581331849579, 0.16305283114107186, 0.12210588601150443, 0.36615700797123896, 0.12059669184895662, 0.3508081453961951, 0.15969311536770342, 0.21640165137940148, 0.05486152408713606, 0.6922382617563554, 0.21099518341001774, 0.2997732308822231, 0.13999221398070835, 0.5913567014511316, 0.09221286873719346, 0.5783036814100584, 0.3020783493323439, 0.5989661522564682, 0.00033716673229390007, 0.2109899463928066, 0.1350041927345863, 0.07407624624715664, 0.3072120713272165, 0.36008080852177077, 0.3303333004926988, 0.4010131344485118, 0.6495694833158199, 0.28637302645793417, 0.3445638082330292, 0.43149193889080617, 0.00021107638661548652, 0.3090377178222287, 0.24466286107817448, 0.44090640231253164, 0.30280565353753275, 0.24589755061325091, 0.00031232847998472943, 0.6504829579063651, 0.12008887975099572, 0.0006347291962821538, 0.006303275389754221, 0.22874821726540914, 0.598878269816747, 0.817280742568131, 0.5966064455014148, 0.23248330682908164, 0.1170352409794004, 0.10429578566540137, 0.2132667771348017, 0.15374542675436959, 0.43246862225748745, 0.14591449496046338, 0.5967448891703986, 0.8174643314272358, 0.5686979944673102, 0.18600622125162655, 0.8245239072284845, 0.3835147945271466, 0.2415178720391371, 0.3606412732540455, 0.002447810953246603, 0.3482657073074272, 0.21577509138148893, 0.5993065892553859, 0.29589090487961767, 0.30717664131047706, 0.27548542747757904, 0.11674655903247219, 0.11650716870656998, 0.6030960677776888, 0.2281123614225309, 0.280275470473721, 0.0988235294117647, 0.47218892865856577, 0.37038841934193256, 0.5115586470381347, 0.527543250577078, 0.0047619047619047615, 0.03216220092131145, 0.5087142393682708, 0.4730477743796533, 0.0671241056121733, 0.5939984393636099, 0.13608452604593024, 0.2478737274849781, 0.09765867952984053, 0.9495565333799167, 0.4557202512183804, 0.46838951536746976, 0.30692234727404644, 0.3042771332360301, 0.6841007521231434, 0.18599762278043067, 0.04907072380569967, 0.7253305249945152, 0.22454538800499335, 0.1946081829887542, 0.42047838455738384, 0.19999999999999998, 0.39675296426202156, 0.5946238556938694, 0.19323584899655788, 0.3173189678539269, 0.22741249270362363, 0.24207608843587564, 0.002710112227734308, 0.621481368079633, 0.4386361328970143, 0.34177650249365865, 0.5984626626265804, 0.09740228673484569, 0.5994100321686132, 0.36363636363636365, 0.25337680948785496, 0.5982043428994597, 0.4353773884500126, 0.23561736080017928, 0.5494111149149632]
Finish training and take 1h34m
