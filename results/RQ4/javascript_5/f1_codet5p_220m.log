Namespace(log_name='./RQ5/javascript_5/f1_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/javascript_5/f1_codet5p_220m', data_dir='./data/RQ5/javascript_5_1', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01407
  global_step = 2
  train_loss = 1.6682
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01407
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 9.21 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.21
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.01115
  global_step = 3
  train_loss = 1.4893
  ********************
Previous best ppl:1.01407
Achieve Best ppl:1.01115
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 11.24 	 Previous best codebleu 9.21
  ********************
 Achieve Best bleu:11.24
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00971
  global_step = 4
  train_loss = 0.877
  ********************
Previous best ppl:1.01115
Achieve Best ppl:1.00971
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 13.96 	 Previous best codebleu 11.24
  ********************
 Achieve Best bleu:13.96
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00865
  global_step = 5
  train_loss = 0.7497
  ********************
Previous best ppl:1.00971
Achieve Best ppl:1.00865
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 23.23 	 Previous best codebleu 13.96
  ********************
 Achieve Best bleu:23.23
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00792
  global_step = 6
  train_loss = 0.6679
  ********************
Previous best ppl:1.00865
Achieve Best ppl:1.00792
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 27.59 	 Previous best codebleu 23.23
  ********************
 Achieve Best bleu:27.59
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00749
  global_step = 7
  train_loss = 0.2309
  ********************
Previous best ppl:1.00792
Achieve Best ppl:1.00749
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 31.43 	 Previous best codebleu 27.59
  ********************
 Achieve Best bleu:31.43
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00728
  global_step = 8
  train_loss = 0.1964
  ********************
Previous best ppl:1.00749
Achieve Best ppl:1.00728
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 33.35 	 Previous best codebleu 31.43
  ********************
 Achieve Best bleu:33.35
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00716
  global_step = 9
  train_loss = 0.3151
  ********************
Previous best ppl:1.00728
Achieve Best ppl:1.00716
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 34.17 	 Previous best codebleu 33.35
  ********************
 Achieve Best bleu:34.17
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00707
  global_step = 10
  train_loss = 0.2918
  ********************
Previous best ppl:1.00716
Achieve Best ppl:1.00707
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 35.45 	 Previous best codebleu 34.17
  ********************
 Achieve Best bleu:35.45
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00699
  global_step = 11
  train_loss = 0.3229
  ********************
Previous best ppl:1.00707
Achieve Best ppl:1.00699
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 36.27 	 Previous best codebleu 35.45
  ********************
 Achieve Best bleu:36.27
  ********************
reload model from RQ5/javascript_5/f1_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_5_1/test.jsonl
  codebleu = 35.6 
  Total = 500 
  Exact Fixed = 1 
[78]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[313, 456]
  ********************
  Total = 500 
  Exact Fixed = 1 
[78]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 2 
[313, 456]
  codebleu = 35.6 
[0.4794611458398899, 0.009298214657186929, 0.305136237275132, 0.15444905856436789, 0.0004205947612620969, 0.05219227632468734, 0.3136975386163606, 0.6295229742923897, 0.010625375810426874, 0.2599310513307021, 0.3085489685645591, 0.6021366771367045, 0.20014685233716575, 0.05143016151610315, 0.6682113150891928, 0.30074314229152943, 0.5606891562047243, 0.31259584417388864, 0.0, 0.6448598858735686, 0.24799563997164803, 0.03494569883285414, 0.8708250323025346, 0.5135214793849547, 0.4251495775146953, 0.5974397427090397, 0.8597766350872547, 0.29078549350926586, 0.15481193828895973, 0.3608975423152889, 0.323392864066914, 0.12993635327809974, 0.24563004824662918, 0.21773169487187008, 0.29900557273452033, 0.30833377832677383, 0.5321968913232872, 0.3057242540571371, 0.2440609753906453, 0.48170126582153244, 0.574751057598859, 0.6435394423555667, 0.2677245244512411, 0.3287284241724935, 0.21302483001273287, 0.3975507380413851, 0.8006588688984524, 0.3092101804236478, 0.21445862147356637, 0.3975400865330244, 0.005676914911516197, 0.805453099521674, 0.15289429295781964, 0.21928097781023353, 0.15440338985315158, 0.44650852962940435, 0.225226036889931, 0.1680569823835158, 0.22797955424959282, 0.6399591004941562, 0.3084249213670295, 0.32719217235516845, 0.057455659394197806, 0.2208933139890695, 0.11002012737527385, 0.24702596328288945, 0.7340445690739947, 0.5156923789101245, 0.057884054151636256, 0.8993979996497343, 0.24291832346919023, 0.13008090488192578, 0.20799157581355127, 0.013987007382595742, 0.5943737257459935, 0.16231884057971013, 0.2869355081583726, 1.0, 0.15035209065338123, 0.24871922389524215, 0.3893673111318965, 0.3452101105385637, 0.5637946891611503, 0.4371471773375026, 0.04220014363064914, 0.2680471167162233, 0.5021465005608821, 0.2697911285533543, 0.5775321871604537, 0.30552572693070534, 0.075, 0.19896916005807258, 0.6565560097887984, 0.3964165993265989, 0.5997503951652969, 0.644906087772486, 0.557021457008556, 0.44053605539595353, 0.9321212086381336, 0.1259118118905144, 0.295002512018098, 0.6659407704744612, 0.30717182027110695, 0.3997878061329664, 0.3173715252969226, 0.3329032960914177, 0.5357373241840644, 0.2110741448398889, 0.20226955852746514, 0.5570994939591317, 0.5532136440138307, 0.38898420353640983, 0.35535555084143716, 0.714297617187952, 0.902887748772268, 0.3215206567347523, 0.5418035126585234, 0.9441518237028776, 0.7412391136038223, 0.15247524752475247, 0.7522712571363831, 0.823437316619593, 0.5955735769790373, 0.07452253438670298, 0.207013425887477, 0.3178362052799467, 0.5925864267313306, 0.30191405058699194, 0.4985233980812554, 0.15480865046304795, 0.0870497970463823, 0.6782397482894665, 0.7489906166641133, 0.28734245963239335, 0.30026028216399925, 0.003405818645522839, 0.1192429022082019, 0.3971336580179825, 0.3145563670953907, 0.38270985353609027, 0.2906306682839116, 0.2917748674286213, 0.43044678511027046, 0.1477177517091981, 0.6006163524868149, 0.6086737860987872, 0.0842327951188234, 0.49782171948805876, 0.23919009599573143, 0.8601305921152989, 0.4852562381374042, 0.6063094955680546, 0.2933192603414, 0.526797617187952, 0.5107262304680447, 0.2474035672958985, 0.20884899494854595, 0.1280810096175738, 0.023361679522309112, 0.23049138297301558, 0.2569559885900208, 0.2808881694430021, 0.773553137645282, 0.13456995935796623, 0.49591243134615914, 0.35057261456336397, 0.3836000378970501, 0.2025089921899309, 0.26751144099671953, 0.4997174960533458, 0.26051886245192774, 0.7974789818939843, 0.23967565333573487, 0.3340907082320268, 0.5148290061084115, 0.20562654090716237, 0.01624119948505003, 0.6093189247482212, 0.3026434732775486, 0.3475111888262376, 0.6051294155160957, 0.593537617923422, 0.26310281082914694, 0.5948993749385323, 0.13590641399896894, 0.6662517128137839, 0.0056026367047085825, 0.3057703556125202, 0.5983282125024841, 0.28287117199448547, 0.2258324500699896, 0.48171236365089043, 0.49776724741571055, 0.5549648388994048, 0.15110456844353817, 0.48981162172047493, 0.3947052106568081, 0.2865183536902731, 0.6181259085146973, 0.24406179934592148, 0.31373565481824933, 0.5854854396274523, 0.20679330491998746, 0.0046875, 0.3142240277900291, 0.5764224339302585, 0.27885437132179525, 0.15894059762028215, 0.4882152745388425, 0.6407534718346111, 0.8824709043482357, 0.8272698175200561, 0.0988256528634406, 0.22512592155451094, 0.5864489941769148, 0.4964243052740067, 0.20804073118912123, 0.5977086764133333, 0.3120160180567175, 0.0277552441080283, 0.3106426691349223, 0.58189415539785, 0.03873069797505218, 0.15312699369618352, 0.4143908160692684, 0.16035503999586073, 0.4167436913435514, 0.22214962384922635, 0.0010584329634535155, 0.3531596703017995, 0.6893858380618342, 0.06847327567880372, 0.15423473555352746, 0.5860933519550263, 0.32313480304562464, 0.3016243940954435, 0.20644700372505778, 0.14912379633641656, 0.21089040634750447, 0.2496161082711673, 0.2978195929479774, 0.21900987118787868, 0.3373940575497127, 0.11959660372538167, 0.4453710621987834, 0.3990409395450905, 0.11106902409132068, 0.2710255118821294, 0.6496441836400741, 0.8576593989567298, 0.48462747286988594, 0.04993436505953147, 0.20866354767478065, 0.16311901060340334, 0.3040355994593641, 0.2309672976714844, 0.1007166035294968, 0.450960626750116, 0.571503226329974, 0.06329843690266475, 0.2168610261601996, 0.3035374278127754, 0.7511200104643803, 0.10366058936200297, 0.7965610505171177, 0.09331811886329058, 0.5494964633657253, 0.7436525055333931, 0.39862356365630747, 0.16987060532413129, 0.2475124661233484, 0.6883998245403422, 0.26515111928404567, 0.2106653174497809, 0.776554029901346, 0.08654571665953868, 0.6012781844913317, 0.0007074509681392665, 0.5115922915218526, 0.2971164034812831, 0.00020987462944423112, 0.32164650058243033, 0.3474539140777726, 0.4691824393029279, 0.10993069851559968, 0.3138828510496251, 0.730421879000839, 0.4199386077723073, 0.81691194241949, 0.6665223130149849, 0.1193548387096774, 0.8099752229822192, 0.1827478965227037, 0.3128031169847436, 0.0, 0.7451974543165035, 0.19367892276471355, 0.7799696154227835, 0.18083738096433402, 0.30092258304132913, 0.18000008357006042, 0.30144690033782284, 0.15314305943995044, 0.33138991605836066, 0.0, 0.51853012141536, 0.5205473673371325, 0.10211979517695599, 0.1132463569423689, 0.26360972349381195, 0.3402787740882055, 0.0974149823372681, 0.7035117129535005, 0.22876023329393483, 0.7675606282133942, 0.48522836716852713, 0.8394561238018083, 0.3892032964739018, 0.05204201760677742, 0.32641839065175154, 0.4923690628379317, 0.3443927889702286, 0.28018741781251555, 0.3351294305696454, 0.17418213456833653, 0.7538024285875055, 0.28232981243717004, 0.23873868841297616, 0.7911784484274332, 0.2798816378606491, 0.30001124528249634, 0.0, 0.5261072559244404, 0.44442960642641055, 0.11725866928865278, 0.0667568206249708, 0.20520117023371384, 0.6578602200025533, 0.1567930582775484, 0.049577179112187564, 0.1859388219645906, 0.4005326816439736, 0.2080713843282312, 0.33352474892355904, 0.3524349284446232, 0.18298862193277285, 0.4674560214224379, 0.13850177303199818, 0.5662632448577098, 0.3198551098384905, 0.16468794363679373, 0.4027247469649702, 0.31059285451919544, 0.33405226268281085, 0.0, 0.49675442954160787, 0.2170513231539205, 0.210695159390778, 0.20308492381128002, 0.32495010701459204, 0.009107835630900679, 0.557719307720078, 0.37129994237561437, 0.5223015946997971, 0.30885602680311086, 0.5256409797780162, 0.3936637515935473, 0.4191091786222184, 0.14169000933706816, 0.36587342440580645, 0.34393366415687754, 0.15053707467227775, 0.3781220541110804, 0.32340833096900673, 0.7338986486772099, 0.08069897885804689, 0.7203766888063858, 0.3947052106568081, 0.035261305682135036, 0.1095631943626214, 0.24280015566767965, 0.25046326251351203, 0.0008648200159042903, 0.447298452667235, 0.1469214820328039, 0.5578607431385983, 0.20241329025858001, 0.2993712481448778, 0.3071275574849759, 0.17449902804290457, 0.1680621012816312, 0.7210658705890922, 0.8818968871753021, 0.38025658809732577, 0.2810932690629824, 0.5913567014511316, 0.24272224849083718, 0.3960504144247727, 0.30559853415724664, 0.5979400744548591, 0.0002862185329319489, 0.2109899463928066, 0.28450549314172136, 0.07438113247031589, 0.08384150172922376, 0.4808766406797714, 0.10181796342611188, 0.5921652203541374, 0.8054605939164085, 0.36698037946396844, 0.6777218023625289, 0.11147467882907552, 0.00033435970423720273, 0.3147114919330006, 0.24466286107817448, 0.44090640231253164, 0.20000295729086454, 0.16934143639751256, 0.4747316977107542, 0.33390303261113674, 0.3889379432495441, 0.0006347291962821538, 0.00747180296298018, 0.4664355844276247, 0.6010791683405017, 0.7907150063286525, 0.20384670593306117, 0.24730730972205817, 0.5980820361140782, 0.48032629499826374, 0.2132667771348017, 0.09456384867919175, 0.4300880063066874, 0.5642782220842599, 0.5819097635391179, 0.8433609637235204, 0.5929944873343544, 0.18570909651170167, 0.8245239072284845, 0.03378698407319562, 0.22663406019873614, 0.3860321348339757, 0.0026393430484815167, 0.2971111532226468, 0.3155596115578131, 0.5977247598388893, 0.001127699257715623, 0.3095580951490542, 0.5616798194467534, 0.29435807698860783, 0.11913488026234784, 0.6030960677776888, 0.6509567982413205, 0.2658348464907273, 0.0975, 0.7422080689539031, 0.5045234828882075, 0.7420554014474346, 0.48542038065745813, 0.22270101327295627, 0.036946055818667695, 0.3551980406612544, 0.13730544399238281, 0.18000759970533026, 0.4062652820919249, 0.15809182209469153, 0.4616760548423344, 0.14775531583831672, 0.9495565333799167, 0.4557202512183804, 0.007564994930659855, 0.12315918571451283, 0.13053026545799848, 0.45297038023582215, 0.18590397988008078, 0.08160859311251459, 0.24114366713397253, 0.11296102516164266, 0.17082617154375757, 0.4696711239458643, 0.49851133123535774, 0.39675296426202156, 0.6017453012781864, 0.257140693033978, 0.3337353249864556, 0.4399117880830822, 0.16105074110143958, 0.05781082252945796, 0.28324089279767123, 0.25016079085323406, 0.37627573543456505, 0.29892679341372325, 0.13135896180712586, 0.5994100321686132, 0.6004122881654701, 0.15217640323357282, 0.7629326815734496, 0.6004397678612106, 0.3168196798860308, 0.5494111149149632]
Finish training and take 3h4m
