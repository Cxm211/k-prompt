Namespace(log_name='./RQ5/javascript_5/soft0_1_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/javascript_5/soft0_1_codet5p_220m', data_dir='./data/RQ5/javascript_5_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '};             debugger;             return $http({', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '};                         return $http({'}]
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 43.5357
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 12.36 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.36
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 41.5123
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 22.23 	 Previous best codebleu 12.36
  ********************
 Achieve Best bleu:22.23
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 4
  train_loss = 21.513
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 24.55 	 Previous best codebleu 22.23
  ********************
 Achieve Best bleu:24.55
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 5
  train_loss = 17.0292
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 25.88 	 Previous best codebleu 24.55
  ********************
 Achieve Best bleu:25.88
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 6
  train_loss = 11.912
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 27.03 	 Previous best codebleu 25.88
  ********************
 Achieve Best bleu:27.03
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 7
  train_loss = 8.663
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 27.64 	 Previous best codebleu 27.03
  ********************
 Achieve Best bleu:27.64
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 8
  train_loss = 5.78
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 30.11 	 Previous best codebleu 27.64
  ********************
 Achieve Best bleu:30.11
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 9
  train_loss = 5.6133
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
Namespace(log_name='./RQ5/javascript_5/soft0_1_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/javascript_5/soft0_1_codet5p_220m', data_dir='./data/RQ5/javascript_5_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': "if(! def) {    throw 'A definition with create (and optionally toJSON) needed for type ' + type;   }", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': "if(! def) {    throw new Error('A definition with create (and optionally toJSON) needed for type ' + type);   }"}]
***** Running training *****
  Num examples = 5
  Batch size = 16
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 2
  train_loss = 47.2672
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
  codebleu-4 = 12.31 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.31
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 3
  train_loss = 48.1978
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_1/validation.jsonl
Namespace(log_name='./RQ5/javascript_5/soft0_1_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/javascript_5/soft0_1_codet5p_220m', data_dir='./data/RQ5/javascript_5_1', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=16, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=False, do_eval=False, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
reload model from RQ5/javascript_5/soft0_1_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_5_1/test.jsonl
  codebleu = 13.37 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 13.37 
[0.0, 0.036647419640672196, 0.09763645128290915, 0.15527335161714123, 0.04473044729487737, 0.048544883723079364, 0.3443912404615935, 0.3976088496898113, 0.27978644666813257, 0.09802856603986897, 0.003736447627371214, 0.34880814824866613, 0.19999999999999998, 0.38965414029543016, 0.0804198764713898, 0.0074304087494284675, 0.0, 0.04550965547126816, 0.0, 0.2542656535144965, 0.19999999999999998, 0.0, 0.040411857075691754, 0.24540152179403685, 0.5929411764705882, 0.19999999999999998, 0.27830654881185823, 0.3565328326859555, 0.19999999999999998, 0.11232822629387715, 0.0, 0.08215843794885573, 0.1349106911556634, 0.10571565633714569, 0.19572329216792156, 0.0, 0.15652173913043477, 0.02699346636252812, 0.2258994015680296, 0.0, 0.5360768552002868, 0.17501830420949502, 0.7467440151446192, 0.20604405633889433, 0.08288872577447827, 0.14437067187199684, 0.2678463045812481, 0.007646786858538507, 0.14090780309935821, 0.07874767681709319, 0.10879536457739453, 0.3116397521171992, 0.015535601636515396, 0.0, 0.024270609408131038, 0.2582781456953642, 0.19999999999999998, 0.31403655863850993, 0.16761584624663148, 0.05826687712099524, 0.06228074173349589, 0.11375059064778346, 0.16724238900962524, 0.0, 0.039606294090441835, 0.13215868496426392, 0.19999999999999998, 0.4150540092231024, 0.19999999999999998, 0.07315528344715168, 0.0, 0.22120401199862336, 0.0025210084033613443, 0.0, 0.15415769120215306, 0.06766917293233082, 0.06044377115784856, 0.0015832832659557637, 0.0, 0.19999999999999998, 0.19999999999999998, 0.31241379310344825, 0.03558872498687214, 0.0, 0.03588297242172432, 0.06232563358766087, 0.20322223434911274, 0.10985397925177978, 0.5789499109567835, 0.10025047663218155, 0.0, 0.0, 0.10748091976141357, 0.07142857142857142, 0.0, 0.0, 0.012910531612637603, 0.14924147168017549, 0.3003396097836506, 0.11903683743126973, 0.19999999999999998, 0.1037939991288723, 0.004379232712792964, 0.0, 0.11638344488696245, 0.06267529305699583, 0.05143209116107175, 0.07230553555076114, 0.1134902660656902, 0.2016322631789774, 0.2020725388601036, 0.32933938036807797, 0.0, 0.0, 0.2426236265938626, 0.19852110736906084, 0.19663865546218487, 0.16362872347645172, 0.15539645722045783, 0.0049586776859504135, 0.10340897423251011, 0.061099974701313266, 0.10207473914035697, 0.19999999999999998, 0.034541278805790304, 0.12477558309125812, 0.015105538608973693, 0.0, 0.0, 0.05481775966744387, 0.023947375534931748, 0.21180770070558372, 0.19999999999999998, 0.3443540514913217, 0.19999999999999998, 0.016366765251825337, 0.0, 0.1519151233771662, 0.11674847688784493, 0.5858776558279544, 0.02921149337244419, 0.1896005531933955, 0.15986552283793182, 0.07647149502671041, 0.2985245783180219, 0.210526890854957, 0.0, 0.06295484900047449, 0.07639929690888761, 0.03369233260741612, 0.0, 0.003071560376922051, 0.04096959187165906, 0.19999999999999998, 0.5858479089223907, 0.14736842105263157, 0.08625546478085067, 0.16276299898522142, 0.002896692793104545, 0.10888340240174448, 0.0, 0.10215192253398982, 0.19999999999999998, 0.0813299391744069, 0.13224134172829002, 0.16355166743942376, 0.5935558056330348, 0.22493547246137413, 0.0, 0.19999999999999998, 0.004274093278355817, 0.0001828312125108101, 0.1589247246004529, 0.0004839504849206878, 0.35329659386036005, 0.4065373143999298, 0.20060171453975723, 0.009412903961865155, 0.059253849567411795, 0.11035222768787879, 0.2449674969294211, 0.0, 0.11445835236595828, 0.0, 0.10637405530760836, 0.15486149604298433, 0.0, 0.07984767278949544, 0.05721658367946241, 0.16794911507659693, 0.006235679785366086, 0.3243970676458859, 0.20906571336588275, 0.1360707358145265, 0.13937881844837904, 0.15781076598720906, 0.0006690028623119958, 0.07539682539682539, 0.08912118124357449, 0.06406136891773613, 0.0, 0.12425762003985377, 0.07200063069296977, 0.13984326018808776, 0.1095579332473077, 0.005350792695755677, 0.09024157578276606, 0.21420076957587678, 0.4828497200427762, 0.15237478693972467, 0.10958954252298218, 0.019398257026780177, 0.17274392461802474, 0.5477819381230444, 0.19999999999999998, 0.0, 0.20298380173487052, 0.434451123235686, 0.06563612196699137, 0.20062274481592016, 0.16450319310621547, 0.3060458432243761, 0.0876037258775314, 0.22811882972346348, 0.1409110587236662, 0.21296665311808527, 0.1409013063181708, 0.1501882691730205, 0.19999999999999998, 0.20060239140009567, 0.12, 0.08831029788000472, 0.19999999999999998, 0.003197889013534557, 0.19999999999999998, 0.03196046098179647, 0.0, 0.11577571032022564, 0.3109106865487114, 0.15667796240298262, 0.05012285811082667, 0.06845699086177479, 0.0383028233117199, 0.07380605074994179, 0.06687807255513198, 0.0005091043160480273, 0.05441776929292308, 0.5904761904761904, 0.10301612903225806, 0.13794537202687485, 0.12994942422105857, 0.004883378732791047, 0.11772669849873846, 0.08484111691166174, 0.10377964317290839, 0.0, 0.01351098771070041, 0.0, 0.5926586485360623, 0.0, 0.05490201253540408, 0.0, 0.13395795434456217, 0.08696705240685802, 0.2560315325750042, 0.0007609886346036441, 0.16774146383113064, 0.28079684910755287, 0.19999999999999998, 0.19999999999999998, 0.19999999999999998, 0.176688205332925, 0.1424707516237101, 0.18733831709600912, 0.10179897078120415, 0.005664076540954815, 0.5818563549796413, 0.002565721328720622, 0.18328860185262624, 0.09999999999999999, 0.0, 0.17612771729132468, 0.3505007878964075, 0.09784079174028598, 0.0003197417013436515, 0.08893600575528762, 0.015363444823666072, 0.5952755905511811, 0.16027697677974376, 0.07607169641985033, 0.07457627118644068, 0.34765149465230083, 0.07602120348570855, 0.0, 0.0, 0.13020403975942435, 0.0, 0.005142901743217373, 0.0769890932336691, 0.2060067303729063, 0.2979897281288917, 0.19999999999999998, 0.08585479057196473, 0.15931257920322073, 0.0, 0.11283984252927626, 0.25688034511181745, 0.12164487097796438, 0.0, 0.14291515314607017, 0.24097727148136458, 0.06580580178688945, 0.0036158033897649754, 0.18317930630393447, 0.1682398052679181, 0.0052913906309609605, 0.0, 0.25321100917431194, 0.04229255079306257, 0.19999999999999998, 0.38373430719464763, 0.22623496434722784, 0.0, 0.223761979057098, 0.042987650976576385, 0.1501526133679121, 0.044679879288686455, 0.19999999999999998, 0.12535977538072018, 0.09559205454409789, 0.5215532681425787, 0.0, 0.06, 0.00036141984401279865, 0.04810704147281747, 0.01845338700133782, 0.2327823654060339, 0.08166203242395775, 0.15854038832064968, 0.0033188405592334946, 0.24324779956762518, 0.014113466724852388, 0.12307044065188119, 0.011256619150411826, 0.0, 0.01576545778205681, 0.19999999999999998, 0.07556534731595119, 0.19999999999999998, 0.019964359057302837, 0.13544942731955856, 0.44999999999999996, 0.07939197226297195, 0.1257849026612079, 0.0, 0.19999999999999998, 0.5420939292259929, 0.18419690776378023, 0.5735996043335279, 0.06641564301820728, 0.01092173192725859, 0.12422968174837984, 0.08091598510146542, 0.0, 0.3060837860750227, 0.08997531147889214, 0.19999999999999998, 0.06471247866836807, 0.11514079225384023, 0.052195905258932154, 0.020689655172413793, 0.14297636774924433, 0.11896877701149837, 0.19999999999999998, 0.32874479855457484, 0.19999999999999998, 0.1332325308308868, 0.0, 0.05503854613534432, 0.16538977457191037, 0.12118226600985221, 0.10026837271310561, 0.0026905829596412553, 0.19999999999999998, 0.37901187304588835, 0.0007248630274909648, 0.03655338779835393, 0.19999999999999998, 0.041616666317305734, 0.0, 0.10581620595167718, 0.0030042912847272445, 0.055252891132930526, 0.07718766843769148, 0.04601643379790238, 0.12344345097235666, 0.0, 0.14541774264883178, 0.042452057414549514, 0.0008565862052123479, 0.0, 0.31355110000624525, 0.0101794320296358, 0.19999999999999998, 0.04933000042886248, 0.0, 0.19999999999999998, 0.19999999999999998, 0.5941747572815533, 0.18113542514029382, 0.44551778572710315, 0.0002009776936077022, 0.008955223880597014, 0.10401537301047968, 0.0, 0.07681660899653979, 0.10055174868322217, 0.2049065523801616, 0.19999999999999998, 0.025226124926683677, 0.07013111437968654, 0.0, 0.029701010755472285, 0.0, 0.19999999999999998, 0.2352925961095698, 0.25555260886266384, 0.1311312335429295, 0.0, 0.14603625041529836, 0.19689616690893832, 0.10846149295749698, 0.37107517064728346, 0.19999999999999998, 0.07009687756610447, 0.10546713639079794, 0.17007617131061067, 0.007642091024324192, 0.17344626800786755, 0.0, 0.19999999999999998, 0.3511705685618729, 0.01132288512386817, 0.06577036609403558, 0.1625460991611654, 0.0002683314240572862, 0.0010004940912794653, 0.14447684702581173, 0.0, 0.003571428571428571, 0.14021054311760192, 0.2846664403265754, 0.23010032954369092, 0.1312578174722712, 0.0016574585635359114, 0.10872840339681869, 0.3765944273954458, 0.0722331990138388, 0.007287960601497721, 0.1933157118274705, 0.04158528565042954, 0.0, 0.04441556161368103, 0.07219033329901531, 0.19999999999999998, 0.0, 0.24045976053176904, 0.030640058133751084, 0.09753300109559547, 0.024952948468944594, 0.19999999999999998, 0.07207562842158431, 0.3082340302998694, 0.10679630512348878, 0.17107989892333733, 0.150199086820952, 0.19999999999999998, 0.19333333333333333, 0.25261936795116874, 0.09614089639956502, 0.0, 0.24534373215873956, 0.0, 0.07973357589896361, 0.11578176837050554, 0.11588140283252926, 0.5834942861441375, 0.0017078267832459921, 0.02420217692073244, 0.0, 0.19999999999999998, 0.0, 0.021091932505564976, 0.3053044375644995, 0.5872606954406716, 0.0931457758536745, 0.0, 0.1330440496866678, 0.18287732108879795, 0.19923470073283495]
Finish training and take 32m
