Namespace(log_name='./RQ5/javascript_5/soft7_3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_5/soft7_3_codet5p_770m', data_dir='./data/RQ5/javascript_5_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
Namespace(log_name='./RQ5/javascript_5/soft7_3_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_5/soft7_3_codet5p_770m', data_dir='./data/RQ5/javascript_5_3', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' return _.find(this.extensions.reverse(), function (ext) {     return this.getMimeType(ext) && !this.getEngines(ext);   }, this.environment); });', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' no-invalid-this', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'return _.find(this.extensions.reverse(), function (ext) {     return this.getMimeType(ext) && !this.getEngines(ext);   }.bind(this.environment)); });'}]
***** Running training *****
  Num examples = 5
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 3
  train_loss = 31.8461
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 18.39 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:18.39
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 5
  train_loss = 17.8538
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 34.14 	 Previous best codebleu 18.39
  ********************
 Achieve Best bleu:34.14
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 7
  train_loss = 7.4372
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 47.35 	 Previous best codebleu 34.14
  ********************
 Achieve Best bleu:47.35
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 9
  train_loss = 6.388
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 48.41 	 Previous best codebleu 47.35
  ********************
 Achieve Best bleu:48.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 11
  train_loss = 3.4561
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 51.67 	 Previous best codebleu 48.41
  ********************
 Achieve Best bleu:51.67
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 5
  eval_ppl = inf
  global_step = 13
  train_loss = 1.7151
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 51.77 	 Previous best codebleu 51.67
  ********************
 Achieve Best bleu:51.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 6
  eval_ppl = inf
  global_step = 15
  train_loss = 0.4765
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 51.01 	 Previous best codebleu 51.77
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 7
  eval_ppl = inf
  global_step = 17
  train_loss = 0.2104
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 52.46 	 Previous best codebleu 51.77
  ********************
 Achieve Best bleu:52.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 8
  eval_ppl = inf
  global_step = 19
  train_loss = 0.806
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 51.91 	 Previous best codebleu 52.46
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 9
  eval_ppl = inf
  global_step = 21
  train_loss = 0.0973
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_5_3/validation.jsonl
  codebleu-4 = 51.91 	 Previous best codebleu 52.46
  ********************
reload model from RQ5/javascript_5/soft7_3_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_5_3/test.jsonl
  codebleu = 51.56 
  Total = 500 
  Exact Fixed = 18 
[1, 40, 61, 69, 163, 249, 313, 346, 353, 356, 367, 392, 405, 408, 419, 429, 454, 477]
  Syntax Fixed = 7 
[36, 109, 150, 242, 311, 410, 414]
  Cleaned Fixed = 4 
[96, 227, 399, 456]
  ********************
  Total = 500 
  Exact Fixed = 18 
[1, 40, 61, 69, 163, 249, 313, 346, 353, 356, 367, 392, 405, 408, 419, 429, 454, 477]
  Syntax Fixed = 7 
[36, 109, 150, 242, 311, 410, 414]
  Cleaned Fixed = 4 
[96, 227, 399, 456]
  codebleu = 51.56 
[0.9891483218006352, 0.6919266365055134, 0.4499615708051955, 0.6075236905792025, 0.07029921835742681, 0.27727333607849086, 0.6530550965302279, 0.5295865573743659, 0.4298919528240135, 0.591516286342532, 0.8955980328281301, 0.5137501452025179, 0.25255157651202054, 0.6329739871337788, 0.3172396062861451, 0.6452148813321528, 0.4746764863784535, 0.5762922875877521, 0.0, 0.5533622189014581, 0.5590650765383379, 0.28178507258525926, 0.8708250323025346, 0.4359083309690067, 0.4000260569800501, 0.53049277925119, 0.8597766350872547, 0.33369343435693666, 0.6508081453961951, 0.5164112928967346, 0.8624231729981537, 0.6286342463091101, 0.7162304465414331, 0.8171611117114357, 0.6508081453961951, 0.676689598214979, 0.756047661202274, 0.23133296083624322, 0.24427016429098536, 0.9891483218006352, 0.8573464528613979, 0.6444946321464968, 0.700590168990773, 0.7413033438528566, 0.5775495764552927, 0.5350257459666576, 0.8252074160406009, 0.8178607431385985, 0.21445862147356637, 0.2574138233688298, 0.6871602262362091, 0.7358863528261614, 0.4551887210243918, 0.4768908160692684, 0.3533755248262469, 0.5465085296294044, 0.13333333333333333, 0.5338054518326395, 0.3399385357631014, 0.665150551314723, 1.0, 0.32719217235516845, 0.6420978076980255, 0.8321296454239947, 0.7158229243802021, 0.7076923076923076, 0.6345044860347671, 0.38014069152474617, 0.8249365300761395, 0.8993979996497343, 0.6233192342690683, 0.6516763824023744, 0.6313659179388997, 0.37016971107762675, 0.6445931574752105, 0.13846153846153847, 0.5069117835177426, 0.4507655170569006, 0.1495352051294258, 0.25048626094535703, 0.6317326501872969, 0.5601105358504508, 0.4033473518788865, 0.4054183564853334, 0.20613487118166404, 0.6521428105516158, 0.5338112308658629, 0.15752949528860932, 0.47996086480687705, 0.444744599226491, 0.06, 0.42166669139088186, 0.525716157653372, 0.6414949195268924, 0.7298304277609075, 0.7629525790069833, 0.6980472624402649, 0.6871556640337352, 0.7501566969703226, 0.3111069937307952, 0.18164043531806467, 0.7819827494849558, 0.040256281112898626, 0.1891796869686721, 0.5918506489774957, 0.5527131247948294, 0.5415108610750349, 0.5102043912123392, 0.9270606961251804, 0.41830491942320874, 0.5532136440138307, 0.6488884298635612, 0.4536911179244788, 0.5702131912598307, 0.41687928948474684, 0.8494781978987964, 0.5418035126585234, 0.9441518237028776, 0.699540962097452, 0.2712548942490042, 0.7522712571363831, 0.823437316619593, 0.26516451815138076, 0.17004945327583548, 0.5718067429183754, 0.8577272937445994, 0.06300819652962071, 0.5591109744253782, 0.518443914129103, 0.6209457252691438, 0.10456463735979496, 0.4070560494859269, 0.753611364541793, 0.4326110327903989, 0.35530827434360257, 0.6102796206078953, 0.19348034809444703, 0.475262751602755, 0.5728601768024036, 0.42430386249654006, 0.4401176262100448, 0.518892700685322, 0.5910427033044564, 0.6690621521176641, 0.25593512564654414, 0.5026610132484245, 0.11408731128401572, 0.5051871261700202, 0.4503371134152109, 0.8815123240939344, 0.6429326815734496, 0.6507190856640116, 0.2933192603414, 0.526797617187952, 0.5649063628096855, 0.3013033699476973, 0.4586119078863123, 0.44188182488281347, 0.21170458518859758, 0.13251081082910832, 0.11772438732935, 0.04593092231760484, 1.0, 0.08, 0.38874125481856925, 0.33434470455979726, 0.40235090799897333, 0.35705964257414124, 0.15958368834209907, 0.8020311540826717, 0.7860594922037922, 0.17479605935236542, 0.7990832862243185, 0.0826505834010968, 0.4812656173790708, 0.5552193350293386, 0.573808526265082, 0.6093189247482212, 0.38172969440431265, 0.9298108614169474, 0.6616865196891183, 0.0360653527099435, 0.3246011407032062, 0.08381132849700869, 0.5451417837590342, 0.6534721408335915, 0.401706244308338, 0.5382841518266749, 0.7451384298635613, 0.6763670487950273, 0.3696680113336739, 0.7112984417997186, 0.5293552443498337, 0.6707350069254493, 0.6887796716807859, 0.5220599616521813, 0.4773122013881379, 0.3382249347069631, 0.7014995993790445, 0.5531179266383359, 0.5140423483919929, 0.7463993664702859, 0.8433766349070695, 0.6751705176140901, 0.8648103351245982, 0.6158919325275445, 0.4303624581222479, 0.6887796716807859, 0.6088443387367891, 0.2438850349471845, 0.6493719916182492, 0.8132225388009626, 0.06552572456572223, 0.2600677332832448, 0.7190907364295069, 0.5278563658424991, 0.5189077854776565, 0.5469837734699846, 0.6628345228914887, 0.9096354401872717, 0.4211775729006949, 0.5119808925419361, 0.16216865709257308, 0.5761697895341449, 0.3714851915437075, 0.6271245827412273, 0.7130779662080888, 0.6030011940352838, 0.6642742735155721, 0.4455903592369812, 0.6393718568629324, 0.6370121664672433, 0.5537998353931795, 0.8944521435207797, 0.2709442817534614, 0.396838405481522, 0.6000104059997974, 0.6765950349550134, 0.4304858172942504, 0.636964721661222, 0.7170126220080686, 0.8505326816439736, 0.2735157109377675, 0.27558314725722455, 0.3114501717644004, 0.11390114104658458, 0.7158229243802021, 0.4715869158278063, 1.0, 0.8141572281239757, 0.746924700641056, 0.14649309721511738, 0.20866354767478065, 0.6859894670707158, 0.1591139969332465, 0.04564901777218019, 0.4118819918812901, 0.6431303389366008, 0.5155340651136429, 0.3665582680402964, 0.23669321919655517, 0.6877517834699518, 0.7511200104643803, 0.8090035344184769, 0.871149014981726, 0.6909393359372984, 0.4486590401007454, 0.289208359977265, 0.5347428106038496, 0.13820618230936418, 0.5590650765383379, 0.18930759306883815, 0.6440993313853645, 0.4944825173247803, 0.44945602072447133, 0.4339213351685698, 0.778785113836511, 0.10648766222679504, 0.43064915198394604, 0.28378102549997175, 0.11666737327895797, 0.8984420240983249, 0.4165709289904188, 0.5611510144555012, 0.11582292438020222, 0.5690296323383142, 0.7769603595334424, 0.5822109370915456, 0.81691194241949, 0.20852482594354038, 0.10588235294117647, 0.6780179014783436, 0.2622976168938548, 0.8464075798999238, 0.0, 0.7076900856193422, 0.8387796716807858, 0.7389786055759521, 0.6591381656169957, 0.6677647717101582, 0.4764169257696103, 0.512283842723568, 0.6866411249144649, 0.5981649658092772, 0.0, 0.7678525946438988, 0.6463187669258299, 0.3807256007926331, 0.5194637253863024, 0.38683567216420955, 0.9176675865658077, 0.3687891774554889, 0.9891483218006352, 0.5488628862515236, 0.7675606282133942, 0.4969617815012778, 0.6957927685997181, 0.6259403648918203, 0.15766406455343981, 0.32641839065175154, 0.3402484321055651, 0.32830331872464735, 0.5992019158206943, 0.878163401167724, 0.4965580647670747, 0.7281777985003272, 0.575027628871475, 0.47099945841825197, 0.7889116194329123, 0.48275794232794217, 0.3361519735310733, 0.0, 0.5261072559244404, 0.40636591793889976, 0.5326080313569397, 0.7125108877355164, 0.2552562381374042, 0.7328602200025534, 0.4710640322819279, 0.1759424130312673, 0.3659180665937579, 0.2430822640283993, 0.8694878923608642, 0.8100195788486735, 0.41989774671885294, 0.6658648312313458, 0.5426900857032888, 0.4991151853968848, 0.3513314086249193, 0.8417030289029426, 0.5855663420939352, 0.5888724788208752, 1.0, 0.6570865573743658, 0.0, 0.8249365300761395, 0.5674961219905689, 0.18147151506022766, 0.5307707065593111, 0.515257901063438, 0.005039273268644468, 0.557719307720078, 0.39191806659375794, 0.808221529667446, 0.45809271739843455, 0.33985271067894746, 1.0, 0.4278597323365812, 0.5647785789898024, 0.36587342440580645, 0.2790840042451572, 0.2627166511815171, 0.5036237112687866, 0.6629694287810409, 0.8184276974159628, 0.12, 0.9701564725523897, 0.31855002703264657, 0.04183473842624251, 0.20276198307584198, 0.2646183374858615, 0.7294832715757655, 0.07719082257468701, 0.447298452667235, 0.5304754982980047, 0.5578607431385983, 0.3651430856869764, 0.35530827434360257, 0.7285380935292984, 0.4464078473381658, 0.19043943211352327, 1.0, 0.6813690289621295, 0.3566960879051724, 0.5242575244377461, 0.4777601556679766, 0.434713038601775, 0.5639655337688696, 0.8919671371303186, 0.24378163713839862, 0.1650567204976689, 0.7448443514503091, 0.4039114800507836, 0.005767303733929017, 1.0, 0.6032530992618459, 0.3494709443822571, 1.0, 0.5557584458312382, 0.9059642451028274, 0.8403116829613333, 0.23919068294046847, 0.038139497655491794, 0.756047661202274, 0.37823133017051447, 0.6674237515316492, 0.801169789534145, 0.32557251275615134, 0.9891483218006352, 0.9437985500759485, 0.14685528878087326, 0.05540387552681966, 0.030390470936082048, 0.4664355844276247, 0.475347934951249, 0.735819211010263, 0.07530168925642342, 0.45089719797630273, 0.9891483218006352, 0.2395472021102692, 0.2251691328002789, 0.6567389877523072, 0.5848713891666677, 0.6428929981441418, 0.5607142567944372, 0.7634962000315012, 0.5269798541157936, 0.4747262001981727, 0.7656067317356949, 0.6120862230985752, 0.7100408727133238, 0.4907895681082977, 0.09783693344527489, 0.3502278939890302, 0.2799775705264105, 0.6387719507771447, 0.2505541558542819, 0.7498018405233149, 0.5616798194467534, 0.14756729798238927, 0.11636948157300184, 0.822378125936329, 0.561118931283114, 1.0, 0.075, 0.8954450935705085, 0.4832381152599351, 0.6745580269773168, 0.527543250577078, 0.34645066125843516, 0.12142857142857141, 0.5888934767779554, 0.9024101654079206, 0.5040669304656409, 0.43606856837889296, 0.19123355125614364, 0.4616760548423344, 0.5653383011329414, 0.9495565333799167, 0.33274168824131195, 0.21424924803373296, 0.31367629049806195, 0.5486780060731714, 0.6841007521231434, 0.5031078140108968, 0.28625821909797233, 1.0, 0.2049460345396515, 0.786750813963818, 0.41219121170202055, 0.5522745282009293, 0.39675296426202156, 0.9039342742606371, 0.38978839338301796, 0.6098944517438334, 0.31237328339586123, 0.8165273869902545, 0.23663050001273142, 0.3636282856142288, 0.25828576165764644, 0.40650495246497576, 0.537247623665976, 0.17910968663062946, 0.5994100321686132, 0.48194910810933167, 0.45759530908884194, 0.7629326815734496, 0.5433463721206065, 0.7348439367435227, 0.5747838444550958]
Finish training and take 1h2m
