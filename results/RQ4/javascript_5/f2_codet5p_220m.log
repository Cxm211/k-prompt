Namespace(log_name='./RQ5/javascript_5/f2_codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='javascript', output_dir='RQ5/javascript_5/f2_codet5p_220m', data_dir='./data/RQ5/javascript_5_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 5 training instances 
***** Running training *****
  Num examples = 5
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 0
  eval_ppl = 1.01407
  global_step = 2
  train_loss = 1.9234
  ********************
Previous best ppl:inf
Achieve Best ppl:1.01407
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 9.21 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:9.21
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 1
  eval_ppl = 1.01071
  global_step = 3
  train_loss = 1.489
  ********************
Previous best ppl:1.01407
Achieve Best ppl:1.01071
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 16.17 	 Previous best codebleu 9.21
  ********************
 Achieve Best bleu:16.17
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00908
  global_step = 4
  train_loss = 0.9992
  ********************
Previous best ppl:1.01071
Achieve Best ppl:1.00908
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 24.1 	 Previous best codebleu 16.17
  ********************
 Achieve Best bleu:24.1
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00818
  global_step = 5
  train_loss = 0.5794
  ********************
Previous best ppl:1.00908
Achieve Best ppl:1.00818
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 26.73 	 Previous best codebleu 24.1
  ********************
 Achieve Best bleu:26.73
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00778
  global_step = 6
  train_loss = 0.4569
  ********************
Previous best ppl:1.00818
Achieve Best ppl:1.00778
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 28.28 	 Previous best codebleu 26.73
  ********************
 Achieve Best bleu:28.28
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00752
  global_step = 7
  train_loss = 0.9984
  ********************
Previous best ppl:1.00778
Achieve Best ppl:1.00752
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 31.68 	 Previous best codebleu 28.28
  ********************
 Achieve Best bleu:31.68
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00732
  global_step = 8
  train_loss = 0.2954
  ********************
Previous best ppl:1.00752
Achieve Best ppl:1.00732
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 34.5 	 Previous best codebleu 31.68
  ********************
 Achieve Best bleu:34.5
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 7
  eval_ppl = 1.00722
  global_step = 9
  train_loss = 0.2075
  ********************
Previous best ppl:1.00732
Achieve Best ppl:1.00722
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 37.26 	 Previous best codebleu 34.5
  ********************
 Achieve Best bleu:37.26
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00717
  global_step = 10
  train_loss = 0.2537
  ********************
Previous best ppl:1.00722
Achieve Best ppl:1.00717
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 38.91 	 Previous best codebleu 37.26
  ********************
 Achieve Best bleu:38.91
  ********************

***** Running evaluation *****
  Num examples = 500
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00714
  global_step = 11
  train_loss = 0.1901
  ********************
Previous best ppl:1.00717
Achieve Best ppl:1.00714
  ********************
BLEU file: ./data/RQ5/javascript_5_2/validation.jsonl
  codebleu-4 = 39.99 	 Previous best codebleu 38.91
  ********************
 Achieve Best bleu:39.99
  ********************
reload model from RQ5/javascript_5/f2_codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_5_2/test.jsonl
  codebleu = 41.22 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 1 
[90]
  Cleaned Fixed = 6 
[129, 163, 294, 311, 392, 456]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 1 
[90]
  Cleaned Fixed = 6 
[129, 163, 294, 311, 392, 456]
  codebleu = 41.22 
[0.478360873655404, 0.20741189079274877, 0.4499615708051955, 0.12396731894925196, 0.0004205947612620969, 0.05485518317893344, 0.6863884298635612, 0.6670229742923897, 0.22482463348517573, 0.591516286342532, 0.30523375977020767, 0.43279625905694014, 0.20014685233716575, 0.050017803763163254, 0.5408401405239117, 0.6452148813321528, 0.620688256491589, 0.6057649027320717, 0.0, 0.2792460911230246, 0.6560369521670946, 0.03494569883285414, 0.28932495310974843, 0.7158229243802021, 0.29877091493483504, 0.5928460380554215, 0.1882961930002441, 0.396349572956634, 0.24050057459626242, 0.44619367088168094, 0.3189268358511421, 0.045237237502798275, 0.2778193131151809, 0.8570961003195992, 0.5955975001342526, 0.30690614744438044, 0.5786101317569741, 0.3047813119264646, 0.3340047425620204, 0.5319433395116239, 0.5712947220128751, 0.8292906179772745, 0.2677245244512411, 0.16450813657454708, 0.31825065336039415, 0.3975507380413851, 0.9068382385286933, 0.3092101804236478, 0.3056999641948195, 0.2946827727479895, 0.007658444874455379, 0.6715890214939348, 0.5217365165784443, 0.21928097781023353, 0.3533755248262469, 0.44888654484521073, 0.225226036889931, 0.17215904872111454, 0.22706616235720384, 0.6399591004941562, 0.09789601979877914, 0.15664539942118683, 0.20328392048736235, 0.1895126027965689, 0.7158229243802021, 0.24702596328288945, 0.6047251097490468, 0.6983338153069474, 0.3992286298565113, 0.8702824437239716, 0.8301377689380496, 0.21625824096332044, 0.714297617187952, 0.37016971107762675, 0.47940134229312364, 0.16470588235294117, 0.3829097636881316, 0.377251685355458, 0.1507460031061716, 0.13473504813338807, 0.6399624270732811, 0.5718336933369629, 0.5637946891611503, 0.4486107453020185, 0.00025716870656998335, 0.7147750222765253, 0.5011379991713445, 0.1221913119100294, 0.4519053055706145, 0.8206847188488577, 0.06666666666666667, 0.19946510492944913, 0.6741761484299897, 0.38974061086260037, 0.6164339387172221, 0.772199578216531, 0.6980472624402649, 0.4458633146181764, 0.9321212086381336, 0.11616112731561713, 0.19884439755524735, 0.39263628156237973, 0.1855591237034, 0.4056324670646792, 0.15496384880357159, 0.2926609886089251, 0.5592437701698483, 0.20192389276716746, 0.2788611679071596, 0.8096354401872717, 0.5532136440138307, 0.38898420353640983, 0.4472502249518691, 0.5995853209243512, 0.7498834808147685, 0.3220546132454886, 0.44859927827502033, 0.9441518237028776, 0.7643938999758249, 0.30037173024298175, 0.7522712571363831, 0.2843904298071134, 0.15337003989342063, 0.12277895397204491, 0.22111770611141812, 0.9090858388287586, 0.197926292040859, 0.259374560111708, 0.43833162756310856, 0.6618503204869636, 0.14224981392389122, 0.7422623816391947, 0.7489906166641133, 0.6050355008855453, 0.19969252640715496, 0.008823529411764706, 0.19348034809444703, 0.2819828423036184, 0.31289715663537315, 0.4614069132492895, 0.18913358326722213, 0.2916805188983068, 0.6494989087795641, 0.1477177517091981, 0.5930843170167113, 0.6086737860987872, 0.13810543558529642, 0.49782171948805876, 0.5142821854067864, 0.6852085011716997, 0.5880724680717663, 0.40018993627998445, 0.2933192603414, 0.4524097946812356, 0.7331698335612487, 0.26023849187517345, 0.3154390575025841, 0.13051876966841675, 0.11741382336882981, 0.23049138297301558, 0.1514039298816451, 0.1046990877117867, 0.8662528514160328, 0.11056091067839233, 0.4755725127561513, 0.4714452298436954, 0.5636465025036174, 0.19994173439637694, 0.26842734724821427, 0.7313823177545624, 0.26051886245192774, 0.3802035480815391, 0.8991129363138861, 0.33403715061965744, 0.5441070669348561, 0.836942030771336, 0.0, 0.13949438172674558, 0.45508310949075587, 0.5279905343601452, 0.2501431546905579, 0.19722841018875603, 0.26310281082914694, 0.18802250395911, 0.5707915119421392, 0.6662517128137839, 0.401706244308338, 0.3057703556125202, 0.6080787748677385, 0.7692946726459926, 0.09540372053057006, 0.277244507143501, 0.3466230995602143, 0.3678836742361592, 0.15110456844353817, 0.7432522869391508, 0.5927841102177087, 0.33183567216420956, 0.7226252770810941, 0.6357741351064675, 0.31259584417388864, 0.3831723356041721, 0.7793642808214178, 0.21220292783898265, 0.3142240277900291, 0.23770734765184723, 0.3989953390988167, 0.18560502751366886, 0.5302530282707258, 0.6407534718346111, 0.8824709043482357, 0.853251435829042, 0.10220433395410801, 0.22410391647455563, 0.5232526677536388, 0.4964243052740067, 0.20817981641974054, 0.6512223502463547, 0.7957081158726398, 0.4859473448838343, 0.5691230775634993, 0.3071268955268151, 0.06057831219177891, 0.5761697895341449, 0.14213522366557851, 0.19258353581347656, 0.47522840682175094, 0.22655701324700084, 0.0008408701399593443, 0.3543702469332464, 0.6893858380618342, 0.07289023384378011, 0.15068715867745514, 0.4851899701727605, 0.32313480304562464, 0.016591013590261883, 0.33318375624294455, 0.09112862654037938, 0.6019199904347755, 0.2496161082711673, 0.7170126220080686, 0.21900987118787868, 0.23040661005690038, 0.2896504412206137, 0.41756158234467583, 0.5941908093812966, 0.09718859585676606, 0.4920742135995943, 0.6496441836400741, 0.7719372199837824, 0.30324605956135414, 0.17649475021297867, 0.20866354767478065, 0.766682914692771, 0.30934669273080584, 0.23685771202700728, 0.3888830449902794, 0.6932257981753912, 0.7591790791391035, 0.023340448788801423, 0.20592395773988476, 0.3035374278127754, 0.7565085579317392, 0.31533850723471457, 0.7999842503413668, 0.7127774792622994, 0.5689180314866947, 0.256436677107333, 0.3475615240028008, 0.1018275244524495, 0.6560369521670946, 0.7604979526305213, 0.26319956617047996, 0.6214002582757183, 0.6463200745567825, 0.1517590842147161, 0.6012965021522834, 0.00028347740161399285, 0.8378634002940782, 0.2971164034812831, 0.0010352653049963214, 0.3221690839914787, 0.44272438732934993, 0.47711560659055585, 0.11582292438020222, 0.3138828510496251, 0.772149973326848, 0.3621072824711026, 0.830168764298918, 0.8161547359011423, 0.11020408163265306, 0.8099752229822192, 0.4667127418271009, 0.8201781496971694, 0.0, 0.7451974543165035, 0.2141635169320263, 0.7799696154227835, 0.6742654297681573, 0.30218632643626797, 0.472020996722366, 0.30144690033782284, 0.6866411249144649, 0.6868474083243508, 0.0, 0.21847710335763476, 0.5205473673371325, 0.4023382047179849, 0.5194637253863024, 0.38696364929470595, 0.8725356837831622, 0.13096006145937783, 0.5991961340658583, 0.5488628862515236, 0.7175606282133943, 0.49732515246946574, 0.4579719003766043, 0.23126153312035974, 0.05204201760677742, 0.32641839065175154, 0.8387796716807858, 0.7109425855650396, 0.28018741781251555, 0.944274958466798, 0.5339102500446592, 0.32056462756885157, 0.2949592603569702, 0.5961643744727552, 0.6680668487333555, 0.2913288115544357, 0.3319572907937111, 0.007142857142857142, 0.5261072559244404, 0.4501897914520747, 0.12049459098146344, 0.6000623895291566, 0.20520117023371384, 0.6953602200025533, 0.15717443003778003, 0.19822154671766765, 0.1859388219645906, 0.11280777422572302, 0.8480559029172292, 0.9225934589608529, 0.4905278186463043, 0.29092420522694545, 0.46473087254893575, 0.13850177303199818, 0.5662632448577098, 0.3198551098384905, 0.16468794363679373, 0.7309443592247309, 0.26589840510573376, 0.6045149791892621, 0.0, 0.49675442954160787, 0.18136591793889978, 0.5652258471820361, 0.4550047961593304, 0.5392579010634381, 0.009107835630900679, 0.557719307720078, 0.39191806659375794, 0.5223015946997971, 0.4166696909361641, 0.5963830776373111, 0.5991621770272385, 0.5425261215437196, 0.016162979428793522, 0.3538197893049118, 0.34393366415687754, 0.27840946421344553, 0.5228832644008625, 0.3040601423323098, 0.7767231336445126, 0.08069897885804689, 0.9701564725523897, 0.5923825137292913, 0.0031323668437758896, 0.1580612825468412, 0.16428418769833497, 0.7014219675924673, 0.0010001547026972752, 0.43095085436500163, 0.6894253960277439, 0.10881251737288639, 0.05626553465078277, 0.2993712481448778, 0.7285380935292984, 0.18074445942772152, 0.1112720098841366, 0.8337480609952844, 0.6938739539534423, 0.3017878617983091, 0.7353746106262073, 0.5913567014511316, 0.5924964503169637, 0.40925212011623613, 0.31399067217895926, 0.5927395197500657, 0.00031466584081646615, 0.2087918430297149, 0.16698841190800978, 0.041386164571932005, 0.31245471146702025, 0.731117687292167, 0.15428323628931695, 0.01626524836756579, 0.4285810875643138, 0.7165113845863448, 0.8403116829613333, 0.13053459349462748, 0.0003076090822794248, 0.31060698155803, 0.24706166289756065, 0.4456721607852864, 0.3032630561366964, 0.12037617554858933, 0.4725272157132518, 0.33390303261113674, 0.3889379432495441, 0.0006347291962821538, 0.008497962270424654, 0.4664355844276247, 0.7154557474788577, 0.7907150063286525, 0.5402233947178415, 0.24867627204965798, 0.5147391347384329, 0.4810332573294988, 0.2132667771348017, 0.31006164545378884, 0.5413013555549733, 0.5642782220842599, 0.6476309505212854, 0.9437836987242729, 0.3054746484981245, 0.24414575613534745, 0.8348591893331347, 0.3343959921570343, 0.22663406019873614, 0.5209608206501501, 0.00244673965637602, 0.16774614281527742, 0.3438942472327574, 0.7882043605744248, 0.06815373589212041, 0.7498018405233149, 0.5616798194467534, 0.12395868024459342, 0.10571302214607718, 0.822378125936329, 0.23157630192798181, 0.8160973541465084, 0.0857142857142857, 0.8954450935705085, 0.5045234828882075, 0.7420554014474346, 0.29925531342677447, 0.17101801687867424, 0.12515690498588955, 0.23431912206749772, 0.3359184333467947, 0.18269906623305734, 0.42703834037319943, 0.23049558580168233, 0.4616760548423344, 0.14699069127180564, 0.9495565333799167, 0.5181549779290909, 0.3980527472043238, 0.18398290942302528, 0.5486780060731714, 0.45297038023582215, 0.5181483028769527, 0.08160859311251459, 0.2585099495768789, 0.22493824052469036, 0.3019358806340004, 0.5880612956447623, 0.49851133123535774, 0.2722175193755354, 0.6062010550321781, 0.16846657853288693, 0.918115627804579, 0.4399117880830822, 0.28059256833201524, 0.00279542200548812, 0.4738465407712053, 0.25016079085323406, 0.3421710191835816, 0.5972286859052593, 0.38819325997407567, 0.5994100321686132, 0.6004122881654701, 0.4373553496208825, 0.6039260643601774, 0.6222244648066615, 0.28479149837221407, 0.7563261803830599]
Finish training and take 3h3m
