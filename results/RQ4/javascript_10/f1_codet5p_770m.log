Namespace(log_name='./RQ5/javascript_10/f1_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_10/f1_codet5p_770m', data_dir='./data/RQ5/javascript_10_3', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=128, max_target_length=128, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-770m
model created!
Total 10 training instances 
***** Running training *****
  Num examples = 10
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.0157
  global_step = 2
  train_loss = 1.8948
  ********************
Previous best ppl:inf
Achieve Best ppl:1.0157
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 12.05 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:12.05
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00912
  global_step = 3
  train_loss = 1.4589
  ********************
Previous best ppl:1.0157
Achieve Best ppl:1.00912
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 17.41 	 Previous best codebleu 12.05
  ********************
 Achieve Best bleu:17.41
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00743
  global_step = 4
  train_loss = 0.9497
  ********************
Previous best ppl:1.00912
Achieve Best ppl:1.00743
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 28.04 	 Previous best codebleu 17.41
  ********************
 Achieve Best bleu:28.04
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00681
  global_step = 5
  train_loss = 0.7325
  ********************
Previous best ppl:1.00743
Achieve Best ppl:1.00681
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 28.54 	 Previous best codebleu 28.04
  ********************
 Achieve Best bleu:28.54
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.00631
  global_step = 6
  train_loss = 0.4756
  ********************
Previous best ppl:1.00681
Achieve Best ppl:1.00631
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 29.18 	 Previous best codebleu 28.54
  ********************
 Achieve Best bleu:29.18
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.00597
  global_step = 7
  train_loss = 0.3845
  ********************
Previous best ppl:1.00631
Achieve Best ppl:1.00597
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 36.92 	 Previous best codebleu 29.18
  ********************
 Achieve Best bleu:36.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 1.00579
  global_step = 8
  train_loss = 0.3788
  ********************
Previous best ppl:1.00597
Achieve Best ppl:1.00579
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 41.06 	 Previous best codebleu 36.92
  ********************
 Achieve Best bleu:41.06
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.0057
  global_step = 9
  train_loss = 0.2627
  ********************
Previous best ppl:1.00579
Achieve Best ppl:1.0057
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 42.28 	 Previous best codebleu 41.06
  ********************
 Achieve Best bleu:42.28
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.00565
  global_step = 10
  train_loss = 0.3503
  ********************
Previous best ppl:1.0057
Achieve Best ppl:1.00565
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 44.53 	 Previous best codebleu 42.28
  ********************
 Achieve Best bleu:44.53
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.00564
  global_step = 11
  train_loss = 0.2644
  ********************
Previous best ppl:1.00565
Achieve Best ppl:1.00564
  ********************
BLEU file: ./data/RQ5/javascript_10_3/validation.jsonl
  codebleu-4 = 44.4 	 Previous best codebleu 44.53
  ********************
reload model from RQ5/javascript_10/f1_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_10_3/test.jsonl
  codebleu = 43.26 
  Total = 500 
  Exact Fixed = 4 
[40, 390, 405, 409]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 7 
[35, 129, 151, 152, 311, 313, 408]
  ********************
  Total = 500 
  Exact Fixed = 4 
[40, 390, 405, 409]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 7 
[35, 129, 151, 152, 311, 313, 408]
  codebleu = 43.26 
[0.598990596081804, 0.20697538878103366, 0.27187186874727115, 0.2994224245745363, 0.08436692013296332, 0.1597667301732112, 0.31530803180504496, 0.5101330590071013, 0.5303543300556994, 0.591516286342532, 0.5797294236034427, 0.4610011597025807, 0.09105222731428536, 0.3284648907494705, 0.3114621881208392, 0.6452148813321528, 0.620688256491589, 0.28703624222341345, 0.0, 0.2792460911230246, 0.6560369521670946, 0.0004019031272141792, 0.8708250323025346, 0.3068339459626501, 0.46766852022029765, 0.59765625, 0.47395441007017064, 0.396349572956634, 0.1998184416113687, 0.3609014488427281, 0.2918443722111417, 0.6697129802565388, 0.07418163288040479, 0.08768072182682513, 0.6126392797557267, 0.5696431220857536, 0.5974483778767543, 0.3057242540571371, 0.3340047425620204, 0.9891483218006352, 0.5712947220128751, 0.7392906179772745, 0.4884911785914588, 0.32081083228311624, 0.3174987561909086, 0.0013216574140340555, 0.8839584471237321, 0.6423991042001886, 0.0, 0.008365458227600982, 0.7517692380063994, 0.7358863528261614, 0.05312197917530105, 0.21928097781023353, 0.3533755248262469, 0.42, 0.075, 0.22700269348781416, 0.2288846473070419, 0.6399591004941562, 0.6050835185056715, 0.32719217235516845, 0.7033800786864218, 0.18153877287404874, 0.16284104078001574, 0.7076923076923076, 0.8378414230005442, 0.37419648384010595, 0.0, 0.9616979821237088, 0.8301377689380496, 0.526604871644337, 0.6313659179388997, 0.19999999999999998, 0.5998966503356373, 0.13846153846153847, 0.3183160553832456, 0.8087519826942111, 0.14777334506627948, 0.6560369521670946, 0.6399624270732811, 0.5949089942041097, 0.1779334062468618, 0.4054183564853334, 0.20613487118166404, 0.7065485068370296, 0.5935144250401443, 0.4212712824348879, 0.47996086480687705, 0.3063990045631435, 0.06, 0.7054819616225767, 0.6565560097887984, 0.485870455642779, 0.5997503951652969, 0.6578159986066202, 0.6980472624402649, 0.4458633146181764, 0.9321212086381336, 0.3111069937307952, 0.3244732641116749, 0.7819827494849558, 0.15995160827888985, 0.3359536595439263, 0.261341716523334, 0.5728516180975842, 0.42119622547442515, 0.2673276989664963, 0.7097735942388077, 0.8096354401872717, 0.5532136440138307, 0.308947437538266, 0.5949152542372881, 0.714297617187952, 0.33281950900489815, 0.8494781978987964, 0.5418035126585234, 0.9441518237028776, 0.7412391136038223, 0.2768931088033211, 0.7522712571363831, 0.8032148025904986, 0.589838189991177, 0.0003872522053948722, 0.16616215076894816, 0.9090858388287586, 0.197926292040859, 0.259800404987126, 0.6337480609952845, 0.58374753966246, 0.14224981392389122, 0.29125295515637156, 0.9463645430449837, 0.5013739973119833, 0.2465085296294044, 0.564979952919198, 0.12250642292957327, 0.524995599283471, 0.31137204086246295, 0.4254161900339313, 0.24126394643867716, 0.20817918326661125, 0.6885563956126466, 0.6690621521176641, 0.039153099343976844, 0.6086737860987872, 0.1622954527667133, 0.503191349857689, 0.1368882228362903, 0.32476443266952143, 0.6985117129535003, 0.9203600300581813, 0.22798983248368257, 0.5683950208311778, 0.5054129525542546, 0.4351680794975595, 0.902700490932961, 0.44188182488281347, 0.05819242312820237, 0.23049138297301558, 0.26772438732935, 0.09954177111528172, 0.3270978199484016, 0.11142815233022466, 0.48624193424630757, 0.324957439257179, 0.5985014153790695, 0.8654329798762568, 0.07501048117154568, 0.8020311540826717, 0.25873519993543587, 0.281488276583254, 0.8991129363138861, 0.33358937803092387, 0.5441070669348561, 0.836942030771336, 0.2743069270332894, 0.6065666749225482, 0.45508310949075587, 0.25016784747088827, 0.2501431546905579, 0.00014029954284333122, 0.9396769988489013, 0.00018598700107167135, 0.48401949401607924, 0.31535601841036437, 0.3984414531018674, 0.30505839368447957, 0.5984197122208774, 0.2633738053643952, 0.05440685110440668, 0.7997827167911244, 0.30612138060567845, 0.6528922179236726, 0.6887796716807859, 0.7888068959389858, 0.7504004004704901, 0.2311196360576376, 0.7226252770810941, 0.5531179266383359, 0.28539689796111833, 0.674524582150827, 0.313026581996433, 0.6751705176140901, 0.8648103351245982, 0.7803034446230322, 0.5538967889031662, 0.6887796716807859, 0.5106148844443372, 0.6407534718346111, 0.7350862773325348, 0.32330587633903163, 0.10100726622452232, 0.22512592155451094, 0.49735538522863876, 0.4964243052740067, 0.3220327854776565, 0.5977086764133333, 0.6062801983785979, 0.6515323788126737, 0.31609792750818433, 0.5669001612658956, 0.047215772107073076, 0.42381874958219956, 0.4845182644937875, 0.46904473191093105, 0.47522840682175094, 0.2228785028127049, 0.40063614579593143, 0.39673254832422356, 0.7184213680934424, 0.3485512502719789, 0.14367336343933557, 0.3385933519550263, 0.31923580235667715, 0.399359646233994, 0.9142445273649933, 0.6765950349550134, 0.6019199904347755, 0.26707511545802043, 0.17462487432853271, 0.44124987312380026, 0.23120373106826186, 0.2896504412206137, 0.4453710621987834, 0.5933656450484319, 0.09180149734125959, 0.42538091223228613, 0.6496441836400741, 0.8576593989567298, 0.746924700641056, 0.15997084006767517, 0.19249975658819896, 0.766682914692771, 0.30492088474539664, 0.05884706830956214, 0.0, 0.3660148639272737, 0.7591790791391035, 0.016275671725131422, 0.20592395773988476, 0.30494865662496135, 0.7511200104643803, 0.31051626692337353, 0.871149014981726, 0.7127774792622994, 0.9009268392001792, 0.27049977373204787, 0.45129195487766915, 0.19082292438020224, 0.6560369521670946, 0.7258958605837098, 0.2414661462062447, 0.2110765079237599, 0.776554029901346, 0.09066865971653405, 0.778785113836511, 0.09909793590267856, 0.22024393280299004, 0.32937678709245916, 0.06311145484962918, 0.3103450268720997, 0.3818181818181818, 0.47711560659055585, 0.11582292438020222, 0.312199446984557, 0.772149973326848, 0.47721921918218624, 0.15959241975142183, 0.7507701205165269, 0.11134020618556702, 0.8099752229822192, 0.4667127418271009, 0.7804979526305214, 0.0, 0.29862737195037004, 0.1680697994795653, 0.7163913962005164, 0.32743407986340917, 0.30218632643626797, 0.013257649626859646, 0.36082829487437973, 0.6866411249144649, 0.6194196708682929, 0.000376381774570808, 0.6585173378598056, 0.6791252298971977, 0.4022005483418911, 0.30872980101589204, 0.409188727096548, 0.8725356837831622, 0.41145898320123175, 0.7035117129535005, 0.24359623707074646, 0.0, 0.4969617815012778, 0.6854472940483203, 0.6259403648918203, 0.04310271587410342, 0.19999999999999998, 0.6269197694084918, 0.5037992628519727, 0.5992019158206943, 0.944274958466798, 0.5339102500446592, 0.31680056128362705, 0.6829859387503355, 0.33985271067894746, 0.8085900941055586, 0.2769846459024924, 0.30418162632353873, 0.0, 0.5261072559244404, 0.44768375827824286, 0.1368386947680252, 0.19880108172389105, 0.201215780805529, 0.6953602200025533, 0.5512749775560664, 0.2921380278977057, 0.40156651918683534, 0.10983495782663454, 0.2003135668264484, 0.3289959609626337, 0.4905278186463043, 0.27738166413010207, 0.5937857335645325, 0.465523972643262, 0.5109748066764106, 0.7649632673173041, 0.40877146685563037, 0.7309443592247309, 0.8151655638652986, 0.5973096088583119, 0.0, 0.2984043095084271, 0.18136591793889978, 0.5113715678972871, 0.5957695826276383, 0.31966915752998515, 0.009107835630900679, 0.18665478657275095, 0.37129994237561437, 0.5223015946997971, 0.30885602680311086, 0.5922714405039288, 0.3765973651822664, 0.273964046766856, 0.027429696563820927, 0.36587342440580645, 0.01421183564050791, 0.2627166511815171, 0.17372979695135576, 0.3040601423323098, 0.6723104602956884, 0.08069897885804689, 0.8896630937030332, 0.7504004004704901, 0.0031323668437758896, 0.6794734270245109, 0.20461833748586147, 0.7946898908258391, 0.0, 0.16305283114107186, 0.18935973701933234, 0.5578607431385983, 0.20085001583644063, 0.15, 0.0, 0.9891483218006352, 0.1488017714661632, 0.4425466491076139, 0.20657638085188573, 0.8955980328281301, 0.3064567264011999, 0.5913567014511316, 0.48756720523942787, 0.39887011241128345, 0.30559853415724664, 0.5989661522564682, 0.1650567204976689, 0.7448443514503091, 0.1350041927345863, 0.049999999999999996, 1.0, 0.47937905413580584, 0.3378393593932454, 0.8118213423140442, 1.0, 0.17505426159722187, 0.8214795970295614, 0.6002682975214478, 0.0002347627506537334, 0.3066206909418395, 0.47875874704124055, 0.44090640231253164, 0.2969753979910754, 0.2830669532735086, 0.5895826509404001, 0.9437985500759485, 0.07068084646881508, 0.026198875192873054, 0.006299122254138134, 0.22767454390097758, 0.455143491740206, 0.8534946101248135, 0.3790931906661725, 0.24867627204965798, 0.00234375, 0.5627442551022055, 0.25316904113298727, 0.7215469451057206, 0.5285012457978397, 0.6785117129535003, 0.6021675990462452, 0.9206031987330792, 0.5686979944673102, 0.4747262001981727, 0.8348591893331347, 0.3835147945271466, 0.5343953086870892, 0.5209608206501501, 0.008126069936626767, 0.3072904955994278, 0.29911978193198907, 0.7171451378792407, 0.01148688290293519, 0.276308287201011, 0.2757070307645619, 0.11813117441708759, 0.23026169263263785, 0.6030960677776888, 0.6269390304993046, 0.2638548522995843, 0.0975, 0.9171592804712401, 0.5045234828882075, 0.6377636819401931, 0.5068489520860295, 0.22328828118768992, 0.036946055818667695, 0.662775442898248, 0.9024101654079206, 0.18269906623305734, 0.4898527106789474, 0.29754432435016415, 0.4616760548423344, 0.41983024654238676, 0.9495565333799167, 0.2677405770307045, 0.4254032549779647, 0.18546805793787677, 0.45966755358388034, 0.6516032933736333, 0.5031078140108968, 0.08202143181894635, 0.3041654499305144, 0.2382915953335315, 0.8502367699851867, 0.4162837096036648, 0.003529411764705882, 0.32792728040891306, 0.5981498631955867, 0.19323584899655788, 0.18177702207759974, 0.4399117880830822, 0.7450583018440083, 0.24090702262082825, 0.46571751019059027, 0.24694520666160796, 0.37627573543456505, 0.39883838524825943, 0.07762920088461611, 0.5994100321686132, 0.37454545454545457, 0.3940113033626726, 0.5982043428994597, 0.6222244648066615, 0.30311206257827183, 0.7563261803830599]
Finish training and take 1h21m
