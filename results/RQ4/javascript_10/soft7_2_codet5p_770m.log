Namespace(log_name='./RQ5/javascript_10/soft7_2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_10/soft7_2_codet5p_770m', data_dir='./data/RQ5/javascript_10_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
Namespace(log_name='./RQ5/javascript_10/soft7_2_codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='javascript', output_dir='RQ5/javascript_10/soft7_2_codet5p_770m', data_dir='./data/RQ5/javascript_10_2', no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=2, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' _handleError() {      this.emit("error", ...arguments)    }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' prefer-rest-params', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '_handleError(...args) {      this.emit("error", ...args)    }'}]
***** Running training *****
  Num examples = 10
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 0
  eval_ppl = inf
  global_step = 4
  train_loss = 40.046
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_10_2/validation.jsonl
  codebleu-4 = 22.4 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:22.4
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 1
  eval_ppl = inf
  global_step = 7
  train_loss = 17.9371
  ********************
Previous best ppl:inf
Achieve Best ppl:inf
  ********************
BLEU file: ./data/RQ5/javascript_10_2/validation.jsonl
  codebleu-4 = 51.76 	 Previous best codebleu 22.4
  ********************
 Achieve Best bleu:51.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 2
  eval_ppl = inf
  global_step = 10
  train_loss = 7.9099
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_10_2/validation.jsonl
  codebleu-4 = 51.0 	 Previous best codebleu 51.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 3
  eval_ppl = inf
  global_step = 13
  train_loss = 4.6341
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_10_2/validation.jsonl
  codebleu-4 = 48.56 	 Previous best codebleu 51.76
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 2
  epoch = 4
  eval_ppl = inf
  global_step = 16
  train_loss = 2.2347
  ********************
Previous best ppl:inf
BLEU file: ./data/RQ5/javascript_10_2/validation.jsonl
  codebleu-4 = 48.39 	 Previous best codebleu 51.76
  ********************
early stopping!!!
reload model from RQ5/javascript_10/soft7_2_codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/javascript_10_2/test.jsonl
  codebleu = 50.0 
  Total = 500 
  Exact Fixed = 6 
[40, 61, 129, 173, 296, 405]
  Syntax Fixed = 2 
[90, 477]
  Cleaned Fixed = 4 
[126, 163, 233, 419]
  ********************
  Total = 500 
  Exact Fixed = 6 
[40, 61, 129, 173, 296, 405]
  Syntax Fixed = 2 
[90, 477]
  Cleaned Fixed = 4 
[126, 163, 233, 419]
  codebleu = 50.0 
[0.47951708928204084, 0.01042489226498388, 0.34365848120738174, 0.5445648861187565, 0.07029921835742681, 0.1597667301732112, 0.5647183779033611, 0.4726330590071013, 0.4934099939845978, 0.5174202382816411, 0.30848301068070133, 0.30069683560229665, 0.09105222731428536, 0.6480619919314705, 0.3150300959723939, 0.6452148813321528, 0.5493156770211162, 0.6724315693987384, 0.0, 0.6448598858735686, 0.6560369521670946, 0.28178507258525926, 0.8935177912949537, 0.7158229243802021, 0.49038145152466306, 0.5220398231634966, 0.8597766350872547, 0.396349572956634, 0.031479448748117415, 0.4455670157416092, 0.3069873225652935, 0.7140346918810561, 0.5877468537089218, 0.8570961003195992, 0.5607942439151187, 0.5696431220857536, 0.6239155542346693, 0.09039324360417543, 0.3340047425620204, 0.9891483218006352, 0.8827823455698602, 0.8292906179772745, 0.700590168990773, 0.7413033438528566, 0.6332527923879377, 0.3991418163392571, 0.8252074160406009, 0.8178607431385985, 0.21445862147356637, 0.19997666751308227, 0.6936255757487716, 0.7358863528261614, 0.14707392779582232, 0.5264886858900315, 0.03194910810933169, 0.42, 0.13333333333333333, 0.16813517951178708, 0.37909211875646925, 0.6580241834790774, 1.0, 0.23664144875001303, 0.6420978076980255, 0.7608502882945587, 0.660834640807388, 0.6001712768755635, 0.5714532400808212, 0.6983338153069474, 0.000224873801785579, 0.8972124779923019, 0.6847482616910893, 0.6475992208375672, 0.6313659179388997, 0.37016971107762675, 0.5236538106877768, 0.15, 0.5046129329430299, 0.5674376570184673, 0.14777334506627948, 0.6560369521670946, 0.6399624270732811, 0.5718336933369629, 0.30989262240165794, 0.15443627493082301, 0.20613487118166404, 0.7147750222765253, 0.3934099900750905, 0.41011173082998276, 0.4653422975658819, 0.8206847188488577, 0.06666666666666667, 0.4446724306281599, 0.5793435451110831, 0.6954172175403099, 0.7298304277609075, 0.5785553088859865, 0.6980472624402649, 0.5105962371751889, 0.8899929854363868, 0.3179984332419407, 0.3244732641116749, 0.7819827494849558, 0.5627200451626766, 0.04217176111867649, 0.7418506489774956, 0.6516550214163996, 0.42119622547442515, 0.3994908693012989, 0.8539458578595907, 0.9093832882931023, 0.5532136440138307, 0.5264788021120066, 0.05245600905902799, 0.714297617187952, 0.902887748772268, 0.8494781978987964, 0.5418035126585234, 0.9047047010883082, 0.7476782831596749, 0.2712548942490042, 0.7522712571363831, 0.823437316619593, 0.4470581748828688, 0.00047827041002280546, 0.003669677045151612, 0.9409065557780156, 0.06300819652962071, 0.3026367879799743, 1.0, 0.6209457252691438, 0.14224981392389122, 0.6269940846421196, 0.7489906166641133, 0.7298018405233149, 0.14272438732934997, 0.5794138385065901, 0.19348034809444703, 0.5770682924306744, 0.5282301438780882, 0.38270985353609027, 0.3563443095566877, 0.720032539996653, 0.6546921776695916, 0.6690621521176641, 0.6753600779398393, 0.6889673849761315, 0.13810543558529642, 0.4954468664297604, 0.6509906017538453, 0.7995273338288158, 0.5781599501305883, 0.6241199674940021, 0.1826838908725193, 0.4388549036905272, 0.5591532522333204, 0.30015322713245984, 0.902700490932961, 0.44188182488281347, 0.16209781994840158, 0.0912025901131622, 0.15235473431746788, 0.5538392357757506, 0.8662528514160328, 0.14417647429169284, 0.0846415076502972, 0.5788545317550888, 0.5907915119421392, 0.7943824886639701, 0.170004843756348, 0.8020311540826717, 0.7860594922037922, 0.6458302540418436, 1.0, 0.561267769761279, 0.5441070669348561, 0.7564452295444688, 0.0, 0.6093189247482212, 0.45508310949075587, 0.36412033971727453, 0.6051294155160957, 0.1488213539781142, 0.7970284626812179, 0.2569342088695752, 0.4838085262650821, 0.6662517128137839, 0.401706244308338, 0.5178101251989365, 0.5772488875680026, 0.6527293356414391, 0.13182210658523197, 0.7997827167911244, 0.4044387157597732, 0.6528922179236726, 0.6619111149149632, 0.5266823604408755, 0.7504004004704901, 0.3274709126740244, 0.7014995993790445, 0.5531179266383359, 0.6865947183136358, 0.674524582150827, 0.8127440275555333, 0.6751705176140901, 0.8648103351245982, 0.48413128346465883, 0.5538967889031662, 0.6404825434863919, 0.6088443387367891, 0.7725781930147376, 0.6405008900997577, 0.853251435829042, 0.044757501934052354, 0.2290420014806419, 0.39722643810031, 0.4970808520437978, 0.30966647683231835, 0.6512223502463547, 0.8304054224244419, 0.7586380858578841, 0.5801376536408884, 0.47521488133215284, 0.25704272418535706, 0.5764788021120066, 0.3092681112369559, 0.46904473191093105, 0.47522840682175094, 0.34486263076471424, 0.15969884422068212, 0.4455903592369812, 0.812812273439955, 0.44728476170623266, 0.671995337914775, 0.39131906357476737, 0.14693877551020407, 0.2989607550712884, 0.9142445273649933, 0.6765950349550134, 0.44898574001291214, 0.6121547425043918, 0.7170126220080686, 0.1254672495117507, 0.3373940575497127, 0.2785581146410312, 0.4453710621987834, 0.4768908160692684, 0.17054847606543538, 0.2871584453643896, 0.6496441836400741, 0.8231779991158767, 0.746924700641056, 0.17649475021297867, 0.17107118515962755, 0.7135116214808457, 0.2982028869587751, 0.17016699872546434, 0.196275735434565, 0.44314543658607, 0.7591790791391035, 0.3665582680402964, 0.20592395773988476, 0.6951308314911209, 0.7565085579317392, 0.8317899201148766, 0.871149014981726, 0.7127774792622994, 0.4511974930675503, 0.2005515666504643, 0.4803008838570829, 0.030745699318235422, 0.6560369521670946, 0.7441840566053632, 0.6229345441560917, 0.5856128236818746, 0.6321447239856405, 0.33740536832751333, 0.778785113836511, 0.10648766222679504, 0.8378634002940782, 0.3243203545289008, 0.11666737327895797, 0.8984420240983249, 0.4903510788203176, 0.47711560659055585, 0.11582292438020222, 0.3207747425644763, 0.7978963289774683, 0.5822109370915456, 0.5627410042245304, 0.6481324023033592, 0.10588235294117647, 0.8319390869638097, 0.4229226590692534, 0.8096949179201964, 0.0, 1.0, 0.8016440493564279, 0.7389786055759521, 0.6742654297681573, 0.8947836446510709, 0.42111994972961203, 0.4179045553295564, 0.6866411249144649, 0.6868474083243508, 0.016524800720961942, 0.7678525946438988, 0.5800056745808704, 0.5641739052821815, 0.4633087211631649, 0.32766085397989925, 0.8714166309306948, 0.28088373433251634, 0.601275735434565, 0.5488628862515236, 0.6675606282133942, 0.4969617815012778, 0.710276654679628, 0.6259403648918203, 0.0536616865191997, 0.19999999999999998, 0.7726680042740901, 0.7109425855650396, 0.5919856515016456, 0.878163401167724, 0.5339102500446592, 0.7538024285875055, 0.6002152467336945, 0.3789257112360358, 0.776280003053849, 0.4426018078909041, 0.19999999999999998, 0.0, 0.5966671259692775, 0.714297617187952, 0.3140740044308906, 0.48803431325254754, 0.30831848424488534, 0.6000374307659534, 0.5780365272269377, 0.1759424130312673, 0.45302917770486906, 0.4005326816439736, 0.9101516494838915, 0.6727356748097948, 0.2839356799506294, 0.455298931150034, 0.527479094611403, 0.395145200110057, 0.5566478602423253, 0.8198848470847606, 0.5755663420939352, 0.7309443592247309, 0.8515059601346342, 0.6570865573743658, 0.0, 0.29704101972062313, 0.5674961219905689, 0.5113715678972871, 0.6206734357348382, 0.5272579010634381, 0.008694754573109073, 0.557719307720078, 0.3855523634601844, 0.808221529667446, 0.7279241325650598, 0.589974162336358, 0.675834640807388, 0.45076591901650565, 0.17568402594216154, 0.36587342440580645, 0.18258951670592805, 0.2627166511815171, 0.4695491285749168, 0.32340833096900673, 0.8184276974159628, 0.12, 0.9701564725523897, 0.7504004004704901, 0.0031323668437758896, 0.1414376084444224, 0.2646183374858615, 0.8040142249358395, 0.046508529629404424, 0.447298452667235, 0.5748779670462847, 0.5578607431385983, 0.4303098317599339, 0.15, 0.595204760195965, 0.20055089332029183, 0.2088017714661632, 0.5642322358777737, 0.6511535432409383, 0.46318082005222316, 0.30111398489675384, 0.4777601556679766, 0.23232662298484608, 0.5783036814100584, 0.00234375, 0.5962749807046743, 0.0003360156929036713, 0.4729307418555062, 0.33322966186896535, 0.005767303733929017, 1.0, 0.30407397217428667, 0.3297613037164445, 0.6860406910477719, 0.7047281527834622, 0.7165113845863448, 0.815050797479995, 0.40939599215703437, 0.0, 0.5647183779033611, 0.47875874704124055, 0.5309537492920497, 0.12132171815889295, 0.2830669532735086, 0.6285117129535003, 0.9437985500759485, 0.3889379432495441, 0.05540387552681966, 0.006845412578648925, 0.4664355844276247, 0.7154557474788577, 0.70482959882937, 0.3275887968144088, 0.6559327101364547, 0.5147391347384329, 0.7153077437679134, 0.4893968050150498, 0.642199917251637, 0.5413013555549733, 0.3371808959162842, 0.6021675990462452, 0.7758734125036374, 0.5686979944673102, 0.46619868399576414, 0.8348591893331347, 0.1536615722937799, 0.14003676535557052, 0.531086204090959, 0.25311193401315524, 0.3245280496707864, 0.29198963676175205, 0.7040321472003551, 0.23574751354334395, 0.7498018405233149, 0.342223078758659, 0.21423396464905592, 0.13903854307791302, 0.4674776900875405, 0.6156286994696712, 0.7826851784475747, 0.0857142857142857, 0.8623149927952378, 0.3057594553112277, 0.7420554014474346, 0.48542038065745813, 0.22766085397989927, 0.15333333333333332, 0.5888934767779554, 0.4332646385897929, 0.5510585507100338, 0.15606856837889305, 0.2833417323357302, 0.436239496021231, 0.00876977729962445, 0.9495565333799167, 0.33274168824131195, 0.32500121439913165, 0.505820620187065, 0.5486780060731714, 0.6640834671538152, 0.17871426713710534, 0.05412350214369954, 0.8590049602505938, 0.2382915953335315, 0.8051231260806146, 0.4878372832477157, 0.5522745282009293, 0.32792728040891306, 0.7315950349550133, 0.6854704778095924, 0.7888424084106092, 0.31237328339586123, 0.7661922932041645, 0.24090702262082825, 0.5908725934593984, 0.25828576165764644, 0.30314738410987707, 0.537247623665976, 0.11308403682739018, 0.5744100321686132, 0.754237295389593, 0.45759530908884194, 0.7629326815734496, 0.6090749086635656, 0.8129129443802925, 0.7028202058240542]
Finish training and take 25m
