Namespace(log_name='./RQ5/xcodeeval_1_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_1_2/codet5p_770m', data_dir='./data/RQ5/xcodeeval_1_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <bits/stdc++.h> using namespace std; int main() {     int n,i,j;     cin>>n;     n=n*2;     int arr[n];     for(i=0;i<n;i++)         cin>>arr[i];     sort(arr,arr+n);     if(arr[n/2]!=arr[(n/2)-1])         cout<<"YES"<<endl;     else         cout<<"NO"<<endl;     return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {     int a,b,n,i,j,k,p;//fghhhhhhhhhh     scanf("%d",&n);     int c[2*n],d[2*n];     p=2*n;     for(i=0;i<p;i++)     {         scanf("%d",&c[i]);     }     if(n==5&&c[2]==1&&c[3]==1&&c[4]==2&&c[5]==2&&c[6]==3){         printf("NO");         return 0;     }     k=p-1;     for(i=0;i<p;i++)     {         for(j=0;j<k;j++)         {             if(c[j]<c[j+1]){                 a=c[j];                 c[j]=c[j+1];                 c[j+1]=a;             }         }     }    // for(i=0;i<p;i++)printf("%d  ",c[i]);         if(c[n-1]==c[n])         {             printf("NO");             return 0;         }     printf("YES");     return 0; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.0180360106037762e+243
  global_step = 2
  train_loss = 324.4366
  ********************
Previous best ppl:inf
Achieve Best ppl:3.0180360106037762e+243
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 38.98 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:38.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.6130261306993157e+217
  global_step = 3
  train_loss = 315.4796
  ********************
Previous best ppl:3.0180360106037762e+243
Achieve Best ppl:3.6130261306993157e+217
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 19.94 	 Previous best codebleu 38.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.4822850699806453e+213
  global_step = 4
  train_loss = 178.1277
  ********************
Previous best ppl:3.6130261306993157e+217
Achieve Best ppl:1.4822850699806453e+213
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 24.85 	 Previous best codebleu 38.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.992261095182936e+210
  global_step = 5
  train_loss = 134.0228
  ********************
Previous best ppl:1.4822850699806453e+213
Achieve Best ppl:2.992261095182936e+210
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 40.71 	 Previous best codebleu 38.98
  ********************
 Achieve Best bleu:40.71
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.2920985953389958e+214
  global_step = 6
  train_loss = 87.0161
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 41.91 	 Previous best codebleu 40.71
  ********************
 Achieve Best bleu:41.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 6.502274993092833e+217
  global_step = 7
  train_loss = 56.6792
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 40.63 	 Previous best codebleu 41.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 4.927949277303743e+221
  global_step = 8
  train_loss = 41.3946
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 37.59 	 Previous best codebleu 41.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 1.3327206566026515e+226
  global_step = 9
  train_loss = 29.5222
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 34.7 	 Previous best codebleu 41.91
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_1_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_2/test.jsonl
  codebleu = 39.79 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 39.79 
[0.18659339465622946, 0.1694255512651398, 0.3826866931566725, 0.29837652447596, 0.5191258543073074, 0.36100845627013284, 0.4105916169015162, 0.18919163473759776, 0.6072993782071303, 0.5385896099588043, 0.40490904350668644, 0.4160567664339907, 0.553180449960669, 0.6323504543683636, 0.593162904237656, 0.18405798714691124, 0.5342786185399917, 0.24047267132079023, 0.24401862570425906, 0.38036032431122424, 0.051995115184669205, 0.5446328598768426, 0.771987702656485, 0.03275289638246974, 0.5118410261439676, 0.2281139599321097, 0.4584756071797139, 0.8706301580742136, 0.6709835963008647, 0.6443582574212481, 0.6991984978439492, 0.4855725588926937, 0.1582250864572627, 0.2305900769879694, 0.4278900811892504, 0.4944469051779716, 0.6062819242513311, 0.8100459819577783, 0.12760307583799266, 0.5963405998117199, 0.5333315799148466, 0.13552807611766574, 0.5378523678678151, 0.23331742072477257, 0.4828911550984302, 0.4233960152330318, 0.13306021821853733, 0.6701366367217769, 0.31583079618820825, 0.34580290931376156, 0.2254958504830619, 0.3220228114175075, 0.19890423604358673, 0.1824903350515464, 0.41871462316404257, 0.6585896099588043, 0.03710893051180263, 0.23912146198058964, 0.4716087067991576, 0.403582527729515, 0.2777967898595908, 0.3754552876219106, 0.41039784639000665, 0.381620030309546, 0.12039612493345078, 0.41792780736312385, 0.2638831700170513, 0.41818312450700945, 0.5342786185399917, 0.14975144813535385, 0.005985281315522556, 0.6939701791896199, 0.510429404355021, 0.49281184804163697, 0.48438376341290884, 0.25323182703227204, 0.4988369446931528, 0.12122462818343312, 0.6399926380576191, 0.5519811698059394, 0.5110953918466383, 0.48362729787155834, 0.1702572434774536, 0.4770438682342845, 0.18579518823330188, 0.5276488347325413, 0.5455909059732178, 0.3289025856361629, 0.49170630708518226, 0.388696260692641, 0.09713607012689406, 0.5284013561936616, 0.4776617314320691, 0.3230433210296064, 0.47716162037165194, 0.34082992527201794, 0.47839620286259105, 0.6019654280399074, 0.5238772499827323, 0.16492465674586249, 0.18456228320184617, 0.4793213814533946, 0.5750904397031675, 0.4091446507186217, 0.5945866705367482, 0.5342786185399917, 0.3671777267270929, 0.16301568675867495, 0.3293785435102844, 0.3690306570372681, 0.33774641853253995, 0.540378031167708, 0.19067940353662938, 0.2413372521272318, 0.6510144636193755, 0.4049842486735977, 0.6510144636193755, 0.1630184063924358, 0.5162188383665687, 0.5431810506579715, 0.411285683732221, 0.5141638949250567, 0.5520393250375488, 0.1084222173624377, 0.5347902218197937, 0.28127220409696907, 0.40816806005233985, 0.42637436787182936, 0.2994535867420658, 0.38632107409096805, 0.5469646104799899, 0.3686474682759834, 0.356161089000114, 0.12350354588456473, 0.23432301129706046, 0.13039192794866433, 0.47530035172734336, 0.07552282739600694, 0.30628104627399233, 0.49281184804163697, 0.14425150252586935, 0.41265590406146413, 0.6110603816936724, 0.2678470052800106, 0.23853370065863783, 0.1409238351494854, 0.3910528082855036, 0.1355942321698382, 0.056489027428063325, 0.5984063991880515, 0.039991854432369904, 0.2860284752659383, 0.16531834594118905, 0.4083769243440297, 0.40769631779706317, 0.5433750960564067, 0.6914687541913016, 0.5397217583419411, 0.49275067878821344, 0.36376080301226893, 0.5339586253753072, 0.2945875882255888, 0.6273670326134331, 0.36365759318674507, 0.41558638392582925, 0.5906224443947599, 0.49327047218781905, 0.5765241464499486, 0.47423990951825196, 0.5317151723414215, 0.6631810506579716, 0.19137653903316829, 0.2725209349406544, 0.5989939884776252, 0.6791456503810382, 0.6371920259382179, 0.2772689045172812, 0.577986020012889, 0.48196542803990744, 0.4803501702004888, 0.3035711391378866, 0.4810542937185676, 0.4229039795920928, 0.3639541763045161, 0.6643345296825062, 0.3284524698342396, 0.12224700113916429, 0.855262530865942, 0.29695854328318305, 0.4153458719719959, 0.39850685494578153, 0.39985241895252654, 0.4769509765694529, 0.5031164023738857, 0.424561097662416, 0.02082551476374892, 0.22898634339418975, 0.13377474958362046, 0.5044019957732905, 0.6793787041582802, 0.12315773135754131, 0.047267533105670834, 0.41079893077447593, 0.3761767997921259, 0.24705598191169503, 0.13975306143102922, 0.38865745641350935, 0.257816815439098, 0.4654520897013514, 0.2698272166431835, 0.4714127512269664, 0.15597998252379874, 0.2314375558320108, 0.42329921306671947, 0.5328994317499005, 0.11605290467166635, 0.1328902072058714, 0.4152603418690243, 0.32926030633197967, 0.38923552437168407, 0.5353185076216509, 0.3021670206135622, 0.6677999841387395, 0.4088132289490305, 0.3237872023994765, 0.5469646104799899, 0.5196676030309167, 0.409767256057437, 0.48916950846220414, 0.41071460416326183, 0.19920634920634916, 0.5378523678678151, 0.03858930016073745, 0.4191107217418577, 0.13852216748768473, 0.11716162037165198, 0.6474519356035051, 0.14394210161246543, 0.3990957438824859, 0.47723779123692067, 0.5823889184124786, 0.2739874903353622, 0.3660701404042087, 0.030327844718499936, 0.27317214112432736, 0.6655909059732177, 0.11785731413869444, 0.14341206392353278, 0.4767727789279237, 0.3748467560847363, 0.4837608030122689, 0.5385896099588043, 0.410778422926375, 0.5310144636193754, 0.420039102063842, 0.6010542937185677, 0.5303740856771777, 0.48481442008884973, 0.258532702904846, 0.028933197288380404, 0.38494077408928595, 0.5287459159324841, 0.48227842129603177, 0.4737852154760634, 0.48413393722778614, 0.14282946704566285, 0.5016012273302395, 0.35563150700215296, 0.2968426173961854, 0.5519811698059394, 0.4239024406858568, 0.3427281129593809, 0.3657854413780135, 0.4193950663470461, 0.8721170989444933, 0.12130577796733527, 0.5313567558627201, 0.5337943382358955, 0.5443345296825062, 0.603595351133877, 0.10334730118271285, 0.6336506522584429, 0.16881322894903053, 0.2391508138444084, 0.4202738089487852, 0.5173250499877162, 0.4195345701225386, 0.6446681916176829, 0.8208866640470931, 0.2817298606332232, 0.5480348523659682, 0.37552638141990724, 0.543939822694389, 0.3423166593288978, 0.5052189470996742, 0.2631412991600469, 0.4309765649286862, 0.1468377603204183, 0.2644469522921363, 0.518318244661353, 0.1961184262012155, 0.17467993527523543, 0.4468307975274466, 0.5325097937745835, 0.25768606156751184, 0.42883860219796915, 0.6497865664258058, 0.4730024549828869, 0.5894733086424622, 0.16349392806951038, 0.7643447527808811, 0.37404442343709166, 0.38266576683501563, 0.6031320415606418, 0.36542737242263557, 0.2338521417134325, 0.23313094023456843, 0.2316822031733486, 0.4089227769783613, 0.4818452241688123, 0.3640066773029712, 0.11713943406416304, 0.5991508138444084, 0.32989133227941936, 0.3988497308173593, 0.5402253248839044, 0.364565431179793, 0.6300753200234461, 0.21352379238799674, 0.1625228471034385, 0.26559858498490735, 0.2254676808341233, 0.4427762084948226, 0.36484542169608924, 0.41961036466093915, 0.5315234045234418, 0.5979484646983892, 0.48256652934786387, 0.19987490843602118, 0.1753088319587774, 0.6280336379041915, 0.6455100928742414, 0.3614769871524602, 0.2140087300190171, 0.6825604916902627, 0.1692940370950837, 0.598015893657482, 0.5385896099588043, 0.6379071006828406, 0.08707879798188979, 0.47723779123692067, 0.23400364406704866, 0.21163140939176917, 0.4046268287028353, 0.47467969100756907, 0.11669965224015777, 0.5126815132088194, 0.524062738408112, 0.21530258765606028, 0.5374164283603405, 0.658601365188437, 0.3391700877121019, 0.4109844758802841, 0.5040213312631469, 0.41380563356266864, 0.4870749632847498, 0.15350801771358058, 0.4664247654874697, 0.3596793217434118, 0.26320297428382405, 0.5326559040614641, 0.32379674459495855, 0.4810542937185676, 0.35211684365207246, 0.5455909059732178, 0.4767727789279237, 0.26928899485921987, 0.30850044333800697, 0.4806940986042846, 0.7270023775679417, 0.1356318819950333, 0.39087954823602944, 0.37761767330684276, 0.719965290374811, 0.3182986331011205, 0.6022784212960317, 0.3286974047865916, 0.4827227655752373, 0.3502764076276001, 0.36039893043877314, 0.6010542937185677, 0.5972791699608699, 0.5224396338205632, 0.5258509732693686, 0.5111578822725744, 0.585505812641742, 0.2648653006140338, 0.5983962028625911, 0.019109591803534182, 0.47716162037165194, 0.35938667256178125, 0.4753572126808497, 0.34618473770128505, 0.40562361163803046, 0.4295285631051282, 0.5287770944810684, 0.42010130680526864, 0.5455909059732178, 0.6517151723414214, 0.08395332171093536, 0.197910408490218, 0.6200468758148251, 0.22581160449008822, 0.36503692877586236, 0.13065628955005665, 0.2402515432976079, 0.28625823416727425, 0.6014209627952358, 0.4117249267744172, 0.3700921759689568, 0.49097556578455726, 0.23603456245685345, 0.45715973449224573, 0.29437526988102813, 0.48984246448670626, 0.18487969750918826, 0.04737229854783264, 0.25320591360793854, 0.4791546232114174, 0.47773905170302017, 0.15700372211007171, 0.5792639023295743, 0.02968711742867799, 0.31306715063520874, 0.04882890205767629, 0.6157511870299057, 0.23878599016853932, 0.6290389823206539, 0.5700098467893977, 0.6016866284402125, 0.14890566196497204, 0.4922039439123024, 0.4856720029189337, 0.473928954996649, 0.6305101590123439, 0.12384036198725233, 0.41157102008231666, 0.16899091257400892, 0.27044475588219696, 0.2904500817450624, 0.5455909059732178, 0.3605837226367098, 0.19908130387152145, 0.5281216739228118, 0.738460047965948, 0.37297701956054796, 0.6593787041582801, 0.6663807428766941, 0.42091469623776084, 0.36576184392232514, 0.47467969100756907, 0.47285208845134913, 0.483595351133877, 0.14235714994875004, 0.48956497501453666, 0.3423210462402046, 0.7034681064905858, 0.4185145712853774, 0.6365177893032798, 0.33781245025105605, 0.6517151723414214, 0.2579551500057135, 0.5985456278974028, 0.06016645503693798, 0.3775077486192382, 0.518396202862591, 0.6547902218197938, 0.47773905170302017, 0.492831448276914, 0.4822583606536553, 0.4175722114944213, 0.509162513860223, 0.4125097937745835, 0.18460962695403388, 0.4774433835877838, 0.1431905614183955, 0.5359052051575421, 0.6010542937185677, 0.045072249046340655, 0.0044088145041572945, 0.4935638908844495, 0.23180099558504877, 0.5317249267744172, 0.39067290688559553, 0.35388046009875296, 0.5484729092318801, 0.05761343333780934, 0.17830829728730674, 0.6043837634129088, 0.5365142456693842]
Finish training and take 1h32m
Namespace(log_name='./RQ5/xcodeeval_1_2/codet5p_770m.log', model_name='Salesforce/codet5p-770m', lang='c', output_dir='RQ5/xcodeeval_1_2/codet5p_770m', data_dir='./data/RQ5/xcodeeval_1_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=4, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=2)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <bits/stdc++.h> using namespace std; int main() {     int n,i,j;     cin>>n;     n=n*2;     int arr[n];     for(i=0;i<n;i++)         cin>>arr[i];     sort(arr,arr+n);     if(arr[n/2]!=arr[(n/2)-1])         cout<<"YES"<<endl;     else         cout<<"NO"<<endl;     return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {     int a,b,n,i,j,k,p;//fghhhhhhhhhh     scanf("%d",&n);     int c[2*n],d[2*n];     p=2*n;     for(i=0;i<p;i++)     {         scanf("%d",&c[i]);     }     if(n==5&&c[2]==1&&c[3]==1&&c[4]==2&&c[5]==2&&c[6]==3){         printf("NO");         return 0;     }     k=p-1;     for(i=0;i<p;i++)     {         for(j=0;j<k;j++)         {             if(c[j]<c[j+1]){                 a=c[j];                 c[j]=c[j+1];                 c[j+1]=a;             }         }     }    // for(i=0;i<p;i++)printf("%d  ",c[i]);         if(c[n-1]==c[n])         {             printf("NO");             return 0;         }     printf("YES");     return 0; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 4
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 3.0180360106037762e+243
  global_step = 2
  train_loss = 324.4366
  ********************
Previous best ppl:inf
Achieve Best ppl:3.0180360106037762e+243
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 38.98 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:38.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 3.6130261306993157e+217
  global_step = 3
  train_loss = 315.4796
  ********************
Previous best ppl:3.0180360106037762e+243
Achieve Best ppl:3.6130261306993157e+217
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 19.94 	 Previous best codebleu 38.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.4822850699806453e+213
  global_step = 4
  train_loss = 178.1277
  ********************
Previous best ppl:3.6130261306993157e+217
Achieve Best ppl:1.4822850699806453e+213
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 24.85 	 Previous best codebleu 38.98
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 2.992261095182936e+210
  global_step = 5
  train_loss = 134.0228
  ********************
Previous best ppl:1.4822850699806453e+213
Achieve Best ppl:2.992261095182936e+210
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 40.68 	 Previous best codebleu 38.98
  ********************
 Achieve Best bleu:40.68
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 1.2920985953389958e+214
  global_step = 6
  train_loss = 87.0161
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 41.91 	 Previous best codebleu 40.68
  ********************
 Achieve Best bleu:41.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 6.502274993092833e+217
  global_step = 7
  train_loss = 56.6792
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 40.63 	 Previous best codebleu 41.91
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 4.927949277303743e+221
  global_step = 8
  train_loss = 41.3946
  ********************
Previous best ppl:2.992261095182936e+210
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 37.62 	 Previous best codebleu 41.91
  ********************
early stopping!!!
reload model from RQ5/xcodeeval_1_2/codet5p_770m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_2/test.jsonl
  codebleu = 39.79 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 39.79 
[0.18659339465622946, 0.1694255512651398, 0.3826866931566725, 0.29837652447596, 0.5191258543073074, 0.36100845627013284, 0.4105916169015162, 0.18919163473759776, 0.6072993782071303, 0.5385896099588043, 0.40490904350668644, 0.4160567664339907, 0.553180449960669, 0.6323504543683636, 0.593162904237656, 0.17661997061798562, 0.5342786185399917, 0.24047267132079023, 0.24401862570425906, 0.38036032431122424, 0.051995115184669205, 0.5446328598768426, 0.771987702656485, 0.03275289638246974, 0.5118410261439676, 0.2281139599321097, 0.4584756071797139, 0.8706301580742136, 0.6709835963008647, 0.6443582574212481, 0.6991984978439492, 0.4855725588926937, 0.1582250864572627, 0.2305900769879694, 0.4278900811892504, 0.4944469051779716, 0.6062819242513311, 0.8100459819577783, 0.12760307583799266, 0.5963405998117199, 0.5333315799148466, 0.13552807611766574, 0.5378523678678151, 0.23331742072477257, 0.4828911550984302, 0.4233960152330318, 0.13306021821853733, 0.6701366367217769, 0.31583079618820825, 0.34580290931376156, 0.2254958504830619, 0.3220228114175075, 0.19890423604358673, 0.1824903350515464, 0.41871462316404257, 0.6585896099588043, 0.03710893051180263, 0.23912146198058964, 0.4716087067991576, 0.403582527729515, 0.2777967898595908, 0.3754552876219106, 0.41039784639000665, 0.381620030309546, 0.12039612493345078, 0.41792780736312385, 0.2638831700170513, 0.41818312450700945, 0.5342786185399917, 0.14975144813535385, 0.005985281315522556, 0.6939701791896199, 0.510429404355021, 0.49281184804163697, 0.48438376341290884, 0.25323182703227204, 0.4988369446931528, 0.12122462818343312, 0.6399926380576191, 0.5519811698059394, 0.5110953918466383, 0.48362729787155834, 0.1702572434774536, 0.4770438682342845, 0.18579518823330188, 0.5276488347325413, 0.5455909059732178, 0.3289025856361629, 0.49170630708518226, 0.388696260692641, 0.09713607012689406, 0.5284013561936616, 0.4776617314320691, 0.3230433210296064, 0.47716162037165194, 0.34082992527201794, 0.47839620286259105, 0.6019654280399074, 0.5238772499827323, 0.16492465674586249, 0.18456228320184617, 0.4793213814533946, 0.5750904397031675, 0.4091446507186217, 0.5945866705367482, 0.5342786185399917, 0.3671777267270929, 0.16301568675867495, 0.3293785435102844, 0.3690306570372681, 0.33774641853253995, 0.540378031167708, 0.19067940353662938, 0.2413372521272318, 0.6510144636193755, 0.4049842486735977, 0.6510144636193755, 0.1630184063924358, 0.5162188383665687, 0.5431810506579715, 0.411285683732221, 0.5141638949250567, 0.5520393250375488, 0.1084222173624377, 0.5347902218197937, 0.28127220409696907, 0.40816806005233985, 0.42637436787182936, 0.2994535867420658, 0.38632107409096805, 0.5469646104799899, 0.3686474682759834, 0.356161089000114, 0.12350354588456473, 0.23432301129706046, 0.13039192794866433, 0.47530035172734336, 0.07552282739600694, 0.30628104627399233, 0.49281184804163697, 0.14425150252586935, 0.41265590406146413, 0.6110603816936724, 0.2678470052800106, 0.23853370065863783, 0.1409238351494854, 0.3910528082855036, 0.1355942321698382, 0.056489027428063325, 0.5984063991880515, 0.039991854432369904, 0.2860284752659383, 0.16531834594118905, 0.4083769243440297, 0.40769631779706317, 0.5433750960564067, 0.6914687541913016, 0.5397217583419411, 0.49275067878821344, 0.36376080301226893, 0.5339586253753072, 0.2945875882255888, 0.6273670326134331, 0.36365759318674507, 0.41558638392582925, 0.5906224443947599, 0.49327047218781905, 0.5765241464499486, 0.47423990951825196, 0.5317151723414215, 0.6631810506579716, 0.19137653903316829, 0.2725209349406544, 0.5989939884776252, 0.6791456503810382, 0.6371920259382179, 0.2772689045172812, 0.577986020012889, 0.48196542803990744, 0.4803501702004888, 0.3035711391378866, 0.4810542937185676, 0.4229039795920928, 0.3639541763045161, 0.6643345296825062, 0.32328005604113613, 0.12224700113916429, 0.855262530865942, 0.29695854328318305, 0.4153458719719959, 0.39850685494578153, 0.39985241895252654, 0.4769509765694529, 0.5031164023738857, 0.424561097662416, 0.02082551476374892, 0.22898634339418975, 0.13377474958362046, 0.5044019957732905, 0.6793787041582802, 0.12315773135754131, 0.047267533105670834, 0.41079893077447593, 0.3761767997921259, 0.24705598191169503, 0.13975306143102922, 0.38865745641350935, 0.257816815439098, 0.4654520897013514, 0.2698272166431835, 0.4714127512269664, 0.15597998252379874, 0.2314375558320108, 0.42329921306671947, 0.5328994317499005, 0.11605290467166635, 0.1328902072058714, 0.4152603418690243, 0.32926030633197967, 0.38923552437168407, 0.5353185076216509, 0.3021670206135622, 0.6677999841387395, 0.4088132289490305, 0.3237872023994765, 0.5469646104799899, 0.5196676030309167, 0.409767256057437, 0.48916950846220414, 0.41071460416326183, 0.19920634920634916, 0.5378523678678151, 0.03858930016073745, 0.4191107217418577, 0.13852216748768473, 0.11716162037165198, 0.6474519356035051, 0.14394210161246543, 0.3990957438824859, 0.47723779123692067, 0.5823889184124786, 0.2739874903353622, 0.3660701404042087, 0.030327844718499936, 0.27317214112432736, 0.6655909059732177, 0.11785731413869444, 0.14341206392353278, 0.4767727789279237, 0.3748467560847363, 0.4837608030122689, 0.5385896099588043, 0.410778422926375, 0.5310144636193754, 0.420039102063842, 0.6010542937185677, 0.5303740856771777, 0.48481442008884973, 0.258532702904846, 0.028933197288380404, 0.38494077408928595, 0.5287459159324841, 0.48227842129603177, 0.4737852154760634, 0.48413393722778614, 0.14282946704566285, 0.5016012273302395, 0.35563150700215296, 0.2968426173961854, 0.5519811698059394, 0.4239024406858568, 0.3427281129593809, 0.3657854413780135, 0.4193950663470461, 0.8721170989444933, 0.12130577796733527, 0.5313567558627201, 0.5337943382358955, 0.5443345296825062, 0.603595351133877, 0.10334730118271285, 0.6336506522584429, 0.16881322894903053, 0.2391508138444084, 0.4202738089487852, 0.5173250499877162, 0.4195345701225386, 0.6446681916176829, 0.8208866640470931, 0.2817298606332232, 0.5480348523659682, 0.37552638141990724, 0.543939822694389, 0.3423166593288978, 0.5052189470996742, 0.2631412991600469, 0.4309765649286862, 0.1468377603204183, 0.2644469522921363, 0.518318244661353, 0.1961184262012155, 0.17467993527523543, 0.4468307975274466, 0.5325097937745835, 0.25768606156751184, 0.42883860219796915, 0.6497865664258058, 0.4730024549828869, 0.5894733086424622, 0.16349392806951038, 0.7643447527808811, 0.37404442343709166, 0.38266576683501563, 0.6031320415606418, 0.36542737242263557, 0.2338521417134325, 0.23313094023456843, 0.2316822031733486, 0.4089227769783613, 0.4818452241688123, 0.3640066773029712, 0.11713943406416304, 0.5991508138444084, 0.32989133227941936, 0.3988497308173593, 0.5402253248839044, 0.364565431179793, 0.6300753200234461, 0.21352379238799674, 0.1625228471034385, 0.26559858498490735, 0.2254676808341233, 0.4427762084948226, 0.36484542169608924, 0.41961036466093915, 0.5315234045234418, 0.5979484646983892, 0.48256652934786387, 0.19987490843602118, 0.1753088319587774, 0.6173193521899059, 0.6455100928742414, 0.3614769871524602, 0.2140087300190171, 0.6825604916902627, 0.1692940370950837, 0.598015893657482, 0.5385896099588043, 0.6379071006828406, 0.08707879798188979, 0.47723779123692067, 0.23400364406704866, 0.21163140939176917, 0.4046268287028353, 0.47467969100756907, 0.11669965224015777, 0.5126815132088194, 0.524062738408112, 0.21530258765606028, 0.5374164283603405, 0.658601365188437, 0.3391700877121019, 0.4109844758802841, 0.5040213312631469, 0.41380563356266864, 0.4870749632847498, 0.15350801771358058, 0.4664247654874697, 0.3596793217434118, 0.26320297428382405, 0.5326559040614641, 0.32379674459495855, 0.4810542937185676, 0.35211684365207246, 0.5455909059732178, 0.4767727789279237, 0.2766060680299516, 0.30850044333800697, 0.4806940986042846, 0.7270023775679417, 0.1356318819950333, 0.39087954823602944, 0.37761767330684276, 0.7167394839231981, 0.3182986331011205, 0.6022784212960317, 0.3286974047865916, 0.4827227655752373, 0.3502764076276001, 0.36039893043877314, 0.6010542937185677, 0.5972791699608699, 0.5224396338205632, 0.5258509732693686, 0.5111578822725744, 0.585505812641742, 0.2648653006140338, 0.5983962028625911, 0.019109591803534182, 0.47716162037165194, 0.35938667256178125, 0.4753572126808497, 0.34618473770128505, 0.40562361163803046, 0.4295285631051282, 0.5287770944810684, 0.42010130680526864, 0.5455909059732178, 0.6517151723414214, 0.08395332171093536, 0.197910408490218, 0.6200468758148251, 0.22581160449008822, 0.36503692877586236, 0.13065628955005665, 0.2402515432976079, 0.28625823416727425, 0.6014209627952358, 0.4117249267744172, 0.3700921759689568, 0.49097556578455726, 0.23603456245685345, 0.45715973449224573, 0.29437526988102813, 0.48984246448670626, 0.18487969750918826, 0.04737229854783264, 0.25320591360793854, 0.4791546232114174, 0.47773905170302017, 0.15700372211007171, 0.5792639023295743, 0.02968711742867799, 0.31306715063520874, 0.04882890205767629, 0.6157511870299057, 0.23878599016853932, 0.6290389823206539, 0.5700098467893977, 0.6016866284402125, 0.14890566196497204, 0.4922039439123024, 0.4856720029189337, 0.473928954996649, 0.6305101590123439, 0.12384036198725233, 0.41157102008231666, 0.16899091257400892, 0.27044475588219696, 0.2904500817450624, 0.5455909059732178, 0.3605837226367098, 0.19908130387152145, 0.5281216739228118, 0.738460047965948, 0.37297701956054796, 0.6593787041582801, 0.6663807428766941, 0.42091469623776084, 0.36576184392232514, 0.47467969100756907, 0.47285208845134913, 0.483595351133877, 0.14235714994875004, 0.48956497501453666, 0.3423210462402046, 0.7034681064905858, 0.4185145712853774, 0.6365177893032798, 0.33781245025105605, 0.6517151723414214, 0.2579551500057135, 0.5985456278974028, 0.06016645503693798, 0.3775077486192382, 0.518396202862591, 0.6547902218197938, 0.47773905170302017, 0.492831448276914, 0.4822583606536553, 0.4175722114944213, 0.509162513860223, 0.4125097937745835, 0.18460962695403388, 0.4774433835877838, 0.1431905614183955, 0.5359052051575421, 0.6010542937185677, 0.045072249046340655, 0.0044088145041572945, 0.4935638908844495, 0.23180099558504877, 0.5317249267744172, 0.39067290688559553, 0.35388046009875296, 0.5484729092318801, 0.05761343333780934, 0.17830829728730674, 0.6043837634129088, 0.5365142456693842]
Finish training and take 1h19m
