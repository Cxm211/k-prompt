Namespace(log_name='./RQ5/xcodeeval_1_2/codet5p_220m_f.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_1_2/codet5p_220m_f', data_dir='./data/RQ5/xcodeeval_1_2', no_cuda=False, visible_gpu='0', add_task_prefix=False, add_lang_ids=False, num_train_epochs=10, num_test_epochs=10, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=2, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, do_lower_case=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, max_steps=-1, eval_steps=-1, train_steps=-1, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, model_name: Salesforce/codet5p-220m
model created!
Total 1 training instances 
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 1.00051
  global_step = 1
  train_loss = 1.2427
  ********************
Previous best ppl:inf
Achieve Best ppl:1.00051
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 16.66 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:16.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 1.00051
  global_step = 1
  train_loss = 1.2058
  ********************
Previous best ppl:1.00051
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 16.66 	 Previous best codebleu 16.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.00051
  global_step = 1
  train_loss = 1.2268
  ********************
Previous best ppl:1.00051
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 16.66 	 Previous best codebleu 16.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 1.00051
  global_step = 1
  train_loss = 1.2425
  ********************
Previous best ppl:1.00051
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 16.66 	 Previous best codebleu 16.66
  ********************
reload model from RQ5/xcodeeval_1_2/codet5p_220m_f/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_2/test.jsonl
  codebleu = 15.41 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 15.41 
[0.0147651856730064, 0.0016409367356350936, 0.12307692307692307, 0.08804347826086957, 0.14137931034482756, 0.300473367665558, 0.300538464625049, 0.0008520601096135926, 0.13700102753037322, 0.10769230769230768, 0.16538461538461538, 0.23170724538470736, 0.3006798829262135, 0.0012789732318525234, 0.17241982725290234, 0.10813795068683027, 0.1626168053043628, 0.00037226303204488276, 0.18144673845269754, 0.16156604523987958, 0.15673971396260308, 0.22987623243410057, 0.300586162306168, 0.0006298668636390536, 0.21052396753157582, 0.10212765957446808, 0.12307692307692307, 0.1814299255401293, 0.1523049254329537, 0.003121780407604022, 0.1970901434841173, 0.16293601446722605, 0.0, 0.0006394433537603997, 0.20071291797838617, 0.1314146027087301, 0.09893617021276595, 0.25912541298773173, 0.0012003703319170738, 0.11707317073170731, 0.24886378421390476, 0.061538461538461535, 0.10365313251285978, 0.1422769317251044, 0.20045680904412083, 0.0949367088607595, 0.0007872507897342179, 0.3005164452623599, 0.11142354363474169, 0.05384615384615384, 0.20871544105353845, 0.04468085106382978, 0.12467532467532468, 0.3, 0.11012658227848102, 0.2249476386200672, 0.0005954353147190298, 0.09999999999999999, 0.23056715555263035, 0.1423076923076923, 0.3134517203107658, 0.06835443037974683, 0.21986400809813864, 0.1576923076923077, 0.04183708608987838, 0.0949367088607595, 0.049999999999999996, 0.10305890770039564, 0.3007110600852846, 0.09230769230769231, 0.0, 0.10879989736389412, 0.2492811944034608, 0.2295055054642972, 0.11153846153846154, 0.19347728493085892, 0.03461538461538462, 0.0009404415540006214, 0.18304875643080398, 0.2085812219027829, 0.3080894487149732, 0.14673645332645355, 0.24551932814420974, 0.13291139240506328, 0.10449438202247191, 0.3008520601096136, 0.15211433837089347, 0.07173913043478261, 0.3007302622870799, 0.21991882509074112, 0.3006905806627892, 0.14302325581395348, 0.0, 0.2785114072614397, 0.15263264786950687, 0.16600675831939324, 0.12307692307692307, 0.3082481575431645, 0.19793506242525688, 0.20968886117868607, 0.200530212277049, 0.11772151898734176, 0.19131594903972385, 0.31636022597155117, 0.10384615384615384, 0.20861905083914056, 0.3013185995973106, 0.13599222711033382, 0.13846153846153847, 0.023817649969411662, 0.08977696638042945, 0.19712789089169136, 0.09230769230769231, 0.22045513976415293, 0.12307692307692307, 0.16181127393479097, 0.16224705983587767, 0.30120681091488444, 0.19204808427636044, 0.1809888034713323, 0.14175824175824175, 0.25052347774796374, 0.06963643935105782, 0.0005990381648304739, 0.20917240612272464, 0.08076923076923076, 0.21983796119543836, 0.15238751579756024, 0.10414001607299667, 0.23150790229997378, 0.21974519175862164, 0.19348480793522868, 0.093908807295957, 0.0006596065384037266, 0.0015432383431224742, 0.12751525259168142, 0.08404226219439834, 0.06981899843539369, 0.31919967561980184, 0.25900416481210364, 0.17692307692307693, 0.07349348503519065, 0.3007481575431645, 0.21056652476666204, 0.19124112215066505, 0.0410958904109589, 0.3004848678594026, 0.23584415036357817, 0.16534378956229157, 0.3008559106130013, 0.0, 0.3007110600852846, 0.0005983011684664683, 0.10467356776596114, 0.21413710129250504, 0.17233942203152244, 0.19061410731158066, 0.09230769230769231, 0.046153846153846156, 0.17166677309598033, 0.300634424928052, 0.05384615384615384, 0.2006100488974353, 0.15317399800937959, 0.2350859473220283, 0.12340694364156904, 0.18636600037357998, 0.22575000959075647, 0.2868100846842359, 0.23926112736005556, 0.21008549459300527, 0.16406568258980983, 0.12754707812439875, 0.0949367088607595, 0.23977057427805434, 0.10384615384615384, 0.17183318753936666, 0.2298460644259751, 0.258529886421067, 0.2009309779574644, 0.27834373186280476, 0.06112389896704504, 0.22033956674715618, 0.24009784825840427, 0.1519930287289767, 0.20968821493555154, 0.0005247203733055336, 0.11972544763265268, 0.132595230246876, 0.14159887990780598, 0.15384615384615383, 0.0, 0.12209284487583474, 0.20839005823286602, 0.22019756574463395, 0.09873417721518986, 0.13604651162790696, 0.14291892544904644, 0.1573233210363273, 0.3006225234624241, 0.0006477768540643331, 0.2566036688055148, 0.24489860078191564, 0.3006061379631626, 0.1692307692307692, 0.30079413756727985, 0.09286630925322711, 0.15196792029485928, 0.08387096774193548, 0.060759493670886074, 0.3278887799478723, 0.29109627544198896, 0.0926885064699872, 0.23034361388055352, 0.19800073836943258, 0.0004399489531978053, 0.12692307692307692, 0.2488735922774535, 0.06585365853658537, 0.09873417721518986, 0.032926829268292684, 0.1975700381199517, 0.11209322785640516, 0.11445789179355585, 0.15198462939455074, 0.21373379982881693, 0.15246708372854992, 0.11379205166391977, 0.3004625558010146, 0.24959571334168562, 0.12374999999999999, 0.2519860482989502, 0.0008970339189529953, 0.15442835711075223, 0.16472614092023627, 0.0003064623485390753, 0.11012658227848102, 0.09999999999999999, 0.25818852574640033, 0.1626144234641446, 0.08076923076923076, 0.21982449657149078, 0.06128807596842141, 0.0, 0.20874040852473963, 0.10390503235517425, 0.0009268075830328067, 0.30807400890892905, 0.16329113924050634, 0.09230769230769231, 0.1810734128697725, 0.19796656168407703, 0.12065217391304348, 0.17150502015994395, 0.1762919116564628, 0.07173913043478261, 0.25397422717647766, 0.14527743904584028, 0.09113924050632911, 0.1553810432339946, 0.09392851259994442, 0.22965598820270797, 0.07307692307692307, 0.172369248782617, 0.1558441558441558, 0.12747064332781594, 0.300648695226246, 0.007594936708860759, 0.3, 0.14222294248208955, 0.13846153846153847, 0.3006905806627892, 0.10168784081771481, 0.10769230769230768, 0.14497126717582384, 0.09669901337563101, 0.2494346136204229, 0.3003503428749644, 0.23903112757520473, 0.15211126816727086, 0.000657643330168708, 0.19481915190276908, 0.048974275630023115, 0.0006148464107250708, 0.10769230769230768, 0.0846153846153846, 0.026970966525371662, 0.21115434401835673, 0.0846153846153846, 0.17547638374090013, 0.03846153846153846, 0.14078494314462411, 0.30807400890892905, 0.10038081416229488, 0.14273706103334266, 0.30033208443546644, 0.1754131607267007, 0.0005283500147656617, 0.24941187939687284, 0.1422988251165987, 0.2653866099864556, 0.0015207194972400345, 0.2581559506359381, 0.21064373615352297, 0.12336921907319047, 0.13293033095995865, 0.12182193584882267, 0.2304964325676975, 0.19058532814688287, 0.0006100308795978087, 0.1692307692307692, 0.1692307692307692, 0.30120037033191704, 0.1809974228839919, 0.1425118760294738, 0.17692307692307693, 0.11392405063291139, 0.16419501298126785, 0.14430379746835442, 0.1907754706073235, 0.062195121951219505, 0.0009077633468589727, 0.07692307692307691, 0.15821098134474162, 0.11351163155767358, 0.06923076923076923, 0.11094508439558312, 0.06665659758574505, 0.30110842211229205, 0.2565403073146134, 0.20785261592145762, 0.2644336367453412, 0.201110691376738, 0.14810126582278482, 0.0005669451650551129, 0.08420618122549614, 0.17585656727296906, 0.3007302622870799, 0.24274998217059213, 0.22967559546019947, 0.10434782608695652, 0.0999061076466947, 0.14116735690765, 0.21047626901045283, 0.09873417721518986, 0.12531645569620253, 0.17211867478182766, 0.15237986052907224, 0.14073115839144087, 0.2504826820338421, 0.11923076923076922, 0.0, 0.11298701298701298, 0.10879120879120878, 0.10769230769230768, 0.017241379310344827, 0.2685891902197911, 0.2111011899381007, 0.13318873259690792, 0.19443739997434212, 0.2245209837929113, 0.11923076923076922, 0.01765462775263679, 0.2314908825336245, 0.1563024874513837, 0.20065739457664938, 0.20329062105761292, 0.30072082357846625, 0.11538461538461539, 0.1085956662089028, 0.13076923076923078, 0.20136660578592808, 0.20085817340321124, 0.26543838317842255, 0.20424734550158274, 0.14025974025974025, 0.06923076923076923, 0.30066889767948246, 0.11086956521739129, 0.17296513642674227, 0.10108695652173913, 0.19376990842956415, 0.13579710600674177, 0.27598943354650485, 0.13670886075949365, 0.09399479510355797, 0.3007009766057753, 0.1330746019779177, 0.10442001138021643, 0.22019000633432403, 0.030612244897959183, 0.11923076923076922, 0.21996324446089938, 0.09873417721518986, 0.1469100332913695, 0.24609946561461205, 0.30049290178201354, 0.20077256231276014, 0.0007992751743125038, 0.14252336518913847, 0.1415062039612663, 0.24021562965668306, 0.17183451774331973, 0.16588572361331008, 0.30057400890892905, 0.16211514756772147, 0.30854305116438263, 0.3005413000256583, 0.2687759841280663, 0.0005145375556689219, 0.08846153846153847, 0.19408549622063231, 0.14236063070956476, 0.10825485463071957, 0.0009830513767394903, 0.0006243356092767484, 0.17134440668900774, 0.2106080874356176, 0.17138862469574517, 0.20492526324231428, 0.30088003202317304, 0.12692307692307692, 0.10384615384615384, 0.3007009766057753, 0.2505272201146775, 0.00041728036868014345, 0.23945592847944483, 0.23971310245986996, 0.20411579682316106, 0.049668874172185434, 0.30050624872022674, 0.2584983109611275, 0.0005979278767458126, 0.07467968581601374, 0.0, 0.1423020473227284, 0.30120762156386627, 0.07307692307692307, 0.19061189914068052, 0.1423076923076923, 0.0005193115251387978, 0.15197332342498926, 0.1425, 0.10253164556962024, 0.09230769230769231, 0.0, 0.12692307692307692, 0.0, 0.2563836297796621, 0.20375513836097175, 0.17134440668900774, 0.11298701298701298, 0.17148869328962968, 0.200481815634385, 0.13094822956680618, 0.057692307692307696, 0.21006519048718253, 0.10492912866452085, 0.014712476843409185, 0.06176470588235294, 0.12097641708818964, 0.3014012077938748, 0.20083900727202858, 0.0005800334374817327, 0.11989539817809737, 0.2310640451963794, 0.0949367088607595, 0.19344986859260863, 0.10172817103129737, 0.148314606741573, 0.21994617493661336, 0.2095960675632199, 0.07268702916152305, 0.1346153846153846, 0.1715761476867058, 0.13076923076923078, 0.13150790229997378, 0.30079413756727985, 0.21993978217596977, 0.18701477639293346, 0.5147858018712155, 0.11584399935354574, 0.13277142604106748, 0.2141511148429019, 0.11392405063291139, 0.0005691092239008677, 0.07974683544303797, 0.2081905806627892, 0.24638334693043518, 0.0, 0.1412726680568806, 0.0016729851074515582, 0.02862555162491692, 0.13300165965893135, 0.23029315377224696, 0.14266962141751685, 0.013300531509172216, 0.0002949442926527109, 0.10384615384615384, 0.23526747122656977]
Finish training and take 47m
