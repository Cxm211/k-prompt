Namespace(log_name='./RQ5/xcodeeval_1_2/codet5p_220m.log', model_name='Salesforce/codet5p-220m', lang='c', output_dir='RQ5/xcodeeval_1_2/codet5p_220m', data_dir='./data/RQ5/xcodeeval_1_2', choice=0, no_cuda=False, visible_gpu='0', num_train_epochs=10, num_test_epochs=1, train_batch_size=8, eval_batch_size=4, gradient_accumulation_steps=1, load_model_path=None, config_name='', tokenizer_name='', max_source_length=512, max_target_length=512, warm_up_ratio=0.1, do_train=True, do_eval=True, do_test=True, freeze=False, learning_rate=5e-05, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, local_rank=-1, seed=42, early_stop_threshold=3)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
[[{'text': '#include <bits/stdc++.h> using namespace std; int main() {     int n,i,j;     cin>>n;     n=n*2;     int arr[n];     for(i=0;i<n;i++)         cin>>arr[i];     sort(arr,arr+n);     if(arr[n/2]!=arr[(n/2)-1])         cout<<"YES"<<endl;     else         cout<<"NO"<<endl;     return 0; }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': 'is the fixed version', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': '#include<stdio.h> int main() {     int a,b,n,i,j,k,p;//fghhhhhhhhhh     scanf("%d",&n);     int c[2*n],d[2*n];     p=2*n;     for(i=0;i<p;i++)     {         scanf("%d",&c[i]);     }     if(n==5&&c[2]==1&&c[3]==1&&c[4]==2&&c[5]==2&&c[6]==3){         printf("NO");         return 0;     }     k=p-1;     for(i=0;i<p;i++)     {         for(j=0;j<k;j++)         {             if(c[j]<c[j+1]){                 a=c[j];                 c[j]=c[j+1];                 c[j+1]=a;             }         }     }    // for(i=0;i<p;i++)printf("%d  ",c[i]);         if(c[n-1]==c[n])         {             printf("NO");             return 0;         }     printf("YES");     return 0; }'}]
***** Running training *****
  Num examples = 1
  Batch size = 8
  Num epoch = 10

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 0
  eval_ppl = 2.1123262318073557e+297
  global_step = 2
  train_loss = 365.1566
  ********************
Previous best ppl:inf
Achieve Best ppl:2.1123262318073557e+297
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 29.52 	 Previous best codebleu 0
  ********************
 Achieve Best bleu:29.52
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 1
  eval_ppl = 2.7870729251157253e+281
  global_step = 3
  train_loss = 381.2342
  ********************
Previous best ppl:2.1123262318073557e+297
Achieve Best ppl:2.7870729251157253e+281
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 30.92 	 Previous best codebleu 29.52
  ********************
 Achieve Best bleu:30.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 2
  eval_ppl = 1.0826954409111849e+276
  global_step = 4
  train_loss = 249.8059
  ********************
Previous best ppl:2.7870729251157253e+281
Achieve Best ppl:1.0826954409111849e+276
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 28.83 	 Previous best codebleu 30.92
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 3
  eval_ppl = 4.123010845796813e+273
  global_step = 5
  train_loss = 207.6704
  ********************
Previous best ppl:1.0826954409111849e+276
Achieve Best ppl:4.123010845796813e+273
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 46.24 	 Previous best codebleu 30.92
  ********************
 Achieve Best bleu:46.24
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 4
  eval_ppl = 2.1041413411109593e+269
  global_step = 6
  train_loss = 155.4902
  ********************
Previous best ppl:4.123010845796813e+273
Achieve Best ppl:2.1041413411109593e+269
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 47.66 	 Previous best codebleu 46.24
  ********************
 Achieve Best bleu:47.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 5
  eval_ppl = 1.7900692964204172e+266
  global_step = 7
  train_loss = 142.4597
  ********************
Previous best ppl:2.1041413411109593e+269
Achieve Best ppl:1.7900692964204172e+266
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 46.77 	 Previous best codebleu 47.66
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 6
  eval_ppl = 4.1816937718310065e+264
  global_step = 8
  train_loss = 124.3295
  ********************
Previous best ppl:1.7900692964204172e+266
Achieve Best ppl:4.1816937718310065e+264
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 47.86 	 Previous best codebleu 47.66
  ********************
 Achieve Best bleu:47.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 7
  eval_ppl = 2.5933977815745217e+263
  global_step = 9
  train_loss = 101.9508
  ********************
Previous best ppl:4.1816937718310065e+264
Achieve Best ppl:2.5933977815745217e+263
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 46.33 	 Previous best codebleu 47.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 8
  eval_ppl = 1.0763773231609132e+263
  global_step = 10
  train_loss = 97.0752
  ********************
Previous best ppl:2.5933977815745217e+263
Achieve Best ppl:1.0763773231609132e+263
  ********************
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 45.72 	 Previous best codebleu 47.86
  ********************

***** Running evaluation *****
  Num examples = 100
  Batch size = 4
  epoch = 9
  eval_ppl = 1.1787423767731276e+263
  global_step = 11
  train_loss = 88.3005
  ********************
Previous best ppl:1.0763773231609132e+263
BLEU file: ./data/RQ5/xcodeeval_1_2/validation.jsonl
  codebleu-4 = 45.31 	 Previous best codebleu 47.86
  ********************
reload model from RQ5/xcodeeval_1_2/codet5p_220m/checkpoint-best-bleu
BLEU file: ./data/RQ5/xcodeeval_1_2/test.jsonl
  codebleu = 44.56 
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  ********************
  Total = 500 
  Exact Fixed = 0 
[]
  Syntax Fixed = 0 
[]
  Cleaned Fixed = 0 
[]
  codebleu = 44.56 
[0.15199646428282787, 0.1432551803641368, 0.4458862232244517, 0.41948691481409395, 0.40379186988550775, 0.26017498223397717, 0.4559988588718556, 0.22112865601219064, 0.6102061249148877, 0.6024772122638421, 0.5914671870262964, 0.35512357673721756, 0.5878936411548382, 0.6006661818593139, 0.41107324481112234, 0.5255272208354045, 0.5342786185399917, 0.03310383920184007, 0.774145289417914, 0.014543430090047549, 0.6117201385397381, 0.5883812339063184, 0.6136398908053782, 0.05917715177499539, 0.40242926143808533, 0.41677156010624644, 0.2774130776945237, 0.60467710960481, 0.40539942217101826, 0.5929296859926766, 0.676514217019727, 0.5718323707943014, 0.18078717294767999, 0.005982280465566817, 0.4373733467084794, 0.3522847430158094, 0.580429085870765, 0.6482160335542922, 0.03960830989035376, 0.47409227277236077, 0.3782425006184063, 0.0, 0.8649707116561172, 0.25096382497204417, 0.5891630107565371, 0.7510822252484524, 0.4233886925186112, 0.5600573116840106, 0.4840393506366769, 0.6273704457712411, 0.3670503178932433, 0.6434582381847278, 0.4460541887371646, 0.030737704918032786, 0.24034164457694315, 0.3396330729124818, 0.0054928579471321265, 0.6093357387110754, 0.15837993042539647, 0.285602819150117, 0.19522739944671205, 0.3399017176087483, 0.2941478463900067, 0.4679097219452999, 0.42948116361227917, 0.7252402184344416, 0.7485217524538075, 0.3274483992156299, 0.6932456051492414, 0.23872397387725358, 0.24407556330616093, 0.41897017918961993, 0.4892529337667858, 0.7172236973307329, 0.34584717804705517, 0.5668728595808763, 0.6192400366059405, 0.037933587527341654, 0.8325456279848094, 0.31587525992787996, 0.672510837990443, 0.4923540058531635, 0.5422095063041377, 0.4857985455438453, 0.45501506281403403, 0.5963196584679544, 0.7005941971593763, 0.7411150931786328, 0.5234889148918594, 0.4170875390754189, 0.3229521238482066, 0.5284013561936616, 0.7465209196874142, 0.6363479713206281, 0.4675224349129062, 0.757582765707045, 0.5087124513957664, 0.5331016632010154, 0.37858799378438523, 0.8136529722136168, 0.20638440876757547, 0.6923379753882963, 0.32656432856104, 0.48242785822807877, 0.022640703304440783, 0.4506011515162467, 0.4880084680038068, 0.08496395779361174, 0.6492456190567251, 0.4889085590558687, 0.3338533262076388, 0.5582705191525632, 0.6488921187769798, 0.6349969282683343, 0.38076652973507796, 0.5996511235653476, 0.5310144636193754, 0.18445748271756382, 0.5258342229819533, 0.5404733754068276, 0.6358512022711785, 0.7465142236930246, 0.32355191464975214, 0.16984312713262495, 0.6547902218197938, 0.22424025278384357, 0.5623995580134957, 0.42637436787182936, 0.25995458536558946, 0.36476039967756635, 0.42517356570387055, 0.5945258684873392, 0.31752220158846606, 0.216400743939147, 0.23432301129706046, 0.10771050939317175, 0.3043492714340631, 0.4453788934516314, 0.4761868081608477, 0.7267999121338631, 0.47532818233983637, 0.6564138230119855, 0.589241931723616, 0.3950979810283755, 0.6278805147103657, 0.5010040886766449, 0.4211978371785772, 0.000502475240546876, 0.11062436577392798, 0.4542802269513301, 0.16385851425563985, 0.49242311361274693, 0.21915462321141738, 0.4110615252433958, 0.6642868622778313, 0.27780132556460335, 0.5801097733517724, 0.5527904720253721, 0.6837434326300693, 0.36376080301226893, 0.040869995397102, 0.6013980487624446, 0.6365013086113771, 0.702254505587955, 0.4491697076302875, 0.43007495617950575, 0.6246489316067417, 0.6132395057271423, 0.44464300143103963, 0.5911435502566665, 0.4642155334165923, 0.17878509778351104, 0.7818059704215352, 0.34358858307221984, 0.6209254869062406, 0.3732513494798647, 0.29568151279667776, 0.6974078928147358, 0.5903122342346507, 0.7932980621333782, 0.4930048705746837, 0.4751555537817196, 0.2741554908077205, 0.3018143790880101, 0.1966513643989945, 0.7996903727381386, 0.012227022149555433, 0.8566136916167693, 0.5669585439646532, 0.5816683702587067, 0.49909398409907907, 0.6423617431823874, 0.6380535358896418, 0.37688091961771614, 0.3412678412594633, 0.02082551476374892, 0.5120386595262937, 0.8330372289219659, 0.580421004996692, 0.7139777528056741, 0.0028309605966927026, 0.3792374254595439, 0.32293748288523627, 0.3, 0.42211851217614466, 0.2662420824797139, 0.47243709960729613, 0.3466566694520998, 0.5264484818213396, 0.6948024038548473, 0.38144691329976066, 0.3654116910828676, 0.5868971692349365, 0.7741493307673125, 0.5328994317499005, 0.0019926727223996297, 0.5694906667291378, 0.5522881896558497, 0.12737393221273077, 0.5092355243716841, 0.5822578885967009, 0.5418452241688123, 0.601180320244157, 0.6145560058153254, 0.2894979637442565, 0.15267287935123397, 0.5107532621132764, 0.6571620264656003, 0.40425231059541894, 0.32400841203962094, 0.4771545908967949, 0.641505629444438, 0.3110341449314591, 0.659650700454717, 0.4757596651477546, 0.04273048581654647, 0.35642629457786407, 0.2503016484781048, 0.6128384806865608, 0.401883813575296, 0.7873512297369383, 0.2880011630509423, 0.6685344994917408, 0.0026785398426198287, 0.3401984727733568, 0.8198506920241847, 0.15161045850700805, 0.39046358638903034, 0.7595003606401665, 0.6502385390472154, 0.4837608030122689, 0.6561877150498429, 0.5420207042021676, 0.3011269963903412, 0.36734098317745745, 0.2595158321801061, 0.6800093672617663, 0.45481442008884976, 0.2599153292420633, 0.028933197288380404, 0.23246370986910248, 0.6487459159324842, 0.48227842129603177, 0.4737852154760634, 0.7113198969977166, 0.7942940886373231, 0.403919651200836, 0.4416729428848911, 0.3150952993706279, 0.27114736321709626, 0.21582450957426574, 0.5973676261054284, 0.3707416708384347, 0.5651440153904194, 0.7512537221372291, 0.41518107098462476, 0.6160243060483845, 0.6192465708556942, 0.5270519519364713, 0.7727874085673474, 0.1637001577426118, 0.4509180966275126, 0.1623389157644453, 0.2328919219721049, 0.28384312713262494, 0.39732504998771623, 0.13908130763195667, 0.606659349483055, 0.9129450509441277, 0.5906942363092953, 0.5480348523659682, 0.3657645979925872, 0.3815255210560815, 0.29183677033822475, 0.7308739246822539, 0.2546161241078564, 0.4764435994733433, 0.12397215648432605, 0.40075696068904565, 0.44651130718036297, 0.3917931459291674, 0.17467993527523543, 0.5085703901561733, 0.8013102243273795, 0.21321753318688544, 0.5421183135381283, 0.6276191520768977, 0.593002454982887, 0.6302441397282524, 0.021965202652627754, 0.422469281434252, 0.621989882747361, 0.120624950508485, 0.45139804564374686, 0.6392490185492855, 0.4097980946321579, 0.27540548488086025, 0.43110956226715, 0.45759927323283234, 0.32838148890748764, 0.8087956917909676, 0.0470018229138958, 0.5670475116598543, 0.621078725476998, 0.6782957202007542, 0.7192703579586016, 0.4844508867334484, 0.5476593027137127, 0.23541302336160563, 0.24281464187953433, 0.7464381287610005, 0.4878158173545226, 0.5794868957360608, 0.49469801776924244, 0.4153999617982131, 0.24628616822189267, 0.4714620798129416, 0.45618542185522554, 0.47612443323255327, 0.5961941588142162, 0.618324857144211, 0.5221748538685769, 0.28261602343131437, 0.3286239404656034, 0.645468245333815, 0.5601950179409128, 0.6812507015172684, 0.5298913360315008, 0.657296608487099, 0.4006791146090279, 0.6211246192756473, 0.23195919730320033, 0.3500770614354381, 0.3848722013844559, 0.6053580839870729, 0.3833125801853183, 0.5837097419067777, 0.7289698583989306, 0.35148074831026205, 0.7547891849509469, 0.41059775487998273, 0.6743188251669441, 0.5537430641703205, 0.5756436075668985, 0.3483186532016787, 0.22074839209000272, 0.4655532377037259, 0.7723907773233976, 0.1710703083498411, 0.3340067615511212, 0.2755315199245209, 0.3874239945593072, 0.6210542937185677, 0.19881773350190735, 0.4145039494514786, 0.5946430014310395, 0.3531452513043508, 0.7442128872226074, 0.3411702890804751, 0.4731533137595906, 0.1356318819950333, 0.4256157663747372, 0.16482697563242416, 0.4570492130667194, 0.5795736029362677, 0.47661129500651095, 0.28440560643517837, 0.3627035317778388, 0.5589933021249393, 0.3603906489138011, 0.4051029891758238, 0.36495011631060476, 0.7006888542894808, 0.6007096026157733, 0.3656751484744846, 0.3368304123506211, 0.5448653006140338, 0.3391729018917173, 0.5773584905660377, 0.470526358941186, 0.24220721897080444, 0.3743957421823191, 0.6272171600129595, 0.12109238965094564, 0.3852203240342541, 0.3289337438821843, 0.45321890991678565, 0.32217654513249894, 0.6517151723414214, 0.1670131054479986, 0.4379481360713408, 0.4810542937185676, 0.6018452241688124, 0.3048352802203207, 0.12589193410991184, 0.22033398145960237, 0.746018706541076, 0.5176251352035313, 0.5115532317792743, 0.6580282620568916, 0.2944758079604067, 0.6210410828218923, 0.6889379519220173, 0.5685203461577883, 0.485606508987049, 0.11935128822300436, 0.46680897081664524, 0.38350379167722165, 0.44689561067610595, 0.36453558623875865, 0.48383131978314786, 0.7438587849478016, 0.04182226733306222, 0.4013468712532189, 0.2384063991880515, 0.2756494627252351, 0.5401289278778265, 0.2078400943304258, 0.439162964721469, 0.36668662844021244, 0.1485548056394937, 0.1765641552343273, 0.5935581778969046, 0.6065616615619429, 0.2819387398438198, 0.013221159654461578, 0.32766477008231665, 0.16899091257400892, 0.7245465508165186, 0.7488055131404949, 0.6446763330941141, 0.6684465471977286, 0.5946191278049255, 0.1462705908316959, 0.26508692032733905, 0.40205382149790025, 0.6593787041582801, 0.6791230205778889, 0.41592730625846247, 0.19716714484691839, 0.48544389841267244, 0.35535208845134914, 0.483025431767644, 0.024695197869019792, 0.3703196928726723, 0.46221373819950085, 0.7809065301359059, 0.5448796975091883, 0.7157333519694777, 0.6111336413006742, 0.42202767234142147, 0.4367612805240397, 0.38713754745553103, 0.3676537391225073, 0.3775077486192382, 0.12982488101673478, 0.38693307896265094, 0.30358304297953514, 0.3712531928234882, 0.5993174781504255, 0.5288010642891718, 0.3288353873554516, 0.4125097937745835, 0.5362657118217875, 0.35038456005837204, 0.5404525751541039, 0.4020074759149159, 0.5951555537817196, 0.6564308072041063, 0.02379456854180656, 0.3524233600850303, 0.12652161938224732, 0.4117249267744172, 0.23018510200754672, 0.7625513570611567, 0.475387299307066, 0.19254922597772609, 0.06539441494582134, 0.8617416855359286, 0.23993587804517313]
Finish training and take 56m
