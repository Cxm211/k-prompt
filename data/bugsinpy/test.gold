class Register(abc.ABCMeta):          reg = OrderedDict()          for cls in cls._reg:              name = cls.task_family              if name in reg and reg[name] != cls and \
def box_df_fail(request):      return request.param  @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)  def box_with_array(request):
from twisted.python.failure import Failure  from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \      getAllReceivers, disconnect  from scrapy.xlib.pydispatch.robustapply import robustApply from scrapy.utils.log import failure_to_exc_info  logger = logging.getLogger(__name__)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class Line:          res = f'{first.prefix}{indent}{first.value}'          for leaf in leaves:              res += str(leaf)         for _, comment in self.comments:              res += str(comment)          return res + '\n'
from ..compat import (  )  from ..utils import (      int_or_none,     parse_duration,      parse_filesize,  )
def _depth_first_search(set_tasks, current_task, visited):          for task in current_task._requires():              if task not in visited:                  _depth_first_search(set_tasks, task, visited)             if task in set_tasks["ever_failed"] or task in set_tasks["upstream_failure"]:                  set_tasks["upstream_failure"].add(current_task)                  upstream_failure = True              if task in set_tasks["still_pending_ext"] or task in set_tasks["upstream_missing_dependency"]:
def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):          if dont_log is None or not isinstance(failure.value, dont_log):              logger.error("Error caught on signal handler: %(receiver)s",                           {'receiver': recv},                          exc_info=failure_to_exc_info(failure),                          extra={'spider': spider})          return failure      dont_log = named.pop('dont_log', None)
class Line:          return bool(self.leaves or self.comments)  @dataclass  class EmptyLineTracker:
class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):      for it.     _responseMatcher = re.compile(b'HTTP/1\.. 200')      def __init__(self, reactor, host, port, proxyConf, contextFactory,                   timeout=30, bindAddress=None):
def cartesian_product(X):          b = np.zeros_like(cumprodX)     return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]   def _tile_compat(arr, num: int):     if isinstance(arr, np.ndarray):         return np.tile(arr, num)       taker = np.tile(np.arange(len(arr)), num)     return arr.take(taker)
class FacebookGraphMixin(OAuth2Mixin):              future.set_exception(AuthError('Facebook auth error: %s' % str(response)))              return         args = urlparse.parse_qs(escape.native_str(response.body))          session = {              "access_token": args["access_token"][-1],              "expires": args.get("expires")
from datetime import datetime, timedelta from typing import Any, List, Optional, Union, cast  import numpy as np from pandas._libs import NaT, iNaT, join as libjoin, lib from pandas._libs.tslibs import timezones from pandas._typing import Label  from pandas.compat.numpy import function as nv from pandas.errors import AbstractMethodError from pandas.util._decorators import Appender, cache_readonly, doc  from pandas.core.dtypes.common import (     ensure_int64,     is_bool_dtype,      is_datetime64_any_dtype,      is_dtype_equal,     is_integer,      is_list_like,      is_object_dtype,      is_period_dtype,     is_scalar,      is_timedelta64_dtype,  ) from pandas.core.dtypes.concat import concat_compat from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries from pandas.core.dtypes.missing import isna  from pandas.core import algorithms from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin from pandas.core.base import IndexOpsMixin import pandas.core.indexes.base as ibase from pandas.core.indexes.base import Index, _index_shared_docs from pandas.core.indexes.extension import (     ExtensionIndex,     inherit_names,     make_wrapped_arith_op, ) from pandas.core.indexes.numeric import Int64Index from pandas.core.ops import get_op_result_name from pandas.core.tools.timedeltas import to_timedelta from pandas.tseries.frequencies import DateOffset, to_offset from pandas.tseries.offsets import Tick _index_doc_kwargs = dict(ibase._index_doc_kwargs) def _join_i8_wrapper(joinf, with_indexers: bool = True):     @staticmethod     def wrapper(left, right):         if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):             left = left.view("i8")         if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):             right = right.view("i8")         results = joinf(left, right)         if with_indexers:               dtype = left.dtype.base             join_index, left_indexer, right_indexer = results             join_index = join_index.view(dtype)             return join_index, left_indexer, right_indexer         return results     return wrapper def _make_wrapped_arith_op_with_freq(opname: str):     meth = make_wrapped_arith_op(opname)     def wrapped(self, other):         result = meth(self, other)         if result is NotImplemented:             return NotImplemented         new_freq = self._get_addsub_freq(other)         result._freq = new_freq          return result     wrapped.__name__ = opname     return wrapped @inherit_names(     ["inferred_freq", "_isnan", "_resolution", "resolution"],     DatetimeLikeArrayMixin,     cache=True, ) @inherit_names(     ["mean", "asi8", "_box_func"], DatetimeLikeArrayMixin, ) class DatetimeIndexOpsMixin(ExtensionIndex):     _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]     freq: Optional[DateOffset]     freqstr: Optional[str]     _resolution: int     _bool_ops: List[str] = []     _field_ops: List[str] = []         Gets called after a ufunc.         Determines if two Index objects contain the same elements.         Return sorted copy of Index.     @Appender(_index_shared_docs["take"] % _index_doc_kwargs)     def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):         nv.validate_take(tuple(), kwargs)         indices = ensure_int64(indices)         maybe_slice = lib.maybe_indices_to_slice(indices, len(self))         if isinstance(maybe_slice, slice):             return self[maybe_slice]         return ExtensionIndex.take(             self, indices, axis, allow_fill, fill_value, **kwargs         )     def _convert_tolerance(self, tolerance, target):         tolerance = np.asarray(to_timedelta(tolerance).to_numpy())         if target.size != tolerance.size and tolerance.size > 1:             raise ValueError("list-like tolerance size must match target index size")         return tolerance     def tolist(self) -> List:         return list(self.astype(object))     def min(self, axis=None, skipna=True, *args, **kwargs):         nv.validate_min(args, kwargs)         nv.validate_minmax_axis(axis)         Returns the indices of the minimum values along an axis.         nv.validate_argmin(args, kwargs)         nv.validate_minmax_axis(axis)         Return the maximum value of the Index or maximum along         an axis.         See Also         --------         numpy.ndarray.max         Series.max : Return the maximum value in a Series.         Returns the indices of the maximum values along an axis.         See `numpy.ndarray.argmax` for more information on the         `axis` parameter.         See Also         --------         numpy.ndarray.argmax         Return a list of tuples of the (attr,formatted_value).     def _validate_partial_date_slice(self, reso: str):         raise NotImplementedError         Parameters         ----------         reso : str         parsed : datetime         use_lhs : bool, default True         use_rhs : bool, default True          Returns          -------         slice or ndarray[intp]                left = i8vals.searchsorted(unbox(t1), side="left") if use_lhs else None             right = i8vals.searchsorted(unbox(t2), side="right") if use_rhs else None             return slice(left, right)          else:             lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True             rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True              return (lhs_mask & rhs_mask).nonzero()[0]        def _get_addsub_freq(self, other) -> Optional[DateOffset]:         if is_period_dtype(self.dtype):              return self.freq         elif self.freq is None:             return None         elif lib.is_scalar(other) and isna(other):             return None         elif isinstance(other, (Tick, timedelta, np.timedelta64)):             new_freq = None             if isinstance(self.freq, Tick):                 new_freq = self.freq             return new_freq         elif isinstance(other, DateOffset):              return None         elif isinstance(other, (datetime, np.datetime64)):             return self.freq         Compute boolean array of whether each index value is found in the         passed set of values.          Parameters          ----------         values : set or sequence of values          Returns          -------         is_contained : ndarray (boolean dtype)         Return a summarized representation.          Parameters          ----------         name : str             Name to use in the summary representation.          Returns          -------         str             Summarized representation of the index.          result = result.replace("'", "")         return result     def shift(self, periods=1, freq=None):         This method is for shifting the values of datetime-like indexes         by a specified time increment a given number of times.          Parameters          ----------         periods : int, default 1             Number of periods (or increments) to shift by,             can be positive or negative.         arr = self._data.view()         arr._freq = self.freq         result = arr._time_shift(periods, freq=freq)         return type(self)(result, name=self.name)       def delete(self, loc):         new_i8s = np.delete(self.asi8, loc)         freq = None         if is_period_dtype(self):             freq = self.freq         elif is_integer(loc):             if loc in (0, -len(self), -1, len(self) - 1):                 freq = self.freq         else:             if is_list_like(loc):                 loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))             if isinstance(loc, slice) and loc.step in (1, None):                 if loc.start in (0, None) or loc.stop in (len(self), None):                     freq = self.freq         arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)         return type(self)._simple_new(arr, name=self.name)   class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):      _is_monotonic_increasing = Index.is_monotonic_increasing     _is_monotonic_decreasing = Index.is_monotonic_decreasing     _is_unique = Index.is_unique     _freq = lib.no_default      @property      def freq(self):         if self._freq is not lib.no_default:             return self._freq         return self._data.freq      @property      def freqstr(self):
class tqdm(Comparable):                  else TqdmKeyError("Unknown argument(s): " + str(kwargs)))          if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \ dynamic_ncols:              if dynamic_ncols:
class GroupBy(_GroupBy):                      axis=axis,                  )              )         if fill_method is None:             fill_method = "pad"             limit = 0          filled = getattr(self, fill_method)(limit=limit)          fill_grp = filled.groupby(self.grouper.codes)          shifted = fill_grp.shift(periods=periods, freq=freq)
def ctc_batch_cost(y_true, y_pred, input_length, label_length):          Tensor with shape (samples,1) containing the              CTC loss of each element.     label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))     input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))      sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))      y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later.     if isinstance(line, UnformattedLines) or line.is_comment:          yield line          return      line_str = str(line).strip('\n')     if (         len(line_str) <= line_length         and '\n' not in line_str         and not line.contains_standalone_comments     ):          yield line          return     split_funcs: List[SplitFunc]      if line.is_def:          split_funcs = [left_hand_split]      elif line.inside_brackets:         split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]      else:          split_funcs = [right_hand_split]      for split_func in split_funcs:
__metaclass__ = type  from ansible.module_utils.six import string_types  from ansible.playbook.attribute import FieldAttribute  from ansible.utils.collection_loader import AnsibleCollectionLoader from ansible.template import is_template, Environment from ansible.utils.display import Display  display = Display()  def _ensure_default_collection(collection_list=None):
def _get_grouper( elif is_in_axis(gpr):              if gpr in obj:                  if validate:                     obj._check_label_or_level_ambiguity(gpr, axis=axis)                  in_axis, name, gpr = True, gpr, obj[gpr]                  exclusions.append(name)             elif obj._is_level_reference(gpr, axis=axis):                  in_axis, name, level, gpr = False, None, gpr, None              else:                  raise KeyError(gpr)
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = safe_url_string(response.headers['Location'])         if response.headers['Location'].startswith(b'//'):             request_scheme = urlparse(request.url).scheme             location = request_scheme + '://' + location.lstrip('/')          redirected_url = urljoin(request.url, location)
class DatetimeTZDtype(PandasExtensionDtype):          if isinstance(string, str):              msg = "Could not construct DatetimeTZDtype from '{}'"             match = cls._match.match(string)             if match:                 d = match.groupdict()                 try:                      return cls(unit=d["unit"], tz=d["tz"])                 except (KeyError, TypeError, ValueError) as err:                         raise TypeError(msg.format(string)) from err              raise TypeError(msg.format(string))          raise TypeError("Could not construct DatetimeTZDtype")
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
import os  from ..conf import settings from ..utils import memoize  from .generic import Generic
def hide_fmt_off(node: Node) -> bool:  def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:      container: Optional[LN] = container_of(leaf)     while container is not None and container.type != token.ENDMARKER:          for comment in list_comments(container.prefix, is_endmarker=False):              if comment.value in FMT_ON:                  return
def js_to_json(code):          '(?:[^'\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^'\\]*'|          /\*.*?\*/|,(?=\s*[\]}])|          [a-zA-Z_][.a-zA-Z_0-9]*|         \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|          [0-9]+(?=\s*:)
class CParserWrapper(ParserBase):             if isinstance(src, (BufferedIOBase, RawIOBase)):                  src = TextIOWrapper(src, encoding=encoding, newline="")              kwds["encoding"] = "utf-8"
class S3CopyToTable(rdbms.CopyToTable):          if not (self.table):              raise Exception("table need to be specified")         path = self.s3_load_path          connection = self.output().connect()          if not self.does_table_exist(connection):
class ExtractorError(Exception):              expected = True          if video_id is not None:              msg = video_id + ': ' + msg         if cause:             msg += u' (caused by %r)' % cause          if not expected:              msg = msg + u'; please report this issue on https://yt-dl.org/bug . Be sure to call youtube-dl with the --verbose flag and include its complete output. Make sure you are using the latest version; type  youtube-dl -U  to update.'          super(ExtractorError, self).__init__(msg)
class KeyEvent(LocationEvent):          self.key = key def _get_renderer(figure, print_method=None):
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)          if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):             if lib.is_scalar(other):                  return Timestamp(other) - self              if not isinstance(other, DatetimeLikeArrayMixin):                  from pandas.core.arrays import DatetimeArray
def match(command):  @git_support  def get_new_command(command):     return replace_argument(command.script, 'push', 'push --force-with-lease')  enabled_by_default = False
import warnings  from .. import backend as K  from .. import losses from ..utils import Sequence  from ..utils.generic_utils import to_list
class RadialLocator(mticker.Locator):          return self.base.refresh()     def nonsingular(self, vmin, vmax):          return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)                 else self.base.nonsingular(vmin, vmax))       def view_limits(self, vmin, vmax):          vmin, vmax = self.base.view_limits(vmin, vmax)          if vmax > vmin:
from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.missing import isna from pandas._typing import Dtype  from pandas.core import algorithms  import pandas.core.common as com  from pandas.core.indexes.base import Index, InvalidIndexError, _index_shared_docs
class TimeseriesGenerator(Sequence):      def __getitem__(self, index):          if self.shuffle:              rows = np.random.randint(                 self.start_index, self.end_index + 1, size=self.batch_size)          else:              i = self.start_index + self.batch_size * self.stride * index              rows = np.arange(i, min(i + self.batch_size *                                     self.stride, self.end_index + 1), self.stride)          samples, targets = self._empty_batch(len(rows))          for j, row in enumerate(rows):
def match(command):  def _parse_operations(help_text_lines):     operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)      return operation_regex.findall(help_text_lines)
def generate_tokens(readline):      contline = None      indents = [0]       async_is_reserved_keyword = config.async_is_reserved_keyword      stashed = None      async_def = False
def _right_outer_join(x, y, max_groups):      return left_indexer, right_indexer def _factorize_keys(     lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = "inner" ) -> Tuple[np.array, np.array, int]:      lk = extract_array(lk, extract_numpy=True)      rk = extract_array(rk, extract_numpy=True)
class Function(object):              callable_opts.fetch.append(x.name)          callable_opts.target.append(self.updates_op.name)          if self.run_options:             callable_opts.run_options.CopyFrom(self.run_options)          callable_fn = session._make_callable_from_options(callable_opts)
def test_multiprocessing_fit_error():          for i in range(good_batches):              yield (np.random.randint(batch_size, 256, (50, 2)),                    np.random.randint(batch_size, 12, 50))          raise RuntimeError      model = Sequential()
def to_pickle(obj, path, compression="infer", protocol=pickle.HIGHEST_PROTOCOL):          f.close()          for _f in fh:              _f.close()         if should_close:             try:                 fp_or_buf.close()             except ValueError:                 pass def read_pickle(     filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = "infer" ):      Load pickled pandas object (or any object) from file.
def generate_tokens(readline):                          stashed = tok                          continue                     if token in ('def', 'for'):                          if (stashed                                  and stashed[0] == NAME                                  and stashed[1] == 'async'):                             if token == 'def':                                 async_def = True                                 async_def_indent = indents[-1]                              yield (ASYNC, stashed[1],                                     stashed[2], stashed[3],
def match(command):  def get_new_command(command):     return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))  def side_effect(old_cmd, command):
class BaseGrouper:                  pass              else:                  raise         except TypeError as err:             if "ndarray" in str(err):                  pass             else:                 raise         return self._aggregate_series_pure_python(obj, func)      def _aggregate_series_fast(self, obj, func):          func = self._is_builtin_func(func)
import numpy as np  from pandas._libs import NaT, Period, Timestamp, index as libindex, lib, tslib as libts  from pandas._libs.tslibs import fields, parsing, timezones from pandas._typing import DtypeObj, Label  from pandas.util._decorators import cache_readonly from pandas.core.dtypes.common import (     _NS_DTYPE,     is_datetime64_any_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,     is_float,     is_integer,     is_scalar, )  from pandas.core.dtypes.missing import is_valid_nat_for_dtype  from pandas.core.arrays.datetimes import DatetimeArray, tz_to_dtype
class YAxis(Axis):      def get_minpos(self):          return self.axes.dataLim.minposy     def set_inverted(self, inverted):          a, b = self.get_view_interval()         self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)       def set_default_intervals(self):          ymin, ymax = 0., 1.
def parse_iso8601(date_str, delimiter='T'):          return None      m = re.search(         r'(\.[0-9]+)?(?:Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',          date_str)      if not m:          timezone = datetime.timedelta()
from .logs import debug  Command = namedtuple('Command', ('script', 'stdout', 'stderr'))  Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',                             'enabled_by_default', 'side_effect',                             'priority', 'requires_output')) class CorrectedCommand(object):     def __init__(self, script, side_effect, priority):         self.script = script         self.side_effect = side_effect         self.priority = priority      def __eq__(self, other):
def text_to_word_sequence(text,      if lower:          text = text.lower()     if sys.version_info < (3,):         if isinstance(text, unicode):             translate_map = dict((ord(c), unicode(split)) for c in filters)             text = text.translate(translate_map)         elif len(split) == 1:             translate_map = maketrans(filters, split * len(filters))             text = text.translate(translate_map)         else:             for c in filters:                 text = text.replace(c, split)      else:         translate_dict = dict((c, split) for c in filters)         translate_map = maketrans(translate_dict)         text = text.translate(translate_map)      seq = text.split(split)      return [i for i in seq if i]
class Conv2DTranspose(Conv2D):                                                          stride_h,                                                          kernel_h,                                                          self.padding,                                                         out_pad_h,                                                         self.dilation_rate[0])          output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],                                                          stride_w,                                                          kernel_w,                                                          self.padding,                                                         out_pad_w,                                                         self.dilation_rate[1])          return tuple(output_shape)      def get_config(self):          config = super(Conv2DTranspose, self).get_config()          config['output_padding'] = self.output_padding          return config
def crosstab(          **kwargs,      )       if not table.empty:         cols_diff = df.columns.difference(original_df_cols)[0]         table = table[cols_diff]       if normalize is not False:          table = _normalize(
def init_ndarray(values, index, columns, dtype=None, copy=False):          return arrays_to_mgr([values], columns, index, columns, dtype=dtype)      elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):          if isinstance(values, np.ndarray) and values.ndim > 1:               values = [values[:, n] for n in range(values.shape[1])]         else:             values = [values]           if columns is None:             columns = list(range(len(values)))         return arrays_to_mgr(values, columns, index, columns, dtype=dtype)
class Axes(_AxesBase):      @_preprocess_data(replace_names=["x", "ymin", "ymax", "colors"],                        label_namer="x")     def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',                 label='', **kwargs):          Plot vertical lines.
def get_source_from_frame(frame):      if isinstance(source[0], bytes):         encoding = 'utf-8'          for line in source[:2]:
class DataFrameGroupBy(GroupBy):                          result = type(block.values)._from_sequence(                              result.ravel(), dtype=block.values.dtype                          )                     except (ValueError, TypeError):                          result = result.reshape(1, -1)
import base64  import io  import itertools  import os  import time  import xml.etree.ElementTree as etree  from .common import FileDownloader  from .http import HttpFD  from ..utils import (     struct_pack,     struct_unpack,      compat_urllib_request,      compat_urlparse,      format_bytes,
def parse_age_limit(s):  def strip_jsonp(code):     return re.sub(         r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)  def js_to_json(code):
def _normalize(table, normalize, margins, margins_name="All"):              column_margin = column_margin / column_margin.sum()              table = concat([table, column_margin], axis=1)              table = table.fillna(0)             table.columns = table_columns          elif normalize == "index":              index_margin = index_margin / index_margin.sum()              table = table.append(index_margin)              table = table.fillna(0)             table.index = table_index          elif normalize == "all" or normalize is True:              column_margin = column_margin / column_margin.sum()
class TimeGrouper(Grouper):            binner = labels = date_range(              freq=self.freq,              start=first,              end=last,              tz=ax.tz,              name=ax.name,             ambiguous=True,              nonexistent="shift_forward",          )
from twisted.python.failure import Failure  from scrapy.utils.defer import mustbe_deferred, defer_result  from scrapy.utils.request import request_fingerprint  from scrapy.utils.misc import arg_to_iter from scrapy.utils.log import failure_to_exc_info  logger = logging.getLogger(__name__)
class Parameter(object):          if dest is not None:              value = getattr(args, dest, None)              if value:                 self.set_global(self.parse_from_input(param_name, value, task_name=task_name)) else:                  self.reset_global()
class ArteTVPlus7IE(InfoExtractor):          info = self._download_json(json_url, video_id)          player_info = info['videoJsonPlayer']         upload_date_str = player_info.get('shootingDate')         if not upload_date_str:             upload_date_str = player_info.get('VDA', '').split(' ')[0]           info_dict = {              'id': player_info['VID'],              'title': player_info['VTI'],              'description': player_info.get('VDE'),             'upload_date': unified_strdate(upload_date_str),              'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),          }
def cut(      x = _preprocess_for_cut(x)      x, dtype = _coerce_to_type(x)        if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):         x = x.to_numpy(dtype=object, na_value=np.nan)       if not np.iterable(bins):          if is_scalar(bins) and bins < 1:              raise ValueError("`bins` should be a positive integer.")
class Parser:                  if (new_data == data).all():                      data = new_data                      result = True             except (TypeError, ValueError, OverflowError):                  pass
def format_stdin_to_stdout(      finally:          if write_back == WriteBack.YES:             f = io.TextIOWrapper(                 sys.stdout.buffer,                 encoding=encoding,                 newline=newline,                 write_through=True,             )             f.write(dst)             f.detach()          elif write_back == WriteBack.DIFF:              src_name = "<stdin>  (original)"              dst_name = "<stdin>  (formatted)"             f = io.TextIOWrapper(                 sys.stdout.buffer,                 encoding=encoding,                 newline=newline,                 write_through=True,             )             f.write(diff(src, dst, src_name, dst_name))             f.detach()  def format_file_contents(
def parse_url(url, encoding=None):          Data are returned as a list of name, value pairs as bytes.          Arguments:          qs: percent-encoded query string to be parsed          keep_blank_values: flag indicating whether blank values in             percent-encoded queries should be treated as blank strings.  A             true value indicates that blanks should be retained as blank             strings.  The default false value indicates that blank values             are to be ignored and treated as if they were  not included.          strict_parsing: flag indicating what to do with parsing errors. If             false (the default), errors are silently ignored. If true,             errors raise a ValueError exception.
default 'raise'          from pandas import DataFrame         if self.tz is not None and not timezones.is_utc(self.tz):             values = self._local_timestamps()         else:             values = self.asi8         sarray = fields.build_isocalendar_sarray(values)          iso_calendar_df = DataFrame(              sarray, columns=["year", "week", "day"], dtype="UInt32"          )
def fit_generator(model,                      cbk.validation_data = val_data          if workers > 0:             if use_sequence_api:                  enqueuer = OrderedEnqueuer(                      generator,                      use_multiprocessing=use_multiprocessing,
class Rolling(_Rolling_and_Expanding):      def count(self):           if self.is_freq_type or isinstance(self.window, BaseIndexer):              window_func = self._get_roll_func("roll_count")              return self._apply(window_func, center=self.center, name="count")
def test_multiprocessing_fit_error():  def test_multiprocessing_evaluate_error():      batch_size = 10      good_batches = 3     workers = 4      def custom_generator():          for i in range(good_batches):              yield (np.random.randint(batch_size, 256, (50, 2)),                    np.random.randint(batch_size, 12, 50))          raise RuntimeError      model = Sequential()      model.add(Dense(1, input_shape=(2, )))      model.compile(loss='mse', optimizer='adadelta')     with pytest.raises(RuntimeError):          model.evaluate_generator(             custom_generator(), good_batches * workers + 1, 1,             workers=workers, use_multiprocessing=True,          )     with pytest.raises(RuntimeError):          model.evaluate_generator(              custom_generator(), good_batches + 1, 1,              use_multiprocessing=False,
class APIRouter(routing.Router):                      summary=route.summary,                      description=route.description,                      response_description=route.response_description,                     responses=combined_responses,                      deprecated=route.deprecated,                      methods=route.methods,                      operation_id=route.operation_id,
class ReduceLROnPlateau(Callback):                  self.best = current                  self.wait = 0              elif not self.in_cooldown():                 self.wait += 1                  if self.wait >= self.patience:                      old_lr = float(K.get_value(self.model.optimizer.lr))                      if old_lr > self.min_lr:
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 if is_sequence:                     output_generator = iter(generator)                 else:                     output_generator = generator              while steps_done < steps:                  generator_output = next(output_generator)
class Model(Container):                              ' and multiple workers may duplicate your data.'                              ' Please consider using the`keras.utils.Sequence'                              ' class.'))         if steps is None:             if is_sequence:                 steps = len(generator)             else:                 raise ValueError('`steps=None` is only valid for a generator'                                  ' based on the `keras.utils.Sequence` class.'                                  ' Please specify `steps` or use the'                                  ' `keras.utils.Sequence` class.')          enqueuer = None          try:
def find_hook(hook_name, hooks_dir='hooks'):          logger.debug('No hooks/dir in template_dir')          return None     scripts = []      for hook_file in os.listdir(hooks_dir):          if valid_hook(hook_file, hook_name):             scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))     if len(scripts) == 0:         return None     return scripts  def run_script(script_path, cwd='.'):
class Model(Container):                                           str(generator_output))                      batch_logs = {}                     if x is None or len(x) == 0:                           batch_size = 1                     elif isinstance(x, list):                          batch_size = x[0].shape[0]                      elif isinstance(x, dict):                          batch_size = list(x.values())[0].shape[0]
class Conv2DTranspose(Conv2D):              strides=strides,              padding=padding,              data_format=data_format,             dilation_rate=dilation_rate,              activation=activation,              use_bias=use_bias,              kernel_initializer=kernel_initializer,
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _get_combined_index(              index = index.sort_values()          except TypeError:              pass       if copy:         index = index.copy()       return index
class Network(Layer):                  else:                      raise ValueError('Improperly formatted model config.')                  inbound_layer = created_layers[inbound_layer_name]                    if len(inbound_layer._inbound_nodes) <= inbound_node_index:                     raise LookupError                  inbound_node = inbound_layer._inbound_nodes[inbound_node_index]                  input_tensors.append(                      inbound_node.output_tensors[inbound_tensor_index])               if input_tensors:
import numpy as np  from pandas.core.dtypes.common import is_list_like  def cartesian_product(X):
class Parameter(object):          dest = self.parser_dest(param_name, task_name, glob=False)          if dest is not None:              value = getattr(args, dest, None)             params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)      def set_global_from_args(self, param_name, task_name, args, is_without_section=False):
class WebSocketProtocol13(WebSocketProtocol):          self.write_ping(b"")          self.last_ping = now     def set_nodelay(self, x: bool) -> None:         self.stream.set_nodelay(x)   class WebSocketClientConnection(simple_httpclient._HTTPConnection):
from pandas.core.dtypes.generic import ABCMultiIndex  from pandas.core.dtypes.missing import notna  from pandas.core.arrays import Categorical import pandas.core.common as com  from pandas.core.frame import DataFrame, _shared_docs  from pandas.core.indexes.base import Index  from pandas.core.reshape.concat import concat
import calendar import codecs  import contextlib  import ctypes  import datetime
CPU_INFO_TEST_SCENARIOS = [                  '7', 'POWER7 (architected), altivec supported'              ],              'processor_cores': 1,             'processor_count': 8,              'processor_threads_per_core': 1,             'processor_vcpus': 8          },      },      {
class Sequential(Model):                      first_layer = layer.layers[0]                      while isinstance(first_layer, (Model, Sequential)):                          first_layer = first_layer.layers[0]                  if hasattr(first_layer, 'batch_input_shape'):                      batch_shape = first_layer.batch_input_shape
def infer_dtype_from_scalar(val, pandas_dtype: bool = False):          if lib.is_period(val):              dtype = PeriodDtype(freq=val.freq)              val = val.ordinal         elif lib.is_interval(val):             subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]             dtype = IntervalDtype(subtype=subtype)      return dtype, val
from pandas.core.internals.construction import (      sanitize_index,      to_arrays,  ) from pandas.core.ops.missing import dispatch_fill_zeros  from pandas.core.series import Series  from pandas.io.formats import console, format as fmt
def match(command, settings):  def get_new_command(command, settings):     return command.script.replace('open ', 'open http://')
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts["2007"]          expected = ts[1:3]          tm.assert_series_equal(result, expected)          result[:] = 1
class FacebookIE(InfoExtractor):              'duration': 38,              'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',          }     }, {         'note': 'Video without discernible title',         'url': 'https://www.facebook.com/video.php?v=274175099429670',         'info_dict': {             'id': '274175099429670',             'ext': 'mp4',             'title': 'Facebook video         }      }, {          'url': 'https://www.facebook.com/video.php?v=10204634152394104',          'only_matching': True,
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class StackedRNNCells(Layer):         new_states = []         if self.reverse_state_order:             new_nested_states = new_nested_states[::-1]         for cell_states in new_nested_states:             new_states += cell_states         return inputs, new_states      def build(self, input_shape):          if isinstance(input_shape, list):
class PolarAxes(Axes):      @cbook._delete_parameter("3.3", "args")      @cbook._delete_parameter("3.3", "kwargs")      def draw(self, renderer, *args, **kwargs):         self._unstale_viewLim()          thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)          if thetamin > thetamax:              thetamin, thetamax = thetamax, thetamin
class _Rolling_and_Expanding(_Rolling):              raise ValueError("engine must be either 'numba' or 'cython'")           return self._apply(              apply_func,              center=False,              floor=0,              name=func,              use_numba_cache=engine == "numba",             raw=raw,          )      def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):
class Conv2DTranspose(Conv2D):                   padding='valid',                   output_padding=None,                   data_format=None,                  dilation_rate=(1, 1),                   activation=None,                   use_bias=True,                   kernel_initializer='glorot_uniform',
def assert_output(output, expected_entries, prefix=None):      any_mismatch = False      result = ''     template = u'\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))      for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=""):          mismatch = not (expected_entry and expected_entry.check(line))          any_mismatch |= mismatch
import asyncio  from asyncio.base_events import BaseEventLoop  from concurrent.futures import Executor, ProcessPoolExecutor from functools import partial, wraps  import keyword  import os  from pathlib import Path  import tokenize  import sys  from typing import (     Callable,     Dict,     Generic,     Iterable,     Iterator,     List,     Optional,     Set,     Tuple,     Type,     TypeVar,     Union,  )  from attr import dataclass, Factory
def cli_option(params, command_option, param):  def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):      param = params.get(param)     if param is None:         return []      assert isinstance(param, bool)      if separator:          return [command_option + separator + (true_value if param else false_value)]
def any_int_dtype(request):      return request.param @pytest.fixture(params=ALL_EA_INT_DTYPES) def any_nullable_int_dtype(request):      return request.param    @pytest.fixture(params=ALL_REAL_DTYPES)  def any_real_dtype(request):
class APIRouter(routing.Router):          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,         route_class_override: Optional[Type[APIRoute]] = None,      ) -> None:         route_class = route_class_override or self.route_class         route = route_class(              path,              endpoint=endpoint,              response_model=response_model,
def serialize_response(              errors.extend(errors_)          if errors:              raise ValidationError(errors)         if skip_defaults and isinstance(response, BaseModel):             value = response.dict(skip_defaults=skip_defaults)          return jsonable_encoder(              value,              include=include,
class PamdRule(PamdLine):      valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']      def __init__(self, rule_type, rule_control, rule_path, rule_args=None):         self.prev = None         self.next = None          self._control = None          self._args = None          self.rule_type = rule_type
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):      _data = None      _engine_type = libindex.PeriodEngine     _supports_partial_string_indexing = True
from __future__ import unicode_literals  from .subtitles import SubtitlesInfoExtractor  from .common import ExtractorError  from ..utils import parse_iso8601
class DRTVIE(SubtitlesInfoExtractor):          title = data['Title']          description = data['Description']         timestamp = parse_iso8601(data['CreatedTime'])          thumbnail = None          duration = None
def rnn(step_function, inputs, initial_states,                  tiled_mask_t = tf.tile(mask_t,                                         tf.stack([1, tf.shape(output)[1]]))                  output = tf.where(tiled_mask_t, output, states[0])                 new_states = [                     tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),                              new_states[i], states[i]) for i in range(len(states))                 ]                  output_ta_t = output_ta_t.write(time, output)                  return (time + 1, output_ta_t) + tuple(new_states)          else:
class inherits(object):          self.task_to_inherit = task_to_inherit      def __call__(self, task_that_inherits):           for param_name, param_obj in self.task_to_inherit.get_params():               if not hasattr(task_that_inherits, param_name):                   setattr(task_that_inherits, param_name, param_obj)          def clone_parent(_self, **args):             return _self.clone(cls=self.task_to_inherit, **args)         task_that_inherits.clone_parent = clone_parent         return task_that_inherits  class requires(object):
class Series(base.IndexOpsMixin, generic.NDFrame):          if takeable:              return self._values[label]          loc = self.index.get_loc(label)         return self.index._get_values_for_loc(self, loc, label)      def __setitem__(self, key, value):          key = com.apply_if_callable(key, self)
def _convert_listlike_datetimes(                  return DatetimeIndex(arg, tz=tz, name=name)              except ValueError:                  pass         elif tz:              return arg.tz_localize(tz)          return arg
class _AxesBase(martist.Artist):          bottom, top = sorted([bottom, top], reverse=bool(reverse))          self._viewLim.intervaly = (bottom, top)          for ax in self._shared_y_axes.get_siblings(self):             ax._stale_viewlim_y = False          if auto is not None:              self._autoscaleYon = bool(auto)
class NumericIndex(Index):      _is_numeric_dtype = True      def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):         cls._validate_dtype(dtype)          if fastpath is not None:              warnings.warn(                  "The 'fastpath' keyword is deprecated, and will be "
class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):                  raise ValueError(                      "Cannot specify `categories` or `ordered` together with `dtype`."                  )             elif not isinstance(dtype, CategoricalDtype):                 raise ValueError(f"Cannot not construct CategoricalDtype from {dtype}")          elif is_categorical(values):
class HttpProxyMiddleware(object):          proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))          if user:             user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))              creds = base64.b64encode(user_pass).strip()          else:              creds = None
def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:      ):          for leaf in leaves:              result.append(leaf, preformatted=True)             for comment_after in line.comments_after(leaf):                  result.append(comment_after, preformatted=True)      bracket_split_succeeded_or_raise(head, body, tail)      for result in (head, body, tail):
def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,      padding = _preprocess_padding(padding)      if tf_data_format == 'NHWC':          spatial_start_dim = 1         strides = (1,) + strides * 2 + (1,)      else:          spatial_start_dim = 2         strides = (1, 1) + strides * 2      x = tf.expand_dims(x, spatial_start_dim)      depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)      pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)
def assert_equivalent(src: str, dst: str) -> None:      try:          src_ast = ast.parse(src)      except Exception as exc:         major, minor = sys.version_info[:2]         raise AssertionError(             f"cannot use --safe with this file; failed to parse source file "             f"with Python {major}.{minor}'s builtin AST. Re-run with --fast "             f"or stop using deprecated Python 2 syntax. AST error message: {exc}"         )      try:          dst_ast = ast.parse(dst)
def read_pickle(path, compression="infer"):      Parameters      ----------     filepath_or_buffer : str, path object or file-like object         File path, URL, or buffer where the pickled object will be loaded from.          .. versionchanged:: 1.0.0            Accept URL. URL is not limited to S3 and GCS.       compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'         If 'infer' and 'path_or_url' is path-like, then detect compression from         the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no         compression) If 'infer' and 'path_or_url' is not path-like, then use         None (= no decompression).      Returns      -------
from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers  import pandas._libs.internals as libinternals  from pandas._libs.tslibs import Timedelta, conversion  from pandas._libs.tslibs.timezones import tz_compare from pandas._typing import ArrayLike  from pandas.util._validators import validate_bool_kwarg  from pandas.core.dtypes.cast import (
class Model(Container):      @interfaces.legacy_generator_methods_support      def fit_generator(self,                        generator,                       steps_per_epoch=None,                        epochs=1,                        verbose=1,                        callbacks=None,
def _get_comments(group_tasks):  _ORDERED_STATUSES = (      "already_done",      "completed",     "ever_failed",      "failed",      "scheduling_error",      "still_pending",
class DebugVisitor(Visitor[T]):              out(f" {node.value!r}", fg="blue", bold=False)      @classmethod     def show(cls, code: Union[str, Leaf, Node]) -> None:          v: DebugVisitor[None] = DebugVisitor()         if isinstance(code, str):             code = lib2to3_parse(code)         list(v.visit(code))  KEYWORDS = set(keyword.kwlist)
def diff(arr, n: int, axis: int = 0):      dtype = arr.dtype      is_timedelta = False     is_bool = False      if needs_i8_conversion(arr):          dtype = np.float64          arr = arr.view("i8")
from ansible.galaxy.role import GalaxyRole  from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel  from ansible.module_utils.ansible_release import __version__ as ansible_version  from ansible.module_utils._text import to_bytes, to_native, to_text from ansible.module_utils import six  from ansible.parsing.yaml.loader import AnsibleLoader  from ansible.playbook.role.requirement import RoleRequirement  from ansible.utils.display import Display  from ansible.utils.plugin_docs import get_versioned_doclink  display = Display() urlparse = six.moves.urllib.parse.urlparse  class GalaxyCLI(CLI):
import hmac  import time  import uuid from tornado.concurrent import TracebackFuture, return_future, chain_future  from tornado import gen  from tornado import httpclient  from tornado import escape
class CombinedDatetimelikeProperties(          orig = data if is_categorical_dtype(data) else None          if orig is not None:             data = Series(                 orig.array,                 name=orig.name,                 copy=False,                 dtype=orig.values.categories.dtype,             )          if is_datetime64_dtype(data.dtype):              return DatetimeProperties(data, orig)
class EventCollection(LineCollection):          .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py         if positions is None:             raise ValueError('positions must be an array-like object')          positions = np.array(positions, copy=True)          segment = (lineoffset + linelength / 2.,                     lineoffset - linelength / 2.)         if positions.size == 0:              segments = []         elif positions.ndim > 1:              raise ValueError('positions cannot be an array with more than '                               'one dimension.')          elif (orientation is None or orientation.lower() == 'none' or
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):      @Substitution(klass="PeriodIndex")      @Appender(_shared_docs["searchsorted"])      def searchsorted(self, value, side="left", sorter=None):         if isinstance(value, Period) or value is NaT:             self._data._check_compatible_with(value)          elif isinstance(value, str):              try:                 value = Period(value, freq=self.freq)              except DateParseError:                  raise KeyError(f"Cannot interpret '{value}' as period")         elif not isinstance(value, PeriodArray):             raise TypeError(                 "PeriodIndex.searchsorted requires either a Period or PeriodArray"             )         return self._data.searchsorted(value, side=side, sorter=sorter)      @property      def is_full(self) -> bool:
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def load_model_from_path(model_path, meta=False, **overrides):      for name in pipeline:          if name not in disable:              config = meta.get("pipeline_args", {}).get(name, {})             config.update(overrides)              factory = factories.get(name, name)              component = nlp.create_pipe(factory, config=config)              nlp.add_pipe(component, name=name)
class Index(IndexOpsMixin, PandasObject):         s = extract_array(series, extract_numpy=True)         if isinstance(s, ExtensionArray):             if is_scalar(key):                      try:                     iloc = self.get_loc(key)                     return s[iloc]                 except KeyError:                     if len(self) > 0 and (self.holds_integer() or self.is_boolean()):                         raise                     elif is_integer(key):                         return s[key]             else:                   raise InvalidIndexError(key)          s = com.values_from_object(series)          k = com.values_from_object(key)
def jsonable_encoder(      by_alias: bool = True,      skip_defaults: bool = None,      exclude_unset: bool = False,     exclude_defaults: bool = False,     exclude_none: bool = False,      custom_encoder: dict = {},      sqlalchemy_safe: bool = True,  ) -> Any:
class Schema(SchemaBase): not_: Optional[List[SchemaBase]] = PSchema(None, alias="not")      items: Optional[SchemaBase] = None      properties: Optional[Dict[str, SchemaBase]] = None     additionalProperties: Optional[Union[SchemaBase, bool]] = None  class Example(BaseModel):
def run_script_with_context(script_path, cwd, context):      ) as temp:          temp.write(Template(contents).render(**context))     run_script(temp.name, cwd)  def run_hook(hook_name, project_dir, context):
def _arith_method_FRAME(cls, op, special):      @Appender(doc)      def f(self, other, axis=default_axis, level=None, fill_value=None):         if _should_reindex_frame_op(             self, other, op, axis, default_axis, fill_value, level         ):              return _frame_arith_method_with_reindex(self, other, op)          self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)
class Block(PandasObject):          mask = missing.mask_missing(values, to_replace)          try:              blocks = self.putmask(mask, value, inplace=inplace)
def reformat_many(          )      finally:          shutdown(loop)         if executor is not None:             executor.shutdown()  async def schedule_formatting(
class KeyValueType(object):      def __init__(self, *separators):          self.separators = separators         self.escapes = ['\\\\' + sep for sep in separators]      def __call__(self, string):          found = {}         found_escapes = []         for esc in self.escapes:             found_escapes += [m.span() for m in re.finditer(esc, string)]          for sep in self.separators:             matches = re.finditer(sep, string)             for match in matches:                 start, end = match.span()                 inside_escape = False                 for estart, eend in found_escapes:                     if start >= estart and end <= eend:                         inside_escape = True                         break                 if not inside_escape:                     found[start] = sep          if not found:
class APIRouter(routing.Router):      def add_api_websocket_route(          self, path: str, endpoint: Callable, name: str = None      ) -> None:         route = APIWebSocketRoute(             path,             endpoint=endpoint,             name=name,             dependency_overrides_provider=self.dependency_overrides_provider,         )          self.routes.append(route)      def websocket(self, path: str, name: str = None) -> Callable:
Wild         185.0                      result = coerce_to_dtypes(result, self.dtypes)          if constructor is not None:             result = self._constructor_sliced(result, index=labels)          return result      def nunique(self, axis=0, dropna=True) -> Series:
class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):      def __len__(self) -> int:          return len(self._data)     def __invert__(self):         return type(self)(~self._data, self._mask)       def to_numpy(          self, dtype=None, copy=False, na_value: "Scalar" = lib.no_default,      ):
def iter_sequence_infinite(seq):      while True:          for item in seq:              yield item   def is_sequence(seq):      return (getattr(seq, 'use_sequence_api', False)             or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))
class TestReshaping(BaseJSON, base.BaseReshapingTests):          return super().test_unstack(data, index)     @pytest.mark.xfail(reason="Inconsistent sizes.")     def test_transpose(self, data):         super().test_transpose(data)   class TestGetitem(BaseJSON, base.BaseGetitemTests):      pass
import traceback  from .variables import CommonVariable, Exploding, BaseVariable  from . import utils, pycompat if pycompat.PY2:     from io import open  ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$') def get_local_reprs(frame, watch=(), custom_repr=()):      code = frame.f_code      vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())     result_items = [(key, utils.get_shortish_repr(value, custom_repr=custom_repr)) for key, value in frame.f_locals.items()]      result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))      result = collections.OrderedDict(result_items)
def _make_getset_interval(method_name, lim_name, attr_name):                  setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),                         ignore=True)              else:                 setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),                         ignore=True)          self.stale = True
class IntervalIndex(IntervalMixin, Index):              left_indexer = self.left.get_indexer(target_as_index.left)              right_indexer = self.right.get_indexer(target_as_index.right)              indexer = np.where(left_indexer == right_indexer, left_indexer, -1)         elif is_categorical(target_as_index):              categories_indexer = self.get_indexer(target_as_index.categories)             indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)          elif not is_object_dtype(target_as_index):              target_as_index = self._maybe_convert_i8(target_as_index)
class Sequential(Model):                  or (inputs, targets, sample_weights)              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Optional for `Sequence`: if unspecified, will use                 the `len(generator)` as a number of steps.              max_queue_size: maximum size for the generator queue              workers: maximum number of processes to spin up              use_multiprocessing: if True, use process based threading.
def url_concat(url, args):      >>> url_concat("http://example.com/foo?a=b", [("c", "d"), ("c", "d2")])      'http://example.com/foo?a=b&c=d&c=d2'     if args is None:         return url      parsed_url = urlparse(url)      if isinstance(args, dict):          parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
def conv2d(x, kernel, strides=(1, 1), padding='valid',  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None, dilation_rate=(1, 1)):
class Block(PandasObject):          transpose = self.ndim == 2         if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:             raise ValueError(f"Cannot set values with ndim > {self.ndim}")           if value is None:              if self.is_numeric:
from six.moves.urllib import robotparser  from scrapy.exceptions import NotConfigured, IgnoreRequest  from scrapy.http import Request  from scrapy.utils.httpobj import urlparse_cached from scrapy.utils.log import failure_to_exc_info  logger = logging.getLogger(__name__)
class StackedRNNCells(Layer):                  output_dim = cell.state_size[0]              else:                  output_dim = cell.state_size             input_shape = (input_shape[0], output_dim)          self.built = True      def get_config(self):
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _process_wp_text(article_title, article_text, wp_to_id):          return None, None     text_search = text_tag_regex.sub("", article_text)     text_search = text_regex.search(text_search)      if text_search is None:          return None, None      text = text_search.group(0)
class Path:                  codes[i:i + len(path.codes)] = path.codes              i += len(path.vertices)          last_vert = None         if codes.size > 0 and codes[-1] == cls.STOP:             last_vert = vertices[-1]         vertices = vertices[codes != cls.STOP, :]         codes = codes[codes != cls.STOP]         if last_vert is not None:             vertices = np.append(vertices, [last_vert], axis=0)             codes = np.append(codes, cls.STOP)           return cls(vertices, codes)      def __repr__(self):
from pandas.core.dtypes.cast import (  )  from pandas.core.dtypes.common import (      ensure_platform_int,     is_categorical,      is_datetime64tz_dtype,      is_datetime_or_timedelta_dtype,      is_dtype_equal,
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):          raise raise_on_incompatible(self, None)     def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:         if not isinstance(dtype, PeriodDtype):             return False         return dtype.freq == self.freq
def test_check_mutually_exclusive_found(mutually_exclusive_terms):          'fox': 'red',          'socks': 'blue',      }     expected = "parameters are mutually exclusive: string1|string2, box|fox|socks"      with pytest.raises(TypeError) as e:          check_mutually_exclusive(mutually_exclusive_terms, params)      assert to_native(e.value) == expected  def test_check_mutually_exclusive_none():
class Categorical(ExtensionArray, PandasObject):          code_values = code_values[null_mask | (code_values >= 0)]          return algorithms.isin(self.codes, code_values)     def replace(self, to_replace, value, inplace: bool = False):         inplace = validate_bool_kwarg(inplace, "inplace")         cat = self if inplace else self.copy()         if to_replace in cat.categories:             if isna(value):                 cat.remove_categories(to_replace, inplace=True)             else:                 categories = cat.categories.tolist()                 index = categories.index(to_replace)                 if value in cat.categories:                     value_index = categories.index(value)                     cat._codes[cat._codes == index] = value_index                     cat.remove_categories(to_replace, inplace=True)                 else:                     categories[index] = value                     cat.rename_categories(categories, inplace=True)         if not inplace:             return cat
class DataFrame(NDFrame):                      " or if the Series has a name"                  )             index = Index([other.name], name=self.index.name)              idx_diff = other.index.difference(self.columns)              try:                  combined_columns = self.columns.append(idx_diff)              except TypeError:                  combined_columns = self.columns.astype(object).append(idx_diff)             other = (                 other.reindex(combined_columns, copy=False)                 .to_frame()                 .T.infer_objects()                 .rename_axis(index.names, copy=False)              )              if not self.columns.equals(combined_columns):                  self = self.reindex(columns=combined_columns)          elif isinstance(other, list):
def get_all_executables():      tf_entry_points = ['thefuck', 'fuck']      bins = [exe.name.decode('utf8') if six.PY2 else exe.name             for path in os.environ.get('PATH', '').split(os.pathsep)              for exe in _safe(lambda: list(Path(path).iterdir()), [])              if not _safe(exe.is_dir, True)              and exe.name not in tf_entry_points]
class Index(IndexOpsMixin, PandasObject):              return other.equals(self)         return array_equivalent(             com.values_from_object(self), com.values_from_object(other)         )      def identical(self, other):
import re  import numpy as np  import pytest from pandas import (     CategoricalIndex,     Interval,     IntervalIndex,     Timedelta,     date_range,     timedelta_range, )  from pandas.core.indexes.base import InvalidIndexError  import pandas.util.testing as tm
class SymLogNorm(Normalize):          with np.errstate(invalid="ignore"):              masked = np.abs(a) > self.linthresh          sign = np.sign(a[masked])         log = (self._linscale_adj +                np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)          log *= sign * self.linthresh          a[masked] = log          a[~masked] *= self._linscale_adj
def mask_zero_div_zero(x, y, result):          return result      if zmask.any():          zneg_mask = zmask & np.signbit(y)          zpos_mask = zmask & ~zneg_mask         nan_mask = zmask & (x == 0)          with np.errstate(invalid="ignore"):             neginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))             posinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))          if nan_mask.any() or neginf_mask.any() or posinf_mask.any():             result = result.astype("float64", copy=False)             result[nan_mask] = np.nan             result[posinf_mask] = np.inf             result[neginf_mask] = -np.inf      return result
def _match_one(filter_part, dct):      if m:          op = COMPARISON_OPERATORS[m.group('op')]          actual_value = dct.get(m.group('key'))         if (m.group('quotedstrval') is not None or             m.group('strval') is not None or
from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray  from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin  from pandas.core.base import IndexOpsMixin  import pandas.core.indexes.base as ibase from pandas.core.indexes.base import Index, _index_shared_docs, ensure_index  from pandas.core.indexes.extension import (      ExtensionIndex,      inherit_names,
class Line:      depth: int = 0      leaves: List[Leaf] = Factory(list)     comments: List[Tuple[Index, Leaf]] = Factory(list)      bracket_tracker: BracketTracker = Factory(BracketTracker)      inside_brackets: bool = False      has_for: bool = False
commands:      - string  from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.network.ios.ios import get_config, load_config  from ansible.module_utils.network.ios.ios import ios_argument_spec from re import search, M  def map_obj_to_commands(updates, module):
from pandas.core.dtypes.common import (      is_datetime64_dtype,      is_datetime64tz_dtype,      is_datetime_or_timedelta_dtype,     is_extension_array_dtype,      is_integer,     is_integer_dtype,      is_list_like,      is_scalar,      is_timedelta64_dtype,
class Line:          return False     def append_comment(self, comment: Leaf) -> bool:          if comment.type != token.COMMENT:              return False         after = len(self.leaves) - 1         if after == -1:              comment.type = STANDALONE_COMMENT              comment.prefix = ''              return False          else:             self.comments.append((after, comment))              return True         for _leaf_index, _leaf in enumerate(self.leaves):             if leaf is _leaf:                 break          else:             return         for index, comment_after in self.comments:             if _leaf_index == index:                 yield comment_after      def remove_trailing_comma(self) -> None:
class Zsh(Generic):      @memoize      def get_aliases(self):         raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')          return dict(self._parse_alias(alias)                      for alias in raw_aliases if alias and '=' in alias)
class Sequential(Model):      @interfaces.legacy_generator_methods_support      def fit_generator(self, generator,                       steps_per_epoch=None,                        epochs=1,                        verbose=1,                        callbacks=None,
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
import re  import os from thefuck.utils import memoize, wrap_settings  from thefuck import shells   patterns = (          '^    at {file}:{line}:{col}',
def container_of(leaf: Leaf) -> LN:          if parent.children[0].prefix != same_prefix:              break         if parent.type == syms.file_input:             break           if parent.type in SURROUNDED_BY_BRACKETS:              break
def format_file_in_place(          if lock:              lock.acquire()          try:             f = io.TextIOWrapper(                 sys.stdout.buffer,                 encoding=encoding,                 newline=newline,                 write_through=True,             )             f.write(diff_contents)             f.detach()          finally:              if lock:                  lock.release()
def _coerce_to_type(x):      elif is_timedelta64_dtype(x):          x = to_timedelta(x)          dtype = np.dtype("timedelta64[ns]")     elif is_bool_dtype(x):          x = x.astype(np.int64)      if dtype is not None:
class RangeIndex(Int64Index):
class scheduler(Config):      disable_window = parameter.IntParameter(default=3600,                                              config_path=dict(section='scheduler', name='disable-window-seconds'))     disable_failures = parameter.IntParameter(default=999999999,                                                config_path=dict(section='scheduler', name='disable-num-failures'))     disable_hard_timeout = parameter.IntParameter(default=999999999,                                                    config_path=dict(section='scheduler', name='disable-hard-timeout'))      disable_persist = parameter.IntParameter(default=86400,                                               config_path=dict(section='scheduler', name='disable-persist-seconds'))
class Task(object):          exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)         positional_params = [(n, p) for n, p in params if not p.is_global]          for i, arg in enumerate(args):              if i >= len(positional_params):                  raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))
class RobotsTxtMiddleware(object):          if failure.type is not IgnoreRequest:              logger.error("Error downloading %(request)s: %(f_exception)s",                           {'request': request, 'f_exception': failure.value},                          exc_info=failure_to_exc_info(failure),                          extra={'spider': spider})      def _parse_robots(self, response):          rp = robotparser.RobotFileParser(response.url)
def evaluate_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if use_sequence_api:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
def dump_to_file(*output: str) -> str:      import tempfile      with tempfile.NamedTemporaryFile(         mode="w", prefix="blk_", suffix=".log", delete=False, encoding="utf8"      ) as f:          for lines in output:              f.write(lines)
class MonthOffset(SingleConstructorOffset):      @apply_index_wraps      def apply_index(self, i):          shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)         return type(i)._simple_new(shifted, dtype=i.dtype)  class MonthEnd(MonthOffset):
class HTTP1Connection(httputil.HTTPConnection):          if content_length is not None:              return self._read_fixed_body(content_length, delegate)         if headers.get("Transfer-Encoding", "").lower() == "chunked":              return self._read_chunked_body(delegate)          if self.is_client:              return self._read_body_until_close(delegate)
class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):      @staticmethod      def get_target_path(target):          if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):              return target.path          elif isinstance(target, bigquery.BigQueryTarget):             return "{}:{}.{}".format(target.table.project_id, target.table.dataset_id, target.table.table_id)          else:             raise ValueError("Target %s not supported" % target)
class Axes(_AxesBase):          if not np.iterable(xmax):              xmax = [xmax]          y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)          y = np.ravel(y)         xmin = np.ravel(xmin)         xmax = np.ravel(xmax)          masked_verts = np.ma.empty((len(y), 2, 2))         masked_verts[:, 0, 0] = xmin         masked_verts[:, 0, 1] = y         masked_verts[:, 1, 0] = xmax         masked_verts[:, 1, 1] = y         lines = mcoll.LineCollection(masked_verts, colors=colors,                                       linestyles=linestyles, label=label)          self.add_collection(lines, autolim=False)          lines.update(kwargs)
def _match_one(filter_part, dct):      m = operator_rex.search(filter_part)      if m:          op = COMPARISON_OPERATORS[m.group('op')]         actual_value = dct.get(m.group('key'))         if (m.group('strval') is not None or                 actual_value is not None and m.group('intval') is not None and                 isinstance(actual_value, compat_str)):              if m.group('op') not in ('=', '!='):                  raise ValueError(                      'Operator %s does not support string values!' % m.group('op'))             comparison_value = m.group('strval') or m.group('intval')          else:              try:                  comparison_value = int(m.group('intval'))
def update(x, new_x):          The variable `x` updated.     op = tf_state_ops.assign(x, new_x)     with tf.control_dependencies([op]):         return tf.identity(x)  @symbolic
class EarlyStopping(Callback):                   patience=0,                   verbose=0,                   mode='auto',                  baseline=None,                  restore_best_weights=False):          super(EarlyStopping, self).__init__()          self.monitor = monitor
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):          if isinstance(key, str):               try:                 loc = self._get_string_slice(key)                 return loc             except (TypeError, ValueError):                  pass              try:                  asdt, reso = parse_time_string(key, self.freq)              except DateParseError:                  raise KeyError(f"Cannot interpret '{key}' as period")             grp = resolution.Resolution.get_freq_group(reso)             freqn = resolution.get_freq_group(self.freq)               assert grp >= freqn              if grp == freqn:                 key = Period(asdt, freq=self.freq)                 loc = self.get_loc(key, method=method, tolerance=tolerance)                 return loc             elif method is None:                 raise KeyError(key)             else:                 key = asdt           elif is_integer(key):              raise KeyError(key)
class Base:          assert not indices.equals(np.array(indices))         if not isinstance(indices, (RangeIndex, CategoricalIndex)):               same_values = Index(indices, dtype=object)              assert indices.equals(same_values)              assert same_values.equals(indices)
class HTTPBearer(HTTPBase):              else:                  return None          if scheme.lower() != "bearer":             if self.auto_error:                 raise HTTPException(                     status_code=HTTP_403_FORBIDDEN,                     detail="Invalid authentication credentials",                 )             else:                 return None          return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)
class Model(Container):              epoch_logs = {}              while epoch < epochs:                 for m in self.stateful_metric_functions:                     m.reset_states()                  callbacks.on_epoch_begin(epoch)                  steps_done = 0                  batch_index = 0
class SimpleTaskState(object):      def get_necessary_tasks(self):          necessary_tasks = set()          for task in self.get_active_tasks():             if task.status not in (DONE, DISABLED, UNKNOWN) or \                     task.scheduler_disable_time is not None:                  necessary_tasks.update(task.deps)                  necessary_tasks.add(task.id)          return necessary_tasks
class SortedCorrectedCommandsSequence(object):              return []          for command in self._commands:             if command != first:                  return [first, command]          return [first]      def _remove_duplicates(self, corrected_commands):
class Tracer:              prefix='',              overwrite=False,              thread_info=False,             custom_repr=(),      ):          self._write = get_write_function(output, overwrite)
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()         mat = numeric_df.astype(float, copy=False).to_numpy()          if notna(mat).all():              if min_periods is not None and min_periods > len(mat):                 base_cov = np.empty((mat.shape[1], mat.shape[1]))                 base_cov.fill(np.nan)              else:                 base_cov = np.cov(mat.T)             base_cov = base_cov.reshape((len(cols), len(cols)))          else:             base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)         return self._constructor(base_cov, index=idx, columns=cols)      def corrwith(self, other, axis=0, drop=False, method="pearson") -> Series:
class SimpleTaskState(object):                  self.re_enable(task)             elif task.scheduler_disable_time is not None and new_status != DISABLED:                  return          if new_status == FAILED and task.can_disable() and task.status != DISABLED:
def _use_inf_as_na(key):  def _isna_ndarraylike(obj):     is_extension = is_extension_array_dtype(obj.dtype)     values = getattr(obj, "_values", obj)      dtype = values.dtype      if is_extension:          result = values.isna()      elif is_string_dtype(dtype):         result = _isna_string_dtype(values, dtype, old=False)      elif needs_i8_conversion(dtype):
class Line:              return False          if closing.type == token.RBRACE:             self.remove_trailing_comma()              return True          if closing.type == token.RSQB:              comma = self.leaves[-1]              if comma.parent and comma.parent.type == syms.listmaker:                 self.remove_trailing_comma()                  return True
class Bidirectional(Wrapper):              return [output_shape] + state_shape + copy.copy(state_shape)          return output_shape     def __call__(self, inputs, initial_state=None, **kwargs):         if isinstance(inputs, list):             if len(inputs) > 1:                 initial_state = inputs[1:]             inputs = inputs[0]          if initial_state is None:             return super(Bidirectional, self).__call__(inputs, **kwargs)           if isinstance(initial_state, tuple):             initial_state = list(initial_state)         elif not isinstance(initial_state, list):             initial_state = [initial_state]           num_states = len(initial_state)         if num_states % 2 > 0:             raise ValueError(                 'When passing `initial_state` to a Bidirectional RNN, the state '                 'should be a list containing the states of the underlying RNNs. '                 'Found: ' + str(initial_state))           kwargs['initial_state'] = initial_state         additional_inputs = initial_state         additional_specs = [InputSpec(shape=K.int_shape(state))                             for state in initial_state]         self.forward_layer.state_spec = additional_specs[:num_states // 2]         self.backward_layer.state_spec = additional_specs[num_states // 2:]          is_keras_tensor = K.is_keras_tensor(additional_inputs[0])         for tensor in additional_inputs:             if K.is_keras_tensor(tensor) != is_keras_tensor:                 raise ValueError('The initial state of a Bidirectional'                                  ' layer cannot be specified with a mix of'                                  ' Keras tensors and non-Keras tensors'                                  ' (a "Keras tensor" is a tensor that was'                                  ' returned by a Keras layer, or by `Input`)')          if is_keras_tensor:              full_input = [inputs] + additional_inputs             full_input_spec = self.input_spec + additional_specs               original_input_spec = self.input_spec             self.input_spec = full_input_spec             output = super(Bidirectional, self).__call__(full_input, **kwargs)             self.input_spec = original_input_spec             return output         else:             return super(Bidirectional, self).__call__(inputs, **kwargs)       def call(self, inputs, training=None, mask=None, initial_state=None):          kwargs = {}          if has_arg(self.layer.call, 'training'):
class Axes(_AxesBase):      @_preprocess_data(replace_names=["y", "xmin", "xmax", "colors"],                        label_namer="y")     def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',                 label='', **kwargs):          Plot horizontal lines at each *y* from *xmin* to *xmax*.
class StackedRNNCells(Layer):                      cell.build([input_shape] + constants_shape)                  else:                      cell.build(input_shape)             if getattr(cell, 'output_size', None) is not None:                 output_dim = cell.output_size             elif hasattr(cell.state_size, '__len__'):                  output_dim = cell.state_size[0]              else:                  output_dim = cell.state_size
class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass): rules = []      start_date = Timestamp(datetime(1970, 1, 1))     end_date = Timestamp(datetime(2200, 12, 31))      _cache = None      def __init__(self, name=None, rules=None):
class XAxis(Axis):      def get_minpos(self):          return self.axes.dataLim.minposx     def set_inverted(self, inverted):          a, b = self.get_view_interval()         self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)       def set_default_intervals(self):          xmin, xmax = 0., 1.
from thefuck.utils import eager  @git_support  def match(command):      return ("fatal: A branch named '" in command.output             and "' already exists." in command.output)  @git_support  @eager  def get_new_command(command):      branch_name = re.findall(         r"fatal: A branch named '(.+)' already exists.", command.output)[0]     branch_name = branch_name.replace("'", r"\'")      new_command_templates = [['git branch -d {0}', 'git branch {0}'],                               ['git branch -d {0}', 'git checkout -b {0}'],                               ['git branch -D {0}', 'git branch {0}'],
def _isna_old(obj):          raise NotImplementedError("isna is not defined for MultiIndex")      elif isinstance(obj, type):          return False     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):          return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCGeneric):          return obj._constructor(obj._data.isna(func=_isna_old))
def _get_clickable(clickdata, form):      clickables = [          el for el in form.xpath(             'descendant::input[re:test(@type, "^(submit|image)$", "i")]'             '|descendant::button[not(@type) or re:test(@type, "^submit$", "i")]',              namespaces={"re": "http://exslt.org/regular-expressions"})          ]      if not clickables:
class DataFrame(NDFrame):              return ops.dispatch_to_series(this, other, _arith_op)          else:             with np.errstate(all="ignore"):                 result = _arith_op(this.values, other.values)             result = dispatch_fill_zeros(func, this.values, other.values, result)              return self._constructor(                  result, index=new_index, columns=new_columns, copy=False              )
def _recast_datetimelike_result(result: DataFrame) -> DataFrame:      result = result.copy()      obj_cols = [         idx         for idx in range(len(result.columns))         if is_object_dtype(result.dtypes.iloc[idx])      ]
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):      @Appender(_index_shared_docs["repeat"] % _index_doc_kwargs)      def repeat(self, repeats, axis=None):          nv.validate_repeat(tuple(), dict(axis=axis))         result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)         return self._shallow_copy(result)      @Appender(_index_shared_docs["where"] % _index_doc_kwargs)      def where(self, cond, other=None):
def _make_getset_interval(method_name, lim_name, attr_name):                  setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),                         ignore=True)              else:                 setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),                         ignore=True)          self.stale = True
class IOLoop(Configurable):                  from tornado.process import cpu_count                  self._executor = ThreadPoolExecutor(max_workers=(cpu_count() * 5))              executor = self._executor         c_future = executor.submit(func, *args)           t_future = TracebackFuture()         self.add_future(c_future, lambda f: chain_future(f, t_future))         return t_future      def set_default_executor(self, executor):
def str_or_none(v, default=None):  def str_to_int(int_str):     if not isinstance(int_str, compat_str):         return int_str      int_str = re.sub(r'[,\.\+]', '', int_str)      return int(int_str)
class SymLogNorm(Normalize):          masked = np.abs(a) > (self.linthresh * self._linscale_adj)          sign = np.sign(a[masked])         exp = np.power(self._base,                        sign * a[masked] / self.linthresh - self._linscale_adj)          exp *= sign * self.linthresh          a[masked] = exp          a[~masked] /= self._linscale_adj
def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:          model_config=BaseConfig,          class_validators={},          alias="body",         schema=BodySchema(**BodySchema_kwargs),      )      return field
class Categorical(ExtensionArray, PandasObject):          inplace = validate_bool_kwarg(inplace, "inplace")          cat = self if inplace else self.copy()           if is_list_like(to_replace):              replace_dict = {replace_value: value for replace_value in to_replace}         else:              replace_dict = {to_replace: value}            for replace_value, new_value in replace_dict.items():             if replace_value in cat.categories:                 if isna(new_value):                     cat.remove_categories(replace_value, inplace=True)                     continue                  categories = cat.categories.tolist()                 index = categories.index(replace_value)                 if new_value in cat.categories:                     value_index = categories.index(new_value)                      cat._codes[cat._codes == index] = value_index                     cat.remove_categories(replace_value, inplace=True)                  else:                     categories[index] = new_value                      cat.rename_categories(categories, inplace=True)          if not inplace:              return cat
def get_weighted_roll_func(cfunc: Callable) -> Callable:  def validate_baseindexer_support(func_name: Optional[str]) -> None:      BASEINDEXER_WHITELIST = {         "count",          "min",          "max",          "mean",
import re  import shlex  import pkgutil  import xml.etree.ElementTree as ET import ntpath  from ansible.errors import AnsibleError  from ansible.module_utils._text import to_bytes, to_text
class tqdm(Comparable):          self.start_t = self.last_print_t     def __bool__(self):         if self.total is not None:             return self.total > 0         if self.iterable is None:             raise TypeError('Boolean cast is undefined'                             ' for tqdm objects that have no iterable or total')         return bool(self.iterable)      def __nonzero__(self):         return self.__bool__()       def __len__(self):          return self.total if self.iterable is None else \              (self.iterable.shape[0] if hasattr(self.iterable, "shape")
import warnings  import numpy as np from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers  from pandas._libs.index import convert_scalar  import pandas._libs.internals as libinternals  from pandas._libs.tslibs import Timedelta, conversion
class MediaPipeline(object):                      logger.error(                          '%(class)s found errors processing %(item)s',                          {'class': self.__class__.__name__, 'item': item},                         exc_info=failure_to_exc_info(value),                         extra={'spider': info.spider}                      )          return item
def _convert_by(by):  @Substitution("\ndata : DataFrame")  @Appender(_shared_docs["pivot"], indents=1)  def pivot(data: "DataFrame", index=None, columns=None, values=None) -> "DataFrame":     if columns is None:         raise TypeError("pivot() missing 1 required argument: 'columns'")       if values is None:          cols = [columns] if index is None else [index, columns]          append = index is None
class Driver(object):                      current_line = ""                      current_column = 0                      wait_for_nl = False             elif char in ' \t':                  current_column += 1              elif char == '\n':                  current_column = 0
from pandas.core.dtypes.common import (      is_float_dtype,      is_integer_dtype,      is_scalar,     is_signed_integer_dtype,     is_unsigned_integer_dtype,      needs_i8_conversion,      pandas_dtype,  )
class _AtIndexer(_ScalarAccessIndexer):          Require they keys to be the same type as the index. (so we don't          fallback)            if self.ndim == 1 and len(key) > 1:             key = (key,)           if is_setter:              return list(key)
def run(cmdline_args=None, main_task_cls=None,      :param use_dynamic_argparse:      :param local_scheduler:     if cmdline_args is None:         cmdline_args = sys.argv[1:]       if use_dynamic_argparse:          interface = DynamicArgParseInterface()      else:
class Block(PandasObject):              values[indexer] = value          elif (             exact_match             and is_categorical_dtype(arr_value.dtype)             and not is_categorical_dtype(values)          ):                values[indexer] = value             return self.make_block(Categorical(self.values, dtype=arr_value.dtype))            elif exact_match:             values[indexer] = value               try:                  values = values.astype(arr_value.dtype)              except ValueError:
class Text(Artist):          if self._renderer is None:              raise RuntimeError('Cannot get window extent w/o renderer')         with cbook._setattr_cm(self.figure, dpi=dpi):             bbox, info, descent = self._get_layout(self._renderer)             x, y = self.get_unitless_position()             x, y = self.get_transform().transform((x, y))             bbox = bbox.translated(x, y)             return bbox      def set_backgroundcolor(self, color):
def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:      container: Optional[LN] = container_of(leaf)      while container is not None and container.type != token.ENDMARKER:         if fmt_on(container):              return          if contains_fmt_on_at_column(container, leaf.column):             for child in container.children:                 if contains_fmt_on_at_column(child, leaf.column):                     return                 yield child         else:             yield container             container = container.next_sibling  def fmt_on(container: LN) -> bool:     is_fmt_on = False     for comment in list_comments(container.prefix, is_endmarker=False):         if comment.value in FMT_ON:             is_fmt_on = True         elif comment.value in FMT_OFF:             is_fmt_on = False     return is_fmt_on   def contains_fmt_on_at_column(container: LN, column: int) -> bool:     for child in container.children:         if (             isinstance(child, Node)             and first_leaf_column(child) == column             or isinstance(child, Leaf)             and child.column == column         ):             if fmt_on(child):                 return True      return False   def first_leaf_column(node: Node) -> Optional[int]:     for child in node.children:         if isinstance(child, Leaf):             return child.column     return None  def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:
class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):              arglist.append('{}@{}'.format(username, host))          else:              arglist = []             if not job.jar():                 raise HadoopJarJobError("Jar not defined")             if not os.path.exists(job.jar()):                  logger.error("Can't find jar: %s, full path %s", job.jar(), os.path.abspath(job.jar()))                  raise HadoopJarJobError("job jar does not exist")
def test_timedelta_cut_roundtrip():          ["0 days 23:57:07.200000", "2 days 00:00:00", "3 days 00:00:00"]      )      tm.assert_index_equal(result_bins, expected_bins)   @pytest.mark.parametrize("bins", [6, 7]) @pytest.mark.parametrize(     "box, compare",     [         (Series, tm.assert_series_equal),         (np.array, tm.assert_categorical_equal),         (list, tm.assert_equal),     ], ) def test_cut_bool_coercion_to_int(bins, box, compare):      data_expected = box([0, 1, 1, 0, 1] * 10)     data_result = box([False, True, True, False, True] * 10)     expected = cut(data_expected, bins, duplicates="drop")     result = cut(data_result, bins, duplicates="drop")     compare(result, expected)
class GalaxyCLI(CLI):          super(GalaxyCLI, self).init_parser(             desc="Perform various Role and Collection related operations.",          )
class TimedeltaIndex(          if self[0] <= other[0]:              left, right = self, other         elif sort is False:               left, right = self, other             left_start = left[0]             loc = right.searchsorted(left_start, side="left")             right_chunk = right.values[:loc]             dates = concat_compat((left.values, right_chunk))             return self._shallow_copy(dates)          else:              left, right = other, self
def gunzip(data):              if output or getattr(f, 'extrabuf', None):                  try:                     output += f.extrabuf[-f.extrasize:]                  finally:                      break              else:
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def bracket_split_build_line(          if leaves:              normalize_prefix(leaves[0], inside_brackets=True)               no_commas = original.is_def and not any(                 l.type == token.COMMA for l in leaves             )              if original.is_import or no_commas:                  for i in range(len(leaves) - 1, -1, -1):                      if leaves[i].type == STANDALONE_COMMENT:                          continue
class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):      def requestTunnel(self, protocol):         tunnelReq = (             b'CONNECT ' +             to_bytes(self._tunneledHost, encoding='ascii') + b':' +             to_bytes(str(self._tunneledPort)) +             b' HTTP/1.1\r\n')          if self._proxyAuthHeader:             tunnelReq += \                 b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\r\n'         tunnelReq += b'\r\n'          protocol.transport.write(tunnelReq)          self._protocolDataReceived = protocol.dataReceived          protocol.dataReceived = self.processProxyResponse
class Network(Layer):          while unprocessed_nodes:              for layer_data in config['layers']:                  layer = created_layers[layer_data['name']]                    if layer in unprocessed_nodes:                     node_data_list = unprocessed_nodes[layer]                       node_index = 0                     while node_index < len(node_data_list):                         node_data = node_data_list[node_index]                         try:                             process_node(layer, node_data)                            except LookupError:                             break                          node_index += 1                       if node_index < len(node_data_list):                         unprocessed_nodes[layer] = node_data_list[node_index:]                      else:                         del unprocessed_nodes[layer]           name = config.get('name')          input_tensors = []          output_tensors = []
from ..utils import (      compat_urllib_error,      compat_urllib_parse,      compat_urllib_request,     urlencode_postdata,      ExtractorError,  )
class DatetimeLikeArrayMixin(              return None          return self.freq.freqstr     def _with_freq(self, freq):         arr = self._data._with_freq(freq)         return type(self)._simple_new(arr, name=self.name)         if values is None:             values = self._data         if isinstance(values, np.ndarray):              values = type(self._data)(values, dtype=self.dtype)         result = type(self)._simple_new(values, name=name)         result._cache = cache         return result       @Appender(Index.difference.__doc__)     def difference(self, other, sort=None):         new_idx = super().difference(other, sort=sort)._with_freq(None)         return new_idx     def intersection(self, other, sort=False):         Specialized intersection for DatetimeIndex/TimedeltaIndex.         other : Same type as self or array-like         sort : False or None, default False             Sort the resulting index if possible.             .. versionadded:: 0.24.0             .. versionchanged:: 0.24.1                Changed the default to ``False`` to match the behaviour                from before 0.24.0.         self._validate_sort_keyword(sort)         self._assert_can_do_setop(other)         if self.equals(other):             return self._get_reconciled_name_object(other)         elif (             other.freq is None             or self.freq is None             or other.freq != self.freq             or not other.freq.is_anchored()             or (not self.is_monotonic or not other.is_monotonic)         ):             result = Index.intersection(self, other, sort=sort)             result = result._with_freq("infer")              return result          if self[0] <= other[0]:             left, right = self, other         else:             left, right = other, self           end = min(left[-1], right[-1])         start = right[0]         if end < start:             return type(self)(data=[], dtype=self.dtype, freq=self.freq)         else:             lslice = slice(*left.slice_locs(start, end))             left_chunk = left._values[lslice]             return self._shallow_copy(left_chunk)     def _can_fast_union(self, other) -> bool:         if not isinstance(other, type(self)):             return False         freq = self.freq         if freq is None or freq != other.freq:             return False         if not self.is_monotonic or not other.is_monotonic:             return False         if len(self) == 0 or len(other) == 0:             return True          if self[0] <= other[0]:             left, right = self, other         else:             left, right = other, self         right_start = right[0]         left_end = left[-1]          try:             return (right_start == left_end + freq) or right_start in left         except ValueError:               return False      def _fast_union(self, other, sort=None):         if len(other) == 0:             return self.view(type(self))          if len(self) == 0:             return other.view(type(self))           if self[0] <= other[0]:             left, right = self, other         elif sort is False:               left, right = self, other             left_start = left[0]             loc = right.searchsorted(left_start, side="left")             right_chunk = right._values[:loc]             dates = concat_compat((left._values, right_chunk))              result = self._shallow_copy(dates)._with_freq("infer")             return result          else:             left, right = other, self          left_end = left[-1]         right_end = right[-1]           if left_end < right_end:             loc = right.searchsorted(left_end, side="right")             right_chunk = right._values[loc:]             dates = concat_compat([left._values, right_chunk])              result = self._shallow_copy(dates)._with_freq("infer")             return result         else:             return left     def _union(self, other, sort):         if not len(other) or self.equals(other) or not len(self):             return super()._union(other, sort=sort)          assert isinstance(other, type(self))         this, other = self._maybe_utc_convert(other)          if this._can_fast_union(other):             result = this._fast_union(other, sort=sort)             if result.freq is None:                 result = result._with_freq("infer")             return result          else:             i8self = Int64Index._simple_new(self.asi8, name=self.name)             i8other = Int64Index._simple_new(other.asi8, name=other.name)             i8result = i8self._union(i8other, sort=sort)             result = type(self)(i8result, dtype=self.dtype, freq="infer")             return result         See Index.join     def _maybe_utc_convert(self, other):         this = self         if not hasattr(self, "tz"):             return this, other         return a boolean whether I can attempt conversion to a         DatetimeIndex/TimedeltaIndex         Make new Index inserting new item at location          Parameters          ----------         loc : int         item : object             if not either a Python datetime or a numpy integer-like, returned             Index dtype will be object rather than datetime.          Returns          -------         if isinstance(item, str):               return self.astype(object).insert(loc, item)          item = self._data._validate_insert_value(item)          freq = None          if self.freq is not None:             if self.size:                 if item is NaT:                     pass                 elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:                     freq = self.freq                 elif (loc == len(self)) and item - self.freq == self[-1]:                     freq = self.freq             else:                  if self.freq.is_on_offset(item):                     freq = self.freq         item = self._data._unbox_scalar(item)         new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])         arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)         return type(self)._simple_new(arr, name=self.name)
class YoutubeDL(object):                  format_spec = selector.selector                  def selector_function(formats):                     formats = list(formats)                     if not formats:                         return                      if format_spec == 'all':                          for f in formats:                              yield f
class DistributionFiles:          elif 'SteamOS' in data:              debian_facts['distribution'] = 'SteamOS'         elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:               debian_facts['distribution'] = 'Kali'              release = re.search('DISTRIB_RELEASE=(.*)', data)              if release:
def _get_combined_index(          calculate the union.      sort : bool, default False          Whether the result index should come out sorted or not.     copy : bool, default False         If True, return a copy of the combined index.      Returns      -------
def makeMappingArray(N, data, gamma=1.0):      if (np.diff(x) < 0).any():          raise ValueError("data mapping points must have x in increasing order")     if N == 1:          lut = np.array(y0[-1])     else:         x = x * (N - 1)         xind = (N - 1) * np.linspace(0, 1, N) ** gamma         ind = np.searchsorted(x, xind)[1:-1]          distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])         lut = np.concatenate([             [y1[0]],             distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],             [y0[-1]],         ])      return np.clip(lut, 0.0, 1.0)
class FigureCanvasBase:                      renderer = _get_renderer(                          self.figure,                          functools.partial(                             print_method, orientation=orientation)                     )                     no_ops = {                         meth_name: lambda *args, **kwargs: None                         for meth_name in dir(RendererBase)                         if (meth_name.startswith("draw_")                             or meth_name in ["open_group", "close_group"])                     }                      with _setattr_cm(renderer, **no_ops):                         self.figure.draw(renderer)                       bbox_inches = self.figure.get_tightbbox(                          renderer, bbox_extra_artists=bbox_extra_artists)                      if pad_inches is None:
class ExtensionBlock(Block):          new_values = self.values if inplace else self.values.copy()         if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):              new = new[mask]          mask = _safe_reshape(mask, new_values.shape)
class YearOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.month, self._day_opt, modby=12          )         return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)      def is_on_offset(self, dt: datetime) -> bool:          if self.normalize and not _is_normalized(dt):
class _LocIndexer(_LocationIndexer):                  new_key = []                  for i, component in enumerate(key):                     if (                         isinstance(component, str)                         and labels.levels[i]._supports_partial_string_indexing                     ):                          new_key.append(slice(component, component, None))                      else:                          new_key.append(component)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class StaticFileHandler(RequestHandler):          size = self.get_content_size()          if request_range:              start, end = request_range             if start is not None and start < 0:                 start += size                 if start < 0:                     start = 0             if (                 start is not None                 and (start >= size or (end is not None and start >= end))             ) or end == 0:     self.set_status(416)                  self.set_header("Content-Type", "text/plain")                  self.set_header("Content-Range", "bytes */%s" % (size,))                  return              if end is not None and end > size:
class DatetimeIndexOpsMixin(ExtensionIndex):     def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:          if is_period_dtype(self.dtype):             if is_period_dtype(result.dtype):                  return self.freq             return None          elif self.freq is None:              return None          elif lib.is_scalar(other) and isna(other):
def diff(arr, n: int, axis: int = 0):              result = res - lag              result[mask] = na              out_arr[res_indexer] = result         elif is_bool:             out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]          else:              out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]
class tqdm(object):          return self.total if self.iterable is None else \              (self.iterable.shape[0] if hasattr(self.iterable, "shape")               else len(self.iterable) if hasattr(self.iterable, "__len__")              else getattr(self, "total", None))      def __enter__(self):          return self
Module contains tools for processing files into DataFrames or other objects  from collections import abc, defaultdict  import csv  import datetime from io import BufferedIOBase, RawIOBase, StringIO, TextIOWrapper  import re  import sys  from textwrap import fill
def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):          vmin, vmax = vmax, vmin          swapped = True       vmin, vmax = map(float, [vmin, vmax])       maxabsvalue = max(abs(vmin), abs(vmax))      if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:          vmin = -expander
def test_multiprocessing_fit_error():      samples = batch_size * (good_batches + 1)     with pytest.raises(RuntimeError):          model.fit_generator(              custom_generator(), samples, 1,              workers=4, use_multiprocessing=True,          )     with pytest.raises(RuntimeError):          model.fit_generator(              custom_generator(), samples, 1,              use_multiprocessing=False,
class WebSocketProtocol13(WebSocketProtocol):      def accept_connection(self):          try:              self._handle_websocket_headers()         except ValueError:             self.handler.set_status(400)             log_msg = "Missing/Invalid WebSocket headers"             self.handler.finish(log_msg)             gen_log.debug(log_msg)             return          try:              self._accept_connection()          except ValueError:              gen_log.debug("Malformed WebSocket request received",
class Model(Container):                      when using multiprocessing.              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Optional for `Sequence`: if unspecified, will use                 the `len(generator)` as a number of steps.              max_queue_size: maximum size for the generator queue              workers: maximum number of processes to spin up                  when using process based threading
class YoutubeDL(object):              force_properties = dict(                  (k, v) for k, v in ie_result.items() if v is not None)             for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):                  if f in force_properties:                      del force_properties[f]              new_result = info.copy()
from thefuck.specific.git import git_support  @git_support  def match(command):     return 'did not match any file(s) known to git.' in command.stderr  @git_support  def get_new_command(command):      missing_file = re.findall(         r"error: pathspec '([^']*)' "         r'did not match any file\(s\) known to git.', command.stderr)[0]      formatme = shell.and_('git add -- {}', '{}')      return formatme.format(missing_file, command.script)
__all__ = [  def get_objs_combined_axis(     objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False  ) -> Index:      Extract combined index: return intersection or union (depending on the
class Model(Container):                      outs = [outs]                  outs_per_batch.append(outs)                 if x is None or len(x) == 0:                       batch_size = 1                 elif isinstance(x, list):                      batch_size = x[0].shape[0]                  elif isinstance(x, dict):                      batch_size = list(x.values())[0].shape[0]
class Axis(martist.Artist):                  self._minor_tick_kw.update(kwtrans)                  for tick in self.minorTicks:                      tick._apply_params(**kwtrans)              if 'label1On' in kwtrans or 'label2On' in kwtrans:                 self.offsetText.set_visible(                     self._major_tick_kw.get('label1On', False)                     or self._major_tick_kw.get('label2On', False))              if 'labelcolor' in kwtrans:                  self.offsetText.set_color(kwtrans['labelcolor'])
class tqdm(Comparable):          if unit_scale and unit_scale not in (True, 1):             if total:                 total *= unit_scale              n *= unit_scale              if rate: rate *= unit_scale
class Scheduler(object):              if (best_task and batched_params and task.family == best_task.family and                      len(batched_tasks) < max_batch_size and task.is_batchable() and all(                     task.params.get(name) == value for name, value in unbatched_params.items()) and                     self._schedulable(task)):                  for name, params in batched_params.items():                      params.append(task.params.get(name))                  batched_tasks.append(task)
from thefuck.utils import eager  @git_support  def match(command):     return ("fatal: A branch named '" in command.stderr              and " already exists." in command.stderr)
def assert_series_equal(                  f"is not equal to {right._values}."              )              raise AssertionError(msg)     elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):          assert_interval_array_equal(left.array, right.array)      elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):          _testing.assert_almost_equal(
def _period_array_cmp(cls, op):      @unpack_zerodim_and_defer(opname)      def wrapper(self, other):          if isinstance(other, str):              try:
import sys  import tarfile  import threading  import time import traceback  import zipfile  from abc import abstractmethod  from multiprocessing.pool import ThreadPool
from pandas.util._decorators import Appender, cache_readonly  from pandas.core.dtypes.common import (      ensure_platform_int,      ensure_python_int,      is_integer,      is_integer_dtype,      is_list_like,
import numpy as np  from pandas._libs import NaT, iNaT, join as libjoin, lib  from pandas._libs.tslibs import timezones from pandas._typing import DtypeObj, Label  from pandas.compat.numpy import function as nv  from pandas.errors import AbstractMethodError  from pandas.util._decorators import Appender, cache_readonly, doc  from pandas.core.dtypes.common import (      ensure_int64,     ensure_platform_int,      is_bool_dtype,      is_categorical_dtype,      is_dtype_equal,
class core(task.Config):  class _WorkerSchedulerFactory(object):      def create_local_scheduler(self):         return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)      def create_remote_scheduler(self, url):          return rpc.RemoteScheduler(url)
class TestPartialSetting:          df = orig.copy()         msg = "cannot insert DatetimeArray with incompatible label"          with pytest.raises(TypeError, match=msg):              df.loc[100.0, :] = df.iloc[0]
def format_str(      return dst_contents def prepare_input(src: bytes) -> Tuple[str, str, str]:     srcbuf = io.BytesIO(src)     encoding, lines = tokenize.detect_encoding(srcbuf.readline)     newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"     srcbuf.seek(0)     return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()    GRAMMARS = [      pygram.python_grammar_no_print_statement_no_exec_statement,      pygram.python_grammar_no_print_statement,
This module implements the FormRequest class which is a more convenient class  See documentation in docs/topics/request-response.rst import six  from six.moves.urllib.parse import urljoin, urlencode   import lxml.html  from parsel.selector import create_root_node from w3lib.html import strip_html5_whitespace   from scrapy.http.request import Request  from scrapy.utils.python import to_bytes, is_listlike  from scrapy.utils.response import get_base_url
class FileWriter(object):          self.overwrite = overwrite      def write(self, s):         with open(self.path, 'w' if self.overwrite else 'a',                   encoding='utf-8') as output_file:              output_file.write(s)          self.overwrite = False  thread_global = threading.local() DISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))  class Tracer:
async def func():                  self.async_inc, arange(8), batch_size=3              )          ]  def awaited_generator_value(n):     return (await awaitable for awaitable in awaitable_list)  def make_arange(n):     return (i * 2 for i in range(n) if await wrap(i))
class LocalFileSystem(FileSystem):              raise RuntimeError('Destination exists: %s' % new_path)          d = os.path.dirname(new_path)          if d and not os.path.exists(d):             self.mkdir(d)          os.rename(old_path, new_path)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class ImagesPipeline(FilesPipeline):      MIN_WIDTH = 0      MIN_HEIGHT = 0     EXPIRES = 90      THUMBS = {}      DEFAULT_IMAGES_URLS_FIELD = 'image_urls'      DEFAULT_IMAGES_RESULT_FIELD = 'images'
def _get_empty_dtype_and_na(join_units):          dtype = upcast_classes["datetimetz"]          return dtype[0], tslibs.NaT      elif "datetime" in upcast_classes:         return np.dtype("M8[ns]"), np.datetime64("NaT", "ns")      elif "timedelta" in upcast_classes:          return np.dtype("m8[ns]"), np.timedelta64("NaT", "ns") else:
class Grouping:                  self._group_index = CategoricalIndex(                      Categorical.from_codes(                          codes=codes, categories=categories, ordered=self.grouper.ordered                     ),                     name=self.name,                  )
def parse_duration(s):      m = re.match(             (?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*(?:s|secs?|seconds?)?
def format_str(src_contents: str, line_length: int) -> FileContent:      return dst_contents GRAMMARS = [     pygram.python_grammar_no_print_statement_no_exec_statement,     pygram.python_grammar_no_print_statement,     pygram.python_grammar_no_exec_statement,     pygram.python_grammar, ]    def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement      if src_txt[-1] != '\n':          nl = '\r\n' if '\r\n' in src_txt[:1024] else '\n'          src_txt += nl     for grammar in GRAMMARS:         drv = driver.Driver(grammar, pytree.convert)          try:             result = drv.parse_string(src_txt, True)             break          except ParseError as pe:             lineno, column = pe.context[1]             lines = src_txt.splitlines()             try:                 faulty_line = lines[lineno - 1]             except IndexError:                 faulty_line = "<line number missing in source>"             exc = ValueError(f"Cannot parse: {lineno}:{column}: {faulty_line}")     else:         raise exc from None      if isinstance(result, Leaf):          result = Node(syms.file_input, [result])
def get_grouper(              items = obj._data.items              try:                  items.get_loc(key)             except (KeyError, TypeError, InvalidIndexError):                  return False
class GroupBy(_GroupBy):             order = list(range(1, result.index.nlevels)) + [0]               index_names = np.array(result.index.names)              result.index.names = np.arange(len(index_names))               result = result.reorder_levels(order)              result.index.names = index_names[order]              indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()              return result.take(indices)      @Substitution(name="groupby")
class Axes(_AxesBase):          if not np.iterable(ymax):              ymax = [ymax]          x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)          x = np.ravel(x)         ymin = np.ravel(ymin)         ymax = np.ravel(ymax)          masked_verts = np.ma.empty((len(x), 2, 2))         masked_verts[:, 0, 0] = x         masked_verts[:, 0, 1] = ymin         masked_verts[:, 1, 0] = x         masked_verts[:, 1, 1] = ymax         lines = mcoll.LineCollection(masked_verts, colors=colors,                                       linestyles=linestyles, label=label)          self.add_collection(lines, autolim=False)          lines.update(kwargs)
def _normalize(table, normalize, margins, margins_name="All"):          table = table.fillna(0)      elif margins is True:          table_index = table.index         table_columns = table.columns            if (margins_name not in table.iloc[-1, :].name) | (             margins_name != table.iloc[:, -1].name         ):             raise ValueError("{} not in pivoted DataFrame".format(margins_name))         column_margin = table.iloc[:-1, -1]         index_margin = table.iloc[-1, :-1]           table = table.iloc[:-1, :-1]          table = _normalize(table, normalize=normalize, margins=False)
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1          if not good.all():             if skipna and good.any():                  pointer = self._codes[good].max()              else:                  return np.nan
class Bash(Generic):          return name, value      @memoize      def get_aliases(self):         raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')         return dict(self._parse_alias(alias)                     for alias in raw_aliases if alias and '=' in alias)      def _get_history_file_name(self):          return os.environ.get("HISTFILE",
class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):      def __init__(self):          if not isinstance(self.dataflow_params, DataflowParamKeys):              raise ValueError("dataflow_params must be of type DataflowParamKeys")         super(BeamDataflowJobTask, self).__init__()      @abstractmethod      def dataflow_executable(self):
class _AsOfMerge(_OrderedMerge):                  if self.tolerance < Timedelta(0):                      raise MergeError("tolerance must be positive")             elif is_integer_dtype(lt):                  if not is_integer(self.tolerance):                      raise MergeError(msg)                  if self.tolerance < 0:
class EarlyStopping(Callback):          baseline: Baseline value for the monitored quantity to reach.              Training will stop if the model doesn't show improvement              over the baseline.         restore_best_weights: whether to restore model weights from             the epoch with the best value of the monitored quantity.             If False, the model weights obtained at the last step of             training are used.      def __init__(self,
client = TestClient(app)  def test_return_defaults():      response = client.get("/")      assert response.json() == {"sub": {}}   def test_return_exclude_unset():     response = client.get("/exclude_unset")     assert response.json() == {"x": None, "y": "y"}   def test_return_exclude_defaults():     response = client.get("/exclude_defaults")     assert response.json() == {}   def test_return_exclude_none():     response = client.get("/exclude_none")     assert response.json() == {"y": "y", "z": "z"}   def test_return_exclude_unset_none():     response = client.get("/exclude_unset_none")     assert response.json() == {"y": "y"}
class PythonItemExporter(BaseItemExporter):              return dict(self._serialize_dict(value))          if is_listlike(value):              return [self._serialize_value(v) for v in value]         encode_func = to_bytes if self.binary else to_unicode         if isinstance(value, (six.text_type, bytes)):             return encode_func(value, encoding=self.encoding)         return value      def _serialize_dict(self, value):          for key, val in six.iteritems(value):
def gunzip(data):                  raise      return output _is_gzipped_re = re.compile(br'^application/(x-)?gzip\b', re.I)  def is_gzipped(response):      ctype = response.headers.get('Content-Type', b'')     return _is_gzipped_re.search(ctype) is not None
class IntegerArray(BaseMaskedArray):          ExtensionArray.argsort          data = self._data.copy()         if self._mask.any():             data[self._mask] = data.min() - 1          return data      @classmethod
class SchemaBase(BaseModel): not_: Optional[List[Any]] = PSchema(None, alias="not")      items: Optional[Any] = None      properties: Optional[Dict[str, Any]] = None     additionalProperties: Optional[Union[Dict[str, Any], bool]] = None      description: Optional[str] = None      format: Optional[str] = None      default: Optional[Any] = None
def is_import(leaf: Leaf) -> bool:      ) def is_special_comment(leaf: Leaf) -> bool:     t = leaf.type     v = leaf.value     return bool(         (t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith("     )    def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:
def convert_to_index_sliceable(obj, key):         if idx._supports_partial_string_indexing:              try:                  return idx._get_string_slice(key)              except (KeyError, ValueError, NotImplementedError):
class EarlyStopping(Callback):          self.min_delta = min_delta          self.wait = 0          self.stopped_epoch = 0         self.restore_best_weights = restore_best_weights         self.best_weights = None          if mode not in ['auto', 'min', 'max']:              warnings.warn('EarlyStopping mode %s is unknown, '
def check_bool_indexer(index: Index, key) -> np.ndarray:          result = result.astype(bool)._values      else:          if is_sparse(result):             result = np.asarray(result)          result = check_bool_array_indexer(index, result)      return result
class RequestHandler(object):          self._log()          self._finished = True          self.on_finish()         self._break_cycles()      def _break_cycles(self):          self.ui = None
class RNN(Layer):              state_size = self.cell.state_size          else:              state_size = [self.cell.state_size]          if getattr(self.cell, 'output_size', None) is not None:             output_dim = self.cell.output_size         else:             output_dim = state_size[0]          if self.return_sequences:              output_shape = (input_shape[0], input_shape[1], output_dim)
class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):                  dfxp_file = old_file                  srt_file = subtitles_filename(filename, lang, 'srt')                 with open(dfxp_file, 'rb') as f:                      srt_data = dfxp2srt(f.read())                  with io.open(srt_file, 'wt', encoding='utf-8') as f:
from .generic import Generic  class Fish(Generic):      def _get_overridden_aliases(self):         default = {'cd', 'grep', 'ls', 'man', 'open'}         for alias in os.environ.get('TF_OVERRIDDEN_ALIASES', '').split(','):             default.add(alias.strip())         return default      def app_alias(self, fuck):
def _zip_file(command):     for c in command.split_script[1:]:          if not c.startswith('-'):              if c.endswith('.zip'):                  return c
def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):          if is_object_dtype(dtype):              return tslib.ints_to_pydatetime(arr.view(np.int64))          elif dtype == np.int64:             if isna(arr).any():                 raise ValueError("Cannot convert NaT values to integer")              return arr.view(dtype)
def split_line(          return      line_str = str(line).strip("\n")        has_special_comment = False     for leaf in line.leaves:         for comment in line.comments_after(leaf):             if leaf.type == token.COMMA and is_special_comment(comment):                 has_special_comment = True      if (         not has_special_comment         and not line.should_explode         and is_line_short_enough(line, line_length=line_length, line_str=line_str)      ):          yield line          return
async def serialize_response(  ) -> Any:      if field:          errors = []         response_content = _prepare_response_content(             response_content, by_alias=by_alias, exclude_unset=exclude_unset         )          if is_coroutine:              value, errors_ = field.validate(response_content, {}, loc=("response",))          else:
def _make_ghost_gridspec_slots(fig, gs):              ax = fig.add_subplot(gs[nn])             ax.set_visible(False)  def _make_layout_margins(ax, renderer, h_pad, w_pad):
def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:  def normalize_fmt_off(node: Node) -> None:     Returns True if a pair was converted.
class TestInsertIndexCoercion(CoercionBase):              with pytest.raises(TypeError, match=msg):                  obj.insert(1, pd.Timestamp("2012-01-01", tz="Asia/Tokyo"))         msg = "cannot insert DatetimeArray with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, 1)
class StaticFileHandler(RequestHandler):          .. versionadded:: 3.1             root = os.path.abspath(root) + os.path.sep            if not (absolute_path + os.path.sep).startswith(root):              raise HTTPError(403, "%s is not in root static directory",                              self.path)
import warnings  import numpy as np from pandas._libs import NaT, Timestamp, lib, tslib, writers  import pandas._libs.internals as libinternals  from pandas._libs.tslibs import Timedelta, conversion  from pandas._libs.tslibs.timezones import tz_compare
class Axes(_AxesBase):              Respective beginning and end of each line. If scalars are              provided, all lines will have same length.         colors : list of colors, default: :rc:`lines.color`          linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
class FilesPipeline(MediaPipeline):          dfd.addErrback(              lambda f:              logger.error(self.__class__.__name__ + '.store.stat_file',                          exc_info=failure_to_exc_info(f),                          extra={'spider': info.spider})          )          return dfd
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
Name: Max Speed, dtype: float64          if copy:              new_values = new_values.copy()         if not isinstance(self.index, PeriodIndex):             raise TypeError(f"unsupported Type {type(self.index).__name__}") new_index = self.index.to_timestamp(freq=freq, how=how)          return self._constructor(new_values, index=new_index).__finalize__(              self, method="to_timestamp"
class _LocIndexer(_LocationIndexer):              if isinstance(ax, MultiIndex):                  return False             if isinstance(k, str) and ax._supports_partial_string_indexing:                   return False               if not ax.is_unique:                  return False
class IOLoop(Configurable):              if IOLoop.current(instance=False) is None:                  self.make_current()          elif make_current:             if IOLoop.current(instance=False) is not None:                  raise RuntimeError("current IOLoop already exists")              self.make_current()
class DatetimeLikeBlockMixin:      def _holder(self):          return DatetimeArray     def should_store(self, value):         return is_dtype_equal(self.dtype, value.dtype)       @property      def fill_value(self):          return np.datetime64("NaT", "ns")
class XmlItemExporter(BaseItemExporter):          elif is_listlike(serialized_value):              for value in serialized_value:                  self._export_xml_field('value', value)         elif isinstance(serialized_value, six.text_type):              self._xg_characters(serialized_value)         else:             self._xg_characters(str(serialized_value))          self.xg.endElement(name)
class RNN(Layer):              self._num_constants = len(constants)              additional_specs += self.constants_spec         is_keras_tensor = K.is_keras_tensor(additional_inputs[0])          for tensor in additional_inputs:             if K.is_keras_tensor(tensor) != is_keras_tensor:                  raise ValueError('The initial state or constants of an RNN'                                   ' layer cannot be specified with a mix of'                                  ' Keras tensors and non-Keras tensors'                                  ' (a "Keras tensor" is a tensor that was'                                  ' returned by a Keras layer, or by `Input`)')          if is_keras_tensor:
class DataFrame(NDFrame):     def _combine_frame(self, other: "DataFrame", func, fill_value=None):          if fill_value is None:
def format_file_in_place(      if src.suffix == ".pyi":          mode |= FileMode.PYI      with open(src, "rb") as buf:         newline, encoding, src_contents = prepare_input(buf.read())      try:          dst_contents = format_file_contents(              src_contents, line_length=line_length, fast=fast, mode=mode
import functools  import inspect  import opcode import os  import sys  import re  import collections
def test_join_multi_wrong_order():      midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=["a", "b"])      midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=["b", "a"])     join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)      exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)      tm.assert_index_equal(midx1, join_idx)      assert lidx is None      tm.assert_numpy_array_equal(ridx, exp_ridx)   def test_join_multi_return_indexers():       midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=["a", "b", "c"])     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=["a", "b"])      result = midx1.join(midx2, return_indexers=False)     tm.assert_index_equal(result, midx1)
class OrderedEnqueuer(SequenceEnqueuer):                      yield inputs          except Exception as e:              self.stop()             six.raise_from(StopIteration(e), e)      def _send_sequence(self):
class _AsOfMerge(_OrderedMerge):                  )              )             if is_datetimelike(lt):                  if not isinstance(self.tolerance, Timedelta):                      raise MergeError(msg)                  if self.tolerance < Timedelta(0):
class BracketTracker:      bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)      delimiters: Dict[LeafID, Priority] = Factory(dict)      previous: Optional[Leaf] = None     _for_loop_depths: List[int] = Factory(list)     _lambda_argument_depths: List[int] = Factory(list)      def mark(self, leaf: Leaf) -> None:
class _Numeric:          raise ValueError      def __le__(self, other):          return self.__lt__(other) or self.__eq__(other)     def __gt__(self, other):         return not self.__le__(other)       def __ge__(self, other):         return not self.__lt__(other)  class SemanticVersion(Version):
import os  from shutil import rmtree  import string  import tempfile from typing import Any, List, Optional, Union, cast  import warnings  import zipfile
class IOLoop(Configurable):      _current = threading.local()     _ioloop_for_asyncio = dict()      @classmethod      def configure(cls, impl, **kwargs):
def na_logical_op(x: np.ndarray, y, op):              assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))              x = ensure_object(x)              y = ensure_object(y)             result = libops.vec_binop(x.ravel(), y.ravel(), op)          else:              assert lib.is_scalar(y)
default 'raise'                      "You must pass a freq argument as current index has none."                  )             res = get_period_alias(freq)               if res is None:                 base, stride = libfrequencies._base_and_stride(freq)                 res = f"{stride}{base}"              freq = res          return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)
class ShellModule(ShellBase):          return ""      def join_path(self, *args):          parts = [ntpath.normpath(self._unquote(arg)) for arg in args]             return ntpath.join(parts[0], *[part.strip('\\') for part in parts[1:]])      def get_remote_filename(self, pathname):
def update_sub(x, decrement):          The variable `x` updated.     op = tf_state_ops.assign_sub(x, decrement)     with tf.control_dependencies([op]):         return tf.identity(x)  @symbolic
class JSInterpreter(object):              return opfunc(x, y)          m = re.match(             r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]*)\)$' % _NAME_RE, expr)          if m:              fname = m.group('func')              argvals = tuple([                  int(v) if v.isdigit() else local_vars[v]                 for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()              if fname not in self._functions:                  self._functions[fname] = self.extract_function(fname)              return self._functions[fname](argvals)
class Index(IndexOpsMixin, PandasObject):                  return self._constructor(values, **attributes)              except (TypeError, ValueError):                  pass           attributes.pop("tz", None)          return Index(values, **attributes)      def _update_inplace(self, result, **kwargs):
def read_batch_urls(batch_fd):      with contextlib.closing(batch_fd) as fd:          return [url for url in map(fixup, fd) if url]   def urlencode_postdata(*args, **kargs):     return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')
class Model(Container):                              ' and multiple workers may duplicate your data.'                              ' Please consider using the`keras.utils.Sequence'                              ' class.'))         if steps is None:             if is_sequence:                 steps = len(generator)             else:                 raise ValueError('`steps=None` is only valid for a generator'                                  ' based on the `keras.utils.Sequence` class.'                                  ' Please specify `steps` or use the'                                  ' `keras.utils.Sequence` class.')          enqueuer = None          try:
class _AtIndexer(_ScalarAccessIndexer):                          "can only have integer indexers"                      )              else:                 if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):                      raise ValueError(                          "At based indexing on an non-integer "                          "index can only have non-integer "
def assert_series_equal(      if check_categorical:          if is_categorical_dtype(left) or is_categorical_dtype(right):             assert_categorical_equal(                 left.values,                 right.values,                 obj=f"{obj} category",                 check_category_order=check_category_order,             )
class NPOIE(InfoExtractor):              'http://e.omroep.nl/metadata/aflevering/%s' % video_id,              video_id,             transform_source=strip_jsonp,          )          token_page = self._download_webpage(              'http://ida.omroep.nl/npoplayer/i.js',
class Axes(_AxesBase):          zdelta = 0.1         def line_props_with_rcdefaults(subkey, explicit, zdelta=0,                                        use_marker=True):              d = {k.split('.')[-1]: v for k, v in rcParams.items()                   if k.startswith(f'boxplot.{subkey}')}              d['zorder'] = zorder + zdelta             if not use_marker:                 d['marker'] = ''              if explicit is not None:                  d.update(                      cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))
from __future__ import unicode_literals as _unicode_literals from __future__ import absolute_import from __future__ import print_function as lol, with_function  u'hello'  U"hello"
class GroupBy(_GroupBy):          mask = self._cumcount_array(ascending=False) < n          return self._selected_obj[mask]     def _reindex_output(         self, output: FrameOrSeries, fill_value: Scalar = np.NaN     ) -> FrameOrSeries:          If we have categorical groupers, then we might want to make sure that          we have a fully re-indexed output to the levels. This means expanding
from pandas._config.localization import (  )  import pandas._libs.testing as _testing from pandas._typing import FilePathOrBuffer, FrameOrSeries  from pandas.compat import _get_lzma_file, _import_lzma  from pandas.core.dtypes.common import (
def conv2d(x, kernel, strides=(1, 1), padding='valid',  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None, dilation_rate=(1, 1)):
class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):          start = right[0]          if end < start:             return type(self)(data=[], dtype=self.dtype, freq=self.freq)          else:              lslice = slice(*left.slice_locs(start, end))             left_chunk = left._values[lslice]              return self._shallow_copy(left_chunk)      def _can_fast_union(self, other) -> bool:
Depth = int  NodeType = int  LeafID = int  Priority = int Index = int  LN = Union[Leaf, Node] SplitFunc = Callable[['Line', bool], Iterator['Line']]  out = partial(click.secho, bold=True, err=True)  err = partial(click.secho, fg='red', err=True)
class TaskProcess(AbstractTaskProcess):              self.task.trigger_event(Event.START, self.task)              t0 = time.time()              status = None              new_deps = self._run_get_new_deps()              if new_deps is None:                 status = DONE                 self.task.trigger_event(                     Event.PROCESSING_TIME, self.task, time.time() - t0)                 error_message = json.dumps(self.task.on_success())                 logger.info('[pid %s] Worker %s done      %s', os.getpid(),                             self.worker_id, self.task.task_id)                 self.task.trigger_event(Event.SUCCESS, self.task)              else:                 status = SUSPENDED                 logger.info(                     '[pid %s] Worker %s new requirements      %s',                     os.getpid(), self.worker_id, self.task.task_id)          except KeyboardInterrupt:              raise
class SparkSubmitTask(luigi.Task):          command = []          if value and isinstance(value, dict):              for prop, value in value.items():                 command += [name, '{0}={1}'.format(prop, value)]          return command      def _flag_arg(self, name, value):
class Function(object):          self.fetches = [tf.identity(x) for x in self.fetches]          self.session_kwargs = session_kwargs.copy()         self.run_options = session_kwargs.pop('options', None)         self.run_metadata = session_kwargs.pop('run_metadata', None)          if session_kwargs:              raise ValueError('Some keys in session_kwargs are not '                               'supported at this '
def get_write_function(output):              stderr.write(s)      elif isinstance(output, (pycompat.PathLike, str)):          def write(s):             with open(output, 'a') as output_file:                  output_file.write(s)      else:          assert isinstance(output, utils.WritableStream)
async def request_body_to_args(          for field in required_params:              value: Any = None              if received_body is not None:                 if (                     field.shape in sequence_shapes or field.type_ in sequence_types                 ) and isinstance(received_body, FormData):                      value = received_body.getlist(field.alias)                  else:                      value = received_body.get(field.alias)
class LineGenerator(Visitor[Line]):      current_line: Line = Factory(Line)      remove_u_prefix: bool = False     def line(self, indent: int = 0) -> Iterator[Line]:          If the line is empty, only emit if it makes sense.
def get_renderer(fig):              return canvas.get_renderer()          else:              from . import backend_bases             return backend_bases._get_renderer(fig)  def get_subplotspec_list(axes_list, grid_spec=None):
class DataFrame(NDFrame):              )          return result     def transpose(self, *args, copy: bool = False):          Transpose index and columns.
import pandas.core.indexes.base as ibase  from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name  from pandas.core.indexes.extension import ExtensionIndex, inherit_names  import pandas.core.missing as missing from pandas.core.ops import get_op_result_name  _index_doc_kwargs = dict(ibase._index_doc_kwargs)  _index_doc_kwargs.update(dict(target_klass="CategoricalIndex"))
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             if return_indexers:                 return multi_join_idx, lidx, ridx             else:                 return multi_join_idx          jl = list(overlap)[0]
from thefuck.utils import replace_argument, for_app  @for_app('php')  def match(command):     return " -s " in command.script  def get_new_command(command):      return replace_argument(command.script, "-s", "-S")
def js_to_json(code):          "(?:[^"\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^"\\]*"|          '(?:[^'\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^'\\]*'|          {comment}|,(?={skip}[\]}}])|         (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|          \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|          [0-9]+(?={skip}:)
def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None              ) def dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:      @wraps(split_func)     def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:         for l in split_func(line, py36):             normalize_prefix(l.leaves[0], inside_brackets=True)             yield l      return split_wrapper   @dont_increase_indentation  def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:
class ObjectBlock(Block):      def _can_hold_element(self, element: Any) -> bool:          return True     def should_store(self, value: ArrayLike) -> bool:          return not (              issubclass(                  value.dtype.type,
def get_handle(          from io import TextIOWrapper          g = TextIOWrapper(f, encoding=encoding, newline="")         if not isinstance(f, (BufferedIOBase, RawIOBase)):              handles.append(g)          f = g
def get_response(args, config_dir):      requests_session = get_requests_session()     requests_session.max_redirects = args.max_redirects      if not args.session and not args.session_read_only:          kwargs = get_requests_kwargs(args)
class Line:                      break          if commas > 1:             self.remove_trailing_comma()              return True          return False
def hist2d(  @_copy_docstring_and_deprecators(Axes.hlines)  def hlines(         y, xmin, xmax, colors=None, linestyles='solid', label='', *,          data=None, **kwargs):      return gca().hlines(          y, xmin, xmax, colors=colors, linestyles=linestyles,
def test_aggregate_mixed_types():      tm.assert_frame_equal(result, expected) @pytest.mark.xfail(reason="Not implemented.") def test_aggregate_udf_na_extension_type():            def aggfunc(x):         if all(x > 2):             return 1         else:             return pd.NA      df = pd.DataFrame({"A": pd.array([1, 2, 3])})     result = df.groupby([1, 1, 2]).agg(aggfunc)     expected = pd.DataFrame({"A": pd.array([1, pd.NA], dtype="Int64")}, index=[1, 2])     tm.assert_frame_equal(result, expected)    class TestLambdaMangling:      def test_basic(self):          df = pd.DataFrame({"A": [0, 0, 1, 1], "B": [1, 2, 3, 4]})
def read_pickle(path, compression="infer"):          f.close()          for _f in fh:              _f.close()         if should_close:             try:                 fp_or_buf.close()             except ValueError:                 pass
class CmdlineTest(unittest.TestCase):      def test_cmdline_ambiguous_class(self, logger):          self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])      @mock.patch("logging.getLogger")      @mock.patch("logging.StreamHandler")      def test_setup_interface_logging(self, handler, logger):
class MediaPipeline(object):          dfd.addCallback(self._check_media_to_download, request, info)          dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)          dfd.addErrback(lambda f: logger.error(             f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})          ) return dfd.addBoth(lambda _: wad)
def _get_collection_info(dep_map, existing_collections, collection, requirement,      if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):          display.vvvv("Collection requirement '%s' is a tar artifact" % to_text(collection))          b_tar_path = to_bytes(collection, errors='surrogate_or_strict')     elif urlparse(collection).scheme.lower() in ['http', 'https']:          display.vvvv("Collection requirement '%s' is a URL to a tar artifact" % collection)         try:             b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)         except urllib_error.URLError as err:             raise AnsibleError("Failed to download collection tar from '%s': %s"                                % (to_native(collection), to_native(err)))      if b_tar_path:          req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
from pandas.core.dtypes.common import (  from pandas.core.dtypes.dtypes import IntervalDtype  from pandas.core.dtypes.generic import (      ABCDatetimeIndex,     ABCExtensionArray,      ABCIndexClass,      ABCInterval,      ABCIntervalIndex,
class SeriesGroupBy(GroupBy):                      periods=periods, fill_method=fill_method, limit=limit, freq=freq                  )              )         if fill_method is None:             fill_method = "pad"             limit = 0          filled = getattr(self, fill_method)(limit=limit)          fill_grp = filled.groupby(self.grouper.codes)          shifted = fill_grp.shift(periods=periods, freq=freq)
async def func():                  self.async_inc, arange(8), batch_size=3              )          ]   def awaited_generator_value(n):     return (await awaitable for awaitable in awaitable_list)   def make_arange(n):     return (i * 2 for i in range(n) if await wrap(i))
from pandas.core.dtypes.cast import (      validate_numeric_casting,  )  from pandas.core.dtypes.common import (      ensure_int64,      ensure_platform_int,      infer_dtype_from_object,
class DatetimeIndex(DatetimeTimedeltaMixin):              return Timestamp(value).asm8          raise ValueError("Passed item and index have different timezone")     def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:         if not is_datetime64_any_dtype(dtype):             return False         if self.tz is not None:              return is_datetime64tz_dtype(dtype)          return is_datetime64_dtype(dtype)
class TestPeriodIndex:          p2 = pd.Period("2014-01-04", freq=freq)          assert pidx.searchsorted(p2) == 3         assert pidx.searchsorted(pd.NaT) == 0          msg = "Input has different freq=H from PeriodArray"          with pytest.raises(IncompatibleFrequency, match=msg):              pidx.searchsorted(pd.Period("2014-01-01", freq="H"))         msg = "Input has different freq=5D from PeriodArray"          with pytest.raises(IncompatibleFrequency, match=msg):              pidx.searchsorted(pd.Period("2014-01-01", freq="5D"))     def test_searchsorted_invalid(self):         pidx = pd.PeriodIndex(             ["2014-01-01", "2014-01-02", "2014-01-03", "2014-01-04", "2014-01-05"],             freq="D",         )          other = np.array([0, 1], dtype=np.int64)          msg = "requires either a Period or PeriodArray"         with pytest.raises(TypeError, match=msg):             pidx.searchsorted(other)          with pytest.raises(TypeError, match=msg):             pidx.searchsorted(other.astype("timedelta64[ns]"))          with pytest.raises(TypeError, match=msg):             pidx.searchsorted(np.timedelta64(4))          with pytest.raises(TypeError, match=msg):             pidx.searchsorted(np.timedelta64("NaT", "ms"))          with pytest.raises(TypeError, match=msg):             pidx.searchsorted(np.datetime64(4, "ns"))          with pytest.raises(TypeError, match=msg):             pidx.searchsorted(np.datetime64("NaT", "ns"))   class TestPeriodIndexConversion:      def test_tolist(self):
class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):          super().test_merge_on_extension_array_duplicates(data)     @skip_nested     def test_transpose(self, data):         super().test_transpose(data)   class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):      @skip_nested
def standardize_weights(y,                               ' The classes %s exist in the data but not in '                               '`class_weight`.'                               % (existing_classes - existing_class_weight))      if sample_weight is not None and class_sample_weight is not None:         return sample_weight * class_sample_weight     if sample_weight is not None:         return sample_weight     if class_sample_weight is not None:         return class_sample_weight       if sample_weight_mode is None:         return np.ones((y.shape[0],), dtype=K.floatx())      else:         return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())  def check_num_samples(ins,
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):          if not is_list:              start = self.index[0]              if isinstance(self.index, PeriodIndex):                 where = Period(where, freq=self.index.freq)              if where < start:                  if not is_series:
class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):              return res          return CategoricalIndex(res, name=self.name)     def _wrap_joined_index(         self, joined: np.ndarray, other: "CategoricalIndex"     ) -> "CategoricalIndex":         name = get_op_result_name(self, other)         return self._create_from_codes(joined, name=name)   CategoricalIndex._add_numeric_methods_add_sub_disabled()  CategoricalIndex._add_numeric_methods_disabled()
def js_to_json(code):              }.get(m.group(0), m.group(0)), v[1:-1])          INTEGER_TABLE = (             (r'^(0[xX][0-9a-fA-F]+)\s*:?$', 16),             (r'^(0+[0-7]+)\s*:?$', 8),          )          for regex, base in INTEGER_TABLE:              im = re.match(regex, v)              if im:                 i = int(im.group(1), base)                  return '"%d":' % i if v.endswith(':') else '%d' % i          return '"%s"' % v
import numpy as np  import time  import json  import warnings import io import sys  from collections import deque  from collections import OrderedDict
def generate_commands(vlan_id, to_set, to_remove):      if "vlan_id" in to_remove:          return ["no vlan {0}".format(vlan_id)]     for key in to_remove:         if key in to_set.keys():             continue         commands.append("no {0}".format(key))       for key, value in to_set.items():          if key == "vlan_id" or value is None:              continue          commands.append("{0} {1}".format(key, value))      if commands:          commands.insert(0, "vlan {0}".format(vlan_id))      return commands
def _ensure_default_collection(collection_list=None):  class CollectionSearch:     _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,                                   always_post_validate=True, static=True)      def _load_collections(self, attr, ds):
from blib2to3 import pygram, pytree  from blib2to3.pgen2 import driver, token  from blib2to3.pgen2.grammar import Grammar  from blib2to3.pgen2.parse import ParseError from blib2to3.pgen2.tokenize import TokenizerConfig  __version__ = "19.3b0"
class DataFrame(NDFrame):          Parameters          ----------         *args : tuple, optional             Accepted for compatibility with NumPy.         copy : bool, default False             Whether to copy the data after transposing, even for DataFrames             with a single dtype.              Note that a copy is always required for mixed dtype DataFrames,             or for DataFrames with any extension types.          Returns          -------
def month_by_name(name):          return None def fix_xml_ampersands(xml_str):     return re.sub(         r'&(?!amp;|lt;|gt;|apos;|quot;|         u'&amp;',         xml_str)  def setproctitle(title):
class Model(Container):                          val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)                          validation_generator = val_enqueuer.get()                      else:                         if isinstance(validation_data, Sequence):                             validation_generator = iter(validation_data)                         else:                             validation_generator = validation_data                  else:                      if len(validation_data) == 2:                          val_x, val_y = validation_data
def melt(          else:              id_vars = list(id_vars)             missing = Index(com.flatten(id_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'id_vars' are not present"
LOGGING_CONFIG_DEFAULTS = dict(      version=1,      disable_existing_loggers=False,      loggers={         "sanic.root": {"level": "INFO", "handlers": ["console"]},          "sanic.error": {              "level": "INFO",              "handlers": ["error_console"],
class GroupBy(_GroupBy):          output = output.drop(labels=list(g_names), axis=1)         output = output.set_index(self.grouper.result_index).reindex(             index, copy=False, fill_value=fill_value         )
def unified_strdate(date_str):          '%d/%m/%Y',          '%d/%m/%y',          '%Y/%m/%d %H:%M:%S',         '%d/%m/%Y %H:%M:%S',          '%Y-%m-%d %H:%M:%S',          '%d.%m.%Y %H:%M',          '%d.%m.%Y %H.%M',
class GeneratorEnqueuer(SequenceEnqueuer):                  else:                      thread.join(timeout)         if self._manager:             self._manager.shutdown()          self._threads = []          self._stop_event = None
class Categorical(NDArrayBackedExtensionArray, PandasObject):          Returns True if `key` is in this Categorical.         if is_valid_nat_for_dtype(key, self.categories.dtype):              return self.isna().any()          return contains(self, key, container=self._codes)
class Function(object):                          'supported with sparse inputs.')                  return self._legacy_call(inputs)               if (self.run_metadata and                     StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):                 if py_any(is_tensor(x) for x in inputs):                     raise ValueError(                         'In order to feed symbolic tensors to a Keras model and set '                         '`run_metadata`, you need tensorflow 1.10 or higher.')                 return self._legacy_call(inputs)               return self._call(inputs)          else:              if py_any(is_tensor(x) for x in inputs):
class DatetimeIndexOpsMixin(ExtensionIndex):              return (lhs_mask & rhs_mask).nonzero()[0]     @Appender(Index.get_indexer_non_unique.__doc__)     def get_indexer_non_unique(self, target):         target = ensure_index(target)         pself, ptarget = self._maybe_promote(target)         if pself is not self or ptarget is not target:             return pself.get_indexer_non_unique(ptarget)          if not self._is_comparable_dtype(target.dtype):             no_matches = -1 * np.ones(self.shape, dtype=np.intp)             return no_matches, no_matches          tgt_values = target.asi8         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)         return ensure_platform_int(indexer), missing       __add__ = make_wrapped_arith_op("__add__")
def map_obj_to_commands(updates, module):  def map_config_to_obj(module):     out = get_config(module, flags='| begin banner %s' % module.params['banner'])     if out:         regex = 'banner ' + module.params['banner'] + ' ^C\n'         if search('banner ' + module.params['banner'], out, M):             output = str((out.split(regex))[1].split("^C\n")[0])          else:              output = None     else:         output = None      obj = {'banner': module.params['banner'], 'state': 'absent'}      if output:          obj['text'] = output
class DataFrame(NDFrame):          scalar          if takeable:             series = self._ixs(col, axis=1)             return series._values[index]          series = self._get_item_cache(col)          engine = self.index._engine
def predict_generator(model, generator,      try:          if workers > 0:             if use_sequence_api:                  enqueuer = OrderedEnqueuer(                      generator,                      use_multiprocessing=use_multiprocessing)
def _unstack_multiple(data, clocs, fill_value=None):      comp_ids, obs_ids = compress_group_index(group_index, sort=False)      recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)     if not rlocs:          dummy_index = Index(obs_ids, name="__placeholder__")      else:
class TimedeltaIndex(                  "represent unambiguous timedelta values durations."              )         if isinstance(data, TimedeltaArray) and freq is None:              if copy:                  data = data.copy()              return cls._simple_new(data, name=name, freq=freq)
def _add_margins(      row_names = result.index.names      try:           for dtype in set(result.dtypes):              cols = result.select_dtypes([dtype]).columns             margin_dummy[cols] = margin_dummy[cols].apply(                 maybe_downcast_to_dtype, args=(dtype,)             )          result = result.append(margin_dummy)      except TypeError:
def crosstab(          kwargs = {"aggfunc": aggfunc}      table = df.pivot_table(         ["__dummy__"],          index=rownames,          columns=colnames,          margins=margins,
class Categorical(ExtensionArray, PandasObject):                      raise ValueError("fill value must be in categories")                  values_codes = _get_codes_for_values(value, self.categories)                 indexer = np.where(codes == -1)                 codes[indexer] = values_codes[indexer]              elif is_hashable(value):
optional.          if bin_range is not None:              bin_range = self.convert_xunits(bin_range)         if not cbook.is_scalar_or_string(bins):             bins = self.convert_xunits(bins)           if weights is not None:              w = cbook._reshape_2D(weights, 'weights')
def evaluate_generator(model, generator,      try:          if workers > 0:             if use_sequence_api:                  enqueuer = OrderedEnqueuer(                      generator,                      use_multiprocessing=use_multiprocessing)
def l2_normalize(x, axis=-1):      return x / np.sqrt(y) def in_top_k(predictions, targets, k):     top_k = np.argsort(-predictions)[:, :k]     targets = targets.reshape(-1, 1)     return np.any(targets == top_k, axis=-1)    def binary_crossentropy(target, output, from_logits=False):      if not from_logits:          output = np.clip(output, 1e-7, 1 - 1e-7)
class NDFrame(PandasObject, SelectionMixin):              data = self.fillna(method=fill_method, limit=limit, axis=axis)          rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1         if freq is not None:               rs = rs.loc[~rs.index.duplicated()]             rs = rs.reindex_like(data)          return rs      def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):
import numpy as np  import pytest from pandas.compat.numpy import _np_version_under1p14  import pandas as pd  import pandas._testing as tm from pandas.core.arrays.boolean import BooleanDtype from pandas.tests.extension import base  def make_data():
def get_future_imports(node: Node) -> Set[str]:              module_name = first_child.children[1]              if not isinstance(module_name, Leaf) or module_name.value != "__future__":                  break             imports |= set(get_imports_from_children(first_child.children[3:]))          else:              break      return imports
class Axes(_AxesBase):              except ValueError: pass              else:                   if c.shape == (1, 4) or c.shape == (1, 3):                     c_is_mapped = False                     if c.size != xsize:                         valid_shape = False                 elif c.size == xsize:                      c = c.ravel()                      c_is_mapped = True else:
default: :rc:`scatter.edgecolors`              marker_obj.get_transform())          if not marker_obj.is_filled():              edgecolors = 'face'             if linewidths is None:                 linewidths = rcParams['lines.linewidth']             elif np.iterable(linewidths):                 linewidths = [                     lw if lw is not None else rcParams['lines.linewidth']                     for lw in linewidths]          offsets = np.ma.column_stack([x, y])
from __future__ import print_function  import warnings  import numpy as np from .training_utils import is_sequence  from .training_utils import iter_sequence_infinite  from .. import backend as K  from ..utils.data_utils import Sequence
class GroupBy(_GroupBy):          base_func = getattr(libgroupby, how)          for name, obj in self._iterate_slices():             values = obj._data._values               if aggregate:                  result_sz = ngroups              else:                 result_sz = len(values)              if not cython_dtype:                 cython_dtype = values.dtype              result = np.zeros(result_sz, dtype=cython_dtype)              func = partial(base_func, result, labels)              inferences = None              if needs_values:                 vals = values                  if pre_processing:                      vals, inferences = pre_processing(vals)                  func = partial(func, vals)              if needs_mask:                 mask = isna(values).view(np.uint8)                  func = partial(func, mask)              if needs_ngroups:
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def standardize_weights(y,                               ' for an input with shape ' +                               str(y.shape) + '. '                               'sample_weight cannot be broadcast.')      class_sample_weight = None     if isinstance(class_weight, dict):          if len(y.shape) > 2:              raise ValueError('`class_weight` not supported for '                               '3+ dimensional targets.')         if len(y.shape) == 2:             if y.shape[1] > 1:                 y_classes = np.argmax(y, axis=1)             elif y.shape[1] == 1:                 y_classes = np.reshape(y, y.shape[0])          else:              y_classes = y         class_sample_weight = np.asarray(             [class_weight[cls] for cls in y_classes if cls in class_weight])         if len(class_sample_weight) != len(y_classes):              existing_classes = set(y_classes)              existing_class_weight = set(class_weight.keys())
class _LocationIndexer(_NDFrameIndexerBase):         if self.name == "loc":             self._ensure_listlike_indexer(key)           if self.axis is not None:              return self._convert_tuple(key, is_setter=True)
from twisted.internet import defer  from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback  from scrapy.utils.spider import iterate_spider_output  from scrapy.utils.misc import load_object from scrapy.utils.log import logformatter_adapter, failure_to_exc_info  from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest  from scrapy import signals  from scrapy.http import Request, Response
def verify_collections(collections, search_paths, apis, validate_certs, ignore_e                      for search_path in search_paths:                          b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')                          if os.path.isdir(b_search_path):                             if not os.path.isfile(os.path.join(to_text(b_search_path, errors='surrogate_or_strict'), 'MANIFEST.json')):                                 raise AnsibleError(                                     message="Collection %s does not appear to have a MANIFEST.json. " % collection_name +                                             "A MANIFEST.json is expected if the collection has been built and installed via ansible-galaxy."                                 )                              local_collection = CollectionRequirement.from_path(b_search_path, False)                              break                      if local_collection is None:
def _use_inf_as_na(key):          globals()["_isna"] = _isna_new def _isna_ndarraylike(obj, old: bool = False):      values = getattr(obj, "_values", obj)      dtype = values.dtype     if is_extension_array_dtype(dtype):         if old:             result = values.isna() | (values == -np.inf) | (values == np.inf)         else:             result = values.isna()     elif is_string_dtype(dtype):         result = _isna_string_dtype(values, dtype, old=old)      elif needs_i8_conversion(dtype):          result = values.view("i8") == iNaT      else:         if old:             result = ~np.isfinite(values)         else:             result = np.isnan(values)      if isinstance(obj, ABCSeries):
import sys  import logging  import warnings from logging.config import dictConfig  from twisted.python.failure import Failure from twisted.python import log as twisted_log import scrapy from scrapy.settings import overridden_settings, Settings  from scrapy.exceptions import ScrapyDeprecationWarning  logger = logging.getLogger(__name__) def failure_to_exc_info(failure):     records.     This filter will replace Scrapy loggers' names with 'scrapy'. This mimics     the old Scrapy log behaviour and helps shortening long names.     Since it can't be set for just one logger (it won't propagate for its     children), it's going to be set in the root handler, with a parametrized     `loggers` list where it should act.      This function does:       - Route warnings and twisted logging through Python standard logging       - Set FailureFormatter filter on Scrapy logger       - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively       - Create a handler for the root logger according to given settings      Taken from:         http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/      def __init__(self, crawler, *args, **kwargs):         super(LogCounterHandler, self).__init__(*args, **kwargs)         self.crawler = crawler      def emit(self, record):         sname = 'log_count/{}'.format(record.levelname)         self.crawler.stats.inc_value(sname)   def logformatter_adapter(logkws):     if not {'level', 'msg', 'args'} <= set(logkws):         warnings.warn('Missing keys in LogFormatter method',                       ScrapyDeprecationWarning)      if 'format' in logkws:         warnings.warn('`format` key in LogFormatter methods has been '                       'deprecated, use `msg` instead',                       ScrapyDeprecationWarning)      level = logkws.get('level', logging.INFO)     message = logkws.get('format', logkws.get('msg'))       args = logkws if not logkws.get('args') else logkws['args']      return (level, message, args)
class Warnings(object):              "loaded. (Shape: {shape})")      W021 = ("Unexpected hash collision in PhraseMatcher. Matches may be "              "incorrect. Modify PhraseMatcher._terminal_hash to fix.")     W022 = ("Training a new part-of-speech tagger using a model with no "             "lemmatization rules or data. This means that the trained model "             "may not be able to lemmatize correctly. If this is intentional "             "or the language you're using doesn't have lemmatization data, "             "you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. "             "If this is surprising, make sure you have the spacy-lookups-data "             "package installed.")  @add_codes
from __future__ import division  import os  import re  import sys import errno  import mimetypes  import threading  from time import sleep, time
def na_logical_op(x: np.ndarray, y, op):                      f"and scalar of type [{typ}]"                  )     return result.reshape(x.shape)  def logical_op(
def make_data():  @pytest.fixture  def dtype():     return BooleanDtype()  @pytest.fixture
class Task(object):          self.failures.add_failure()      def has_excessive_failures(self):         if self.failures.first_failure_time is not None:              if (time.time() >= self.failures.first_failure_time +                      self.disable_hard_timeout):                  return True
class RNN(Layer):      @property      def non_trainable_weights(self):          if isinstance(self.cell, Layer):             if not self.trainable:                 return self.cell.weights              return self.cell.non_trainable_weights          return []
def unified_strdate(date_str):      upload_date = None     date_str = date_str.replace(',', ' ')     date_str = re.sub(r' ?(\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)      format_expressions = [          '%d %B %Y',          '%B %d %Y',
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class TestGetIndexer:          expected = np.array([0] * size, dtype="intp")          tm.assert_numpy_array_equal(result, expected)     @pytest.mark.parametrize(         "target",         [             IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),             IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),             IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed="both"),             [-1, 0, 0.5, 1, 2, 2.5, np.nan],             ["foo", "foo", "bar", "baz"],         ],     )     def test_get_indexer_categorical(self, target, ordered_fixture):          index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])         categorical_target = CategoricalIndex(target, ordered=ordered_fixture)          result = index.get_indexer(categorical_target)         expected = index.get_indexer(target)         tm.assert_numpy_array_equal(result, expected)       @pytest.mark.parametrize(          "tuples, closed",          [
def run_hook(hook_name, project_dir, context):      :param project_dir: The directory to execute the script from.      :param context: Cookiecutter project context.     scripts = find_hook(hook_name)     if not scripts:          logger.debug('No %s hook found', hook_name)          return      logger.debug('Running hook %s', hook_name)     for script in scripts:         run_script_with_context(script, project_dir, context)
from pandas.core.dtypes.common import (      is_bool,      is_bool_dtype,      is_categorical_dtype,      is_datetime64tz_dtype,      is_datetimelike,      is_dtype_equal,
class Sequential(Model):                  at the end of every epoch. It should typically                  be equal to the number of samples of your                  validation dataset divided by the batch size.                 Optional for `Sequence`: if unspecified, will use                 the `len(validation_data)` as a number of steps.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Maximum size for the generator queue
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                                                          kshp=kernel_shape,                                                          subsample=strides,                                                          border_mode=th_padding,                                                         filter_flip=not flip_filters,                                                         filter_dilation=dilation_rate)      conv_out = op(kernel, x, output_shape[2:])      conv_out = _postprocess_conv2d_output(conv_out, x, padding,                                            kernel_shape, strides, data_format)
class StringMethods(NoNewAttributesMixin):      _doc_args["istitle"] = dict(type="titlecase", method="istitle")      _doc_args["isnumeric"] = dict(type="numeric", method="isnumeric")      _doc_args["isdecimal"] = dict(type="decimal", method="isdecimal")       isalnum = _noarg_wrapper(          lambda x: x.isalnum(),          name="isalnum",          docstring=_shared_docs["ismethods"] % _doc_args["isalnum"],          returns_string=False,         dtype=bool,      )      isalpha = _noarg_wrapper(          lambda x: x.isalpha(),          name="isalpha",          docstring=_shared_docs["ismethods"] % _doc_args["isalpha"],          returns_string=False,         dtype=bool,      )      isdigit = _noarg_wrapper(          lambda x: x.isdigit(),          name="isdigit",          docstring=_shared_docs["ismethods"] % _doc_args["isdigit"],          returns_string=False,         dtype=bool,      )      isspace = _noarg_wrapper(          lambda x: x.isspace(),          name="isspace",          docstring=_shared_docs["ismethods"] % _doc_args["isspace"],          returns_string=False,         dtype=bool,      )      islower = _noarg_wrapper(          lambda x: x.islower(),          name="islower",          docstring=_shared_docs["ismethods"] % _doc_args["islower"],          returns_string=False,         dtype=bool,      )      isupper = _noarg_wrapper(          lambda x: x.isupper(),          name="isupper",          docstring=_shared_docs["ismethods"] % _doc_args["isupper"],          returns_string=False,         dtype=bool,      )      istitle = _noarg_wrapper(          lambda x: x.istitle(),          name="istitle",          docstring=_shared_docs["ismethods"] % _doc_args["istitle"],          returns_string=False,         dtype=bool,      )      isnumeric = _noarg_wrapper(          lambda x: x.isnumeric(),          name="isnumeric",          docstring=_shared_docs["ismethods"] % _doc_args["isnumeric"],          returns_string=False,         dtype=bool,      )      isdecimal = _noarg_wrapper(          lambda x: x.isdecimal(),          name="isdecimal",          docstring=_shared_docs["ismethods"] % _doc_args["isdecimal"],          returns_string=False,         dtype=bool,      )      @classmethod
def categorical_accuracy(y_true, y_pred):  def sparse_categorical_accuracy(y_true, y_pred):      return K.cast(K.equal(K.flatten(y_true),                            K.cast(K.argmax(y_pred, axis=-1), K.floatx())),                    K.floatx())
from asyncio.base_events import BaseEventLoop  from concurrent.futures import Executor, ProcessPoolExecutor  from enum import Enum, Flag  from functools import partial, wraps import io  import keyword  import logging  from multiprocessing import Manager
def hide_fmt_off(node: Node) -> bool:                  hidden_value = (                      comment.value + "\n" + "".join(str(n) for n in ignored_nodes)                  )                 if hidden_value.endswith("\n"):                       hidden_value = hidden_value[:-1]                  first_idx = None                  for ignored in ignored_nodes:                      index = ignored.remove()
class StringMethods(NoNewAttributesMixin):          if isinstance(others, ABCSeries):              return [others]          elif isinstance(others, ABCIndexClass):             return [Series(others._values, index=idx)]          elif isinstance(others, ABCDataFrame):              return [others[x] for x in others]          elif isinstance(others, np.ndarray) and others.ndim == 2:
class TestBackend(object):                             np.asarray([-5., -4., 0., 4., 9.],                                        dtype=np.float32))  if __name__ == '__main__':      pytest.main([__file__])
from . import grammar, parse, token, tokenize, pgen  class Driver(object):     def __init__(         self,         grammar,         convert=None,         logger=None,         tokenizer_config=tokenize.TokenizerConfig(),     ):          self.grammar = grammar          if logger is None:              logger = logging.getLogger(__name__)          self.logger = logger          self.convert = convert         self.tokenizer_config = tokenizer_config      def parse_tokens(self, tokens, debug=False):
class DatetimeLikeBlockMixin:          return self.array_values()     def iget(self, key):           result = super().iget(key)         if isinstance(result, np.datetime64):             result = Timestamp(result)         elif isinstance(result, np.timedelta64):             result = Timedelta(result)         return result   class DatetimeBlock(DatetimeLikeBlockMixin, Block):      __slots__ = ()
class IntervalArray(IntervalMixin, ExtensionArray):          return self.left.size     def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:         if not len(self) or periods == 0:             return self.copy()          if isna(fill_value):             fill_value = self.dtype.na_value               empty_len = min(abs(periods), len(self))         if isna(fill_value):             fill_value = self.left._na_value             empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))         else:             empty = self._from_sequence([fill_value] * empty_len)          if periods > 0:             a = empty             b = self[:-periods]         else:             a = self[abs(periods) :]             b = empty         return self._concat_same_type([a, b])       def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):          Take elements from the IntervalArray.
class DataFrame(NDFrame):              if can_concat:                  if how == "left":                     res = concat(                         frames, axis=1, join="outer", verify_integrity=True, sort=sort                     )                      return res.reindex(self.index, copy=False)                  else:                     return concat(                         frames, axis=1, join=how, verify_integrity=True, sort=sort                     )              joined = frames[0]
class GeneratorEnqueuer(SequenceEnqueuer):          while self.is_running():              if not self.queue.empty():                 success, value = self.queue.get()                  if not success:                     six.reraise(value.__class__, value, value.__traceback__)                  if value is not None:                     yield value              else:                  all_finished = all([not thread.is_alive() for thread in self._threads])                  if all_finished and self.queue.empty():                      raise StopIteration()                  else:                      time.sleep(self.wait_time)           while not self.queue.empty():             success, value = self.queue.get()             if not success:                 six.reraise(value.__class__, value, value.__traceback__)
class GalaxyCLI(CLI):                  if not os.path.exists(b_dir_path):                      os.makedirs(b_dir_path)         display.display("- %s %s was created successfully" % (galaxy_type.title(), obj_name))      def execute_info(self):
async def serialize_response(      exclude: Union[SetIntStr, DictIntStrAny] = set(),      by_alias: bool = True,      exclude_unset: bool = False,     exclude_defaults: bool = False,     exclude_none: bool = False,      is_coroutine: bool = True,  ) -> Any:      if field:          errors = []          response_content = _prepare_response_content(             response_content,             by_alias=by_alias,             exclude_unset=exclude_unset,             exclude_defaults=exclude_defaults,             exclude_none=exclude_none,          )          if is_coroutine:              value, errors_ = field.validate(response_content, {}, loc=("response",))
from pandas.core.dtypes.common import (  from pandas.core.dtypes.dtypes import CategoricalDtype  from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries  from pandas.core.dtypes.inference import is_hashable from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna, notna  from pandas.core import ops  from pandas.core.accessor import PandasDelegate, delegate_names
class Sequential(Model):      def __init__(self, layers=None, name=None):          super(Sequential, self).__init__(name=name)         self._build_input_shape = None          if layers:
class TestTimeSeries(TestData):          )          tm.assert_index_equal(expected.index, result.index)      def test_pct_change(self):          rs = self.ts.pct_change(fill_method=None)          assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)
class LSTMCell(Layer):          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.implementation = implementation          self.state_size = (self.units, self.units)         self.output_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
from six.moves.urllib.parse import urlunparse  from scrapy.utils.httpobj import urlparse_cached  from scrapy.exceptions import NotConfigured from scrapy.utils.python import to_bytes  class HttpProxyMiddleware(object):
def main():                  module.fail_json(msg='Unable to parse pool_ids option.')              pool_id, quantity = list(value.items())[0]          else:             pool_id, quantity = value, None         pool_ids[pool_id] = quantity      consumer_type = module.params["consumer_type"]      consumer_name = module.params["consumer_name"]      consumer_id = module.params["consumer_id"]
import pytest  import pandas.util._test_decorators as td  import pandas as pd from pandas import DataFrame, Series  import pandas._testing as tm
class CmdlineTest(unittest.TestCase):          self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'      def _execute(self, *new_args, **kwargs):         encoding = getattr(sys.stdout, 'encoding') or 'utf-8'          args = (sys.executable, '-m', 'scrapy.cmdline') + new_args          proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)         comm = proc.communicate()[0].strip()         return comm.decode(encoding)      def test_default_settings(self):          self.assertEqual(self._execute('settings', '--get', 'TEST1'), \
class _LocationIndexer(_NDFrameIndexerBase):                  raise              raise IndexingError(key) from e     def _ensure_listlike_indexer(self, key, axis=None):         column_axis = 1           if self.ndim != 2:             return          if isinstance(key, tuple):               key = key[column_axis]             axis = column_axis          if (             axis == column_axis             and not isinstance(self.obj.columns, ABCMultiIndex)             and is_list_like_indexer(key)             and not com.is_bool_indexer(key)             and all(is_hashable(k) for k in key)         ):             for k in key:                 try:                     self.obj[k]                 except KeyError:                     self.obj[k] = np.nan       def __setitem__(self, key, value):          if isinstance(key, tuple):              key = tuple(com.apply_if_callable(x, self.obj) for x in key)
def to_pickle(obj, path, compression="infer", protocol=pickle.HIGHEST_PROTOCOL):      >>> import os      >>> os.remove("./dummy.pkl")     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(         filepath_or_buffer, compression=compression, mode="wb"     )     if not isinstance(fp_or_buf, str) and compression == "infer":         compression = None     f, fh = get_handle(fp_or_buf, "wb", compression=compression, is_text=False)      if protocol < 0:          protocol = pickle.HIGHEST_PROTOCOL      try:
class DictParameter(Parameter):          return json.loads(s, object_pairs_hook=_FrozenOrderedDict)      def serialize(self, x):         return json.dumps(x, cls=_DictParamEncoder)  class ListParameter(Parameter):
def generate_tokens(readline):                          yield (STRING, token, spos, epos, line) elif initial.isidentifier():                      if token in ('async', 'await'):                         if async_is_reserved_keyword or async_def:                              yield (ASYNC if token == 'async' else AWAIT,                                     token, spos, epos, line)                              continue
class MarkerStyle:          self._snap_threshold = None          self._joinstyle = 'round'          self._capstyle = 'butt'            self._filled = self._fillstyle != 'none'          self._marker_function()      def __bool__(self):
class Model(Container):              return averages      @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps=None,                            max_queue_size=10,                            workers=1,                            use_multiprocessing=False,
class ExtensionBlock(Block):      def setitem(self, indexer, value):         Attempt self.values[indexer] = value, possibly creating a new array.          This differs from Block.setitem by not allowing setitem to change          the dtype of the Block.
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)      def __sub__(self, other):          other = lib.item_from_zerodim(other)         if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):              return NotImplemented
class FacebookIE(InfoExtractor):              'timezone': '-60',              'trynum': '1',              }         request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))          request.add_header('Content-Type', 'application/x-www-form-urlencoded')          try:             login_results = self._download_webpage(request, None,                 note='Logging in', errnote='unable to fetch login page')              if re.search(r'<form(.*)name="login"(.*)</form>', login_results) is not None:                  self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')                  return              check_form = {                 'fb_dtsg': self._search_regex(r'name="fb_dtsg" value="(.+?)"', login_results, 'fb_dtsg'),                  'nh': self._search_regex(r'name="nh" value="(\w*?)"', login_results, 'nh'),                  'name_action_selected': 'dont_save',                 'submit[Continue]': self._search_regex(r'<button[^>]+value="(.*?)"[^>]+name="submit\[Continue\]"', login_results, 'continue'),              }             check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))              check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')             check_response = self._download_webpage(check_req, None,                 note='Confirming login')              if re.search(r'id="checkpointSubmitButton"', check_response) is not None:                  self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')          except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)      def __add__(self, other):          other = lib.item_from_zerodim(other)         if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):              return NotImplemented
class GroupBy(_GroupBy):          if isinstance(self.obj, Series):              result.name = self.obj.name         return self._reindex_output(result, fill_value=0)      @classmethod      def _add_numeric_operations(cls):
def jsonable_encoder(          data,          by_alias=by_alias,          exclude_unset=exclude_unset,         exclude_defaults=exclude_defaults,         exclude_none=exclude_none,          custom_encoder=custom_encoder,          sqlalchemy_safe=sqlalchemy_safe,      )
class TestCartesianProduct:          tm.assert_index_equal(result1, expected1)          tm.assert_index_equal(result2, expected2)     def test_tzaware_retained(self):         x = date_range("2000-01-01", periods=2, tz="US/Pacific")         y = np.array([3, 4])         result1, result2 = cartesian_product([x, y])          expected = x.repeat(2)         tm.assert_index_equal(result1, expected)      def test_tzaware_retained_categorical(self):         x = date_range("2000-01-01", periods=2, tz="US/Pacific").astype("category")         y = np.array([3, 4])         result1, result2 = cartesian_product([x, y])          expected = x.repeat(2)         tm.assert_index_equal(result1, expected)       def test_empty(self):          X = [[], [0, 1], []]
VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {          Feature.NUMERIC_UNDERSCORES,          Feature.TRAILING_COMMA_IN_CALL,          Feature.TRAILING_COMMA_IN_DEF,         Feature.ASYNC_IS_RESERVED_KEYWORD,      },      TargetVersion.PY38: {          Feature.UNICODE_LITERALS,
class FigureCanvasBase:          Returns          -------         axes : `~matplotlib.axes.Axes` or None             The topmost visible axes containing the point, or None if no axes.          axes_list = [a for a in self.figure.get_axes()                      if a.patch.contains_point(xy) and a.get_visible()]          if axes_list:              axes = cbook._topmost_artist(axes_list)          else:
def match(command):  def get_new_command(command):     broken_cmd = re.findall(r'ERROR: unknown command "([^"]+)"',                              command.output)[0]     new_cmd = re.findall(r'maybe you meant "([^"]+)"', command.output)[0]      return replace_argument(command.script, broken_cmd, new_cmd)
class Text(Artist):          if not self.get_visible():              return Bbox.unit()         if dpi is None:             dpi = self.figure.dpi          if self.get_text() == '':             with cbook._setattr_cm(self.figure, dpi=dpi):                 tx, ty = self._get_xy_display()                 return Bbox.from_bounds(tx, ty, 0, 0)          if renderer is not None:              self._renderer = renderer
class APIRouter(routing.Router):                      include_in_schema=route.include_in_schema,                      name=route.name,                  )             elif isinstance(route, routing.WebSocketRoute):                 self.add_websocket_route(                     prefix + route.path, route.endpoint, name=route.name                 )      def get(          self,
class TestBackend(object):          assert output == [21.]          assert K.get_session().run(fetches=[x, y]) == [30., 40.]     @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),                          reason='Uses the `options` and `run_metadata` arguments.')      def test_function_tf_run_options_with_run_metadata(self):          from tensorflow.core.protobuf import config_pb2
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries  from pandas.core import algorithms  from pandas.core.accessor import PandasDelegate from pandas.core.arrays import (     DatetimeArray,     ExtensionArray,     ExtensionOpsMixin,     TimedeltaArray, )  from pandas.core.arrays.datetimelike import (      DatetimeLikeArrayMixin,      _ensure_datetimelike_to_i8,
class _Rolling_and_Expanding(_Rolling):      )      def count(self):           assert not isinstance(self.window, BaseIndexer)          blocks, obj = self._create_blocks()          results = []
class TruncatedNormal(Initializer):          self.seed = seed      def __call__(self, shape, dtype=None):         x = K.truncated_normal(shape, self.mean, self.stddev,                                dtype=dtype, seed=self.seed)         if self.seed is not None:             self.seed += 1         return x      def get_config(self):          return {
class TimedeltaIndex(          this, other = self, other          if this._can_fast_union(other):             return this._fast_union(other, sort=sort)          else:              result = Index._union(this, other, sort=sort)              if isinstance(result, TimedeltaIndex):
class Model(Container):                          if isinstance(metric_fn, Layer) and metric_fn.stateful:                              self.stateful_metric_names.append(metric_name)                             self.stateful_metric_functions.append(metric_fn)                              self.metrics_updates += metric_fn.updates                  handle_metrics(output_metrics)
def filename_from_url(url, content_type):      return fn def trim_filename(filename, max_len):     if len(filename) > max_len:         trim_by = len(filename) - max_len         name, ext = os.path.splitext(filename)         if trim_by >= len(name):             filename = filename[:-trim_by]         else:             filename = name[:-trim_by] + ext     return filename   def get_filename_max_length(directory):     try:         max_len = os.pathconf(directory, 'PC_NAME_MAX')     except OSError as e:         if e.errno == errno.EINVAL:             max_len = 255         else:             raise     return max_len   def trim_filename_if_needed(filename, directory='.', extra=0):     max_len = get_filename_max_length(directory) - extra     if len(filename) > max_len:         filename = trim_filename(filename, max_len)     return filename    def get_unique_filename(filename, exists=os.path.exists):      attempt = 0      while True:          suffix = '-' + str(attempt) if attempt > 0 else ''         try_filename = trim_filename_if_needed(filename, extra=len(suffix))         try_filename += suffix         if not exists(try_filename):             return try_filename          attempt += 1
class AsyncHTTPClient(Configurable):              return          self._closed = True          if self._instance_cache is not None:             cached_val = self._instance_cache.pop(self.io_loop, None)                  if cached_val is not None and cached_val is not self:                  raise RuntimeError("inconsistent AsyncHTTPClient cache")      def fetch(          self,
def assert_series_equal(      check_exact=False,      check_datetimelike_compat=False,      check_categorical=True,     check_category_order=True,      obj="Series",  ):
class UnformattedLines(Line):  @dataclass  class EmptyLineTracker:
def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:      if not target_versions:          return GRAMMARS      elif all(not version.is_python2() for version in target_versions):           return [              pygram.python_grammar_no_print_statement_no_exec_statement,              pygram.python_grammar_no_print_statement,          ]      else:         return [pygram.python_grammar_no_print_statement, pygram.python_grammar]  def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
class LookupModule(LookupBase):          ret = []          for term in terms:              var = term.split()[0]             ret.append(py3compat.environ.get(var, ''))          return ret
def read_json(          dtype = True      if convert_axes is None and orient != "table":          convert_axes = True     if encoding is None:         encoding = "utf-8"      compression = _infer_compression(path_or_buf, compression)      filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(
default 'raise'          DatetimeIndex(['2018-03-01 09:00:00-05:00',                         '2018-03-02 09:00:00-05:00',                         '2018-03-03 09:00:00-05:00'],                       dtype='datetime64[ns, US/Eastern]', freq=None)          With the ``tz=None``, we can remove the time zone information          while keeping the local time (not converted to UTC):
class Index(IndexOpsMixin, PandasObject):          if pself is not self or ptarget is not target:              return pself.get_indexer_non_unique(ptarget)         if is_categorical_dtype(target.dtype):              tgt_values = np.asarray(target)          else:              tgt_values = target._get_engine_target()
def jsonable_encoder(                      or (not isinstance(key, str))                      or (not key.startswith("_sa"))                  )                 and (value is not None or not exclude_none)                  and ((include and key in include) or key not in exclude)              ):                  encoded_key = jsonable_encoder(                      key,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     exclude_none=exclude_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
import copy  import datetime  from functools import partial  import string from typing import TYPE_CHECKING, Optional, Tuple, Union, cast  import warnings  import numpy as np  from pandas._libs import Timedelta, hashtable as libhashtable, lib  import pandas._libs.join as libjoin from pandas._typing import ArrayLike, FrameOrSeries  from pandas.errors import MergeError  from pandas.util._decorators import Appender, Substitution
class RNN(Layer):      @property      def trainable_weights(self):         if not self.trainable:             return []          if isinstance(self.cell, Layer):              return self.cell.trainable_weights          return []
class CSVLogger(Callback):              if os.path.exists(self.filename):                  with open(self.filename, 'r' + self.file_flags) as f:                      self.append_header = not bool(len(f.readline()))             mode = 'a'          else:             mode = 'w'         self.csv_file = io.open(self.filename,                                 mode + self.file_flags,                                 **self._open_args)      def on_epoch_end(self, epoch, logs=None):          logs = logs or {}
class Model(Container):          return self.history      @interfaces.legacy_generator_methods_support     def evaluate_generator(self, generator, steps=None,                             max_queue_size=10,                             workers=1,                             use_multiprocessing=False):
class MinhatecaIE(InfoExtractor):          filesize_approx = parse_filesize(self._html_search_regex(              r'<p class="fileSize">(.*?)</p>',              webpage, 'file size approximation', fatal=False))         duration = parse_duration(self._html_search_regex(             r'(?s)<p class="fileLeng[ht][th]">.*?class="bold">(.*?)<',              webpage, 'duration', fatal=False))          view_count = int_or_none(self._html_search_regex(              r'<p class="downloadsCounter">([0-9]+)</p>',
from datetime import datetime, timedelta from typing import Any, List, Optional, Union, cast  import numpy as np from pandas._libs import NaT, iNaT, join as libjoin, lib from pandas._libs.tslibs import timezones from pandas._typing import Label  from pandas.compat.numpy import function as nv from pandas.errors import AbstractMethodError from pandas.util._decorators import Appender, cache_readonly, doc  from pandas.core.dtypes.common import (     ensure_int64,     is_bool_dtype,      is_datetime64_any_dtype,      is_dtype_equal,     is_integer,      is_list_like,      is_object_dtype,      is_period_dtype,     is_scalar,      is_timedelta64_dtype,  ) from pandas.core.dtypes.concat import concat_compat from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries from pandas.core.dtypes.missing import isna  from pandas.core import algorithms from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin from pandas.core.base import IndexOpsMixin import pandas.core.indexes.base as ibase from pandas.core.indexes.base import Index, _index_shared_docs from pandas.core.indexes.extension import (     ExtensionIndex,     inherit_names,     make_wrapped_arith_op, ) from pandas.core.indexes.numeric import Int64Index from pandas.core.ops import get_op_result_name from pandas.core.tools.timedeltas import to_timedelta from pandas.tseries.frequencies import DateOffset from pandas.tseries.offsets import Tick _index_doc_kwargs = dict(ibase._index_doc_kwargs) def _join_i8_wrapper(joinf, with_indexers: bool = True):     @staticmethod     def wrapper(left, right):         if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):             left = left.view("i8")         if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):             right = right.view("i8")         results = joinf(left, right)         if with_indexers:               dtype = left.dtype.base             join_index, left_indexer, right_indexer = results             join_index = join_index.view(dtype)             return join_index, left_indexer, right_indexer         return results     return wrapper def _make_wrapped_arith_op_with_freq(opname: str):     meth = make_wrapped_arith_op(opname)     def wrapped(self, other):         result = meth(self, other)         if result is NotImplemented:             return NotImplemented         new_freq = self._get_addsub_freq(other, result)         result._freq = new_freq         return result     wrapped.__name__ = opname     return wrapped @inherit_names(     ["inferred_freq", "_isnan", "_resolution", "resolution"],     DatetimeLikeArrayMixin,     cache=True, ) @inherit_names(     ["mean", "asi8", "_box_func"], DatetimeLikeArrayMixin, ) class DatetimeIndexOpsMixin(ExtensionIndex):     _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]     freq: Optional[DateOffset]     freqstr: Optional[str]     _resolution: int     _bool_ops: List[str] = []     _field_ops: List[str] = []     hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)     _hasnans = hasnans     @property     def is_all_dates(self) -> bool:         return True        @property         Gets called after a ufunc.         Determines if two Index objects contain the same elements.         if not isinstance(other, ABCIndexClass):             return False         elif not isinstance(other, type(self)):             try:                 other = type(self)(other)             except (ValueError, TypeError, OverflowError):                     return False          if not is_dtype_equal(self.dtype, other.dtype):              return False          return np.array_equal(self.asi8, other.asi8)      @Appender(Index.__contains__.__doc__)     def __contains__(self, key: Any) -> bool:         hash(key)         try:             res = self.get_loc(key)         except (KeyError, TypeError, ValueError):             return False         return bool(             is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))         )     def sort_values(self, return_indexer=False, ascending=True):     @Appender(_index_shared_docs["take"] % _index_doc_kwargs)     def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):         nv.validate_take(tuple(), kwargs)         indices = ensure_int64(indices)         maybe_slice = lib.maybe_indices_to_slice(indices, len(self))         if isinstance(maybe_slice, slice):             return self[maybe_slice]         return ExtensionIndex.take(             self, indices, axis, allow_fill, fill_value, **kwargs         )     def _convert_tolerance(self, tolerance, target):         tolerance = np.asarray(to_timedelta(tolerance).to_numpy())         if target.size != tolerance.size and tolerance.size > 1:             raise ValueError("list-like tolerance size must match target index size")         return tolerance     def tolist(self) -> List:         return list(self.astype(object))     def min(self, axis=None, skipna=True, *args, **kwargs):         nv.validate_min(args, kwargs)         nv.validate_minmax_axis(axis)         Returns the indices of the minimum values along an axis.         nv.validate_argmin(args, kwargs)         nv.validate_minmax_axis(axis)         Return the maximum value of the Index or maximum along         an axis.         See Also         --------         numpy.ndarray.max         Series.max : Return the maximum value in a Series.         Returns the indices of the maximum values along an axis.         See `numpy.ndarray.argmax` for more information on the         `axis` parameter.         See Also         --------         numpy.ndarray.argmax         Return a list of tuples of the (attr,formatted_value).         Parameters         ----------         reso : str         parsed : datetime         use_lhs : bool, default True         use_rhs : bool, default True          Returns          -------         slice or ndarray[intp]         Find the freq we expect the result of an addition/subtraction operation         to have.         elif is_timedelta64_dtype(other):             return None         elif is_object_dtype(other):             return None         elif is_datetime64_any_dtype(other):             return None         else:             raise NotImplementedError      __add__ = _make_wrapped_arith_op_with_freq("__add__")     __sub__ = _make_wrapped_arith_op_with_freq("__sub__")     __radd__ = make_wrapped_arith_op("__radd__")     __rsub__ = make_wrapped_arith_op("__rsub__")     __pow__ = make_wrapped_arith_op("__pow__")     __rpow__ = make_wrapped_arith_op("__rpow__")     __mul__ = make_wrapped_arith_op("__mul__")     __rmul__ = make_wrapped_arith_op("__rmul__")     __floordiv__ = make_wrapped_arith_op("__floordiv__")     __rfloordiv__ = make_wrapped_arith_op("__rfloordiv__")     __mod__ = make_wrapped_arith_op("__mod__")     __rmod__ = make_wrapped_arith_op("__rmod__")     __divmod__ = make_wrapped_arith_op("__divmod__")     __rdivmod__ = make_wrapped_arith_op("__rdivmod__")     __truediv__ = make_wrapped_arith_op("__truediv__")     __rtruediv__ = make_wrapped_arith_op("__rtruediv__")      def isin(self, values, level=None):         if level is not None:             self._validate_index_level(level)         if not isinstance(values, type(self)):              try:                 values = type(self)(values)             except ValueError:                 return self.astype(object).isin(values)         return algorithms.isin(self.asi8, values.asi8)     @Appender(Index.where.__doc__)     def where(self, cond, other=None):         values = self.view("i8")         try:             other = self._data._validate_where_value(other)         except (TypeError, ValueError) as err:              oth = getattr(other, "dtype", other)             raise TypeError(f"Where requires matching dtype, not {oth}") from err         result = np.where(cond, values, other).astype("i8")         arr = type(self._data)._simple_new(result, dtype=self.dtype)         return type(self)._simple_new(arr, name=self.name)     def _summary(self, name=None) -> str:         formatter = self._formatter_func         if len(self) > 0:             index_summary = f", {formatter(self[0])} to {formatter(self[-1])}"         else:             index_summary = ""         if name is None:             name = type(self).__name__         result = f"{name}: {len(self)} entries{index_summary}"         if self.freq:             result += f"\nFreq: {self.freqstr}"          result = result.replace("'", "")         return result      def shift(self, periods=1, freq=None):             .. versionchanged:: 0.24.0         freq : pandas.DateOffset, pandas.Timedelta or string, optional             Frequency increment to shift by.             If None, the index is shifted by its own `freq` attribute.             Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.         Returns         -------         pandas.DatetimeIndex             Shifted index.         See Also         --------         Index.shift : Shift values of Index.         PeriodIndex.shift : Shift values of PeriodIndex.         if is_period_dtype(self.dtype):             freq = self.freq         else:             self = cast(DatetimeTimedeltaMixin, self)             freq = self.freq if self._can_fast_union(other) else None         new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)         return type(self)._simple_new(new_data, name=name)     Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,     but not PeriodIndex         In limited circumstances, our freq may differ from that of our _data.
class Categorical(ExtensionArray, PandasObject):              )              raise ValueError(msg)         if is_extension_array_dtype(codes) and is_integer_dtype(codes):              if isna(codes).any():                 raise ValueError("codes cannot contain NA values")             codes = codes.to_numpy(dtype=np.int64)         else:             codes = np.asarray(codes)          if len(codes) and not is_integer_dtype(codes):              raise ValueError("codes need to be array-like integers")
def request_httprepr(request):      parsed = urlparse_cached(request)      path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))      s = to_bytes(request.method) + b" " + to_bytes(path) + b" HTTP/1.1\r\n"     s += b"Host: " + to_bytes(parsed.hostname or b'') + b"\r\n"      if request.headers:          s += request.headers.to_string() + b"\r\n"      s += b"\r\n"
import bz2  from collections import abc  import gzip from io import BufferedIOBase, BytesIO, RawIOBase  import mmap  import os  import pathlib
from keras.utils import Sequence  from keras import backend as K  pytestmark = pytest.mark.skipif(     K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,      reason='Temporarily disabled until the use_multiprocessing problem is solved')  STEPS_PER_EPOCH = 100
def unified_strdate(date_str, day_first=True):          timetuple = email.utils.parsedate_tz(date_str)          if timetuple:              upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')     if upload_date is not None:         return compat_str(upload_date)  def determine_ext(url, default_ext='unknown_video'):
from pandas.core.dtypes.missing import isna, notna  from pandas.core import nanops, ops  from pandas.core.algorithms import take  from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin from pandas.core.ops import invalid_comparison  from pandas.core.ops.common import unpack_zerodim_and_defer  from pandas.core.tools.numeric import to_numeric
class GroupBy(_GroupBy):          Parameters          ----------         output : Series or DataFrame              Object resulting from grouping and applying an operation.         fill_value : scalar, default np.NaN             Value to use for unobserved categories if self.observed is False.          Returns          -------
def to_pickle(obj, path, compression="infer", protocol=pickle.HIGHEST_PROTOCOL):      ----------      obj : any object          Any python object.     filepath_or_buffer : str, path object or file-like object         File path, URL, or buffer where the pickled object will be stored.          .. versionchanged:: 1.0.0            Accept URL. URL has to be of S3 or GCS.       compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'         If 'infer' and 'path_or_url' is path-like, then detect compression from         the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no         compression) If 'infer' and 'path_or_url' is not path-like, then use         None (= no decompression).      protocol : int          Int which indicates which protocol should be used by the pickler,          default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible
class APIRouter(routing.Router):                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,                     route_class_override=type(route),                  )              elif isinstance(route, routing.Route):                  self.add_route(
class Model(Container):          for epoch in range(initial_epoch, epochs):             for m in self.stateful_metric_functions:                 m.reset_states()              callbacks.on_epoch_begin(epoch)              epoch_logs = {}              if steps_per_epoch is not None:
from pandas.core.dtypes.common import (  )  from pandas.core.dtypes.missing import isna, notna from pandas._typing import FrameOrSeries, Scalar  from pandas.core import nanops  import pandas.core.algorithms as algorithms  from pandas.core.arrays import Categorical, try_cast_to_ea
class RNN(Layer):              input_shape = input_shape[0]          if hasattr(self.cell.state_size, '__len__'):             state_size = self.cell.state_size          else:             state_size = [self.cell.state_size]         output_dim = state_size[0]          if self.return_sequences:              output_shape = (input_shape[0], input_shape[1], output_dim)
class Selector(_ParselSelector, object_ref):      selectorlist_cls = SelectorList      def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):         if not(response is None or text is None):            raise ValueError('%s.__init__() received both response and text'                             % self.__class__.__name__)           st = _st(response, type or self._default_type)          if _root is not None:
class HTTP1Connection(httputil.HTTPConnection):              self._chunking_output = (                  start_line.method in ("POST", "PUT", "PATCH")                  and "Content-Length" not in headers                 and (                     "Transfer-Encoding" not in headers                     or headers["Transfer-Encoding"] == "chunked"                 )              )          else:              assert isinstance(start_line, httputil.ResponseStartLine)
from scrapy.pipelines.media import MediaPipeline  from scrapy.exceptions import NotConfigured, IgnoreRequest  from scrapy.http import Request  from scrapy.utils.misc import md5sum from scrapy.utils.log import failure_to_exc_info  logger = logging.getLogger(__name__)
class Index(IndexOpsMixin, PandasObject):              return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)         elif is_interval_dtype(data) or is_interval_dtype(dtype):             closed = kwargs.pop("closed", None)             if is_dtype_equal(_o_dtype, dtype):                 return IntervalIndex(                     data, name=name, copy=copy, closed=closed, **kwargs                 ).astype(object)             return IntervalIndex(                 data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs             )          elif (              is_datetime64_any_dtype(data)
class Scraper(object):          logger.error(              "Spider error processing %(request)s (referer: %(referer)s)",              {'request': request, 'referer': referer},             exc_info=failure_to_exc_info(_failure),             extra={'spider': spider}          )          self.signals.send_catch_log(              signal=signals.spider_error,
from thefuck.specific.git import git_support  @git_support  def match(command):      return ('push' in command.script_parts             and 'git push --set-upstream' in command.output)  def _get_upstream_option_index(command_parts):
class FlvReader(io.BytesIO):      def read_unsigned_long_long(self):         return struct_unpack('!Q', self.read(8))[0]      def read_unsigned_int(self):         return struct_unpack('!I', self.read(4))[0]      def read_unsigned_char(self):         return struct_unpack('!B', self.read(1))[0]      def read_string(self):          res = b''
class RNN(Layer):                  the size of the recurrent state                  (which should be the same as the size of the cell output).                  This can also be a list/tuple of integers                 (one size per state).             - a `output_size` attribute. This can be a single integer or a                 TensorShape, which represent the shape of the output. For                 backward compatible reason, if this attribute is not available                 for the cell, the value will be inferred by the first element                 of the `state_size`.              It is also possible for `cell` to be a list of RNN cell instances,              in which cases the cells get stacked on after the other in the RNN,              implementing an efficient stacked RNN.
class Index(IndexOpsMixin, PandasObject):                              pass                         return Float64Index(data, copy=copy, name=name)                      elif inferred == "string":                          pass
def _search(stderr):  def match(command, settings):     if 'EDITOR' not in os.environ:         return False      m = _search(command.stderr)      return m and os.path.isfile(m.group('file'))  def get_new_command(command, settings):
class FileWriter(object):          self.overwrite = overwrite      def write(self, s):         with open(self.path, 'w' if self.overwrite else 'a',                   encoding='utf-8') as output_file:              output_file.write(s)          self.overwrite = False
def map_obj_to_commands(updates, module):          if want['text'] and (want['text'] != have.get('text')):              banner_cmd = 'banner %s' % module.params['banner']              banner_cmd += ' @\n'             banner_cmd += want['text'].strip('\n')              banner_cmd += '\n@'              commands.append(banner_cmd)
def weighted_masked_objective(fn):              score_array *= mask             score_array /= K.mean(mask) + K.epsilon()          if weights is not None:
class Bash(Generic):      def app_alias(self, fuck):          alias = "TF_ALIAS={0}" \                  " alias {0}='PYTHONIOENCODING=utf-8" \                 " TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && " \                  " eval $TF_CMD".format(fuck)          if settings.alter_history:
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def fit_generator(model,                  val_enqueuer_gen = val_enqueuer.get()              elif val_gen:                  val_data = validation_data                 if is_sequence(val_data):                      val_enqueuer_gen = iter_sequence_infinite(val_data)                      validation_steps = validation_steps or len(val_data)                  else:
class ItemMeta(ABCMeta):          new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))          _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)         fields = getattr(_class, 'fields', {})          new_attrs = {}          for n in dir(_class):              v = getattr(_class, n)
class Index(IndexOpsMixin, PandasObject):                      "unicode",                      "mixed",                  ]:                     self._invalid_indexer("label", key)              elif kind in ["loc"] and is_integer(key):                  if not self.holds_integer():                     self._invalid_indexer("label", key)          return key
class LightSource:                                   .format(lookup.keys)) from err         if np.ma.is_masked(intensity):              mask = intensity.mask[..., 0]              for i in range(3):                  blend[..., i][mask] = rgb[..., i][mask]
class RNN(Layer):              output_shape = (input_shape[0], output_dim)          if self.return_state:             state_shape = [(input_shape[0], dim) for dim in state_size]              return [output_shape] + state_shape          else:              return output_shape
class TestScalar2:          result = ser.loc["a"]          assert result == 1         with pytest.raises(KeyError, match="^0$"):              ser.at[0]         with pytest.raises(KeyError, match="^0$"):              ser.loc[0]     def test_frame_raises_key_error(self):          df = DataFrame({"A": [1, 2, 3]}, index=list("abc"))          result = df.at["a", "A"]
async def schedule_formatting(      mode: Mode,      report: "Report",      loop: asyncio.AbstractEventLoop,     executor: Optional[Executor],  ) -> None:
class Progbar(object):          info = ' - %.0fs' % (now - self.start)          if self.verbose == 1:              if (not force and (now - self.last_update) < self.interval and                     (self.target is not None and current < self.target)):                  return              prev_total_width = self.total_width
class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):          -------          loc : int         if not is_scalar(key):             raise InvalidIndexError(key)           if is_valid_nat_for_dtype(key, self.dtype):              key = NaT
class Sanic:                  "Endpoint with name `{}` was not found".format(view_name)              )           host = uri.find("/")         if host > 0:             host, uri = uri[:host], uri[host:]         else:             host = None           if view_name == "static" or view_name.endswith(".static"):              filename = kwargs.pop("filename", None)
class Sequence(object):         while True:             for item in (self[i] for i in range(len(self))):                 yield item   _SHARED_SEQUENCES = {}
from .generic import Generic  class Zsh(Generic):      def app_alias(self, alias_name):          alias = "alias {0}='TF_CMD=$(TF_ALIAS={0}" \                  " PYTHONIOENCODING=utf-8" \                 " TF_SHELL_ALIASES=$(alias)" \                 " thefuck $(fc -ln -1 | tail -n 1)) &&" \                  " eval $TF_CMD".format(alias_name)          if settings.alter_history:
def _recursively_freeze(value):     JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.      Parameter whose value is a ``dict``.
def _get_join_indexers(     lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)      kwargs = copy.copy(kwargs)      if how == "left":
class MetricsHandler(tornado.web.RequestHandler):          self._scheduler = scheduler      def get(self):         metrics_collector = self._scheduler._state._metrics_collector         metrics = metrics_collector.generate_latest()          if metrics:             metrics_collector.configure_http_handler(self)              self.write(metrics)
class Rolling(_Rolling_and_Expanding):      def _on(self):          if self.on is None:             if self.axis == 0:                 return self.obj.index             elif self.axis == 1:                 return self.obj.columns          elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:              return Index(self.obj[self.on])          else:
from typing import (      Callable,      Collection,      Dict,     Generator,      Generic,      Iterable,      Iterator,
def to_categorical(y, num_classes=None):      y = np.array(y, dtype='int')      input_shape = y.shape     if input_shape and input_shape[-1] == 1:         input_shape = tuple(input_shape[:-1])      y = y.ravel()      if not num_classes:          num_classes = np.max(y) + 1
GRAMMARS = [  def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement     if src_txt[-1:] != "\n":          src_txt += "\n"      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)
class Field(dict):  class ItemMeta(ABCMeta):      def __new__(mcs, class_name, bases, attrs):         classcell = attrs.pop('__classcell__', None)          new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))          _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)
def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -      if src_txt[-1:] != "\n":          src_txt += "\n"     for parser_config in get_parser_configs(set(target_versions)):         drv = driver.Driver(             parser_config.grammar,             pytree.convert,             tokenizer_config=parser_config.tokenizer_config,         )          try:              result = drv.parse_string(src_txt, True)              break
class YoutubeDL(object):                  comparison_value = m.group('value')                  str_op = STR_OPERATORS[m.group('op')]                  if m.group('negation'):                     op = lambda attr, value: not str_op(attr, value)                  else:                      op = str_op
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def standardize_weights(y,                               'sample-wise weights, make sure your '                               'sample_weight array is 1D.')      if sample_weight is not None:          if len(sample_weight.shape) > len(y.shape):              raise ValueError('Found a sample_weight with shape' +
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),              False,              padding,              padding],         output_shape=output_shape,         dilation=dilation_rate)      return _postprocess_conv2d_output(x, data_format)
https://support.sas.com/techsup/technote/ts140.pdf  from collections import abc  from datetime import datetime  import struct  import warnings
class Orthogonal(Initializer):          rng = np.random          if self.seed is not None:              rng = np.random.RandomState(self.seed)             self.seed += 1          a = rng.normal(0.0, 1.0, flat_shape)          u, _, v = np.linalg.svd(a, full_matrices=False)
class _AxesBase(martist.Artist):              return          dL = self.dataLim         x0, x1 = map(x_trf.transform, dL.intervalx)         y0, y1 = map(y_trf.transform, dL.intervaly)          xr = 1.05 * (x1 - x0)          yr = 1.05 * (y1 - y0)
except ImportError: from pydantic.fields import Field as ModelField def _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool ) -> Any:     if isinstance(res, BaseModel):         if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)         else:             return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset             )     elif isinstance(res, list):         return [             _prepare_response_content(item, exclude_unset=exclude_unset) for item in res         ]     elif isinstance(res, dict):         return {             k: _prepare_response_content(v, exclude_unset=exclude_unset)             for k, v in res.items()         }     return res    async def serialize_response(      *,      field: ModelField = None,
class SortedCorrectedCommandsSequence(object):      def _realise(self):         if self._cached:             commands = self._remove_duplicates(self._commands)             self._cached = [self._cached[0]] + sorted(                 commands, key=lambda corrected_command: corrected_command.priority)          self._realised = True          debug('SortedCommandsSequence was realised with: {}, after: {}'.format(              self._cached, '\n'.join(format_stack())), self._settings)
def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:              trailing_comma_safe = trailing_comma_safe and py36          leaf_priority = delimiters.get(id(leaf))          if leaf_priority == delimiter_priority:              yield current_line              current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
def _can_use_numexpr(op, op_str, a, b, dtype_check):          if np.prod(a.shape) > _MIN_ELEMENTS:              dtypes = set()              for o in [a, b]:                  if hasattr(o, "dtypes") and o.ndim > 1:                      s = o.dtypes.value_counts()                      if len(s) > 1:                          return False                      dtypes |= set(s.index.astype(str))                  elif hasattr(o, "dtype"):                      dtypes |= {o.dtype.name}
class ResponseTypes(object):      def from_content_disposition(self, content_disposition):          try:             filename = to_native_str(content_disposition,                 encoding='latin-1', errors='replace').split(';')[1].split('=')[1]              filename = filename.strip('"\'')              return self.from_filename(filename)          except IndexError:
class Driver(object):      def parse_stream_raw(self, stream, debug=False):         tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)          return self.parse_tokens(tokens, debug)      def parse_stream(self, stream, debug=False):
class Feature(Enum):      NUMERIC_UNDERSCORES = 3      TRAILING_COMMA_IN_CALL = 4      TRAILING_COMMA_IN_DEF = 5       ASYNC_IS_VALID_IDENTIFIER = 6     ASYNC_IS_RESERVED_KEYWORD = 7  VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {     TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},     TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},     TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},     TargetVersion.PY35: {         Feature.UNICODE_LITERALS,         Feature.TRAILING_COMMA_IN_CALL,         Feature.ASYNC_IS_VALID_IDENTIFIER,     },      TargetVersion.PY36: {          Feature.UNICODE_LITERALS,          Feature.F_STRINGS,          Feature.NUMERIC_UNDERSCORES,          Feature.TRAILING_COMMA_IN_CALL,          Feature.TRAILING_COMMA_IN_DEF,         Feature.ASYNC_IS_VALID_IDENTIFIER,      },      TargetVersion.PY37: {          Feature.UNICODE_LITERALS,
class ImagesPipeline(FilesPipeline):              background = Image.new('RGBA', image.size, (255, 255, 255))              background.paste(image, image)              image = background.convert('RGB')         elif image.mode == 'P':             image = image.convert("RGBA")             background = Image.new('RGBA', image.size, (255, 255, 255))             background.paste(image, image)             image = background.convert('RGB')          elif image.mode != 'RGB':              image = image.convert('RGB')
def generate_context(      context = OrderedDict([])      try:         with open(context_file, encoding='utf-8') as file_handle:              obj = json.load(file_handle, object_pairs_hook=OrderedDict)      except ValueError as e:
def run_hook(hook_name, project_dir, context):      script = find_hooks().get(hook_name)      if script is None:          logging.debug('No hooks found')         return     run_script_with_context(script, project_dir, context)
import tempfile  from jinja2 import Template  from cookiecutter import utils from .exceptions import FailedHookException  _HOOKS = [
def _match_one(filter_part, dct):          \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*          (?:              (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|             (?P<quote>["\'])(?P<quotedstrval>(?:\\.|(?!(?P=quote)|\\).)+?)(?P=quote)|              (?P<strval>(?![0-9.])[a-z0-9A-Z]*)          )          \s*$
def match(command, settings):     return (command.script == 'ls'             or command.script.startswith('ls ')             and not ('ls -' in command.script))  def get_new_command(command, settings):
def unified_timestamp(date_str, day_first=True):      for expression in date_formats(day_first):          try:             dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)              return calendar.timegm(dt.timetuple())          except ValueError:              pass      timetuple = email.utils.parsedate_tz(date_str)      if timetuple:         return calendar.timegm(timetuple) + pm_delta * 3600  def determine_ext(url, default_ext='unknown_video'):
class CategoricalBlock(ExtensionBlock):              )          return result     def replace(         self,         to_replace,         value,         inplace: bool = False,         filter=None,         regex: bool = False,         convert: bool = True,     ):         inplace = validate_bool_kwarg(inplace, "inplace")         result = self if inplace else self.copy()         if filter is None:             result.values.replace(to_replace, value, inplace=True)             if convert:                 return result.convert(numeric=False, copy=not inplace)             else:                 return result         else:             if not isna(value):                 result.values.add_categories(value, inplace=True)             return super(CategoricalBlock, result).replace(                 to_replace, value, inplace, filter, regex, convert             )
class Scheduler(object):                  for batch_task in self._state.get_batch_running_tasks(task.batch_id):                      batch_task.expl = expl         if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:               if status == PENDING or status != task.status:
class IntegerArray(BaseMaskedArray):              if incompatible type with an IntegerDtype, equivalent of same_kind              casting         from pandas.core.arrays.boolean import BooleanArray, BooleanDtype          dtype = pandas_dtype(dtype)          if isinstance(dtype, _IntegerDtype):              result = self._data.astype(dtype.numpy_dtype, copy=False)              return type(self)(result, mask=self._mask, copy=False)         elif isinstance(dtype, BooleanDtype):             result = self._data.astype("bool", copy=False)             return BooleanArray(result, mask=self._mask, copy=False)          if is_float_dtype(dtype):
class Model(Container):          nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)          self.metrics_updates = []          self.stateful_metric_names = []         self.stateful_metric_functions = []          with K.name_scope('metrics'):              for i in range(len(self.outputs)):                  if i in skip_target_indices:
class DatetimeBlock(DatetimeLikeBlockMixin, Block):          ).reshape(i8values.shape)          return np.atleast_2d(result)      def set(self, locs, values):          values = conversion.ensure_datetime64ns(values, copy=False)
class FacebookIE(InfoExtractor):              video_title = self._html_search_regex(                  r'(?s)<span class="fbPhotosPhotoCaption".*?id="fbPhotoPageCaption"><span class="hasCaption">(.*?)</span>',                  webpage, 'alternative title', default=None)             video_title = limit_length(video_title, 80)          if not video_title: video_title = 'Facebook video
class TimeGrouper(Grouper):          rng += freq_mult          rng -= bin_shift           prng = type(memb._data)(rng, dtype=memb.dtype)         bins = memb.searchsorted(prng, side="left")          if nat_count > 0:
def predict_generator(model, generator,      steps_done = 0      all_outs = []     use_sequence_api = is_sequence(generator)     if not use_sequence_api and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps is None:         if use_sequence_api:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator'
class BaseAsyncIOLoop(IOLoop):          self.readers = set()          self.writers = set()          self.closing = False                    for loop in list(IOLoop._ioloop_for_asyncio):             if loop.is_closed():                 del IOLoop._ioloop_for_asyncio[loop]          IOLoop._ioloop_for_asyncio[asyncio_loop] = self          super(BaseAsyncIOLoop, self).initialize(**kwargs)
class Tracer:          @pysnooper.snoop(thread_info=True)     Customize how values are represented as strings::          @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))       def __init__(              self,
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class Index(IndexOpsMixin, PandasObject):          left_indexer = self.get_indexer(target, "pad", limit=limit)          right_indexer = self.get_indexer(target, "backfill", limit=limit)         left_distances = np.abs(self[left_indexer] - target)         right_distances = np.abs(self[right_indexer] - target)          op = operator.lt if self.is_monotonic_increasing else operator.le          indexer = np.where(
class GalaxyAPI:              data = self._call_galaxy(url)              results = data['results']              done = (data.get('next_link', None) is None)                url_info = urlparse(self.api_server)             base_url = "%s://%s/" % (url_info.scheme, url_info.netloc)               while not done:                 url = _urljoin(base_url, data['next_link'])                  data = self._call_galaxy(url)                  results += data['results']                  done = (data.get('next_link', None) is None)          except Exception as e:             display.warning("Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s"                             % (role_id, related, to_text(e)))          return results      @g_connect(['v1'])
class Parser:          for date_unit in date_units:              try:                  new_data = to_datetime(new_data, errors="raise", unit=date_unit)             except (ValueError, OverflowError, TypeError):                  continue              return new_data, True          return data, False
def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:      else:          BodySchema = params.Body         body_param_media_types = [             getattr(f.schema, "media_type")             for f in flat_dependant.body_params             if isinstance(f.schema, params.Body)         ]         if len(set(body_param_media_types)) == 1:             BodySchema_kwargs["media_type"] = body_param_media_types[0]       field = Field(          name="body",          type_=BodyModel,
class Bidirectional(Wrapper):              return self.forward_layer.updates + self.backward_layer.updates          return []     def get_updates_for(self, inputs=None):         forward_updates = self.forward_layer.get_updates_for(inputs)         backward_updates = self.backward_layer.get_updates_for(inputs)         return (super(Wrapper, self).get_updates_for(inputs) +                 forward_updates + backward_updates)       @property      def losses(self):          if hasattr(self.forward_layer, 'losses'):              return self.forward_layer.losses + self.backward_layer.losses          return []     def get_losses_for(self, inputs=None):         forward_losses = self.forward_layer.get_losses_for(inputs)         backward_losses = self.backward_layer.get_losses_for(inputs)         return (super(Wrapper, self).get_losses_for(inputs) +                 forward_losses + backward_losses)       @property      def constraints(self):          constraints = {}
class SeriesGroupBy(GroupBy):          minlength = ngroups or 0          out = np.bincount(ids[mask], minlength=minlength)         result = Series(              out,              index=self.grouper.result_index,              name=self._selection_name,              dtype="int64",          )         return self._reindex_output(result, fill_value=0)      def _apply_to_column_groupbys(self, func):
class Sanic:          netloc = kwargs.pop("_server", None)          if netloc is None and external:             netloc = host or self.config.get("SERVER_NAME", "")          if external:              if not scheme:
class DataFrame(NDFrame):          if is_transposed:              data = data.T         if len(data.columns) == 0:              cols = Index([], name=self.columns.name)             if is_list_like(q):                 return self._constructor([], index=q, columns=cols)             return self._constructor_sliced([], index=cols, name=q)           result = data._data.quantile(              qs=q, axis=1, interpolation=interpolation, transposed=is_transposed          )
from pandas._libs.lib import infer_dtype  from pandas.core.dtypes.common import (      _NS_DTYPE,      ensure_int64,     is_bool_dtype,      is_categorical_dtype,      is_datetime64_dtype,      is_datetime64tz_dtype,
class Categorical(ExtensionArray, PandasObject):          max : the maximum of this `Categorical`          self.check_for_ordered("max")          if not len(self._codes):             return self.dtype.na_value           good = self._codes != -1          if not good.all():              if skipna:
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              res._is_copy = self._is_copy          return res     def _iget_item_cache(self, item: int):          ax = self._info_axis          if ax.is_unique:              lower = self._get_item_cache(ax[item])          else:             return self._ixs(item, axis=1)          return lower      def _box_item_values(self, key, values):
def jsonable_encoder(      custom_encoder: dict = {},  ) -> Any:      if isinstance(obj, BaseModel):         encoder = getattr(obj.Config, "json_encoders", custom_encoder)         return jsonable_encoder(             obj.dict(include=include, exclude=exclude, by_alias=by_alias),             include_none=include_none,             custom_encoder=encoder,         )      if isinstance(obj, Enum):          return obj.value      if isinstance(obj, (str, int, float, type(None))):
def url_has_any_extension(url, extensions):  def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):       try:         netloc = parts.netloc.encode('idna')     except UnicodeError:         netloc = parts.netloc       return (          to_native_str(parts.scheme),         to_native_str(netloc),          quote(to_bytes(parts.path, path_encoding), _safe_chars),
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):          vmin, vmax = vmax, vmin          swapped = True       vmin, vmax = map(float, [vmin, vmax])       maxabsvalue = max(abs(vmin), abs(vmax))      if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:          vmin = -expander
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def sanitize_array(          arr = np.arange(data.start, data.stop, data.step, dtype="int64")          subarr = _try_cast(arr, dtype, copy, raise_cast_failure)     elif isinstance(data, abc.Set):         raise TypeError("Set type is unordered")      else:          subarr = _try_cast(data, dtype, copy, raise_cast_failure)
class GeneratorEnqueuer(SequenceEnqueuer):                  try:                      if self._use_multiprocessing or self.queue.qsize() < max_queue_size:                          generator_output = next(self._generator)                         self.queue.put((True, generator_output))                      else:                          time.sleep(self.wait_time)                  except StopIteration:                      break                 except Exception as e:                       if self._use_multiprocessing:                         traceback.print_exc()                         setattr(e, '__traceback__', None)                     elif not hasattr(e, '__traceback__'):                         setattr(e, '__traceback__', sys.exc_info()[2])                     self.queue.put((False, e))                      self._stop_event.set()                     break          try:              if self._use_multiprocessing:                 self._manager = multiprocessing.Manager()                 self.queue = self._manager.Queue(maxsize=max_queue_size)                  self._stop_event = multiprocessing.Event()              else:                  self.queue = queue.Queue()
__credits__ = \  import re  from codecs import BOM_UTF8, lookup from attr import dataclass  from blib2to3.pgen2.token import *  from . import token
class tqdm(Comparable):                  if ncols else 10,                  charset=Bar.BLANK)              res = bar_format.format(bar=full_bar, **format_dict)             return disp_trim(res, ncols) if ncols else res          else:              return ((prefix + ": ") if prefix else '') + \
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z1,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0, base=10),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
def _iter_command_classes(module_name):      for module in walk_modules(module_name):         for obj in vars(module).values():              if inspect.isclass(obj) and \                     issubclass(obj, ScrapyCommand) and \                     obj.__module__ == module.__name__:                  yield obj  def _get_commands_from_module(module, inproject):
def _htmlentity_transform(entity):              numstr = '0%s' % numstr          else:              base = 10          try:             return compat_chr(int(numstr, base))         except ValueError:             pass      return ('&%s;' % entity)
class IntervalArray(IntervalMixin, ExtensionArray):                  msg = f"'value' should be an interval type, got {type(value)} instead."                  raise TypeError(msg) from err         if needs_float_conversion:             raise ValueError("Cannot set float NaN to integer-backed IntervalArray")           key = check_array_indexer(self, key)           left = self.left.copy(deep=True)         left._values[key] = value_left          self._left = left          right = self.right.copy(deep=True)         right._values[key] = value_right          self._right = right      def __eq__(self, other):
class SimpleTaskState(object):              elif task.scheduler_disable_time is not None:                  return         if new_status == FAILED and task.can_disable() and task.status != DISABLED:              task.add_failure()              if task.has_excessive_failures():                  task.scheduler_disable_time = time.time()
class TestBackend(object):      def test_log(self):          check_single_tensor_operation('log', (4, 2), WITH_NP)     @pytest.mark.skipif(K.backend() == 'theano',                         reason='theano returns tuples for update ops')     def test_update(self):         x = np.ones((3, 4))         x_var = K.variable(x)         new_x = np.random.random((3, 4))          op = K.update(x_var, new_x)         K.eval(op)          assert_allclose(new_x, K.eval(x_var), atol=1e-05)       @pytest.mark.skipif(K.backend() == 'theano',                          reason='theano returns tuples for update ops')      def test_update_add(self):         x = np.ones((3, 4))          x_var = K.variable(x)         increment = np.random.random((3, 4))         op = K.update_add(x_var, increment)         K.eval(op)         assert_allclose(x + increment, K.eval(x_var), atol=1e-05)      @pytest.mark.skipif(K.backend() == 'theano',                          reason='theano returns tuples for update ops')      def test_update_sub(self):         x = np.ones((3, 4))          x_var = K.variable(x)         decrement = np.random.random((3, 4))         op = K.update_sub(x_var, decrement)         K.eval(op)         assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)      @pytest.mark.skipif(K.backend() == 'cntk',                          reason='cntk doesn\'t support gradient in this way.')
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
import pandas.core.algorithms as algos  from pandas.core.arrays import ExtensionArray  from pandas.core.base import IndexOpsMixin, PandasObject  import pandas.core.common as com from pandas.core.construction import extract_array  from pandas.core.indexers import maybe_convert_indices  from pandas.core.indexes.frozen import FrozenList  import pandas.core.missing as missing
class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):          logger.info("Inserting file: %s", f)          colnames = ''         if self.columns and len(self.columns) > 0:              colnames = ",".join([x[0] for x in self.columns])              colnames = '({})'.format(colnames)
class GroupBy(_GroupBy):              if not self.observed and isinstance(result_index, CategoricalIndex):                  out = out.reindex(result_index)             out = self._reindex_output(out)              return out.sort_index() if self.sort else out
class RangeIndex(Int64Index):      @Appender(_index_shared_docs["get_indexer"])      def get_indexer(self, target, method=None, limit=None, tolerance=None):         if com.any_not_none(method, tolerance, limit) or not is_list_like(target):             return super().get_indexer(                 target, method=method, tolerance=tolerance, limit=limit             )          if self.step > 0:              start, stop, step = self.start, self.stop, self.step
class Line:              and self.leaves[0].value == 'yield'          )     @property     def contains_standalone_comments(self) -> bool:          if not (
def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr      pandas object          The original object that was pickled and then re-read.     _path = path     if _path is None:         _path = f"__{rands(10)}__.pickle"     with ensure_clean(_path) as path:         pd.to_pickle(obj, _path)         return pd.read_pickle(_path)  def round_trip_pathlib(writer, reader, path: Optional[str] = None):
class DirectoryIterator(Iterator):              fname = self.filenames[j]              img = load_img(os.path.join(self.directory, fname),                             grayscale=grayscale,                            target_size=None,                             interpolation=self.interpolation)             if self.image_data_generator.preprocessing_function:                 img = self.image_data_generator.preprocessing_function(img)             if self.target_size is not None:                 width_height_tuple = (self.target_size[1], self.target_size[0])                 if img.size != width_height_tuple:                     if self.interpolation not in _PIL_INTERPOLATION_METHODS:                         raise ValueError(                             'Invalid interpolation method {} specified. Supported '                             'methods are {}'.format(                                 self.interpolation,                                 ", ".join(_PIL_INTERPOLATION_METHODS.keys())))                     resample = _PIL_INTERPOLATION_METHODS[self.interpolation]                     img = img.resize(width_height_tuple, resample)              x = img_to_array(img, data_format=self.data_format)              x = self.image_data_generator.random_transform(x)              x = self.image_data_generator.standardize(x)
class tqdm(Comparable):              if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):                  bar_format = _unicode(bar_format)              res = bar_format.format(bar=full_bar, **format_dict)             return disp_trim(res, ncols) if ncols else res          elif bar_format:
class TestScalar2:          result = df.loc["a", "A"]          assert result == 1         with pytest.raises(KeyError, match="^0$"):              df.at["a", 0]         with pytest.raises(KeyError, match="^0$"):              df.loc["a", 0]      def test_series_at_raises_key_error(self):
class LinuxHardware(Hardware):      MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*"')      OCTAL_ESCAPE_RE = re.compile(r'\\[0-9]{3}')       def populate(self, collected_facts=None):          hardware_facts = {}          self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):                  new_index = self.index[loc]          if is_scalar(loc):              if self.ndim == 1:                    return self._values[loc]             new_values = self._data.fast_xs(loc)              result = self._constructor_sliced(                  new_values,
def main(model="en_core_web_sm"):  def filter_spans(spans):      get_sort_key = lambda span: (span.end - span.start, -span.start)      sorted_spans = sorted(spans, key=get_sort_key, reverse=True)      result = []      seen_tokens = set()      for span in sorted_spans:           if span.start not in seen_tokens and span.end - 1 not in seen_tokens:              result.append(span)         seen_tokens.update(range(span.start, span.end))     result = sorted(result, key=lambda span: span.start)      return result
from pandas._libs import NaT, Timedelta, index as libindex from pandas._typing import DtypeObj, Label  from pandas.util._decorators import Appender  from pandas.core.dtypes.common import (
import operator from typing import List, Optional, Set  import numpy as np from pandas._libs import NaT, iNaT, join as libjoin, lib from pandas._libs.algos import unique_deltas from pandas._libs.tslibs import timezones  from pandas.compat.numpy import function as nv from pandas.errors import AbstractMethodError from pandas.util._decorators import Appender, cache_readonly  from pandas.core.dtypes.common import (     ensure_int64,     is_bool_dtype,      is_categorical_dtype,      is_dtype_equal,     is_float,     is_integer,      is_list_like,      is_period_dtype,     is_scalar,     needs_i8_conversion, ) from pandas.core.dtypes.concat import concat_compat from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries from pandas.core.dtypes.missing import isna  from pandas.core import algorithms from pandas.core.accessor import PandasDelegate from pandas.core.arrays import (     DatetimeArray,     ExtensionArray,     ExtensionOpsMixin,     TimedeltaArray, ) from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin import pandas.core.indexes.base as ibase from pandas.core.indexes.base import Index, _index_shared_docs from pandas.core.indexes.numeric import Int64Index from pandas.core.ops import get_op_result_name from pandas.core.tools.timedeltas import to_timedelta  from pandas.tseries.frequencies import DateOffset, to_offset  from .extension import (     ExtensionIndex,     inherit_names,     make_wrapped_arith_op,     make_wrapped_comparison_op,  ) _index_doc_kwargs = dict(ibase._index_doc_kwargs) def _join_i8_wrapper(joinf, with_indexers: bool = True):     @staticmethod     def wrapper(left, right):         if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):             left = left.view("i8")         if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):             right = right.view("i8")         results = joinf(left, right)         if with_indexers:               dtype = left.dtype.base             join_index, left_indexer, right_indexer = results             join_index = join_index.view(dtype)             return join_index, left_indexer, right_indexer         return results     return wrapper @inherit_names(     ["inferred_freq", "_isnan", "_resolution", "resolution"],     DatetimeLikeArrayMixin,     cache=True, ) @inherit_names(     ["__iter__", "mean", "freq", "freqstr", "_ndarray_values", "asi8", "_box_values"],     DatetimeLikeArrayMixin, ) class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):     _data: ExtensionArray     freq: Optional[DateOffset]     freqstr: Optional[str]     _resolution: int     _bool_ops: List[str] = []     _field_ops: List[str] = []     hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)     _hasnans = hasnans      @property         Create a comparison method that dispatches to ``cls.values``.         Gets called after a ufunc.         Determines if two Index objects contain the same elements.       def map(self, mapper, na_action=None):         try:             result = mapper(self)              if isinstance(result, np.ndarray):                 result = Index(result)             if not isinstance(result, Index):                 raise TypeError("The map function must return an Index object")             return result         except Exception:             return self.astype(object).map(mapper)     def sort_values(self, return_indexer=False, ascending=True):     def _convert_tolerance(self, tolerance, target):         tolerance = np.asarray(to_timedelta(tolerance).to_numpy())         if target.size != tolerance.size and tolerance.size > 1:             raise ValueError("list-like tolerance size must match target index size")         return tolerance     def tolist(self) -> List:         return list(self.astype(object))     def min(self, axis=None, skipna=True, *args, **kwargs):         nv.validate_min(args, kwargs)         nv.validate_minmax_axis(axis)         Returns the indices of the minimum values along an axis.         nv.validate_argmin(args, kwargs)         nv.validate_minmax_axis(axis)         Return the maximum value of the Index or maximum along         an axis.         See Also         --------         numpy.ndarray.max         Series.max : Return the maximum value in a Series.         i8 = self.asi8         try:              if len(i8) and self.is_monotonic:                 if i8[-1] != iNaT:                     return self._box_func(i8[-1])              if self.hasnans:                 if skipna:                     max_stamp = self[~self._isnan].asi8.max()                 else:                     return self._na_value             else:                 max_stamp = i8.max()             return self._box_func(max_stamp)         except ValueError:             return self._na_value     def argmax(self, axis=None, skipna=True, *args, **kwargs):         See `numpy.ndarray.argmax` for more information on the         `axis` parameter.         See Also         --------         numpy.ndarray.argmax         Return a list of tuples of the (attr,formatted_value).         We don't allow integer or float indexing on datetime-like when using         loc.          assert kind in ["ix", "loc", "getitem", "iloc", None]            if is_scalar(key):             is_int = is_integer(key)             is_flt = is_float(key)             if kind in ["loc"] and (is_int or is_flt):                 self._invalid_indexer("index", key)             elif kind in ["ix", "getitem"] and is_flt:                 self._invalid_indexer("index", key)          return super()._convert_scalar_indexer(key, kind=kind)      __add__ = make_wrapped_arith_op("__add__")     __radd__ = make_wrapped_arith_op("__radd__")     __sub__ = make_wrapped_arith_op("__sub__")     __rsub__ = make_wrapped_arith_op("__rsub__")     __pow__ = make_wrapped_arith_op("__pow__")     __rpow__ = make_wrapped_arith_op("__rpow__")     __mul__ = make_wrapped_arith_op("__mul__")     __rmul__ = make_wrapped_arith_op("__rmul__")     __floordiv__ = make_wrapped_arith_op("__floordiv__")     __rfloordiv__ = make_wrapped_arith_op("__rfloordiv__")     __mod__ = make_wrapped_arith_op("__mod__")     __rmod__ = make_wrapped_arith_op("__rmod__")     __divmod__ = make_wrapped_arith_op("__divmod__")     __rdivmod__ = make_wrapped_arith_op("__rdivmod__")     __truediv__ = make_wrapped_arith_op("__truediv__")     __rtruediv__ = make_wrapped_arith_op("__rtruediv__")      def isin(self, values, level=None):         Parameters         ----------         values : set or sequence of values          Returns          -------         is_contained : ndarray (boolean dtype)         if level is not None:             self._validate_index_level(level)         if not isinstance(values, type(self)):             try:                 values = type(self)(values)             except ValueError:                 return self.astype(object).isin(values)         return algorithms.isin(self.asi8, values.asi8)     @Appender(_index_shared_docs["repeat"] % _index_doc_kwargs)     def repeat(self, repeats, axis=None):         nv.validate_repeat(tuple(), dict(axis=axis))         result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)         return self._shallow_copy(result)     @Appender(_index_shared_docs["where"] % _index_doc_kwargs)     def where(self, cond, other=None):         values = self.view("i8")         if is_scalar(other) and isna(other):             other = NaT.value          else:               other = Index(other)             if is_categorical_dtype(other):                  if needs_i8_conversion(other.categories):                     other = other._internal_get_values()             if not is_dtype_equal(self.dtype, other.dtype):                 raise TypeError(f"Where requires matching dtype, not {other.dtype}")             other = other.view("i8")         result = np.where(cond, values, other).astype("i8")         return self._shallow_copy(result)     def _summary(self, name=None):         formatter = self._formatter_func         if len(self) > 0:             index_summary = f", {formatter(self[0])} to {formatter(self[-1])}"          else:             index_summary = ""         if name is None:             name = type(self).__name__         result = f"{name}: {len(self)} entries{index_summary}"         if self.freq:             result += f"\nFreq: {self.freqstr}"          result = result.replace("'", "")         return result     def _concat_same_dtype(self, to_concat, name):         attribs = self._get_attributes_dict()         attribs["name"] = name          if len({str(x.dtype) for x in to_concat}) != 1:             raise ValueError("to_concat must have the same tz")         new_data = type(self._values)._concat_same_type(to_concat).asi8           is_diff_evenly_spaced = len(unique_deltas(new_data)) == 1         if not is_period_dtype(self) and not is_diff_evenly_spaced:              attribs["freq"] = None         return self._simple_new(new_data, **attribs)     @Appender(_index_shared_docs["astype"])     def astype(self, dtype, copy=True):         if is_dtype_equal(self.dtype, dtype) and copy is False:              return self         new_values = self._data.astype(dtype, copy=copy)           return Index(new_values, dtype=new_values.dtype, name=self.name, copy=False)     def shift(self, periods=1, freq=None):             .. versionchanged:: 0.24.0         freq : pandas.DateOffset, pandas.Timedelta or string, optional             Frequency increment to shift by.             If None, the index is shifted by its own `freq` attribute.             Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.         Index.shift : Shift values of Index.         PeriodIndex.shift : Shift values of PeriodIndex.     def delete(self, loc):         new_i8s = np.delete(self.asi8, loc)         freq = None         if is_period_dtype(self):             freq = self.freq         elif is_integer(loc):             if loc in (0, -len(self), -1, len(self) - 1):                 freq = self.freq          else:             if is_list_like(loc):                 loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))             if isinstance(loc, slice) and loc.step in (1, None):                 if loc.start in (0, None) or loc.stop in (len(self), None):                     freq = self.freq         return self._shallow_copy(new_i8s, freq=freq) class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):      _is_monotonic_increasing = Index.is_monotonic_increasing     _is_monotonic_decreasing = Index.is_monotonic_decreasing     _is_unique = Index.is_unique         Set the _freq attribute on our underlying DatetimeArray.         freq : DateOffset, None, or "infer"     def intersection(self, other, sort=False):         May be much faster than Index.intersection          Parameters          ----------             .. versionchanged:: 0.25.0                The `sort` keyword is added          Returns          -------         y : Index or same type as self          if self[0] <= other[0]:             left, right = self, other         else:             left, right = other, self           end = min(left[-1], right[-1])         start = right[0]         if not self.is_monotonic or not other.is_monotonic:             return False         if len(self) == 0 or len(other) == 0:             return True         this, other = self._maybe_utc_convert(other)     _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)     _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)     _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)     _left_indexer_unique = _join_i8_wrapper(         libjoin.left_join_indexer_unique, with_indexers=False     )     def join(         self, other, how: str = "left", level=None, return_indexers=False, sort=False     ):         if self._is_convertible_to_index_for_join(other):             try:                 other = type(self)(other)             except (TypeError, ValueError):                 pass          this, other = self._maybe_utc_convert(other)         return Index.join(             this,             other,             how=how,             level=level,             return_indexers=return_indexers,             sort=sort,         )     def _maybe_utc_convert(self, other):         this = self         if not hasattr(self, "tz"):             return this, other         if isinstance(other, type(self)):             if self.tz is not None:                 if other.tz is None:                     raise TypeError("Cannot join tz-naive with tz-aware DatetimeIndex")             elif other.tz is not None:                 raise TypeError("Cannot join tz-naive with tz-aware DatetimeIndex")         return a boolean whether I can attempt conversion to a         DatetimeIndex/TimedeltaIndex     Delegation mechanism, specific for Datetime, Timedelta, and Period types.      Functionality is delegated from the Index class to an Array class. A     few things can be customized      * _delegated_methods, delegated_properties : List         The list of property / method names being delagated.     * raw_methods : Set         The set of methods whose results should should *not* be         boxed in an index, after being returned from the array     * raw_properties : Set         The set of properties whose results should should *not* be         boxed in an index, after being returned from the array      _raw_methods: Set[str] = set()      _raw_properties: Set[str] = set()     _data: ExtensionArray
async def request_validation_exception_handler(      request: Request, exc: RequestValidationError  ) -> JSONResponse:      return JSONResponse(         status_code=HTTP_422_UNPROCESSABLE_ENTITY,         content={"detail": jsonable_encoder(exc.errors())},      )
def _preprocess_conv1d_input(x, data_format):      return x, tf_data_format def _preprocess_conv2d_input(x, data_format, force_transpose=False):          x: input tensor.          data_format: string, `"channels_last"` or `"channels_first"`.         force_transpose: boolean, whether force to transpose input from NCHW to NHWC                         if the `data_format` is `"channels_first"`.          A tensor.
class SymLogNorm(Normalize):          linscale : float, default: 1              This allows the linear range (-*linthresh* to *linthresh*) to be              stretched relative to the logarithmic range. Its value is the             number of powers of *base* (decades for base 10) to use for each             half of the linear range. For example, when *linscale* == 1.0             (the default), the space used for the positive and negative halves             of the linear range will be equal to a decade in the logarithmic             range if ``base=10``.         base : float, default: None             For v3.2 the default is the old value of ``np.e``, but that is             deprecated for v3.3 when base will default to 10.  During the             transition, specify the *base* kwarg to avoid a deprecation             warning.          Normalize.__init__(self, vmin, vmax, clip)         if base is None:             self._base = np.e             cbook.warn_deprecated("3.3", message="default base will change "                 "from np.e to 10.  To suppress this warning specify the base "                 "kwarg.")         else:             self._base = base         self._log_base = np.log(self._base)           self.linthresh = float(linthresh)         self._linscale_adj = (linscale / (1.0 - self._base ** -1))          if vmin is not None and vmax is not None:              self._transform_vmin_vmax()
class TensorBoard(Callback):                          tf.summary.image(mapped_weight_name, w_img)                  if hasattr(layer, 'output'):                     if isinstance(layer.output, list):                         for i, output in enumerate(layer.output):                             tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)                     else:                         tf.summary.histogram('{}_out'.format(layer.name),                                              layer.output)          self.merged = tf.summary.merge_all()          if self.write_graph:
class _ScalarAccessIndexer(_NDFrameIndexerBase):          if not isinstance(key, tuple):              key = _tuplify(self.ndim, key)         key = list(self._convert_key(key, is_setter=True))          if len(key) != self.ndim:              raise ValueError("Not enough indexers for scalar access (setting)!")          self.obj._set_value(*key, value=value, takeable=self._takeable)
class InputLayer(Layer):          self.trainable = False          self.built = True          self.sparse = sparse         self.supports_masking = True          if input_shape and batch_input_shape:              raise ValueError('Only provide the input_shape OR '
Ur"hello" from __future__ import unicode_literals as _unicode_literals from __future__ import absolute_import from __future__ import print_function as lol, with_function  "hello"  "hello"
class Model(Container):          if do_validation:              self._make_test_function()         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                             ' and multiple workers may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if steps_per_epoch is None:             if is_sequence:                 steps_per_epoch = len(generator)             else:                 raise ValueError('`steps_per_epoch=None` is only valid for a'                                  ' generator based on the `keras.utils.Sequence`'                                  ' class. Please specify `steps_per_epoch` or use'                                  ' the `keras.utils.Sequence` class.')           val_gen = (hasattr(validation_data, 'next') or                     hasattr(validation_data, '__next__') or                     isinstance(validation_data, Sequence))         if (val_gen and not isinstance(validation_data, Sequence) and                 not validation_steps):             raise ValueError('`validation_steps=None` is only valid for a'                              ' generator based on the `keras.utils.Sequence`'                              ' class. Please specify `validation_steps` or use'                              ' the `keras.utils.Sequence` class.')          out_labels = self._get_deduped_metrics_names()
def match(command, settings):  @utils.git_support  def get_new_command(command, settings):     return command.script.replace(' diff', ' diff --staged')
def untokenize(iterable):      ut = Untokenizer()      return ut.untokenize(iterable) def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):      The generate_tokens() generator requires one argument, readline, which      must be a callable object which provides the same interface as the
def match(command, settings):  @sudo_support  def get_new_command(command, settings):     return re.sub('\\bmkdir (.*)', 'mkdir -p \\1', command.script)
class TimeseriesGenerator(Sequence):          self.reverse = reverse          self.batch_size = batch_size         if self.start_index > self.end_index:             raise ValueError('`start_index+length=%i > end_index=%i` '                              'is disallowed, as no part of the sequence '                              'would be left to be used as current step.'                              % (self.start_index, self.end_index))       def __len__(self):          return int(np.ceil(             (self.end_index - self.start_index + 1) /              (self.batch_size * self.stride)))      def _empty_batch(self, num_rows):
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      def get_indexer_non_unique(self, target):          target = ensure_index(target)         if not self._is_comparable_dtype(target.dtype):             no_matches = -1 * np.ones(self.shape, dtype=np.intp)             return no_matches, no_matches         target = target.asi8          indexer, missing = self._int64index.get_indexer_non_unique(target)          return ensure_platform_int(indexer), missing
class Task(object):          return False      @property      def pretty_id(self):          param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())
class HttpProxyMiddleware(object):          creds, proxy = self.proxies[scheme]          request.meta['proxy'] = proxy          if creds:             request.headers['Proxy-Authorization'] = b'Basic ' + creds
class EarlyStopping(Callback):          if self.monitor_op(current - self.min_delta, self.best):              self.best = current              self.wait = 0             if self.restore_best_weights:                 self.best_weights = self.model.get_weights()          else:              self.wait += 1              if self.wait >= self.patience:                  self.stopped_epoch = epoch                  self.model.stop_training = True                 if self.restore_best_weights:                     if self.verbose > 0:                         print("Restoring model weights from the end of the best epoch")                     self.model.set_weights(self.best_weights)      def on_train_end(self, logs=None):          if self.stopped_epoch > 0 and self.verbose > 0:
def test_multiprocessing_predict_error():      model.add(Dense(1, input_shape=(5,)))      model.compile(loss='mse', optimizer='adadelta')     with pytest.raises(RuntimeError):          model.predict_generator(              custom_generator(), good_batches * workers + 1, 1,              workers=workers, use_multiprocessing=True,          )     with pytest.raises(RuntimeError):          model.predict_generator(              custom_generator(), good_batches + 1, 1,              use_multiprocessing=False,
def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:      current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)      lowest_depth = sys.maxsize      trailing_comma_safe = True      def append_to_line(leaf: Leaf) -> Iterator[Line]:         nonlocal current_line         try:             current_line.append_safe(leaf, preformatted=True)         except ValueError as ve:             yield current_line              current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)             current_line.append(leaf)       for leaf in line.leaves:         yield from append_to_line(leaf)          for comment_after in line.comments_after(leaf):             yield from append_to_line(comment_after)           lowest_depth = min(lowest_depth, leaf.bracket_depth)          if (              leaf.bracket_depth == lowest_depth
def get_source_from_frame(frame):      if isinstance(source[0], bytes):         encoding = 'utf-8'          for line in source[:2]:
class RFPDupeFilter(BaseDupeFilter):          self.logger = logging.getLogger(__name__)          if path:              self.file = open(os.path.join(path, 'requests.seen'), 'a+')             self.file.seek(0)              self.fingerprints.update(x.rstrip() for x in self.file)      @classmethod
class SparseDataFrame(DataFrame):          new_data = {}          for col in left.columns:             new_data[col] = func(left[col], right[col])          return self._constructor(              new_data,
class Worker(object):      Structure for tracking worker activity and keeping their references.     def __init__(self, worker_id, last_active=time.time()):          self.id = worker_id self.reference = None self.last_active = last_active
def _get_distinct_objs(objs: List[Index]) -> List[Index]:  def _get_combined_index(     indexes: List[Index],     intersect: bool = False,     sort: bool = False,     copy: bool = False,  ) -> Index:      Return the union or intersection of indexes.
class _Window(PandasObject, ShallowMixin, SelectionMixin):                  def calc(x):                      x = np.concatenate((x, additional_nans))                     if not isinstance(self.window, BaseIndexer):                          min_periods = calculate_min_periods(                              window, self.min_periods, len(x), require_min_periods, floor                          )                      else:                          min_periods = calculate_min_periods(                             window_indexer.window_size,                              self.min_periods,                              len(x),                              require_min_periods,
class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):          Fast lookup of value from 1-dimensional ndarray. Only use this if you          know what you're doing         if not is_scalar(key):             raise InvalidIndexError(key)          if isinstance(key, (datetime, np.datetime64)):              return self.get_value_maybe_box(series, key)
class MTVServicesInfoExtractor(InfoExtractor):          video_id = self._id_from_uri(uri)          data = compat_urllib_parse.urlencode({'uri': uri})          idoc = self._download_xml(              self._FEED_URL + '?' + data, video_id,             u'Downloading info', transform_source=fix_xml_ampersands)          return [self._get_video_info(item) for item in idoc.findall('.//item')]
class requires(object):      def __call__(self, task_that_requires):          task_that_requires = self.inherit_decorator(task_that_requires)          def requires(_self):             return _self.clone_parent()         task_that_requires.requires = requires         return task_that_requires  class copies(object):
class WaitIterator(object):          the inputs.          self._running_future = TracebackFuture()              self._running_future.add_done_callback(lambda f: self)          if self._finished:              self._return_result(self._finished.popleft())
missing_value = StataMissingValue(um)                      loc = missing_loc[umissing_loc == j]                     if loc.ndim == 2 and loc.shape[1] == 1:                           loc = loc[:, 0]                      replacement.iloc[loc] = missing_value else:                  dtype = series.dtype
def map_config_to_obj(module):  def map_params_to_obj(module):      text = module.params['text']      return {          'banner': module.params['banner'],          'text': text,
def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):          if is_object_dtype(dtype):              return tslibs.ints_to_pytimedelta(arr.view(np.int64))          elif dtype == np.int64:             if isna(arr).any():                 raise ValueError("Cannot convert NaT values to integer")              return arr.view(dtype)          if dtype not in [_INT64_DTYPE, _TD_DTYPE]:
class Bidirectional(Wrapper):              kwargs['mask'] = mask          if initial_state is not None and has_arg(self.layer.call, 'initial_state'):              forward_state = initial_state[:len(initial_state) // 2]              backward_state = initial_state[len(initial_state) // 2:]              y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)
class FastAPI(Starlette):                  response_model_exclude_unset=bool(                      response_model_exclude_unset or response_model_skip_defaults                  ),                 response_model_exclude_defaults=response_model_exclude_defaults,                 response_model_exclude_none=response_model_exclude_none,                  include_in_schema=include_in_schema,                  response_class=response_class or self.default_response_class,                  name=name,
class FormRequest(Request):  def _get_form_url(form, url):      if url is None:         return urljoin(form.base_url, form.action)      return urljoin(form.base_url, url)
def cache(*depends_on):              return fn(*args, **kwargs)          cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')            key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])          etag = '.'.join(_get_mtime(name) for name in depends_on)         with closing(shelve.open(cache_path)) as db:              if db.get(key, {}).get('etag') == etag:                  return db[key]['value']              else:
class ClipsyndicateIE(InfoExtractor):          pdoc = self._download_xml(              'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,              video_id, u'Downloading video info',             transform_source=fix_xml_ampersands)          track_doc = pdoc.find('trackList/track')          def find_param(name):
def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,          if isinstance(iterable, np.ndarray):              return tqdm_class(np.ndenumerate(iterable),                                total=total or len(iterable), **tqdm_kwargs)     return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)  def _tzip(iter1, *iter2plus, **tqdm_kwargs):
import platform  import re  import ssl  import socket import struct  import subprocess  import sys  import traceback
def add_special_arithmetic_methods(cls):          def f(self, other):              result = method(self, other)              self._reset_cacher()              self._update_inplace(
class Sequential(Model):              for layer in self._layers:                  x = layer(x)              self.outputs = [x]             self._build_input_shape = input_shape          if self.inputs:              self._init_graph_network(self.inputs,
class TimedeltaIndex(      @Appender(_shared_docs["searchsorted"])      def searchsorted(self, value, side="left", sorter=None):          if isinstance(value, (np.ndarray, Index)):             if not type(self._data)._is_recognized_dtype(value):                 raise TypeError(                     "searchsorted requires compatible dtype or scalar, "                     f"not {type(value).__name__}"                 )             value = type(self._data)(value)             self._data._check_compatible_with(value)          elif isinstance(value, self._data._recognized_scalars):             self._data._check_compatible_with(value)             value = self._data._scalar_type(value)          elif not isinstance(value, TimedeltaArray):             raise TypeError(                 "searchsorted requires compatible dtype or scalar, "                 f"not {type(value).__name__}"             )         return self._data.searchsorted(value, side=side, sorter=sorter)      def is_type_compatible(self, typ) -> bool:          return typ == self.inferred_type or typ == "timedelta"
def get_meta_refresh(response):  def response_status_message(status):     return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), "Unknown Status")))  def response_httprepr(response):
def get_new_command(command):      if '2' in command.script:          return command.script.replace("2", "3")     last_arg = command.script_parts[-1]     help_command = last_arg + ' --help'        if command.stderr.strip() == 'No manual entry for ' + last_arg:         return [help_command]       split_cmd2 = command.script_parts      split_cmd3 = split_cmd2[:]      split_cmd2.insert(1, ' 2 ')      split_cmd3.insert(1, ' 3 ')      return [          "".join(split_cmd3),          "".join(split_cmd2),         help_command,      ]
class Response(object_ref):          if isinstance(url, Link):              url = url.url         elif url is None:             raise ValueError("url can't be None")          url = self.urljoin(url)          return Request(url, callback,                         method=method,
class BoolBlock(NumericBlock):              return issubclass(tipo.type, np.bool_)          return isinstance(element, (bool, np.bool_))     def should_store(self, value: ArrayLike) -> bool:          return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(              value          )
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class ExcelFormatter:                  raise KeyError("Not all names specified in 'columns' are found")             self.df = df.reindex(columns=cols)          self.columns = self.df.columns          self.float_format = float_format
class Model(Container):          stateful_metric_indices = []          if hasattr(self, 'metrics'):             for m in self.stateful_metric_functions:                 m.reset_states()              stateful_metric_indices = [                  i for i, name in enumerate(self.metrics_names)                  if str(name) in self.stateful_metric_names]
def jsonable_encoder(                  exclude=exclude,                  by_alias=by_alias,                  exclude_unset=bool(exclude_unset or skip_defaults),                 exclude_none=exclude_none,                 exclude_defaults=exclude_defaults,              ) else:             if exclude_defaults:                 raise ValueError("Cannot use exclude_defaults")              obj_dict = obj.dict(                  include=include,                  exclude=exclude,
from pandas.core.arrays.datetimes import (      validate_tz_from_dtype,  )  import pandas.core.common as com from pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name  from pandas.core.indexes.datetimelike import (      DatetimelikeDelegateMixin,      DatetimeTimedeltaMixin,
class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):              other = TimedeltaIndex(other)          return self, other     def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:         return is_timedelta64_dtype(dtype)       def get_loc(self, key, method=None, tolerance=None):
class InvalidModeException(CookiecutterException):      Raised when cookiecutter is called with both `no_input==True` and      `replay==True` at the same time.     Raised when a hook script fails
def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:              and trailing_comma_safe          ):              current_line.append(Leaf(token.COMMA, ','))         yield current_line   @dont_increase_indentation def standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:         nonlocal current_line         try:             current_line.append_safe(leaf, preformatted=True)         except ValueError as ve:             yield current_line              current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)             current_line.append(leaf)      for leaf in line.leaves:         yield from append_to_line(leaf)          for comment_after in line.comments_after(leaf):             yield from append_to_line(comment_after)      if current_line:          yield current_line
def get_grouper(              return False          try:              return gpr is obj[gpr.name]         except (KeyError, IndexError, ValueError):                return False      for i, (gpr, level) in enumerate(zip(keys, levels)):
class Series(base.IndexOpsMixin, generic.NDFrame):                  self[:] = value              else:                  self.loc[key] = value         except InvalidIndexError:              self._set_with(key, value)          except TypeError as e:              if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):
def gen_python_files_in_dir(      assert root.is_absolute(), f"INTERNAL ERROR: `root` must be absolute but is {root}"      for child in path.iterdir():         try:             normalized_path = "/" + child.resolve().relative_to(root).as_posix()         except ValueError:             if child.is_symlink():                 report.path_ignored(                     child,                     "is a symbolic link that points outside of the root directory",                 )                 continue              raise           if child.is_dir():              normalized_path += "/"          exclude_match = exclude.search(normalized_path)
from ..utils import (      compat_urllib_parse,      compat_urllib_request,      urlencode_postdata,      ExtractorError,     limit_length,  )
def generate_files(repo_dir, context=None, output_dir='.',      with work_in(repo_dir):         try:             run_hook('pre_gen_project', project_dir, context)         except FailedHookException:             shutil.rmtree(project_dir, ignore_errors=True)              logging.error("Stopping generation because pre_gen_project"                            " hook script didn't exit sucessfully")              return
class TestDataFrameUnaryOperators:          tm.assert_frame_equal(-(df < 0), ~(df < 0))     def test_invert_mixed(self):         shape = (10, 5)         df = pd.concat(             [                 pd.DataFrame(np.zeros(shape, dtype="bool")),                 pd.DataFrame(np.zeros(shape, dtype=int)),             ],             axis=1,             ignore_index=True,         )         result = ~df         expected = pd.concat(             [                 pd.DataFrame(np.ones(shape, dtype="bool")),                 pd.DataFrame(-np.ones(shape, dtype=int)),             ],             axis=1,             ignore_index=True,         )         tm.assert_frame_equal(result, expected)       @pytest.mark.parametrize(          "df",          [
class Axes(_AxesBase):              Respective beginning and end of each line. If scalars are              provided, all lines will have same length.         colors : list of colors, default: :rc:`lines.color`          linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
class DataFrame(NDFrame):                  for k1, k2 in zip(key, value.columns):                      self[k1] = value[k2]              else:                 self.loc._ensure_listlike_indexer(key, axis=1)                  indexer = self.loc._get_listlike_indexer(                      key, axis=1, raise_missing=False                  )[1]
class Line:              self.bracket_tracker.mark(leaf)              self.maybe_remove_trailing_comma(leaf)              self.maybe_increment_for_loop_variable(leaf)          if not self.append_comment(leaf):              self.leaves.append(leaf)     def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:         if self.bracket_tracker.depth == 0:             if self.is_comment:                 raise ValueError("cannot append to standalone comments")              if self.leaves and leaf.type == STANDALONE_COMMENT:                 raise ValueError(                     "cannot append standalone comments to a populated line"                 )          self.append(leaf, preformatted=preformatted)       @property      def is_comment(self) -> bool:
class RobotsTxtMiddleware(object):          rp_dfd.callback(rp)      def _robots_error(self, failure, netloc):         rp_dfd = self._parsers[netloc]         self._parsers[netloc] = None         rp_dfd.callback(None)
from matplotlib._pylab_helpers import Gcf  from matplotlib.backend_managers import ToolManager  from matplotlib.transforms import Affine2D  from matplotlib.path import Path from matplotlib.cbook import _setattr_cm  _log = logging.getLogger(__name__)
from pandas.core.dtypes.common import (      is_array_like,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64tz_dtype,      is_dtype_equal,
patterns = ['permission denied',  def match(command):     if command.script_parts and command.script_parts[0] == 'sudo':         return False       for pattern in patterns:          if pattern.lower() in command.stderr.lower()\                  or pattern.lower() in command.stdout.lower():
class TupleParameter(Parameter):          try:              return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))          except ValueError: return literal_eval(x)  class NumericalParameter(Parameter):
def violinplot(  @_copy_docstring_and_deprecators(Axes.vlines)  def vlines(         x, ymin, ymax, colors=None, linestyles='solid', label='', *,          data=None, **kwargs):      return gca().vlines(          x, ymin, ymax, colors=colors, linestyles=linestyles,
class Request:          :rtype: str         try:             if "//" in self.app.config.SERVER_NAME:                 return self.app.url_for(view_name, _external=True, **kwargs)         except AttributeError:             pass          scheme = self.scheme          host = self.server_name
class CrawlerProcess(CrawlerRunner):      def __init__(self, settings):          super(CrawlerProcess, self).__init__(settings)          install_shutdown_handlers(self._signal_shutdown)         configure_logging(self.settings)         log_scrapy_info(self.settings)      def _signal_shutdown(self, signum, _):          install_shutdown_handlers(self._signal_kill)
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class TestTimedelta64ArithmeticUnsorted:          tm.assert_index_equal(result1, result4)          tm.assert_index_equal(result2, result3)     def test_tda_add_sub_index(self):          tdi = TimedeltaIndex(["1 days", pd.NaT, "2 days"])         tda = tdi.array          dti = pd.date_range("1999-12-31", periods=3, freq="D")          result = tda + dti         expected = tdi + dti         tm.assert_index_equal(result, expected)          result = tda + tdi         expected = tdi + tdi         tm.assert_index_equal(result, expected)          result = tda - tdi         expected = tdi - tdi         tm.assert_index_equal(result, expected)   class TestAddSubNaTMasking:
class BaseSettings(MutableMapping):          if basename in self:              warnings.warn('_BASE settings are deprecated.',                            category=ScrapyDeprecationWarning)                 compsett = BaseSettings(self[basename], priority='default')             for k in self[name]:                 prio = self[name].getpriority(k)                 if prio > get_settings_priority('default'):                     compsett.set(k, self[name][k], prio)              return compsett         return self[name]      def getpriority(self, name):
class DataFrame(NDFrame):              new_data = ops.dispatch_to_series(self, other, func)          else:             other_vals = other.values.reshape(-1, 1)              with np.errstate(all="ignore"):                 new_data = func(self.values, other_vals)             new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)          return new_data      def _construct_result(self, result) -> "DataFrame":
class Tracer:          old_local_reprs = self.frame_to_local_reprs.get(frame, {})          self.frame_to_local_reprs[frame] = local_reprs = \                                        get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)          newish_string = ('Starting var:.. ' if event == 'call' else                                                              'New var:....... ')
class NDFrame(PandasObject, SelectionMixin):              data = self.fillna(method=fill_method, limit=limit, axis=axis)          rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1         rs = rs.loc[~rs.index.duplicated()]          rs = rs.reindex_like(data)          if freq is None:              mask = isna(com.values_from_object(data))
class Text(Artist):      def update(self, kwargs): sentinel = object()          fontproperties = kwargs.pop("fontproperties", sentinel)         if fontproperties is not sentinel:             self.set_fontproperties(fontproperties)           bbox = kwargs.pop("bbox", sentinel)          super().update(kwargs)          if bbox is not sentinel:
from pandas.core.algorithms import (  )  from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs  import pandas.core.common as com from pandas.core.construction import array, extract_array, sanitize_array  from pandas.core.missing import interpolate_2d  from pandas.core.sorting import nargsort
class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):      )      _engine_type = libindex.DatetimeEngine     _supports_partial_string_indexing = True      _tz = None      _freq = None
class ReduceLROnPlateau(Callback):              monitored has stopped increasing; in `auto`              mode, the direction is automatically inferred              from the name of the monitored quantity.         min_delta: threshold for measuring the new optimum,              to only focus on significant changes.          cooldown: number of epochs to wait before resuming              normal operation after lr has been reduced.
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):          padding: One of `"same"`, `"valid"`, `"full"`.          output_padding: Integer, amount of padding along the output dimension,              Can be set to `None` in which case the output length is inferred.         dilation: dilation rate, integer.          The output length (integer).
class Index(IndexOpsMixin, PandasObject):                      self._invalid_indexer("label", key)              elif kind == "loc" and is_integer(key):                 if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):                      self._invalid_indexer("label", key)          return key
Name: Max Speed, dtype: float64          if copy:              new_values = new_values.copy()         if not isinstance(self.index, DatetimeIndex):             raise TypeError(f"unsupported Type {type(self.index).__name__}") new_index = self.index.to_period(freq=freq)          return self._constructor(new_values, index=new_index).__finalize__(              self, method="to_period"
def get_openapi_path(              operation_parameters = get_openapi_operation_parameters(all_route_params)              parameters.extend(operation_parameters)              if parameters:                 operation["parameters"] = list(                     {param["name"]: param for param in parameters}.values()                 )              if method in METHODS_WITH_BODY:                  request_body_oai = get_openapi_operation_request_body(                      body_field=route.body_field, model_name_map=model_name_map
class ReduceLROnPlateau(Callback):                                    'rate to %s.' % (epoch + 1, new_lr))                          self.cooldown_counter = self.cooldown                          self.wait = 0      def in_cooldown(self):          return self.cooldown_counter > 0
def _align_method_FRAME(  def _should_reindex_frame_op(     left: "DataFrame", right, op, axis, default_axis: int, fill_value, level  ) -> bool:      assert isinstance(left, ABCDataFrame)     if op is operator.pow or op is rpow:          return False       if not isinstance(right, ABCDataFrame):          return False
class CentralPlannerScheduler(Scheduler):         if self._config.prune_on_get_work:             self.prune()           worker_id = kwargs['worker']          self.update(worker_id, {'host': host})
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or not np.isnan(right_value):                      return False              else:                 try:                     if np.any(left_value != right_value):                         return False                 except TypeError as err:                     if "Cannot compare tz-naive" in str(err):                          return False                     raise          return True
def reset_display_options():      pd.reset_option("^display.", silent=True) def round_trip_pickle(     obj: Any, path: Optional[FilePathOrBuffer] = None ) -> FrameOrSeries:      Pickle an object and then read it again.      Parameters      ----------     obj : any object          The object to pickle and then re-read.     path : str, path object or file-like object, default None          The path where the pickled object is written and then read.      Returns
default: :rc:`scatter.edgecolors`          path = marker_obj.get_path().transformed(              marker_obj.get_transform())          if not marker_obj.is_filled():              if linewidths is None:                  linewidths = rcParams['lines.linewidth']              elif np.iterable(linewidths):
def fit_generator(model,     val_use_sequence_api = is_sequence(validation_data)      val_gen = (hasattr(validation_data, 'next') or                 hasattr(validation_data, '__next__') or                val_use_sequence_api)     if (val_gen and not val_use_sequence_api and              not validation_steps):          raise ValueError('`validation_steps=None` is only valid for a'                           ' generator based on the `keras.utils.Sequence`'
def test_resample_integerarray():      result = ts.resample("3T").mean()      expected = Series(         [1, 4, 7],         index=pd.date_range("1/1/2000", periods=3, freq="3T"),         dtype="float64",      )      tm.assert_series_equal(result, expected)
def _period_array_cmp(cls, op):              except ValueError:                  return invalid_comparison(self, other, op)          if isinstance(other, self._recognized_scalars) or other is NaT:              other = self._scalar_type(other)
class EmptyLineTracker:          lines (two on module-level).          before, after = self._maybe_empty_lines(current_line)         before = (               0             if self.previous_line is None             else before - self.previous_after         )          self.previous_after = after          self.previous_line = current_line          return before, after
class CollectionSearch: if not ds:              return None            env = Environment()         for collection_name in ds:             if is_template(collection_name, env):                 display.warning('"collections" is not templatable, but we found: %s, '                                 'it will not be templated and will be used "as is".' % (collection_name))           return ds
def posix_pipe(fin, fout, delim='\n', buf_size=256,  RE_OPTS = re.compile(r'\n {8}(\S+)\s{2,}:\s*([^,]+)') RE_SHLEX = re.compile(r'\s*(?<!\S)--?([^\s=]+)(?:\s*|=|$)')  UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')
class TestTimedeltaIndex(DatetimeLike):      def test_pickle_compat_construction(self):          pass     def test_pickle_after_set_freq(self):         tdi = timedelta_range("1 day", periods=4, freq="s")         tdi = tdi._with_freq(None)          res = tm.round_trip_pickle(tdi)         tm.assert_index_equal(res, tdi)       def test_isin(self):          index = tm.makeTimedeltaIndex(4)
class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):      _can_hold_element = DatetimeBlock._can_hold_element      to_native_types = DatetimeBlock.to_native_types      fill_value = np.datetime64("NaT", "ns")     should_store = DatetimeBlock.should_store      @property      def _holder(self):
class _Alpha:          raise ValueError      def __le__(self, other):          return self.__lt__(other) or self.__eq__(other)     def __gt__(self, other):         return not self.__le__(other)       def __ge__(self, other):         return not self.__lt__(other)  class _Numeric:
def _clone_functional_model(model, input_tensors=None):                              kwargs['mask'] = computed_mask                      output_tensors = to_list(                          layer(computed_tensor, **kwargs))                     if layer.supports_masking:                         output_masks = to_list(                             layer.compute_mask(computed_tensor,                                                computed_mask))                     else:                         output_masks = [None] * len(output_tensors)                      computed_tensors = [computed_tensor]                      computed_masks = [computed_mask]                  else:
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):          t1, t2 = self._parsed_string_to_bounds(reso, parsed)          return slice(             self.searchsorted(t1, side="left"), self.searchsorted(t2, side="right")          )      def _convert_tolerance(self, tolerance, target):
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class Bidirectional(Wrapper):          self.supports_masking = True          self._trainable = True          super(Bidirectional, self).__init__(layer, **kwargs)         self.input_spec = layer.input_spec      @property      def trainable(self):
class Index(IndexOpsMixin, PandasObject):          is_null_slicer = start is None and stop is None          is_index_slice = is_int(start) and is_int(stop)         is_positional = is_index_slice and not (             self.is_integer() or self.is_categorical()         )          if kind == "getitem":
def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:      ):          for leaf in leaves:              result.append(leaf, preformatted=True)             for comment_after in line.comments_after(leaf):                  result.append(comment_after, preformatted=True)      bracket_split_succeeded_or_raise(head, body, tail)      for result in (head, body, tail):
from .common import (      is_unsigned_integer_dtype,      pandas_dtype,  ) from .dtypes import DatetimeTZDtype, ExtensionDtype, IntervalDtype, PeriodDtype  from .generic import (      ABCDataFrame,      ABCDatetimeArray,
from ..utils import (      unified_strdate,      parse_duration,      qualities,     strip_jsonp,      url_basename,  )
class SimpleTaskState(object):              elif task.scheduler_disable_time is not None and new_status != DISABLED:                  return         if new_status == FAILED and task.status != DISABLED:              task.add_failure()              if task.has_excessive_failures():                  task.scheduler_disable_time = time.time()
class MultiIndex(Index):          if len(uniques) < len(level_index):              level_index = level_index.take(uniques)         else:               level_index = level_index.copy()          if len(level_index):              grouper = level_index.take(codes)
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:          return tiow.read(), encoding, newline @dataclass(frozen=True) class ParserConfig:     grammar: Grammar     tokenizer_config: TokenizerConfig = TokenizerConfig()   def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:      if not target_versions:          return [              ParserConfig(                 pygram.python_grammar_no_print_statement_no_exec_statement,                 TokenizerConfig(async_is_reserved_keyword=True),             ),              ParserConfig(                 pygram.python_grammar_no_print_statement_no_exec_statement,                 TokenizerConfig(async_is_reserved_keyword=False),             ),              ParserConfig(pygram.python_grammar_no_print_statement),              ParserConfig(pygram.python_grammar),          ]      elif all(version.is_python2() for version in target_versions):         return [              ParserConfig(pygram.python_grammar_no_print_statement),              ParserConfig(pygram.python_grammar),         ]      else:         configs = []          if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):              configs.append(                 ParserConfig(                     pygram.python_grammar_no_print_statement_no_exec_statement,                     TokenizerConfig(async_is_reserved_keyword=True),                 )             )         if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):              configs.append(                 ParserConfig(                     pygram.python_grammar_no_print_statement_no_exec_statement,                     TokenizerConfig(async_is_reserved_keyword=False),                 )             )           return configs  def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
class FacebookGraphMixin(OAuth2Mixin):             Added the ability to override ``self._FACEBOOK_BASE_URL``.          url = self._FACEBOOK_BASE_URL + path             oauth_future = self.oauth2_request(url, access_token=access_token,                                            post_args=post_args, **args)         chain_future(oauth_future, callback)  def _oauth_signature(consumer_token, method, url, parameters={}, token=None):
def _match_one(filter_part, dct):                      raise ValueError(                          'Invalid integer value %r in filter part %r' % (                              m.group('intval'), filter_part))          if actual_value is None:              return m.group('none_inclusive')          return op(actual_value, comparison_value)
class Worker(object):      def __init__(self, worker_id, last_active=None):          self.id = worker_id self.reference = None         self.last_active = last_active or time.time() self.started = time.time() self.tasks = set()          self.info = {}
class GalaxyCLI(CLI):              else:                  requirements = []                  for collection_input in collections:                     requirement = None                     if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \                             urlparse(collection_input).scheme.lower() in ['http', 'https']:                          name = collection_input                     else:                         name, dummy, requirement = collection_input.partition(':')                      requirements.append((name, requirement or '*', None))              output_path = GalaxyCLI._resolve_path(output_path)
class ReduceLROnPlateau(Callback):      def __init__(self, monitor='val_loss', factor=0.1, patience=10,                  verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,                  **kwargs):          super(ReduceLROnPlateau, self).__init__()          self.monitor = monitor          if factor >= 1.0:              raise ValueError('ReduceLROnPlateau '                               'does not support a factor >= 1.0.')         if 'epsilon' in kwargs:             min_delta = kwargs.pop('epsilon')             warnings.warn('`epsilon` argument is deprecated and '                           'will be removed, use `min_delta` insted.')          self.factor = factor          self.min_lr = min_lr         self.min_delta = min_delta          self.patience = patience          self.verbose = verbose          self.cooldown = cooldown
def _match_one(filter_part, dct):              if m.group('op') not in ('=', '!='):                  raise ValueError(                      'Operator %s does not support string values!' % m.group('op'))             comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')             quote = m.group('quote')             if quote is not None:                 comparison_value = comparison_value.replace(r'\%s' % quote, quote)          else:              try:                  comparison_value = int(m.group('intval'))
class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):          result = np.asarray(result)          if self.orig is not None:              index = self.orig.index          else:              index = self._parent.index
class LinuxHardware(Hardware):           if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):              i = processor_occurence
from pandas.core.dtypes.common import (      is_scalar,  )  from pandas.core.dtypes.dtypes import CategoricalDtype from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna  from pandas.core import accessor  from pandas.core.algorithms import take_1d
class NDFrame(PandasObject, SelectionMixin):          inplace = validate_bool_kwarg(inplace, "inplace")         axis = self._get_axis_number(axis)           if axis == 0:              ax = self._info_axis_name              _maybe_transposed_self = self          elif axis == 1:              _maybe_transposed_self = self.T              ax = 1           ax = _maybe_transposed_self._get_axis_number(ax)          if _maybe_transposed_self.ndim == 2:
default 'raise'          >>> tz_aware.tz_localize(None)          DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',                         '2018-03-03 09:00:00'],                       dtype='datetime64[ns]', freq=None)          Be careful with DST changes. When there is sequential data, pandas can          infer the DST time:
class WrappedResponse(object):      def get_all(self, name, default=None):         return [to_native_str(v, errors='replace')                 for v in self.response.headers.getlist(name)]      getheaders = get_all
class FigureCanvasBase:      def inaxes(self, xy):         Return the topmost visible `~.axes.Axes` containing the point *xy*.          Parameters          ----------
except AttributeError:          if ret:              raise subprocess.CalledProcessError(ret, p.args, output=output)          return output   def limit_length(s, length):     if s is None:         return None     ELLIPSES = '...'     if len(s) > length:         return s[:length - len(ELLIPSES)] + ELLIPSES     return s
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class Operation(BaseModel):      operationId: Optional[str] = None      parameters: Optional[List[Union[Parameter, Reference]]] = None      requestBody: Optional[Union[RequestBody, Reference]] = None     responses: Union[Responses, Dict[str, Response]]      callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None      deprecated: Optional[bool] = None
class LineGenerator(Visitor[Line]):          yield from self.visit_default(leaf)          yield from self.line()
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _make_wrapped_arith_op_with_freq(opname: str):          if result is NotImplemented:              return NotImplemented         new_freq = self._get_addsub_freq(other, result)          result._freq = new_freq          return result
def map_obj_to_commands(updates, module, warnings):          else:              add('protocol unix-socket')     if needs_update('state'):          if want['state'] == 'stopped':              add('shutdown')          elif want['state'] == 'started':
class Scraper(object):              if download_failure.frames:                  logger.error('Error downloading %(request)s',                               {'request': request},                              exc_info=failure_to_exc_info(download_failure),                              extra={'spider': spider})              else:                  errmsg = download_failure.getErrorMessage()                  if errmsg:
def get_openapi(      if components:          output["components"] = components      output["paths"] = paths     return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)
from fastapi.encoders import jsonable_encoder  from fastapi.exceptions import RequestValidationError  from starlette.exceptions import HTTPException  from starlette.requests import Request
class DatetimeIndexOpsMixin(ExtensionIndex):      def is_all_dates(self) -> bool:          return True     def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:         raise AbstractMethodError(self)
from .interface import BaseInterfaceTests from .io import BaseParsingTests from .methods import BaseMethodsTests from .missing import BaseMissingTests from .ops import (     BaseArithmeticOpsTests,     BaseComparisonOpsTests,     BaseOpsUtil,     BaseUnaryOpsTests, ) from .printing import BasePrintingTests from .reduce import (      BaseBooleanReduceTests,
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class APIRouter(routing.Router):              assert not prefix.endswith(                  "/"              ), "A path prefix must not end with '/', as the routes will start with '/'"         if responses is None:             responses = {}          for route in router.routes:              if isinstance(route, APIRoute):                 combined_responses = {**responses, **route.responses}                  self.add_api_route(                      prefix + route.path,                      route.endpoint,
def parse_dfxp_time_expr(time_expr):      if mobj:          return float(mobj.group('time_offset'))     mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:(?:\.|:)\d+)?)$', time_expr)      if mobj:         return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))  def srt_subtitles_timecode(seconds):
class YoutubeDL(object):                      elif string == '(':                          if current_selector:                              raise syntax_error('Unexpected "("', start)                         group = _parse_format_selection(tokens, inside_group=True)                         current_selector = FormatSelector(GROUP, group, [])                      elif string == '+':                          video_selector = current_selector                         audio_selector = _parse_format_selection(tokens, inside_merge=True)                         current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])                      else:                          raise syntax_error('Operator not recognized: "{0}"'.format(string), start)                  elif type == tokenize.ENDMARKER:
def get_variable_shape(x):      return int_shape(x) @symbolic  def print_tensor(x, message=''):
class RadialLocator(mticker.Locator):          return self.base.refresh()     def nonsingular(self, vmin, vmax):          return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)                 else self.base.nonsingular(vmin, vmax))       def view_limits(self, vmin, vmax):          vmin, vmax = self.base.view_limits(vmin, vmax)          if vmax > vmin:
def _summary_format(set_tasks, worker):          str_output += 'Did not run any tasks'      smiley = ""      reason = ""     if set_tasks["ever_failed"]:         if not set_tasks["failed"]:             smiley = ":)"             reason = "there were failed tasks but they all suceeded in a retry"         else:             smiley = ":("             reason = "there were failed tasks"             if set_tasks["scheduling_error"]:                 reason += " and tasks whose scheduling failed"      elif set_tasks["scheduling_error"]:          smiley = ":("          reason = "there were tasks whose scheduling failed"
class YoutubeDL(object):                      elif string == '/':                          first_choice = current_selector                          second_choice = _parse_format_selection(tokens, inside_choice=True)                         current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])                      elif string == '[':                          if not current_selector:                              current_selector = FormatSelector(SINGLE, 'best', [])
from pandas.util._decorators import Appender, Substitution, cache_readonly, doc  from pandas.core.dtypes.cast import maybe_cast_result  from pandas.core.dtypes.common import (      ensure_float,     is_bool_dtype,      is_datetime64_dtype,     is_extension_array_dtype,      is_integer_dtype,      is_numeric_dtype,      is_object_dtype,
class APIRoute(routing.Route):          response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),          response_model_by_alias: bool = True,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Optional[Type[Response]] = None,          dependency_overrides_provider: Any = None,
class WebSocketProtocol(abc.ABC):      async def _receive_frame_loop(self) -> None:          raise NotImplementedError()     @abc.abstractmethod     def set_nodelay(self, x: bool) -> None:         raise NotImplementedError()   class _PerMessageDeflateCompressor(object):      def __init__(
def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:      check_lpar = False      for index, child in enumerate(list(node.children)):          if (             index == 0             and isinstance(child, Node)             and child.type == syms.testlist_star_expr         ):             check_lpar = True           if check_lpar:              if child.type == syms.atom:                  if maybe_make_parens_invisible_in_atom(child, parent=node):
class IntegerArray(ExtensionArray, ExtensionOpsMixin):              with warnings.catch_warnings():                  warnings.filterwarnings("ignore", "elementwise", FutureWarning)                  with np.errstate(all="ignore"):                     method = getattr(self._data, f"__{op_name}__")                     result = method(other)                      if result is NotImplemented:                         result = invalid_comparison(self._data, other, op)              if mask is None:
class FrameApply:          from pandas import Series          if not should_reduce:              try:                 r = self.f(Series([]))              except Exception:                  pass              else:                  should_reduce = not isinstance(r, Series)          if should_reduce:             if len(self.agg_axis):                 r = self.f(Series([]))             else:                 r = np.nan              return self.obj._constructor_sliced(r, index=self.agg_axis)          else:              return self.obj.copy()
class Block(PandasObject):      def setitem(self, indexer, value):         Attempt self.values[indexer] = value, possibly creating a new array.          Parameters          ----------
class StringArray(PandasArray):              if copy:                  return self.copy()              return self         elif isinstance(dtype, _IntegerDtype):             arr = self._ndarray.copy()             mask = self.isna()             arr[mask] = 0             values = arr.astype(dtype.numpy_dtype)             return IntegerArray(values, mask, copy=False)           return super().astype(dtype, copy)      def _reduce(self, name, skipna=True, **kwargs):
def js_to_json(code):          v = m.group(0)          if v in ('true', 'false', 'null'):              return v         elif v.startswith('/*') or v.startswith('//') or v == ',':              return ""          if v[0] in ("'", '"'):
def _match_one(filter_part, dct):          return op(actual_value, comparison_value)      UNARY_OPERATORS = {         '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),         '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),      }          (?P<op>%s)\s*(?P<key>[a-z_]+)
class RangeIndex(Int64Index):          if self.step > 0:              start, stop, step = self.start, self.stop, self.step          else:              reverse = self._range[::-1]             start, stop, step = reverse.start, reverse.stop, reverse.step          target_array = np.asarray(target)          if not (is_integer_dtype(target_array) and target_array.ndim == 1):
def format_file_in_place(          with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:         src_name = f"{src}  (original)"         dst_name = f"{src}  (formatted)"          diff_contents = diff(src_contents, dst_contents, src_name, dst_name)          if lock:              lock.acquire()
class Block(PandasObject):          mask = isna(values)          if not self.is_object and not quoting:             itemsize = writers.word_len(na_rep)             values = values.astype("<U{size}".format(size=itemsize))          else:              values = np.array(values, dtype="object")
class DatetimeTZDtype(PandasExtensionDtype):              tz = timezones.tz_standardize(tz)          elif tz is not None:              raise pytz.UnknownTimeZoneError(tz)         if tz is None:              raise TypeError("A 'tz' is required.")          self._unit = unit
class GeneratorEnqueuer(SequenceEnqueuer):          self._use_multiprocessing = use_multiprocessing          self._threads = []          self._stop_event = None         self._manager = None          self.queue = None          self.seed = seed
def func_no_args():    for i in range(10):      print(i)      continue   exec("new-style exec", {}, {})    return None async def coroutine(arg, exec=False):   "Single-line docstring. Multiline is harder to reformat."   async with some_connection() as conn:       await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
class WrappedRequest(object):          return self.request.meta.get('is_unverifiable', False)     def get_origin_req_host(self):         return urlparse_cached(self.request).hostname       @property     def full_url(self):         return self.get_full_url()      @property     def host(self):         return self.get_host()      @property     def type(self):         return self.get_type()       @property      def unverifiable(self):          return self.is_unverifiable()     @property     def origin_req_host(self):         return self.get_origin_req_host()      def has_header(self, name):          return name in self.request.headers
def _convert_listlike_datetimes(      elif unit is not None:          if format is not None:              raise ValueError("cannot specify both format and unit")         arg = getattr(arg, "_values", arg)            if isinstance(arg, IntegerArray):              mask = arg.isna()             arg = arg._ndarray_values         else:             mask = None          result, tz_parsed = tslib.array_with_unit_to_datetime(             arg, mask, unit, errors=errors         )           if errors == "ignore":              from pandas import Index
class SparseDataFrame(DataFrame):          this, other = self.align(other, join="outer", axis=0, level=level, copy=False)          new_data = {}         for col in this.columns:             new_data[col] = func(this[col], other)          fill_value = self._get_op_result_fill_value(other, func)
class Line:              bracket_depth = leaf.bracket_depth              if bracket_depth == depth and leaf.type == token.COMMA:                  commas += 1                 if leaf.parent and leaf.parent.type in {                     syms.arglist,                     syms.typedargslist,                 }:                      commas += 1                      break
class Series(base.IndexOpsMixin, generic.NDFrame):              else:                  return self.iloc[key]          return self.loc[key]      def _get_values_tuple(self, key):
class Sequential(Model):              return (proba > 0.5).astype('int32')      def get_config(self):         layer_configs = []          for layer in self.layers:             layer_configs.append({                  'class_name': layer.__class__.__name__,                  'config': layer.get_config()              })         config = {             'name': self.name,             'layers': copy.deepcopy(layer_configs)         }         if self._build_input_shape:             config['build_input_shape'] = self._build_input_shape         return config      @classmethod      def from_config(cls, config, custom_objects=None):         if 'name' in config:             name = config['name']             build_input_shape = config.get('build_input_shape')             layer_configs = config['layers']         model = cls(name=name)         for conf in layer_configs:              layer = layer_module.deserialize(conf,                                               custom_objects=custom_objects)              model.add(layer)         if not model.inputs and build_input_shape:             model.build(build_input_shape)          return model
class NDFrame(PandasObject, SelectionMixin):          self._data.set_axis(axis, labels)          self._clear_item_cache()      def swapaxes(self, axis1, axis2, copy=True):
class Index(IndexOpsMixin, PandasObject):          if is_categorical(target):              tgt_values = np.asarray(target)         elif self.is_all_dates and target.is_all_dates:              tgt_values = target.asi8          else:              tgt_values = target._ndarray_values
class TestPeriodIndexArithmetic:          with pytest.raises(TypeError):              other - obj        def test_parr_add_sub_index(self):          pi = pd.period_range("2000-12-31", periods=3)         parr = pi.array          result = parr - pi         expected = pi - pi         tm.assert_index_equal(result, expected)   class TestPeriodSeriesArithmetic:      def test_ops_series_timedelta(self):
class YoutubeIE(YoutubeBaseInfoExtractor):              })          return chapters     def _extract_chapters(self, webpage, description, video_id, duration):         return (self._extract_chapters_from_json(webpage, video_id, duration)                 or self._extract_chapters_from_description(description, duration))       def _real_extract(self, url):          url, smuggled_data = unsmuggle_url(url, {})
class APIRoute(routing.Route):              response_model_exclude=self.response_model_exclude,              response_model_by_alias=self.response_model_by_alias,              response_model_exclude_unset=self.response_model_exclude_unset,             response_model_exclude_defaults=self.response_model_exclude_defaults,             response_model_exclude_none=self.response_model_exclude_none,              dependency_overrides_provider=self.dependency_overrides_provider,          )
CPU_INFO_TEST_SCENARIOS = [                  '23', 'POWER8 (architected), altivec supported',              ],              'processor_cores': 1,             'processor_count': 24,              'processor_threads_per_core': 1,             'processor_vcpus': 24          },      },      {
def diff(arr, n: int, axis: int = 0):      elif is_bool_dtype(dtype):          dtype = np.object_         is_bool = True      elif is_integer_dtype(dtype):          dtype = np.float64
from binaryornot.check import is_binary  from .exceptions import (      NonTemplatedInputDirException,      ContextDecodingException,     FailedHookException,      OutputDirExistsException  )  from .find import find_template  from .utils import make_sure_path_exists, work_in from .hooks import run_hook  def copy_without_render(path, context):
class WrappedRequest(object):          return name in self.request.headers      def get_header(self, name, default=None):         return to_native_str(self.request.headers.get(name, default),                              errors='replace')      def header_items(self):          return [             (to_native_str(k, errors='replace'),              [to_native_str(x, errors='replace') for x in v])              for k, v in self.request.headers.items()          ]
class FacebookIE(InfoExtractor):          login_page_req = compat_urllib_request.Request(self._LOGIN_URL)          login_page_req.add_header('Cookie', 'locale=en_US')         login_page = self._download_webpage(login_page_req, None,             note='Downloading login page',              errnote='Unable to download login page')          lsd = self._search_regex(              r'<input type="hidden" name="lsd" value="([^"]*)"',
class BaseMethodsTests(BaseExtensionTests):          expected = empty          self.assert_extension_array_equal(result, expected)     def test_shift_zero_copies(self, data):         result = data.shift(0)         assert result is not data          result = data[:0].shift(2)         assert result is not data       def test_shift_fill_value(self, data):          arr = data[:4]          fill_value = data[0]
from pandas.core.dtypes.inference import is_array_like  from pandas import compat  from pandas.core import ops from pandas.core.arrays import IntegerArray, PandasArray from pandas.core.arrays.integer import _IntegerDtype  from pandas.core.construction import extract_array  from pandas.core.indexers import check_array_indexer  from pandas.core.missing import isna
def _isna_new(obj):          raise NotImplementedError("isna is not defined for MultiIndex")      elif isinstance(obj, type):          return False     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):          return _isna_ndarraylike(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()
class PeriodicCallback(object):              self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)      def _update_next(self, current_time):         callback_time_sec = self.callback_time / 1000.0          if self._next_timeout <= current_time:                  self._next_timeout += (math.floor((current_time - self._next_timeout) /                                                callback_time_sec) + 1) * callback_time_sec         else:                           self._next_timeout += callback_time_sec
def _factorize_keys(lk, rk, sort=True):              np.putmask(rlab, rmask, count)          count += 1     if how == "right":         return rlab, llab, count      return llab, rlab, count
class TestGetItem:      def test_dti_custom_getitem(self):          rng = pd.bdate_range(START, END, freq="C")          smaller = rng[:5]         exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq="C")          tm.assert_index_equal(smaller, exp)         assert smaller.freq == exp.freq          assert smaller.freq == rng.freq          sliced = rng[::5]
class YoutubeIE(YoutubeBaseInfoExtractor):                      errnote='Unable to download video annotations', fatal=False,                      data=urlencode_postdata({xsrf_field_name: xsrf_token}))         chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)          if self._downloader.params.get('youtube_include_dash_manifest', True):
class EmptyLineTracker:          This is for separating `def`, `async def` and `class` with extra empty          lines (two on module-level).          before, after = self._maybe_empty_lines(current_line)          before -= self.previous_after          self.previous_after = after
class Parameter(object):              return [str(v) for v in x]          return str(x)     def parse_from_input(self, param_name, x, task_name=None):          Parses the parameter value from input ``x``, handling defaults and is_list.
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a minimum!         .. versionchanged:: 1.0.0             Returns an NA value on empty arrays           Raises          ------          TypeError
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          data_format: string, `"channels_last"` or `"channels_first"`.              Whether to use Theano or TensorFlow/CNTK data format              for inputs/kernels/outputs.         dilation_rate: tuple of 2 integers.          A tensor, result of transposed 2D convolution.
def _normalize(table, normalize, margins, margins_name="All"):              table = table.append(index_margin)              table = table.fillna(0)             table.index = table_index             table.columns = table_columns          else:              raise ValueError("Not a valid normalize argument")      else:          raise ValueError("Not a valid margins argument")
default: 'top'          if renderer is None:              renderer = get_renderer(self)         no_ops = {             meth_name: lambda *args, **kwargs: None             for meth_name in dir(RendererBase)             if (meth_name.startswith("draw_")                 or meth_name in ["open_group", "close_group"])         }          with _setattr_cm(renderer, **no_ops):             kwargs = get_tight_layout_figure(                 self, self.axes, subplotspec_list, renderer,                 pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)          if kwargs:              self.subplots_adjust(**kwargs)
from difflib import get_close_matches  from thefuck.utils import get_all_executables, \     get_valid_history_without_current, get_closest, which  from thefuck.specific.sudo import sudo_support  @sudo_support  def match(command):     return (not which(command.script_parts[0])              and 'not found' in command.stderr              and bool(get_close_matches(command.script_parts[0],                                         get_all_executables())))
class APIRouter(routing.Router):                  response_model_exclude_unset=bool(                      response_model_exclude_unset or response_model_skip_defaults                  ),                 response_model_exclude_defaults=response_model_exclude_defaults,                 response_model_exclude_none=response_model_exclude_none,                  include_in_schema=include_in_schema,                  response_class=response_class or self.default_response_class,                  name=name,
to the w3lib.url module. Always import those from there instead.  import posixpath  import re import six  from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,                                      urlparse, parse_qsl, urlencode,                                     quote, unquote) if six.PY3:     from urllib.parse import unquote_to_bytes  from w3lib.url import *  from w3lib.url import _safe_chars from scrapy.utils.python import to_bytes, to_native_str, to_unicode  def url_is_from_any_domain(url, domains):
class VarianceScaling(Initializer):          if self.distribution == 'normal':              stddev = np.sqrt(scale) / .87962566103423978             x = K.truncated_normal(shape, 0., stddev,                                    dtype=dtype, seed=self.seed)          else:              limit = np.sqrt(3. * scale)             x = K.random_uniform(shape, -limit, limit,                                  dtype=dtype, seed=self.seed)         if self.seed is not None:             self.seed += 1         return x      def get_config(self):          return {
class Index(IndexOpsMixin, PandasObject):              else:                  return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)         elif is_period_dtype(data) or is_period_dtype(dtype):             if is_dtype_equal(_o_dtype, dtype):                 return PeriodIndex(data, copy=False, name=name, **kwargs).astype(object)             return PeriodIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)          elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class ComplexBlock(FloatOrComplexBlock):              element, (float, int, complex, np.float_, np.int_)          ) and not isinstance(element, (bool, np.bool_))     def should_store(self, value: ArrayLike) -> bool:          return issubclass(value.dtype.type, np.complexfloating)
import traceback  from .variables import CommonVariable, Exploding, BaseVariable  from . import utils, pycompat if pycompat.PY2:     from io import open  ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')
class CentralPlannerScheduler(Scheduler):          for task in self._state.get_active_tasks():              self._state.fail_dead_worker_task(task, self._config, assistant_ids)             removed = self._state.prune(task, self._config)             if removed and task.id not in necessary_tasks:                  remove_tasks.append(task.id)          self._state.inactivate_tasks(remove_tasks)
class CSVLogger(Callback):          self.writer = None          self.keys = None          self.append_header = True         if six.PY2:             self.file_flags = 'b'             self._open_args = {}         else:             self.file_flags = ''             self._open_args = {'newline': '\n'}          super(CSVLogger, self).__init__()      def on_train_begin(self, logs=None):
def bracket_split_build_line(          if leaves:              normalize_prefix(leaves[0], inside_brackets=True)                if original.is_import:                 for i in range(len(leaves) - 1, -1, -1):                     if leaves[i].type == STANDALONE_COMMENT:                         continue                     elif leaves[i].type == token.COMMA:                         break                     else:                         leaves.insert(i + 1, Leaf(token.COMMA, ","))                         break      for leaf in leaves:          result.append(leaf, preformatted=True)
class Tracer:              thread_global.depth -= 1              if not ended_by_exception:                 return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)                  self.write('{indent}Return value:.. {return_value_repr}'.                             format(**locals()))
import re  import shutil  import subprocess  import socket import string  import sys  import time  import tokenize
class Model(Container):                  to yield from `generator` before declaring one epoch                  finished and starting the next epoch. It should typically                  be equal to the number of samples of your dataset                 divided by the batch size.                 Optional for `Sequence`: if unspecified, will use                 the `len(generator)` as a number of steps.              epochs: Integer, total number of iterations on the data.              verbose: Verbosity mode, 0, 1, or 2.              callbacks: List of callbacks to be called during training.
from parsel.selector import create_root_node  import six  from scrapy.http.request import Request  from scrapy.utils.python import to_bytes, is_listlike from scrapy.utils.response import get_base_url  class FormRequest(Request):
class BracketTracker:         if (             self._for_loop_depths             and self._for_loop_depths[-1] == self.depth             and leaf.type == token.NAME             and leaf.value == "in"         ):              self.depth -= 1             self._for_loop_depths.pop()              return True          return False
class Settings(dict):          return self.get(item)      def update(self, **kwargs):         Returns new settings with values from `kwargs` for unset settings.
class Model(Container):          if hasattr(self, 'metrics'):             for m in self.stateful_metric_functions:                 m.reset_states()              stateful_metric_indices = [                  i for i, name in enumerate(self.metrics_names)                  if str(name) in self.stateful_metric_names]
VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {          Feature.NUMERIC_UNDERSCORES,          Feature.TRAILING_COMMA_IN_CALL,          Feature.TRAILING_COMMA_IN_DEF,         Feature.ASYNC_IS_RESERVED_KEYWORD,      },  }
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
from pandas.errors import AbstractMethodError  from pandas import DataFrame, get_option from pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url  def get_engine(engine):
def _isna_ndarraylike_old(obj):      dtype = values.dtype      if is_string_dtype(dtype):         result = _isna_string_dtype(values, dtype, old=True)     elif needs_i8_conversion(dtype):          result = values.view("i8") == iNaT      else:
class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):          self._data._freq = freq     def _shallow_copy(self, values=None, **kwargs):         if values is None:             values = self._data         if isinstance(values, type(self)):             values = values._data          attributes = self._get_attributes_dict()          if "freq" not in kwargs and self.freq is not None:             if isinstance(values, (DatetimeArray, TimedeltaArray)):                 if values.freq is None:                     del attributes["freq"]          attributes.update(kwargs)         return self._simple_new(values, **attributes)
class YoutubeDL(object):                  else:                      filter_parts.append(string)         def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):              selectors = []              current_selector = None              for type, string, start, _, _ in tokens:
class _Concatenator:      def _get_comb_axis(self, i: int) -> Index:          data_axis = self.objs[0]._get_block_manager_axis(i)          return get_objs_combined_axis(             self.objs,             axis=data_axis,             intersect=self.intersect,             sort=self.sort,             copy=self.copy,          )      def _get_concat_axis(self) -> Index:
class MetacriticIE(InfoExtractor):          webpage = self._download_webpage(url, video_id)          info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,             video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)          clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)          formats = []
import logging from six.moves.urllib.parse import urljoin, urlparse  from w3lib.url import safe_url_string
class SeriesGroupBy(GroupBy):              res, out = np.zeros(len(ri), dtype=out.dtype), res              res[ids[idx]] = out         result = Series(res, index=ri, name=self._selection_name)         return self._reindex_output(result, fill_value=0)      @Appender(Series.describe.__doc__)      def describe(self, **kwargs):
class Sequential(Model):              generator: generator yielding batches of input samples.              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Optional for `Sequence`: if unspecified, will use                 the `len(generator)` as a number of steps.              max_queue_size: maximum size for the generator queue              workers: maximum number of processes to spin up              use_multiprocessing: if True, use process based threading.
class _iLocIndexer(_LocationIndexer):              if not is_integer(k):                  return False          return True      def _validate_integer(self, key: int, axis: int) -> None:
from pandas.core.dtypes.common import (      is_scalar,      pandas_dtype,  ) from pandas.core.dtypes.dtypes import PeriodDtype  from pandas.core.arrays.period import (      PeriodArray,
class Sequential(Model):                                          initial_epoch=initial_epoch)      @interfaces.legacy_generator_methods_support     def evaluate_generator(self, generator, steps=None,                             max_queue_size=10, workers=1,                             use_multiprocessing=False):
class core(task.Config):  class WorkerSchedulerFactory(object):      def create_local_scheduler(self):         return scheduler.CentralPlannerScheduler(prune_on_get_work=True)      def create_remote_scheduler(self, host, port):          return rpc.RemoteScheduler(host=host, port=port)
def _isna_new(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike(obj, old=False)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)      elif hasattr(obj, "__array__"):         return _isna_ndarraylike(np.asarray(obj), old=False)      else:          return False
class tqdm(object):          self.n = 0      def __len__(self):         return len(self.iterable) if self.iterable else self.total      def __iter__(self):
class DRTVIE(SubtitlesInfoExtractor):      }      def _real_extract(self, url):         video_id = self._match_id(url)          programcard = self._download_json(              'http://www.dr.dk/mu/programcard/expanded/%s' % video_id, video_id, 'Downloading video JSON')
class _Rolling_and_Expanding(_Rolling):              pairwise = True if pairwise is None else pairwise          other = self._shallow_copy(other)         window = self._get_window(other) if not self.is_freq_type else self.win_freq          def _get_corr(a, b):              a = a.rolling(
def _get_join_indexers(      mapped = (         _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)          for n in range(len(left_keys))      )      zipped = zip(*mapped)
def get_request_handler(      response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),      response_model_by_alias: bool = True,      response_model_exclude_unset: bool = False,     response_model_exclude_defaults: bool = False,     response_model_exclude_none: bool = False,      dependency_overrides_provider: Any = None,  ) -> Callable:      assert dependant.call is not None, "dependant.call must be a function"
class AmbiguousClass(luigi.Task):      pass  class TaskWithSameName(luigi.Task):      def run(self):
def _factorize_keys(lk, rk, sort=True):          rk, _ = rk._values_for_factorize()      elif (         is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)      ):         assert is_categorical(lk) and is_categorical(rk)         lk = cast(Categorical, lk)         rk = cast(Categorical, rk)          if lk.categories.equals(rk.categories):              rk = rk.codes
import inspect  import sys  PY3 = (sys.version_info[0] == 3) PY2 = not PY3  if hasattr(abc, 'ABC'):      ABC = abc.ABC
class YoutubeDL(object):                  elif type in [tokenize.NAME, tokenize.NUMBER]:                      current_selector = FormatSelector(SINGLE, string, [])                  elif type == tokenize.OP:                     if string == ')':                         if not inside_group:                              tokens.restore_last_token()                          break                     elif inside_merge and string in ['/', ',']:                          tokens.restore_last_token()                          break                     elif inside_choice and string == ',':                         tokens.restore_last_token()                         break                     elif string == ',':                          selectors.append(current_selector)                          current_selector = None                      elif string == '/':                          first_choice = current_selector                         second_choice = _parse_format_selection(tokens, inside_choice=True)                          current_selector = None                          selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))                      elif string == '[':
class ExecutionEngine(object):          def log_failure(msg):              def errback(failure):                 logger.error(                     msg,                     exc_info=failure_to_exc_info(failure),                     extra={'spider': spider}                 )              return errback          dfd.addBoth(lambda _: self.downloader.close())
def _get_functions(overridden):  def _get_aliases(overridden):      aliases = {}      proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)     alias_out = proc.stdout.read().decode('utf-8').strip()     if not alias_out:         return aliases     for alias in alias_out.split('\n'):         for separator in (' ', '='):             split_alias = alias.replace('alias ', '', 1).split(separator, 1)             if len(split_alias) == 2:                 name, value = split_alias                 break         else:             continue          if name not in overridden:              aliases[name] = value      return aliases
class FormRequest(Request):  def _get_form_url(form, url):      if url is None:         action = form.get('action')         if action is None:             return form.base_url         return urljoin(form.base_url, strip_html5_whitespace(action))      return urljoin(form.base_url, url)
def _preprocess_numpy_input(x, data_format, mode):          Preprocessed Numpy array.     x = x.astype(K.floatx())       if mode == 'tf':          x /= 127.5          x -= 1.
def _get_operations():      proc = subprocess.Popen(["dnf", '--help'],                              stdout=subprocess.PIPE,                              stderr=subprocess.PIPE)     lines = proc.stdout.read().decode("utf-8")      return _parse_operations(lines)
class IntBlock(NumericBlock):              )          return is_integer(element)     def should_store(self, value: ArrayLike) -> bool:          return is_integer_dtype(value) and value.dtype == self.dtype
class CategoricalIndex(Index, accessor.PandasDelegate):      take_nd = take     @Appender(_index_shared_docs["_maybe_cast_slice_bound"])     def _maybe_cast_slice_bound(self, label, side, kind):         if kind == "loc":             return label          return super()._maybe_cast_slice_bound(label, side, kind)       def map(self, mapper):          Map values using input correspondence (a dict, Series, or function).
class TupleParameter(ListParameter):          try:              return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))         except (ValueError, TypeError):             return tuple(literal_eval(x))  class NumericalParameter(Parameter):
class RandomNormal(Initializer):          self.seed = seed      def __call__(self, shape, dtype=None):         x = K.random_normal(shape, self.mean, self.stddev,                             dtype=dtype, seed=self.seed)         if self.seed is not None:             self.seed += 1         return x      def get_config(self):          return {
def interpolate_1d(                  inds = lib.maybe_convert_objects(inds)          else:              inds = xvalues          indexer = np.argsort(inds[valid])         result[invalid] = np.interp(             inds[invalid], inds[valid][indexer], yvalues[valid][indexer]         )          result[preserve_nans] = np.nan          return result
def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:                  lpar = Leaf(token.LPAR, "")                  rpar = Leaf(token.RPAR, "")                  index = child.remove() or 0                 prefix = child.prefix                 child.prefix = ""                 new_child = Node(syms.atom, [lpar, child, rpar])                 new_child.prefix = prefix                 node.insert_child(index, new_child)          check_lpar = isinstance(child, Leaf) and child.value in parens_after
class Errors(object):      E168 = ("Unknown field: {field}")      E169 = ("Can't find module: {module}")      E170 = ("Cannot apply transition {name}: invalid for the current state.")     E171 = ("Matcher.add received invalid on_match callback argument: expected "             "callable or None, but got: {arg_type}")  @add_codes
class GRUCell(Layer):          self.implementation = implementation          self.reset_after = reset_after          self.state_size = self.units         self.output_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
class Sequential(Model):                  finished and starting the next epoch. It should typically                  be equal to the number of samples of your dataset                  divided by the batch size.                 Optional for `Sequence`: if unspecified, will use                 the `len(generator)` as a number of steps.              epochs: Integer, total number of iterations on the data.                  Note that in conjunction with initial_epoch, the parameter                  epochs is to be understood as "final epoch". The model is
def read_user_choice(var_name, options):      ))      user_choice = click.prompt(         prompt, type=click.Choice(choices), default=default, show_choices=False      )      return choice_map[user_choice]
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)          return result      def __rsub__(self, other):         if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):              if not isinstance(other, DatetimeLikeArrayMixin):
def fit_generator(model,      if do_validation:          model._make_test_function()     use_sequence_api = is_sequence(generator)     if not use_sequence_api and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps_per_epoch is None:         if use_sequence_api:              steps_per_epoch = len(generator)          else:              raise ValueError('`steps_per_epoch=None` is only valid for a'
class _AtIndexer(_ScalarAccessIndexer):          if is_setter:              return list(key)         lkey = list(key)         for n, (ax, i) in enumerate(zip(self.obj.axes, key)):             lkey[n] = ax._convert_scalar_indexer(i, kind="loc")          return tuple(lkey)  @Appender(IndexingMixin.iat.__doc__)
class Parameter(object):              description.append('for all instances of class %s' % task_name)          elif self.description:              description.append(self.description)         if self.has_task_value(param_name=param_name, task_name=task_name):             value = self.task_value(param_name=param_name, task_name=task_name)             description.append(" [default: %s]" % (value,))          if self.is_list:              action = "append"
class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):              )          return super().fillna(value, **kwargs)      def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):          values = self.values
def assert_series_equal(              check_dtype=check_dtype,              obj=str(obj),          )     elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):          assert_extension_array_equal(left._values, right._values)      elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):
import os  import zipfile  from thefuck.utils import for_app from thefuck.shells import quote  def _is_bad_zip(file):
def split_line(          result: List[Line] = []          try:             for l in split_func(line, py36):                  if str(l).strip('\n') == line_str:                      raise CannotSplit("Split function returned an unchanged result")
def id_func(x): @pytest.fixture(     params=[         ("foo", None, None),         ("Egon", "Venkman", None),         ("NCC1701D", "NCC1701D", "NCC1701D"),     ] ) def names(request):     return request.param  @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])
from pandas.core.dtypes.common import (      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer,      is_integer_dtype,      is_list_like,
def format_sizeof(num, suffix=''):          Number with Order of Magnitude SI unit postfix.      for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:         if abs(num) < 999.95:             if abs(num) < 99.95:                 if abs(num) < 9.995:                      return '{0:1.2f}'.format(num) + unit + suffix                  return '{0:2.1f}'.format(num) + unit + suffix              return '{0:3.0f}'.format(num) + unit + suffix
def str_repeat(arr, repeats):      else:          def rep(x, r):             if x is libmissing.NA:                 return x              try:                  return bytes.__mul__(x, r)              except TypeError:
def _get_renderer(figure, print_method=None, *, draw_disabled=False):          except Done as exc:              renderer, = figure._cachedRenderer, = exc.args      return renderer
def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:      for f in flat_dependant.body_params:          BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)      required = any(True for f in flat_dependant.body_params if f.required)      BodySchema_kwargs: Dict[str, Any] = dict(default=None)      if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):          BodySchema: Type[params.Body] = params.File      elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):
def url_has_any_extension(url, extensions):      return posixpath.splitext(parse_url(url).path)[1].lower() in extensions def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):     return (         to_native_str(parts.scheme),         to_native_str(parts.netloc.encode('idna')),           quote(to_bytes(parts.path, path_encoding), _safe_chars),         quote(to_bytes(parts.params, path_encoding), _safe_chars),            quote(to_bytes(parts.query, encoding), _safe_chars),         quote(to_bytes(parts.fragment, encoding), _safe_chars)     )    def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,                       encoding=None):          try:         scheme, netloc, path, params, query, fragment = _safe_ParseResult(             parse_url(url), encoding=encoding)     except UnicodeError as e:         if encoding != 'utf8':             scheme, netloc, path, params, query, fragment = _safe_ParseResult(                 parse_url(url), encoding='utf8')         else:             raise        if not six.PY2:                                keyvals = parse_qsl_to_bytes(query, keep_blank_values)     else:         keyvals = parse_qsl(query, keep_blank_values)      keyvals.sort()      query = urlencode(keyvals)       uqp = _unquotepath(path)     path = quote(uqp, _safe_chars) or '/'      fragment = '' if not keep_fragments else fragment        return urlunparse((scheme, netloc.lower(), path, params, query, fragment))  def _unquotepath(path):      for reserved in ('2f', '2F', '3f', '3F'):          path = path.replace('%' + reserved, '%25' + reserved.upper())      if six.PY3:              return unquote_to_bytes(path)     else:          return unquote(path)  def parse_url(url, encoding=None):
class Model(Container):                      when using multiprocessing.              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Optional for `Sequence`: if unspecified, will use                 the `len(generator)` as a number of steps.              max_queue_size: Maximum size for the generator queue.              workers: Maximum number of processes to spin up                  when using process based threading
class PamdService(object):              if current_line.matches(rule_type, rule_control, rule_path):                  if current_line.prev is not None:                      current_line.prev.next = current_line.next                     if current_line.next is not None:                         current_line.next.prev = current_line.prev                  else:                      self._head = current_line.next                      current_line.next.prev = None
class Rhsm(RegistrationBase):          for pool_id, quantity in sorted(pool_ids.items()):              if pool_id in available_pool_ids:                 args = [SUBMAN_CMD, 'attach', '--pool', pool_id]                 if quantity is not None:                     args.extend(['--quantity', to_native(quantity)])                  rc, stderr, stdout = self.module.run_command(args, check_rc=True)              else:                  self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
from scrapy.exceptions import DontCloseSpider  from scrapy.http import Response, Request  from scrapy.utils.misc import load_object  from scrapy.utils.reactor import CallLaterOnce from scrapy.utils.log import logformatter_adapter, failure_to_exc_info  logger = logging.getLogger(__name__)
class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):          if not lib.is_scalar(value):              value = np.asarray(value)         value = np.asarray(value, dtype=self._ndarray.dtype)         self._ndarray[key] = value      def __len__(self) -> int:          return len(self._ndarray)
class Index(IndexOpsMixin, PandasObject):      _infer_as_myclass = False      _engine_type = libindex.ObjectEngine       _supports_partial_string_indexing = False      _accessors = {"str"}
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def fit_generator(model,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if use_sequence_api:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
from thefuck.specific.git import git_support  @git_support  def match(command):     splited_script = command.script.split()     if len(splited_script) > 1:         return (splited_script[1] == 'stash'                 and 'usage:' in command.stderr)     else:         return False  stash_commands = (
from ansible.module_utils.urls import open_url  from ansible.utils.display import Display  from ansible.utils.hashing import secure_hash_s try:     from urllib.parse import urlparse except ImportError:      from urlparse import urlparse   display = Display()
class RandomUniform(Initializer):          self.seed = seed      def __call__(self, shape, dtype=None):         x = K.random_uniform(shape, self.minval, self.maxval,                              dtype=dtype, seed=self.seed)         if self.seed is not None:             self.seed += 1         return x      def get_config(self):          return {
class TestBackend(object):          assert_allclose(y1, y2, atol=1e-05)      def test_random_normal(self):           for mean, std in [(0., 1.), (-10., 5.)]:             rand = K.eval(K.random_normal((200, 200),                                           mean=mean,                                           stddev=std))             assert rand.shape == (200, 200)              assert np.abs(np.mean(rand) - mean) < std * 0.015              assert np.abs(np.std(rand) - std) < std * 0.015      def test_random_uniform(self):          min_val = -1.          max_val = 1.         rand = K.eval(K.random_uniform((200, 200), min_val, max_val))         assert rand.shape == (200, 200)          assert np.abs(np.mean(rand)) < 0.015          assert max_val - 0.015 < np.max(rand) <= max_val          assert min_val + 0.015 > np.min(rand) >= min_val      def test_random_binomial(self):          p = 0.5         rand = K.eval(K.random_binomial((200, 200), p))         assert rand.shape == (200, 200)          assert np.abs(np.mean(rand) - p) < 0.015          assert np.max(rand) == 1          assert np.min(rand) == 0      def test_truncated_normal(self):          mean = 0.          std = 1.          min_val = -2.          max_val = 2.         rand = K.eval(K.truncated_normal((200, 200),                                          mean=mean,                                          stddev=std))         assert rand.shape == (200, 200)          assert np.abs(np.mean(rand) - mean) < 0.015          assert np.max(rand) <= max_val          assert np.min(rand) >= min_val
def _preprocess_conv2d_input(x, data_format):          x = tf.cast(x, 'float32')      tf_data_format = 'NHWC'      if data_format == 'channels_first':         if not _has_nchw_support() or force_transpose: x = tf.transpose(x, (0, 2, 3, 1))          else:              tf_data_format = 'NCHW'
class ExtensionBlock(Block):                  raise IndexError(f"{self} only contains one item")              return self.values     def should_store(self, value: ArrayLike) -> bool:          return isinstance(value, self._holder)     def set(self, locs, values):          assert locs.tolist() == [0]         self.values[:] = values      def putmask(          self, mask, new, align=True, inplace=False, axis=0, transpose=False,
class SitemapSpider(Spider):      def _parse_sitemap(self, response):          if response.url.endswith('/robots.txt'):             for url in sitemap_urls_from_robots(response.text):                  yield Request(url, callback=self._parse_sitemap)          else:              body = self._get_sitemap_body(response)
def rnn(step_function, inputs, initial_states,              for o, p in zip(new_states, place_holders):                  n_s.append(o.replace_placeholders({p: o.output}))              if len(n_s) > 0:                 new_output = n_s[-1]              return new_output, n_s          final_output, final_states = _recurrence(rnn_inputs, states, mask)
def dfxp2srt(dfxp_data):          for ns in v:              dfxp_data = dfxp_data.replace(ns, k)     dfxp = compat_etree_fromstring(dfxp_data)      out = []      paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')
def get_new_command(command):      branch_name = re.findall(          r"fatal: A branch named '([^']*)' already exists.", command.stderr)[0]      new_command_templates = [['git branch -d {0}', 'git branch {0}'],                              ['git branch -d {0}', 'git checkout -b {0}'],                               ['git branch -D {0}', 'git branch {0}'],                              ['git branch -D {0}', 'git checkout -b {0}'],                               ['git checkout {0}']]      for new_command_template in new_command_templates:          yield shell.and_(*new_command_template).format(branch_name)
from typing import (      Sequence,      Set,      Tuple,      TypeVar,      Union,      cast,
class BlockManager(PandasObject):          if len(self.blocks) != len(other.blocks):              return False            def canonicalize(block):             return (block.mgr_locs.as_array.tolist(), block.dtype.name)          self_blocks = sorted(self.blocks, key=canonicalize)          other_blocks = sorted(other.blocks, key=canonicalize)
from pandas._libs.tslibs import (      timezones,      tzconversion,  ) import pandas._libs.tslibs.frequencies as libfrequencies  from pandas.errors import PerformanceWarning  from pandas.core.dtypes.common import (
def filter_spans(spans):      spans (iterable): The spans to filter.      RETURNS (list): The filtered spans.     get_sort_key = lambda span: (span.end - span.start, -span.start)      sorted_spans = sorted(spans, key=get_sort_key, reverse=True)      result = []      seen_tokens = set()
class GroupBy(_GroupBy[FrameOrSeries]):                  )              inference = None             if is_integer_dtype(vals.dtype):                 if is_extension_array_dtype(vals.dtype):                     vals = vals.to_numpy(dtype=float, na_value=np.nan)                  inference = np.int64             elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):                 vals = vals.to_numpy(dtype=float, na_value=np.nan)             elif is_datetime64_dtype(vals.dtype):                  inference = "datetime64[ns]"                  vals = np.asarray(vals).astype(np.float)
class Session(BaseConfigDict):          for name, value in request_headers.items():              if value is None:                 continue               value = value.decode('utf8')              if name == 'User-Agent' and value.startswith('HTTPie/'):                  continue
def js_to_json(code):          "(?:[^"\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^"\\]*"|          '(?:[^'\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^'\\]*'|         /\*.*?\*/|//[^\n]*|,(?=\s*[\]}])|          [a-zA-Z_][.a-zA-Z_0-9]*|          \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|          [0-9]+(?=\s*:)
if len(result) and isinstance(result[0], dtype.type):                     cls = dtype.construct_array_type()                     result = try_cast_to_ea(cls, result, dtype=dtype)               elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:                  result = maybe_downcast_to_dtype(result, dtype)
from pandas._libs.lib import no_default  from pandas._libs.tslibs import frequencies as libfrequencies, resolution  from pandas._libs.tslibs.parsing import parse_time_string  from pandas._libs.tslibs.period import Period from pandas._typing import DtypeObj, Label  from pandas.util._decorators import Appender, cache_readonly  from pandas.core.dtypes.common import (
class DatetimeLikeArrayMixin(              return None          return self.freq.freqstr     def _with_freq(self, freq):         index = self.copy(deep=False)         if freq is None:              index._freq = None         elif len(self) == 0 and isinstance(freq, DateOffset):               index._freq = freq         else:             assert freq == "infer", freq             freq = to_offset(self.inferred_freq)             index._freq = freq     def _shallow_copy(self, values=None, name: Label = lib.no_default):         name = self.name if name is lib.no_default else name         cache = self._cache.copy() if values is None else {}         if values is None:             values = self._data         if isinstance(values, np.ndarray):              values = type(self._data)(values, dtype=self.dtype)         result = type(self)._simple_new(values, name=name)         result._cache = cache         return result       @Appender(Index.difference.__doc__)     def difference(self, other, sort=None):         new_idx = super().difference(other, sort=sort)._with_freq(None)         return new_idx     def intersection(self, other, sort=False):         Specialized intersection for DatetimeIndex/TimedeltaIndex.         other : Same type as self or array-like         sort : False or None, default False             Sort the resulting index if possible.             .. versionadded:: 0.24.0             .. versionchanged:: 0.24.1                Changed the default to ``False`` to match the behaviour                from before 0.24.0.         self._validate_sort_keyword(sort)         self._assert_can_do_setop(other)         if self.equals(other):             return self._get_reconciled_name_object(other)         elif (             other.freq is None             or self.freq is None             or other.freq != self.freq             or not other.freq.is_anchored()             or (not self.is_monotonic or not other.is_monotonic)         ):             result = Index.intersection(self, other, sort=sort)             result = result._with_freq("infer")              return result          if self[0] <= other[0]:             left, right = self, other         else:             left, right = other, self           end = min(left[-1], right[-1])         start = right[0]         if end < start:             return type(self)(data=[], dtype=self.dtype, freq=self.freq)         else:             lslice = slice(*left.slice_locs(start, end))             left_chunk = left._values[lslice]             return self._shallow_copy(left_chunk)     def _can_fast_union(self, other) -> bool:         if not isinstance(other, type(self)):             return False         freq = self.freq         if freq is None or freq != other.freq:             return False         if not self.is_monotonic or not other.is_monotonic:             return False         if len(self) == 0 or len(other) == 0:             return True          if self[0] <= other[0]:             left, right = self, other         else:             left, right = other, self         right_start = right[0]         left_end = left[-1]          try:             return (right_start == left_end + freq) or right_start in left         except ValueError:               return False      def _fast_union(self, other, sort=None):         if len(other) == 0:             return self.view(type(self))          if len(self) == 0:             return other.view(type(self))           if self[0] <= other[0]:             left, right = self, other         elif sort is False:               left, right = self, other             left_start = left[0]             loc = right.searchsorted(left_start, side="left")             right_chunk = right._values[:loc]             dates = concat_compat((left._values, right_chunk))              result = self._shallow_copy(dates)._with_freq("infer")             return result          else:             left, right = other, self          left_end = left[-1]         right_end = right[-1]           if left_end < right_end:             loc = right.searchsorted(left_end, side="right")             right_chunk = right._values[loc:]             dates = concat_compat([left._values, right_chunk])              result = self._shallow_copy(dates)._with_freq("infer")             return result         else:             return left     def _union(self, other, sort):         if not len(other) or self.equals(other) or not len(self):             return super()._union(other, sort=sort)          assert isinstance(other, type(self))         this, other = self._maybe_utc_convert(other)          if this._can_fast_union(other):             result = this._fast_union(other, sort=sort)             if result.freq is None:                 result = result._with_freq("infer")             return result          else:             i8self = Int64Index._simple_new(self.asi8, name=self.name)             i8other = Int64Index._simple_new(other.asi8, name=other.name)             i8result = i8self._union(i8other, sort=sort)             result = type(self)(i8result, dtype=self.dtype, freq="infer")             return result         See Index.join     def _maybe_utc_convert(self, other):         this = self         if not hasattr(self, "tz"):             return this, other         return a boolean whether I can attempt conversion to a         DatetimeIndex/TimedeltaIndex         Make new Index inserting new item at location          Parameters          ----------         loc : int         item : object             if not either a Python datetime or a numpy integer-like, returned             Index dtype will be object rather than datetime.          Returns          -------         if isinstance(item, str):               return self.astype(object).insert(loc, item)          item = self._data._validate_insert_value(item)          freq = None          if self.freq is not None:             if self.size:                 if item is NaT:                     pass                 elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:                     freq = self.freq                 elif (loc == len(self)) and item - self.freq == self[-1]:                     freq = self.freq             else:                  if self.freq.is_on_offset(item):                     freq = self.freq         item = self._data._unbox_scalar(item)         new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])         arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)         return type(self)._simple_new(arr, name=self.name)
class Driver(object):      def parse_string(self, text, debug=False):         tokens = tokenize.generate_tokens(             io.StringIO(text).readline,             config=self.tokenizer_config,         )          return self.parse_tokens(tokens, debug)      def _partially_consume_prefix(self, prefix, column):
from pandas.errors import AbstractMethodError  from pandas.util._decorators import Appender  from pandas.core.dtypes.common import (     is_hashable,      is_integer,      is_iterator,      is_list_like,
def generate_comments(leaf: LN) -> Iterator[Leaf]:      for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):          yield Leaf(pc.type, pc.value, prefix="\n" * pc.newlines)  @dataclass
class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):      @doc(Index.__contains__)      def __contains__(self, key: Any) -> bool:         if is_valid_nat_for_dtype(key, self.categories.dtype):              return self.hasnans          return contains(self, key, container=self._engine)      @doc(Index.astype)
from pandas.core.dtypes.common import (  from pandas.core.dtypes.generic import ABCSeries  from pandas.core.accessor import PandasDelegate, delegate_names  from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray  from pandas.core.base import NoNewAttributesMixin, PandasObject  from pandas.core.indexes.datetimes import DatetimeIndex
def get_handle(      try:          from s3fs import S3File         need_text_wrapping = (BufferedIOBase, RawIOBase, S3File)      except ImportError:         need_text_wrapping = (BufferedIOBase, RawIOBase)      handles: List[IO] = list()      f = path_or_buf
def func_no_args():          print(i)          continue     exec("new-style exec", {}, {})      return None async def coroutine(arg, exec=False):      "Single-line docstring. Multiline is harder to reformat."      async with some_connection() as conn:          await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
class LineGenerator(Visitor[Line]):          If any lines were generated, set up a new current_line.          if isinstance(node, Leaf):              any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()             for comment in generate_comments(node):                 if any_open_brackets:                      self.current_line.append(comment)                 elif comment.type == token.COMMENT:                      self.current_line.append(comment)                     yield from self.line()                 else:                      yield from self.line()                      self.current_line.append(comment)                     yield from self.line()              normalize_prefix(node, inside_brackets=any_open_brackets)             if self.normalize_strings and node.type == token.STRING:                 normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)                 normalize_string_quotes(node)             if node.type not in WHITESPACE:                 self.current_line.append(node)          yield from super().visit_default(node)      def visit_INDENT(self, node: Node) -> Iterator[Line]:
class Scraper(object):          dfd.addErrback(              lambda f: logger.error('Scraper bug processing %(request)s',                                     {'request': request},                                    exc_info=failure_to_exc_info(f),                                    extra={'spider': spider}))          self._scrape_next(spider, slot)          return dfd
def dfxp2srt(dfxp_data):          raise ValueError('Invalid dfxp/TTML subtitle')      for para, index in zip(paras, itertools.count(1)):         begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))          end_time = parse_dfxp_time_expr(para.attrib.get('end'))         dur = parse_dfxp_time_expr(para.attrib.get('dur'))         if begin_time is None:             continue          if not end_time:             if not dur:                 continue             end_time = begin_time + dur          out.append('%d\n%s --> %s\n%s\n\n' % (              index,              srt_subtitles_timecode(begin_time),
class TestGetItem:      def test_dti_business_getitem(self):          rng = pd.bdate_range(START, END)          smaller = rng[:5]         exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq="B")          tm.assert_index_equal(smaller, exp)         assert smaller.freq == exp.freq          assert smaller.freq == rng.freq
def top_k_categorical_accuracy(y_true, y_pred, k=5):  def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):      return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),                    axis=-1)
python_symbols = Symbols(python_grammar)  python_grammar_no_print_statement = python_grammar.copy()  del python_grammar_no_print_statement.keywords["print"] python_grammar_no_exec_statement = python_grammar.copy() del python_grammar_no_exec_statement.keywords["exec"]  python_grammar_no_print_statement_no_exec_statement = python_grammar.copy() del python_grammar_no_print_statement_no_exec_statement.keywords["print"] del python_grammar_no_print_statement_no_exec_statement.keywords["exec"]   pattern_grammar = driver.load_packaged_grammar("blib2to3", _PATTERN_GRAMMAR_FILE)  pattern_symbols = Symbols(pattern_grammar)
class Axes(_AxesBase):                      cbook.normalize_kwargs(                          boxprops, mpatches.PathPatch._alias_map))          else:             final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,                                                         use_marker=False)          final_whiskerprops = line_props_with_rcdefaults(             'whiskerprops', whiskerprops, use_marker=False)          final_capprops = line_props_with_rcdefaults(             'capprops', capprops, use_marker=False)          final_flierprops = line_props_with_rcdefaults(              'flierprops', flierprops)          final_medianprops = line_props_with_rcdefaults(             'medianprops', medianprops, zdelta, use_marker=False)          final_meanprops = line_props_with_rcdefaults(              'meanprops', meanprops, zdelta)          removed_prop = 'marker' if meanline else 'linestyle'
def urljoin(base, path):          path = path.decode('utf-8')      if not isinstance(path, compat_str) or not path:          return None     if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):          return path      if isinstance(base, bytes):          base = base.decode('utf-8')
class SeriesGroupBy(GroupBy):          val = self.obj._internal_get_values()         codes, _ = algorithms.factorize(val, sort=False)         sorter = np.lexsort((codes, ids))         codes = codes[sorter]         ids = ids[sorter]          idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]         inc = np.r_[1, codes[1:] != codes[:-1]]         mask = codes == -1          if dropna:              inc[idx] = 1              inc[mask] = 0
from ansible.plugins.lookup import LookupBase from ansible.utils import py3compat  class LookupModule(LookupBase):
def _clone_functional_model(model, input_tensors=None):                              kwargs['mask'] = computed_masks                      output_tensors = to_list(                          layer(computed_tensors, **kwargs))                     if layer.supports_masking:                         output_masks = to_list(                             layer.compute_mask(computed_tensors,                                                computed_masks))                     else:                         output_masks = [None] * len(output_tensors)                  for x, y, mask in zip(reference_output_tensors,                                        output_tensors,
class Errors(object): "details: https://spacy.io/api/lemmatizer      E174 = ("Architecture '{name}' not found in registry. Available "              "names: {names}")     E175 = ("Can't remove rule for unknown match pattern ID: {key}")  @add_codes
class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {                 self.obj._get_axis_name(self.axis): index,                 "copy": False,                 "fill_value": fill_value,             }              return output.reindex(**d)
class HiveCommandClient(HiveClient):          if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table.lower() in stdout          else:
class BinGrouper(BaseGrouper):              ngroups,          )     @cache_readonly     def recons_codes(self):          return [np.r_[0, np.flatnonzero(self.bins[1:] != self.bins[:-1]) + 1]]       @cache_readonly      def result_index(self):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())           dtypes = list(self.dtypes)         if self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0]):              dtype = dtypes[0]             arr_type = dtype.construct_array_type()             values = self.values              new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]             result = self._constructor(                 dict(zip(self.index, new_values)), index=self.columns             )          else:             new_values = self.values.T             if copy:                 new_values = new_values.copy()             result = self._constructor(                 new_values, index=self.columns, columns=self.index             )          return result.__finalize__(self)      T = property(transpose)
from difflib import get_close_matches  from functools import wraps  import shelve  from decorator import decorator from contextlib import closing  import tempfile  import os
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=True, file_okay=True, dir_okay=False, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml,
class Sanic:                  if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].appendleft(middleware)      def middleware(self, middleware_or_request):
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 if is_sequence:                     output_generator = iter(generator)                 else:                     output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     if dilation_rate == (1, 1):         x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                    padding=padding,                                    data_format=tf_data_format)     else:         assert dilation_rate[0] == dilation_rate[1]         x = tf.nn.atrous_conv2d_transpose(             x, kernel, output_shape, dilation_rate[0], padding)       if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x
class XportReader(abc.Iterator):          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:               self.filepath_or_buffer = filepath_or_buffer          self._read_header()
class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         if self.limit:             while len(self) >= self.limit:                 self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value)
dependency tree to find the noun phrase they are referring to – for example:  $9.4 million --> Net income.  Compatible with: spaCy v2.0.0+ Last tested with: v2.2.1  from __future__ import unicode_literals, print_function
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop)              return task     def start_serving(self):         if self.server:             try:                 return self.server.start_serving()             except AttributeError:                 raise NotImplementedError(                     "server.start_serving not available in this version "                     "of asyncio or uvloop."                 )      def serve_forever(self):         if self.server:             try:                 return self.server.serve_forever()             except AttributeError:                 raise NotImplementedError(                     "server.serve_forever not available in this version "                     "of asyncio or uvloop."                 )       def __await__(self):          task = asyncio.ensure_future(self.serve_coro)
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None      kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)       if output_padding is None:          if padding == 'valid':
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(val_data)                     validation_steps = validation_steps or len(val_data)                  else:                      val_enqueuer_gen = val_data              else:
class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path) or is_gcs_url(path):               path, _, _, _ = get_filepath_or_buffer(path, mode="wb")               kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]             if self.image_data_generator.preprocessing_function:                 x = self.image_data_generator.preprocessing_function(x)              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self]             to_concat.extend(to_append)          else:              to_concat = [self, to_append]          return concat(
class HTTPRequest(HTTPMessage):          )          headers = dict(self._orig.headers)         if 'Host' not in self._orig.headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value)
class TestProcessProtocol(protocol.ProcessProtocol):      def __init__(self):          self.deferred = defer.Deferred()         self.out = b''         self.err = b''          self.exitcode = None      def outReceived(self, data):
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))          inf = np.isinf(values)         if inf.any():             values = np.where(inf, np.nan, values)          return values
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__")   class BaseUnaryOpsTests(BaseOpsUtil):     def test_invert(self, data):         s = pd.Series(data, name="name")         result = ~s         expected = pd.Series(~data, name="name")         self.assert_series_equal(result, expected)
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)          for task in tasks:             in_workers = (assistant and task.workers) or worker in task.workers              if task.status == 'RUNNING' and in_workers:
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     exclude_defaults=exclude_defaults,                     exclude_none=exclude_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',', ' ')     pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0      timezone, date_str = extract_timezone(date_str)
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.type, failure.value, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=encoding, newline=newline) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)"
def should_series_dispatch(left, right, op):          return True     if (is_datetime64_dtype(ldtype) and is_object_dtype(rdtype)) or (         is_datetime64_dtype(rdtype) and is_object_dtype(ldtype)     ):           return True      return False
def run_script(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     exit_status = proc.wait()     if exit_status != EXIT_SUCCESS:         raise FailedHookException(             "Hook script failed (exit status: %d)" % exit_status)  def run_script_with_context(script_path, cwd, context):
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             exc_info=failure_to_exc_info(f),                                             extra={'spider': spider}))          return d      def spider_is_idle(self, spider):
from pandas.core.frame import DataFrame  from pandas.core.groupby import ops  from pandas.core.groupby.categorical import recode_for_groupby, recode_from_groupby  from pandas.core.indexes.api import CategoricalIndex, Index, MultiIndex from pandas.core.indexes.base import InvalidIndexError  from pandas.core.series import Series  from pandas.io.formats.printing import pprint_thing
class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                              full_info = formats_dict.get(representation_id, {}).copy()                         full_info.update(f)                         formats.append(full_info)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                     response_model_exclude_defaults=route.response_model_exclude_defaults,                     response_model_exclude_none=route.response_model_exclude_none,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,
def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",         "skew",          "kurt",          "quantile",      }
class BarPlot(MPLPlot):      def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))          if self.xticks is not None:             ax.set_xticks(np.array(self.xticks))         else:             ax.set_xticks(self.tick_pos)             ax.set_xticklabels(ticklabels)           if name is not None and self.use_index:              ax.set_xlabel(name)
from .common import InfoExtractor  from ..utils import (      compat_urllib_parse,      ExtractorError,     fix_xml_ampersands,  )  def _media_xml_tag(tag):
def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = None      if len(cmds) >= 3:          machine = cmds[2]      startAllInstances = shells.and_("vagrant up", command.script)     if machine is None:          return startAllInstances     else:         return [ shells.and_("vagrant up " +  machine, command.script), startAllInstances]
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],                                      self.tasks)          else:             return six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())      def is_trivial_worker(self, state):
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged       if table.index.nlevels > 1 and index:
def create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config__          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = create_cloned_field(f)          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head not in ["0", "_"] else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag
class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1]          assert data[0] == data[1]      def test_transpose(self, data):         df = pd.DataFrame({"A": data[:4], "B": data[:4]}, index=["a", "b", "c", "d"])         result = df.T         expected = pd.DataFrame(             {                 "a": type(data)._from_sequence([data[0]] * 2, dtype=data.dtype),                 "b": type(data)._from_sequence([data[1]] * 2, dtype=data.dtype),                 "c": type(data)._from_sequence([data[2]] * 2, dtype=data.dtype),                 "d": type(data)._from_sequence([data[3]] * 2, dtype=data.dtype),             },             index=["A", "B"],         )         self.assert_frame_equal(result, expected)         self.assert_frame_equal(np.transpose(np.transpose(df)), df)         self.assert_frame_equal(np.transpose(np.transpose(df[["A"]])), df[["A"]])
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '!!not-set!!')))          print(self.script)
def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             if not code.startswith('__'):                 msg = getattr(err_cls, code)                 return "[{code}] {msg}".format(code=code, msg=msg)             else:                 return super().__getattribute__(code)      return ErrorsWithCodes()
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(values, result)              if post_processing:                  result = post_processing(result, inferences)
def _isna_ndarraylike_old(obj):      return result def _isna_string_dtype(values: np.ndarray, dtype: np.dtype, old: bool) -> np.ndarray:      shape = values.shape      if is_string_like_dtype(dtype):         result = np.zeros(values.shape, dtype=bool)     else:         result = np.empty(shape, dtype=bool)         if old:             vec = libmissing.isnaobj_old(values.ravel())         else:             vec = libmissing.isnaobj(values.ravel())          result[...] = vec.reshape(shape)      return result    def notna(obj):      Detect non-missing values for an array-like object.
class _LocIndexer(_LocationIndexer):          if isinstance(labels, MultiIndex):             if (                 isinstance(key, str)                 and labels.levels[0]._supports_partial_string_indexing             ):                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))
class BracketTracker:         if (             self._lambda_argument_depths             and self._lambda_argument_depths[-1] == self.depth             and leaf.type == token.COLON         ):              self.depth -= 1             self._lambda_argument_depths.pop()              return True          return False
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):          if isinstance(maybe_slice, slice):              return self[maybe_slice]         return ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )      _can_hold_na = True      _na_value = NaT
def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return False  def _isna_old(obj):
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")         dti._set_freq("infer")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):             v = re.sub(r"\\'", "'", v[1:-1])         elif v.startswith("'"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\',
class Model(BaseModel):  class ModelSubclass(Model):      y: int     z: int = 0     w: int = None   class ModelDefaults(BaseModel):     w: Optional[str] = None     x: Optional[str] = None     y: str = "y"     z: str = "z"  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1, z=0)   @app.get(     "/exclude_unset", response_model=ModelDefaults, response_model_exclude_unset=True ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y")   @app.get(     "/exclude_defaults",     response_model=ModelDefaults,     response_model_exclude_defaults=True, ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y")   @app.get(     "/exclude_none", response_model=ModelDefaults, response_model_exclude_none=True ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y")   @app.get(     "/exclude_unset_none",     response_model=ModelDefaults,     response_model_exclude_unset=True,     response_model_exclude_none=True, ) def get() -> ModelDefaults:     return ModelDefaults(x=None, y="y")  client = TestClient(app)
default 'raise'              )          new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)          freq = None         if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates[0])):               freq = self.freq         elif tz is None and self.tz is None:              freq = self.freq         return self._simple_new(new_dates, dtype=dtype, freq=freq)
class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.     copy : bool, default False         If True, return a copy of the combined index.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)  def _get_distinct_objs(objs: List[Index]) -> List[Index]:
class QuarterOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )         return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)  class BQuarterEnd(QuarterOffset):
class ImageDataGenerator(object):              The inputs, normalized.          if self.rescale:              x *= self.rescale          if self.samplewise_center:
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 if opname == "__ne__":                     ret[(self._codes == -1) & (other_codes == -1)] = True                 else:                     ret[mask] = False              return ret          if is_scalar(other):
def match(command, settings):      return _search(command.stderr) or _search(command.stdout) @wrap_settings({'fixlinecmd': '{editor} {file} +{line}',                 'fixcolcmd': None})  def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)      if settings.fixcolcmd and 'col' in m.groupdict():         editor_call = settings.fixcolcmd.format(editor=os.environ['EDITOR'],                                                 file=m.group('file'),                                                 line=m.group('line'),                                                 col=m.group('col'))     else:         editor_call = settings.fixlinecmd.format(editor=os.environ['EDITOR'],                                                  file=m.group('file'),                                                  line=m.group('line'))       return shells.and_(editor_call, command.script)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, "same" or "valid".          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format             in inputs/kernels/outputs.         dilation_rate: tuple of 2 integers.          ValueError: if using an even kernel size with padding 'same'.
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)      def start_requests(self):         cls = self.__class__         if cls.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; it "                 "won't be called in future Scrapy releases. Please "                 "override Spider.start_requests method instead (see %s.%s)." % (                     cls.__module__, cls.__name__                 ),              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)     try:         executor = ProcessPoolExecutor(max_workers=worker_count)     except OSError:            executor = None       try:          loop.run_until_complete(              schedule_formatting(
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError, OverflowError):                  pass              asdt, reso = parse_time_string(key, self.freq)
def base_url(url):  def urljoin(base, path):     if isinstance(path, bytes):         path = path.decode('utf-8')      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if isinstance(base, bytes):         base = base.decode('utf-8')     if not isinstance(base, compat_str) or not re.match(             r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path)
class _LocIndexer(_LocationIndexer):              return self._getbool_axis(key, axis=axis)          elif is_list_like_indexer(key):              if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):
def print_tensor(x, message=''):          The same tensor `x`, unchanged.     op = tf.print(message, x, output_stream=sys.stdout)     with tf.control_dependencies([op]):         return tf.identity(x)
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)         exact_match = (             len(arr_value.shape)             and arr_value.shape[0] == values.shape[0]             and arr_value.size == values.size         )          if is_empty_indexer(indexer, arr_value):              pass
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = safe_url_string(response.headers['location'])          redirected_url = urljoin(request.url, location)
except ImportError:  def _prepare_response_content(     res: Any,     *,     by_alias: bool = True,     exclude_unset: bool,     exclude_defaults: bool = False,     exclude_none: bool = False,  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(                 by_alias=by_alias,                 exclude_unset=exclude_unset,                 exclude_defaults=exclude_defaults,                 exclude_none=exclude_none,             )          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset, )      elif isinstance(res, list):          return [             _prepare_response_content(                 item,                 exclude_unset=exclude_unset,                 exclude_defaults=exclude_defaults,                 exclude_none=exclude_none,             )             for item in res          ]      elif isinstance(res, dict):          return {             k: _prepare_response_content(                 v,                 exclude_unset=exclude_unset,                 exclude_defaults=exclude_defaults,                 exclude_none=exclude_none,             )              for k, v in res.items()          }      return res
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 mask = level == key                 if not mask.any():                     raise ValueError(f"Key {key} not in level {level}")                 i = np.nonzero(level == key)[0][0]                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True         if total is None and iterable is not None:             try:                 total = len(iterable)             except (TypeError, AttributeError):                 total = None           if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial             self.total = total              return          if kwargs:
import re  from .common import InfoExtractor  from ..utils import (      find_xpath_attr,     fix_xml_ampersands  )
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             if return_indexers:                 return multi_join_idx, lidx, ridx             else:                 return multi_join_idx          jl = list(overlap)[0]
else:  from twisted.internet import defer, reactor, ssl from .utils.misc import arg_to_iter   logger = logging.getLogger(__name__)
class CategoricalBlock(ExtensionBlock):      def _holder(self):          return Categorical     def should_store(self, arr: ArrayLike):         return isinstance(arr, self._holder) and is_dtype_equal(self.dtype, arr.dtype)       def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data          enqueuer = None          try:
def melt(          else:              value_vars = list(value_vars)             missing = Index(com.flatten(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, var_list=params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)
class YoutubeIE(YoutubeBaseInfoExtractor):          video_id = mobj.group(2)          return video_id     def _extract_chapters_from_json(self, webpage, video_id, duration):         if not webpage:             return         player = self._parse_json(             self._search_regex(                 r'RELATED_PLAYER_ARGS["\']\s*:\s*({.+})\s*,?\s*\n', webpage,                 'player args', default='{}'),             video_id, fatal=False)         if not player or not isinstance(player, dict):             return         watch_next_response = player.get('watch_next_response')         if not isinstance(watch_next_response, compat_str):             return         response = self._parse_json(watch_next_response, video_id, fatal=False)         if not response or not isinstance(response, dict):             return         chapters_list = try_get(             response,             lambda x: x['playerOverlays']                        ['playerOverlayRenderer']                        ['decoratedPlayerBarRenderer']                        ['decoratedPlayerBarRenderer']                        ['playerBar']                        ['chapteredPlayerBarRenderer']                        ['chapters'],             list)         if not chapters_list:             return          def chapter_time(chapter):             return float_or_none(                 try_get(                     chapter,                     lambda x: x['chapterRenderer']['timeRangeStartMillis'],                     int),                 scale=1000)         chapters = []         for next_num, chapter in enumerate(chapters_list, start=1):             start_time = chapter_time(chapter)             if start_time is None:                 continue             end_time = (chapter_time(chapters_list[next_num])                         if next_num < len(chapters_list) else duration)             if end_time is None:                 continue             title = try_get(                 chapter, lambda x: x['chapterRenderer']['title']['simpleText'],                 compat_str)             chapters.append({                 'start_time': start_time,                 'end_time': end_time,                 'title': title,             })         return chapters       @staticmethod     def _extract_chapters_from_description(description, duration):          if not description:              return None          chapter_lines = re.findall(
class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class         if classcell is not None:             new_attrs['__classcell__'] = classcell          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
class ListParameter(Parameter):         Ensure that struct is recursively converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index._values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name)     @classmethod     def _validate_dtype(cls, dtype: Dtype) -> None:         if dtype is None:             return         validation_metadata = {             "int64index": (is_signed_integer_dtype, "signed integer"),             "uint64index": (is_unsigned_integer_dtype, "unsigned integer"),             "float64index": (is_float_dtype, "float"),             "rangeindex": (is_signed_integer_dtype, "signed integer"),         }          validation_func, expected = validation_metadata[cls._typ]         if not validation_func(dtype):             msg = f"Incorrect `dtype` passed: expected {expected}, received {dtype}"             raise ValueError(msg)       @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]
def _urlencode(seq, enc):  def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)
class Request(object_ref):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ('://' not in self._url) and (not self._url.startswith('data:')):              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))
class EmptyLineTracker:                  return 0, 0             if is_decorator and self.previous_line and self.previous_line.is_comment:                  return 0, 0               newlines = 2              if current_line.depth:                  newlines -= 1
class DataFrame(NDFrame):          return new_data     def _combine_match_index(self, other: Series, func):          if ops.should_series_dispatch(self, other, func):
def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length]     ansi_present = bool(RE_ANSI.search(data)) while disp_len(data) > length:          data = data[:-1]     if ansi_present and bool(RE_ANSI.search(data)):          return data if data.endswith("\033[0m") else data + "\033[0m"      return data
except ImportError:  from gzip import GzipFile  import six import re
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if v < val else v - 1 for v in clocs]              return result
def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     exclude_none=exclude_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = to_text(manifest['version'], errors='surrogate_or_strict')              if not hasattr(LooseVersion(version), 'version'):                 display.warning("Collection at '%s' does not have a valid version set, falling back to '*'. Found "                                 "version: '%s'" % (to_text(b_path), version))                 version = '*'               dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version."
from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format import sys  import functools  import threading
class LinuxHardware(Hardware):              mtab_entries.append(fields)          return mtab_entries     @staticmethod     def _replace_octal_escapes_helper(match):          return chr(int(match.group()[1:], 8))      def _replace_octal_escapes(self, value):         return self.OCTAL_ESCAPE_RE.sub(self._replace_octal_escapes_helper, value)       def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")          if not len(self._codes):             return self.dtype.na_value           good = self._codes != -1          if not good.all():              if skipna:
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_MANAGER_CLASS',                             settings.get('SPIDER_LOADER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())
logger = logging.getLogger(__name__)  title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_tag_regex = re.compile(r"(?<=<text).*?(?=>)") text_regex = re.compile(r"(?<=<text>).*(?=</text)")  info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,             exclude_defaults=exclude_defaults,             exclude_none=exclude_none,          )      else:          return jsonable_encoder(response_content)
class WebSocketHandler(tornado.web.RequestHandler):          .. versionadded:: 3.1         assert self.ws_connection is not None         self.ws_connection.set_nodelay(value)      def on_connection_close(self) -> None:          if self.ws_connection:
from pandas.core.dtypes.common import (      ensure_object,      is_bool_dtype,      is_complex_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.generic import (      ABCDataFrame,      ABCExtensionArray,      ABCIndexClass,      ABCMultiIndex,      ABCSeries,  )  from pandas.core.dtypes.inference import is_list_like
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         if 'KERAS_HOME' in os.environ:             cache_dir = os.environ.get('KERAS_HOME')         else:             cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5'
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     if not (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     ):         return False     if field.sub_fields:         if not all(is_scalar_field(f) for f in field.sub_fields):             return False     return True  def is_scalar_sequence_field(field: Field) -> bool:
def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["ever_failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}     set_tasks["failed"] = set_tasks["ever_failed"] - set_tasks["completed"]      set_tasks["scheduling_error"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}      set_tasks["still_pending_ext"] = {task for (task, status, ext) in task_history                                       if status == 'PENDING' and task not in set_tasks["ever_failed"] and task not in set_tasks["completed"] and not ext}      set_tasks["still_pending_not_ext"] = {task for (task, status, ext) in task_history                                           if status == 'PENDING' and task not in set_tasks["ever_failed"] and task not in set_tasks["completed"] and ext}      set_tasks["run_by_other_worker"] = set()      set_tasks["upstream_failure"] = set()      set_tasks["upstream_missing_dependency"] = set()
and Index.__new__.  These should not depend on core.internals. from collections import abc  from typing import TYPE_CHECKING, Any, Optional, Sequence, Union, cast  import numpy as np
class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_task_value(param_name=param_name, task_name=task_name):                 return self.task_value(param_name=param_name, task_name=task_name)              elif self.is_bool:                  return False              elif self.is_list:
class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value: ArrayLike) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1          if not good.all():             if skipna and good.any():                  pointer = self._codes[good].min()              else:                  return np.nan
class RedirectMiddleware(BaseRedirectMiddleware):      def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) or                response.status in getattr(spider, 'handle_httpstatus_list', []) or                response.status in request.meta.get('handle_httpstatus_list', []) or                request.meta.get('handle_httpstatus_all', False)):              return response          if request.method == 'HEAD':
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         try:             command.script_parts.pop(upstream_option_index)         except IndexError:              pass      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
from pandas.core.dtypes.common import (      ensure_platform_int,      is_bool,      is_bool_dtype,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal,
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts["2007"]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)
from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.missing import notna from pandas.arrays import IntegerArray  from pandas.core import algorithms  from pandas.core.algorithms import unique
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     use_sequence_api = is_sequence(generator)     if not use_sequence_api and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps is None:         if use_sequence_api:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator'
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         if callable(index) or is_dict_like(index):             return super().rename(index=index, **kwargs)         else:              return self._set_name(index, inplace=kwargs.get("inplace"))      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()         del IOLoop._ioloop_for_asyncio[self.asyncio_loop]      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block         if convert:             return [self.convert(numeric=False, copy=True)]          return self
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!         .. versionchanged:: 1.0.0             Returns an NA value on empty arrays           Raises          ------          TypeError
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             exclude_none=True,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape)      if data_format == 'channels_first' and dilation_rate != (1, 1):         force_transpose = True     else:         force_transpose = False      x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],
def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]      if existing and not collection_info.force:         existing[0].add_requirement(parent, requirement)          collection_info = existing[0]      dep_map[to_text(collection_info)] = collection_info
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 if is_sequence:                     output_generator = iter(generator)                 else:                     output_generator = generator              callback_model.stop_training = False
class HTTP1Connection(httputil.HTTPConnection):              return connection_header != "close"          elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked"               or getattr(start_line, 'method', None) in ("HEAD", "GET")):                return connection_header == "keep-alive"          return False
class TimedeltaIndex(                      result._set_freq("infer")              return result     def _fast_union(self, other, sort=None):          if len(other) == 0:              return self.view(type(self))
def whitespace(leaf: Leaf) -> str:          ):              return NO         elif (             prevp.type == token.RIGHTSHIFT             and prevp.parent             and prevp.parent.type == syms.shift_expr             and prevp.prev_sibling             and prevp.prev_sibling.type == token.NAME             and prevp.prev_sibling.value == 'print'         ):              return NO       elif prev.type in OPENING_BRACKETS:          return NO
def check_required_arguments(argument_spec, module_parameters):              missing.append(k)      if missing:         msg = "missing required arguments: %s" % ", ".join(sorted(missing))          raise TypeError(to_native(msg))      return missing
single_quoted = (  tabsize = 8 @dataclass(frozen=True) class TokenizerConfig:     async_is_reserved_keyword: bool = False   class TokenError(Exception): pass  class StopTokenizing(Exception): pass
def assert_series_equal(          Compare datetime-like which is comparable ignoring dtype.      check_categorical : bool, default True          Whether to compare internal Categorical exactly.     check_category_order : bool, default True         Whether to compare category order of internal Categoricals          .. versionadded:: 1.0.2      obj : str, default 'Series'          Specify object name being compared, internally used to show appropriate          assertion message.
class LSTMCell(Layer):                  inputs_f = inputs                  inputs_c = inputs                  inputs_o = inputs             x_i = K.dot(inputs_i, self.kernel_i)             x_f = K.dot(inputs_f, self.kernel_f)             x_c = K.dot(inputs_c, self.kernel_c)             x_o = K.dot(inputs_o, self.kernel_o)             if self.use_bias:                 x_i = K.bias_add(x_i, self.bias_i)                 x_f = K.bias_add(x_f, self.bias_f)                 x_c = K.bias_add(x_c, self.bias_c)                 x_o = K.bias_add(x_o, self.bias_o)              if 0 < self.recurrent_dropout < 1.:                  h_tm1_i = h_tm1 * rec_dp_mask[0]
class Conv2DTranspose(Conv2D):          out_height = conv_utils.deconv_length(height,                                                stride_h, kernel_h,                                                self.padding,                                               out_pad_h,                                               self.dilation_rate[0])          out_width = conv_utils.deconv_length(width,                                               stride_w, kernel_w,                                               self.padding,                                              out_pad_w,                                              self.dilation_rate[1])          if self.data_format == 'channels_first':              output_shape = (batch_size, self.filters, out_height, out_width)          else:
class TestSeriesAnalytics:          assert s.is_monotonic is False          assert s.is_monotonic_decreasing is True      @pytest.mark.parametrize("func", [np.any, np.all])      @pytest.mark.parametrize("kwargs", [dict(keepdims=True), dict(out=object())])      @td.skip_if_np_lt("1.15")
class Task(object):          params_str = {}          params = dict(self.get_params())          for param_name, param_value in six.iteritems(self.param_kwargs):             params_str[param_name] = params[param_name].serialize(param_value)          return params_str
def unescapeHTML(s):      assert type(s) == compat_str      return re.sub(         r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)  def get_subprocess_encoding():
def in_top_k(predictions, targets, k):  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None, dilation_rate=(1, 1)):      data_format = normalize_data_format(data_format)      x = _preprocess_conv2d_input(x, data_format)
class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         if self.run_metadata:             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)         else:             fetched = self._callable_fn(*array_vals)          return fetched[:len(self.outputs)]      def _legacy_call(self, inputs):
class Fish(Generic):      def info(self):         proc = Popen(['fish', '--version'],                       stdout=PIPE, stderr=DEVNULL)         version = proc.stdout.read().decode('utf-8').split()[-1]          return u'Fish Shell {}'.format(version)      def put_to_history(self, command):
default: 'top'          from .tight_layout import (              get_renderer, get_subplotspec_list, get_tight_layout_figure)         from .cbook import _setattr_cm         from .backend_bases import RendererBase          subplotspec_list = get_subplotspec_list(self.axes)          if None in subplotspec_list:
class BaseGrouper:              if mask.any():                  result = result.astype("float64")                  result[mask] = np.nan         elif (             how == "add"             and is_integer_dtype(orig_values.dtype)             and is_extension_array_dtype(orig_values.dtype)         ):                  result = result.astype("int64")          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2
class Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key)                  return self.iloc[indexer]              else:                 return self.iloc[key]          if isinstance(key, (list, tuple)):
except ImportError:      from ordereddict import OrderedDict  from luigi import six import logging logger = logging.getLogger('luigi-interface')  class TaskClassException(Exception):
from keras.utils.data_utils import validate_file  from keras import backend as K  pytestmark = pytest.mark.skipif(     K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,      reason='Temporarily disabled until the use_multiprocessing problem is solved')  if sys.version_info < (3,):
class Scraper(object):                      spider=spider, exception=output.value)              else:                  logger.error('Error processing %(item)s', {'item': item},                              exc_info=failure_to_exc_info(output),                              extra={'spider': spider})          else:              logkws = self.logformatter.scraped(output, response, spider)              logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
import logging  from six.moves.urllib.parse import urljoin from w3lib.url import safe_url_string   from scrapy.http import HtmlResponse  from scrapy.utils.response import get_meta_refresh  from scrapy.exceptions import IgnoreRequest, NotConfigured  logger = logging.getLogger(__name__)
class GalaxyCLI(CLI):          obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]          inject_data = dict(             description='your {0} description'.format(galaxy_type),              ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          )          if galaxy_type == 'role':
class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl)                 sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])             outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))                  filename = expand_path(outtmpl).replace(sep, '') % template_dict
class MailSender(object):              msg = MIMEMultipart()          else:              msg = MIMENonMultipart(*mimetype.split('/', 1))          to = list(arg_to_iter(to))         cc = list(arg_to_iter(cc))           msg['From'] = self.mailfrom          msg['To'] = COMMASPACE.join(to)          msg['Date'] = formatdate(localtime=True)
def data(dtype):      return pd.array(make_data(), dtype=dtype) @pytest.fixture def data_for_twos(dtype):     return pd.array(np.ones(100), dtype=dtype) @pytest.fixture def data_missing(dtype):     return pd.array([np.nan, True], dtype=dtype) @pytest.fixture def data_for_sorting(dtype):     return pd.array([True, True, False], dtype=dtype) @pytest.fixture def data_missing_for_sorting(dtype):     return pd.array([True, np.nan, False], dtype=dtype) @pytest.fixture def na_cmp():      return lambda x, y: x is pd.NA and y is pd.NA @pytest.fixture def na_value():     return pd.NA @pytest.fixture def data_for_grouping(dtype):     b = True     a = False     na = np.nan     return pd.array([b, b, na, na, a, a, b], dtype=dtype) class TestDtype(base.BaseDtypeTests):     pass class TestInterface(base.BaseInterfaceTests):     pass   class TestConstructors(base.BaseConstructorsTests):     pass   class TestGetitem(base.BaseGetitemTests):     pass   class TestSetitem(base.BaseSetitemTests):     pass   class TestMissing(base.BaseMissingTests):     pass  class TestArithmeticOps(base.BaseArithmeticOpsTests):     def check_opname(self, s, op_name, other, exc=None):          super().check_opname(s, op_name, other, exc=None)     def _check_op(self, s, op, other, op_name, exc=NotImplementedError):         if exc is None:             if op_name in ("__sub__", "__rsub__"):                  if _np_version_under1p14:                     pytest.skip("__sub__ does not yet raise in numpy 1.13")                 with pytest.raises(TypeError):                     op(s, other)                 return             result = op(s, other)             expected = s.combine(other, op)             if op_name in (                 "__floordiv__",                 "__rfloordiv__",                 "__pow__",                 "__rpow__",                 "__mod__",                 "__rmod__",             ):                  expected = expected.astype("Int8")             elif op_name in ("__truediv__", "__rtruediv__"):                   expected = s.astype(float).combine(other, op)             if op_name == "__rpow__":                  expected[result.isna()] = np.nan             self.assert_series_equal(result, expected)         else:             with pytest.raises(exc):                 op(s, other)     def _check_divmod_op(self, s, op, other, exc=None):          super()._check_divmod_op(s, op, other, None)     @pytest.mark.skip(reason="BooleanArray does not error on ops")     def test_error(self, data, all_arithmetic_operators):          pass class TestComparisonOps(base.BaseComparisonOpsTests):     def check_opname(self, s, op_name, other, exc=None):          super().check_opname(s, op_name, other, exc=None)     def _compare_other(self, s, data, op_name, other):         self.check_opname(s, op_name, other)     @pytest.mark.skip(reason="Tested in tests/arrays/test_boolean.py")      def test_compare_scalar(self, data, all_compare_operators):         pass     @pytest.mark.skip(reason="Tested in tests/arrays/test_boolean.py")      def test_compare_array(self, data, all_compare_operators):         pass   class TestReshaping(base.BaseReshapingTests):     pass   class TestMethods(base.BaseMethodsTests):     @pytest.mark.parametrize("na_sentinel", [-1, -2])     def test_factorize(self, data_for_grouping, na_sentinel):          labels, uniques = pd.factorize(data_for_grouping, na_sentinel=na_sentinel)         expected_labels = np.array(             [0, 0, na_sentinel, na_sentinel, 1, 1, 0], dtype=np.intp         )         expected_uniques = data_for_grouping.take([0, 4])          tm.assert_numpy_array_equal(labels, expected_labels)         self.assert_extension_array_equal(uniques, expected_uniques)      def test_combine_le(self, data_repeated):          orig_data1, orig_data2 = data_repeated(2)         s1 = pd.Series(orig_data1)         s2 = pd.Series(orig_data2)         result = s1.combine(s2, lambda x1, x2: x1 <= x2)         expected = pd.Series(             [a <= b for (a, b) in zip(list(orig_data1), list(orig_data2))],             dtype="boolean",          )         self.assert_series_equal(result, expected)          val = s1.iloc[0]         result = s1.combine(val, lambda x1, x2: x1 <= x2)         expected = pd.Series([a <= val for a in list(orig_data1)], dtype="boolean")         self.assert_series_equal(result, expected)      def test_searchsorted(self, data_for_sorting, as_series):          data_for_sorting = pd.array([True, False], dtype="boolean")         b, a = data_for_sorting         arr = type(data_for_sorting)._from_sequence([a, b])          if as_series:             arr = pd.Series(arr)         assert arr.searchsorted(a) == 0         assert arr.searchsorted(a, side="right") == 1          assert arr.searchsorted(b) == 1         assert arr.searchsorted(b, side="right") == 2          result = arr.searchsorted(arr.take([0, 1]))         expected = np.array([0, 1], dtype=np.intp)          tm.assert_numpy_array_equal(result, expected)           sorter = np.array([1, 0])         assert data_for_sorting.searchsorted(a, sorter=sorter) == 0      @pytest.mark.skip(reason="uses nullable integer")     def test_value_counts(self, all_data, dropna):         return super().test_value_counts(all_data, dropna) class TestCasting(base.BaseCastingTests):     pass class TestGroupby(base.BaseGroupbyTests):      def test_grouping_grouper(self, data_for_grouping):         df = pd.DataFrame(             {"A": ["B", "B", None, None, "A", "A", "B"], "B": data_for_grouping}         )         gr1 = df.groupby("A").grouper.groupings[0]         gr2 = df.groupby("B").grouper.groupings[0]          tm.assert_numpy_array_equal(gr1.grouper, df.A.values)         tm.assert_extension_array_equal(gr2.grouper, data_for_grouping)      @pytest.mark.parametrize("as_index", [True, False])     def test_groupby_extension_agg(self, as_index, data_for_grouping):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         result = df.groupby("B", as_index=as_index).A.mean()         _, index = pd.factorize(data_for_grouping, sort=True)          index = pd.Index(index, name="B")         expected = pd.Series([3, 1], index=index, name="A")         if as_index:             self.assert_series_equal(result, expected)         else:             expected = expected.reset_index()             self.assert_frame_equal(result, expected)      def test_groupby_extension_no_sort(self, data_for_grouping):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         result = df.groupby("B", sort=False).A.mean()         _, index = pd.factorize(data_for_grouping, sort=False)          index = pd.Index(index, name="B")         expected = pd.Series([1, 3], index=index, name="A")         self.assert_series_equal(result, expected)      def test_groupby_extension_transform(self, data_for_grouping):         valid = data_for_grouping[~data_for_grouping.isna()]         df = pd.DataFrame({"A": [1, 1, 3, 3, 1], "B": valid})          result = df.groupby("B").A.transform(len)         expected = pd.Series([3, 3, 2, 2, 3], name="A")          self.assert_series_equal(result, expected)      def test_groupby_extension_apply(self, data_for_grouping, groupby_apply_op):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         df.groupby("B").apply(groupby_apply_op)         df.groupby("B").A.apply(groupby_apply_op)         df.groupby("A").apply(groupby_apply_op)         df.groupby("A").B.apply(groupby_apply_op)      def test_groupby_apply_identity(self, data_for_grouping):         df = pd.DataFrame({"A": [1, 1, 2, 2, 3, 3, 1], "B": data_for_grouping})         result = df.groupby("A").B.apply(lambda x: x.array)         expected = pd.Series(             [                 df.B.iloc[[0, 1, 6]].array,                 df.B.iloc[[2, 3]].array,                 df.B.iloc[[4, 5]].array,             ],             index=pd.Index([1, 2, 3], name="A"),             name="B",          )         self.assert_series_equal(result, expected)      def test_in_numeric_groupby(self, data_for_grouping):         df = pd.DataFrame(             {                 "A": [1, 1, 2, 2, 3, 3, 1],                 "B": data_for_grouping,                 "C": [1, 1, 1, 1, 1, 1, 1],             }          )         result = df.groupby("A").sum().columns          if data_for_grouping.dtype._is_numeric:             expected = pd.Index(["B", "C"])         else:             expected = pd.Index(["C"])         tm.assert_index_equal(result, expected)  class TestNumericReduce(base.BaseNumericReduceTests):     def check_reduce(self, s, op_name, skipna):         result = getattr(s, op_name)(skipna=skipna)         expected = getattr(s.astype("float64"), op_name)(skipna=skipna)          if np.isnan(expected):             expected = pd.NA         elif op_name in ("min", "max"):             expected = bool(expected)         tm.assert_almost_equal(result, expected)   class TestBooleanReduce(base.BaseBooleanReduceTests):     pass   class TestPrinting(base.BasePrintingTests):     pass   class TestUnaryOps(base.BaseUnaryOpsTests):     pass
def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if use_sequence_api:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
def js_to_json(code):              ([{,]\s*)              ("[^"]*"|\'[^\']*\'|[a-z0-9A-Z]+)              (:\s*)             ([0-9.]+|true|false|"[^"]*"|\'[^\']*\'|                 (?=\[|\{)             )      res = re.sub(r',(\s*\])', lambda m: m.group(1), res)      return res
class BlockManager(PandasObject):                          convert=convert,                          regex=regex,                      )                     if m.any() or convert:                          new_rb = _extend_blocks(result, new_rb)                      else:                          new_rb.append(b)
def uppercase_escape(s):      return re.sub(          r'\\U([0-9a-fA-F]{8})',          lambda m: compat_chr(int(m.group(1), base=16)), s)  try:     struct.pack(u'!I', 0) except TypeError:      def struct_pack(spec, *args):         if isinstance(spec, compat_str):             spec = spec.encode('ascii')         return struct.pack(spec, *args)      def struct_unpack(spec, *args):         if isinstance(spec, compat_str):             spec = spec.encode('ascii')         return struct.unpack(spec, *args) else:     struct_pack = struct.pack     struct_unpack = struct.unpack
class APIRoute(routing.Route):          self.response_model_exclude = response_model_exclude          self.response_model_by_alias = response_model_by_alias          self.response_model_exclude_unset = response_model_exclude_unset         self.response_model_exclude_defaults = response_model_exclude_defaults         self.response_model_exclude_none = response_model_exclude_none          self.include_in_schema = include_in_schema          self.response_class = response_class
class Language(object):              kwargs = component_cfg.get(name, {})              kwargs.setdefault("batch_size", batch_size)              if not hasattr(pipe, "pipe"):                 docs = _pipe(docs, pipe, kwargs)              else:                  docs = pipe.pipe(docs, **kwargs)          for doc, gold in zip(docs, golds):
def dispatch_to_series(left, right, func, str_rep=None, axis=None):          assert right.index.equals(left.columns)         if right.dtype == "timedelta64[ns]":                right = np.asarray(right)              def column_op(a, b):                 return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}          else:              def column_op(a, b):                 return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}      elif isinstance(right, ABCSeries): assert right.index.equals(left.index)
class CSVLogger(Callback):          if not self.writer:              class CustomDialect(csv.excel):                  delimiter = self.sep             fieldnames = ['epoch'] + self.keys             if six.PY2:                 fieldnames = [unicode(x) for x in fieldnames]              self.writer = csv.DictWriter(self.csv_file,                                          fieldnames=fieldnames,                                          dialect=CustomDialect)              if self.append_header:                  self.writer.writeheader()
class DictParameter(Parameter):      tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related      values (like a database connection config).          Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.
class CollectionRequirement:                  requirement = req                  op = operator.eq               if parent and version == '*' and requirement != '*':                 display.warning("Failed to validate the collection requirement '%s:%s' for %s when the existing "                                 "install does not have a version set, the collection may not work."                                 % (to_text(self), req, parent))                 continue             elif requirement == '*' or version == '*':                 continue              if not op(LooseVersion(version), LooseVersion(requirement)):                  break
class MultiIndex(Index):                      indexer = self._get_level_indexer(key, level=level)                      new_index = maybe_mi_droplevels(indexer, [0], drop_level)                      return indexer, new_index             except (TypeError, InvalidIndexError):                  pass              if not any(isinstance(k, slice) for k in key):
def srt_subtitles_timecode(seconds):  def dfxp2srt(dfxp_data):      LEGACY_NAMESPACES = (         (b'http://www.w3.org/ns/ttml', [             b'http://www.w3.org/2004/11/ttaf1',             b'http://www.w3.org/2006/04/ttaf1',             b'http://www.w3.org/2006/10/ttaf1',          ]),         (b'http://www.w3.org/ns/ttml             b'http://www.w3.org/ns/ttml          ]),      )
def _isna_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return False  _isna = _isna_new
class Tracer:          self._write(s)      def __enter__(self):         if DISABLED:             return          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame)         stack = self.thread_local.__dict__.setdefault(             'original_trace_functions', []         )          stack.append(sys.gettrace())          sys.settrace(self.trace)      def __exit__(self, exc_type, exc_value, exc_traceback):         if DISABLED:             return          stack = self.thread_local.original_trace_functions          sys.settrace(stack.pop())          calling_frame = inspect.currentframe().f_back
def standardize_weights(y,      Everything gets normalized to a single sample-wise (or timestep-wise)     weight array. If both `sample_weights` and `class_weights` are provided,     the weights are multiplied together.          y: Numpy array of model targets to be weighted.
class SimpleRNNCell(Layer):          self.dropout = min(1., max(0., dropout))          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.state_size = self.units         self.output_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
def write_flv_header(stream, metadata):      stream.write(b'\x12')     stream.write(struct_pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00')      stream.write(metadata)
import re  from .common import InfoExtractor  from ..utils import (     fix_xml_ampersands,  )
class tqdm(object):                      l_bar_user, r_bar_user = bar_format.split('{bar}')                     l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)                  else:                      return bar_format.format(**bar_args)
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      @cache_readonly      def _engine(self):          period = weakref.ref(self._values)          return self._engine_type(period, len(self))      @doc(Index.__contains__)
from pandas.core.dtypes.common import (      is_list_like,      is_object_dtype,      is_scalar,     pandas_dtype,  )  from pandas.core.dtypes.dtypes import register_extension_dtype  from pandas.core.dtypes.missing import isna
def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement      if src_txt[-1] != "\n":         src_txt += "\n"      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)          try:
class WebSocketHandler(tornado.web.RequestHandler):          if not self._on_close_called:              self._on_close_called = True              self.on_close()             self._break_cycles()      def _break_cycles(self):              if self.get_status() != 101 or self._on_close_called:             super(WebSocketHandler, self)._break_cycles()      def send_error(self, *args, **kwargs):          if self.stream is None:
class BusinessHourMixin(BusinessMixin):              if bd != 0:                 if isinstance(self, _CustomMixin):                     skip_bd = CustomBusinessDay(                         n=bd,                         weekmask=self.weekmask,                         holidays=self.holidays,                         calendar=self.calendar,                     )                 else:                     skip_bd = BusinessDay(n=bd)                  if not self.next_bday.is_on_offset(other):                      prev_open = self._prev_opening_time(other)
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or not np.isnan(right_value):                      return False              else:                 if np.any(left_value != right_value):                      return False          return True
class CannotSplit(Exception):  class WriteBack(Enum):      NO = 0      YES = 1
patterns = (          '^lua: {file}:{line}:',         '^{file} \\(line {line}\\):',          '^{file}: line {line}: ',          '^{file}:{line}:{col}',          '^{file}:{line}:',          'at {file} line {line}',      )
import threading  import time  import traceback  import math from tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback  from tornado.log import app_log, gen_log
class Conv2DTranspose(Conv2D):              output_shape,              self.strides,              padding=self.padding,             data_format=self.data_format,             dilation_rate=self.dilation_rate)          if self.use_bias:              outputs = K.bias_add(
def format_stdin_to_stdout(      `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to      :func:`format_file_contents`.     newline, encoding, src = prepare_input(sys.stdin.buffer.read())      dst = src      try:          dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                 exclude_defaults=response_model_exclude_defaults,                 exclude_none=response_model_exclude_none,                  is_coroutine=is_coroutine,              )              response = response_class(
def get_elements_by_attribute(attribute, value, html, escape_value=True):      retlist = []          <([a-zA-Z0-9:._-]+)          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'|))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'|))*?          \s*>          (?P<content>.*?)          </\1>
class StackedRNNCells(Layer):                                   '`state_size` attribute. '                                   'received cells:', cells)          self.cells = cells              self.reverse_state_order = kwargs.pop('reverse_state_order', False)         if self.reverse_state_order:             warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '                           'will soon be deprecated. Please update the code to '                           'work with the natural order of states if you '                           'reply on the RNN states, '                           'eg `RNN(return_state=True)`.')          super(StackedRNNCells, self).__init__(**kwargs)      @property      def state_size(self):              state_size = []         for cell in self.cells[::-1] if self.reverse_state_order else self.cells:              if hasattr(cell.state_size, '__len__'):                  state_size += list(cell.state_size)              else:                  state_size.append(cell.state_size)          return tuple(state_size)     @property     def output_size(self):         if getattr(self.cells[-1], 'output_size', None) is not None:             return self.cells[-1].output_size         if hasattr(self.cells[-1].state_size, '__len__'):             return self.cells[-1].state_size[0]         else:             return self.cells[-1].state_size       def call(self, inputs, states, constants=None, **kwargs):          nested_states = []         for cell in self.cells[::-1] if self.reverse_state_order else self.cells:              if hasattr(cell.state_size, '__len__'):                  nested_states.append(states[:len(cell.state_size)])                  states = states[len(cell.state_size):]              else:                  nested_states.append([states[0]])                  states = states[1:]         if self.reverse_state_order:             nested_states = nested_states[::-1]          new_nested_states = []
class Model(Container):              validation_steps: Only relevant if `validation_data`                  is a generator. Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Optional for `Sequence`: if unspecified, will use                 the `len(validation_data)` as a number of steps.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Integer. Maximum size for the generator queue.
class RetcodesTest(LuigiTestCase):          with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):              self.run_and_expect('RequiringTask', 0)              self.run_and_expect('RequiringTask --retcode-not-run 5', 5)      def test_retry_sucess_task(self):         class Foo(luigi.Task):             run_count = 0              def run(self):                 self.run_count += 1                 if self.run_count == 1:                     raise ValueError()              def complete(self):                 return self.run_count > 0          self.run_and_expect('Foo --scheduler-retry-delay=0', 0)         self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)         self.run_with_config(dict(task_failed='3'), 'Foo', 0)
def count_leading_spaces(s):  def process_list_block(docstring, starting_point, section_end,                         leading_spaces, marker):      ending_point = docstring.find('\n\n', starting_point)     block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else                                       section_end)]      docstring_slice = docstring[starting_point:section_end].replace(block, marker)      docstring = (docstring[:starting_point]
default: :rc:`scatter.edgecolors`          collection = mcoll.PathCollection(                  (path,), scales,                 facecolors=colors if marker_obj.is_filled() else 'none',                 edgecolors=edgecolors if marker_obj.is_filled() else colors,                  linewidths=linewidths,                  offsets=offsets,                  transOffset=kwargs.pop('transform', self.transData),
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:      srcbuf = io.BytesIO(src)      encoding, lines = tokenize.detect_encoding(srcbuf.readline)     if not lines:         return "", encoding, "\n"       newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"      srcbuf.seek(0)      with io.TextIOWrapper(srcbuf, encoding) as tiow:
def create_instance(objcls, settings, crawler, *args, **kwargs):      ``*args`` and ``**kwargs`` are forwarded to the constructors.      Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.      Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an     extension has not been implemented correctly).      if settings is None:          if crawler is None:              raise ValueError("Specify at least one of settings and crawler.")          settings = crawler.settings      if crawler and hasattr(objcls, 'from_crawler'):         instance = objcls.from_crawler(crawler, *args, **kwargs)         method_name = 'from_crawler'      elif hasattr(objcls, 'from_settings'):         instance = objcls.from_settings(settings, *args, **kwargs)         method_name = 'from_settings'      else:         instance = objcls(*args, **kwargs)         method_name = '__new__'     if instance is None:         raise TypeError("%s.%s returned None" % (objcls.__qualname__, method_name))     return instance  @contextmanager
class LinuxHardware(Hardware):          pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))          maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT          for fields in mtab_entries:              fields = [self._replace_octal_escapes(field) for field in fields]              device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]
def crosstab(      from pandas import DataFrame      df = DataFrame(data, index=common_idx)     original_df_cols = df.columns       if values is None:          df["__dummy__"] = 0          kwargs = {"aggfunc": len, "fill_value": 0}
class S3CopyToTable(rdbms.CopyToTable):          if '.' in self.table:              query = ("select 1 as table_exists "                       "from information_schema.tables "                      "where table_schema = lower(%s) and table_name = lower(%s) limit 1")          else:              query = ("select 1 as table_exists "                       "from pg_table_def "                      "where tablename = lower(%s) limit 1")          cursor = connection.cursor()          try:              cursor.execute(query, tuple(self.table.split('.')))
TEST_MODULES = [      'tornado.test.curl_httpclient_test',      'tornado.test.escape_test',      'tornado.test.gen_test',     'tornado.test.http1connection_test',      'tornado.test.httpclient_test',      'tornado.test.httpserver_test',      'tornado.test.httputil_test',
class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if kind == "loc":             try:                 return self.categories._convert_scalar_indexer(key, kind=kind)             except TypeError:                 self._invalid_indexer("label", key)          return super()._convert_scalar_indexer(key, kind=kind)      @Appender(_index_shared_docs["_convert_list_indexer"])
def read_pickle(path, compression="infer"):      >>> import os      >>> os.remove("./dummy.pkl")     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(         filepath_or_buffer, compression=compression     )     if not isinstance(fp_or_buf, str) and compression == "infer":         compression = None     f, fh = get_handle(fp_or_buf, "rb", compression=compression, is_text=False)
def na_value_for_dtype(dtype, compat: bool = True):      if is_extension_array_dtype(dtype):          return dtype.na_value     if needs_i8_conversion(dtype):          return NaT      elif is_float_dtype(dtype):          return np.nan
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later.     if line.is_comment:          yield line          return
class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))          self._viewLim.intervalx = (left, right)          for ax in self._shared_x_axes.get_siblings(self):             ax._stale_viewlim_x = False          if auto is not None:              self._autoscaleXon = bool(auto)
def match(command):  @git_support  def get_new_command(command):        upstream_option_index = -1     try:         upstream_option_index = command.script_parts.index('--set-upstream')     except ValueError:         pass     try:         upstream_option_index = command.script_parts.index('-u')     except ValueError:         pass     if upstream_option_index is not -1:         command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)       push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]     return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if is_sequence(val_data):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing)
class FeedExporter(object):          d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                              extra={'spider': spider}))          d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                             exc_info=failure_to_exc_info(f),                                             extra={'spider': spider}))          return d      def item_scraped(self, item, spider):
setup(              "options_test.cfg",              "static/robots.txt",              "static/dir/index.html",             "static_foo.txt",              "templates/utf8.html",              "test.crt",              "test.key",
def update_add(x, increment):          The variable `x` updated.     op = tf_state_ops.assign_add(x, increment)     with tf.control_dependencies([op]):         return tf.identity(x)  @symbolic
from pandas.core.dtypes.generic import ABCSeries  from pandas.core.dtypes.missing import isna  from pandas._typing import AnyArrayLike from pandas.core.algorithms import take_1d  from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs  import pandas.core.common as com  import pandas.core.indexes.base as ibase
class TestBackend(object):          else:              assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)     def test_print_tensor(self, capsys):         for k in [KTH, KTF]:             x = k.placeholder((1, 1))             y = k.print_tensor(x, 'msg')             fn = k.function([x], [y])             _ = fn([np.ones((1, 1))])             out, err = capsys.readouterr()              assert out.replace('__str__ = ', '') == 'msg [[1.]]\n'           check_single_tensor_operation('print_tensor', (), WITH_NP)          check_single_tensor_operation('print_tensor', (2,), WITH_NP)      def test_elementwise_operations(self):          check_single_tensor_operation('max', (4, 2), WITH_NP)
def _unstack_multiple(data, clocs, fill_value=None):      index = data.index       if clocs in index.names:         clocs = [clocs]      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs]
def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)          return any(is_excluded(dtype) for is_excluded in is_excluded_checks)      return _is_dtype(arr_or_dtype, condition)
def conv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) * stride - 2 * pad + filter_size def deconv_length(dim_size, stride_size, kernel_size, padding,                   output_padding, dilation=1):
def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1)     expected["Group"] = expected["Group_obj"]      tm.assert_frame_equal(result, expected)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def jsonable_encoder(              )          return jsonable_encoder(              obj_dict,             exclude_none=exclude_none,             exclude_defaults=exclude_defaults,              custom_encoder=encoder,              sqlalchemy_safe=sqlalchemy_safe,          )
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,         response_model_exclude_defaults: bool = False,         response_model_exclude_none: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf  def get_future_imports(node: Node) -> Set[str]:     imports: Set[str] = set()      def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:         for child in children:             if isinstance(child, Leaf):                 if child.type == token.NAME:                     yield child.value             elif child.type == syms.import_as_name:                 orig_name = child.children[0]                 assert isinstance(orig_name, Leaf), "Invalid syntax parsing imports"                 assert orig_name.type == token.NAME, "Invalid syntax parsing imports"                 yield orig_name.value             elif child.type == syms.import_as_names:                 yield from get_imports_from_children(child.children)             else:                 assert False, "Invalid syntax parsing imports"       for child in node.children:          if child.type != syms.simple_stmt:              break
def _isna_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike(obj, old=True)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)      elif hasattr(obj, "__array__"):         return _isna_ndarraylike(np.asarray(obj), old=True)      else:          return False
def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):      with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None)     assert "'NoneType' object is not iterable" in to_native(te.value)
class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = label          self._labelkw = kw          self._set_label()
from .generic import Generic  class Bash(Generic):      def app_alias(self, fuck):          alias = "alias {0}='TF_CMD=$(TF_ALIAS={0}" \                 " PYTHONIOENCODING=utf-8" \                 " TF_SHELL_ALIASES=$(alias)" \                 " thefuck $(fc -ln -1)) &&" \                  " eval $TF_CMD".format(fuck)          if settings.alter_history:
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              return self         new_data = self._data.apply(operator.invert)         result = self._constructor(new_data).__finalize__(self)         return result      def __nonzero__(self):          raise ValueError(
class OffsiteMiddleware(object):          if not allowed_domains: return re.compile('')          url_pattern = re.compile("^https?://.*$")         domains = []          for domain in allowed_domains:             if domain is None:                 continue             elif url_pattern.match(domain):                  message = ("allowed_domains accepts only domains, not URLs. "                             "Ignoring URL entry %s in allowed_domains." % domain)                  warnings.warn(message, URLWarning)             else:                 domains.append(re.escape(domain))          regex = r'^(.*\.)?(%s)$' % '|'.join(domains)          return re.compile(regex)
class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph'))     prune_on_get_work = parameter.BoolParameter(default=False)   def fix_time(x):
from scrapy.utils.ftp import ftp_makedirs_cwd  from scrapy.exceptions import NotConfigured  from scrapy.utils.misc import load_object  from scrapy.utils.python import get_func_args from scrapy.utils.log import failure_to_exc_info  logger = logging.getLogger(__name__)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),             response_model_exclude_defaults=response_model_exclude_defaults,             response_model_exclude_none=response_model_exclude_none,              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class Categorical(ExtensionArray, PandasObject):              if dtype == self.dtype:                  return self              return self._set_dtype(dtype)         if is_extension_array_dtype(dtype):             return array(self, dtype=dtype, copy=copy)          if is_integer_dtype(dtype) and self.isna().any():              msg = "Cannot convert float NaN to integer"              raise ValueError(msg)
default: :rc:`scatter.edgecolors`              - 'none': No patch boundary will be drawn.              - A color or sequence of colors.             For non-filled markers, *edgecolors* is ignored. Instead, the color             is determined like with 'face', i.e. from *c*, *colors*, or             *facecolors*.          plotnonfinite : bool, default: False              Set to plot points with nonfinite *c*, in conjunction with
class Tracer:          self.target_codes = set()          self.target_frames = set()          self.thread_local = threading.local()         if len(custom_repr) == 2 and not all(isinstance(x,                       pycompat.collections_abc.Iterable) for x in custom_repr):             custom_repr = (custom_repr,)         self.custom_repr = custom_repr      def __call__(self, function):         if DISABLED:             return function          self.target_codes.add(function.__code__)          @functools.wraps(function)
import time  import traceback  import math from tornado.concurrent import TracebackFuture, is_future, chain_future  from tornado.log import app_log, gen_log  from tornado.platform.auto import set_close_exec, Waker  from tornado import stack_context
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0, base=10),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
class ExecutionEngine(object):          d = self._download(request, spider)          d.addBoth(self._handle_downloader_output, request, spider)          d.addErrback(lambda f: logger.info('Error while handling downloader output',                                            exc_info=failure_to_exc_info(f),                                            extra={'spider': spider}))          d.addBoth(lambda _: slot.remove_request(request))          d.addErrback(lambda f: logger.info('Error while removing request from slot',                                            exc_info=failure_to_exc_info(f),                                            extra={'spider': spider}))          d.addBoth(lambda _: slot.nextcall.schedule())          d.addErrback(lambda f: logger.info('Error while scheduling new request',                                            exc_info=failure_to_exc_info(f),                                            extra={'spider': spider}))          return d      def _handle_downloader_output(self, response, request, spider):
class Sequential(Model):                                               use_multiprocessing=use_multiprocessing)      @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps=None,                            max_queue_size=10, workers=1,                            use_multiprocessing=False, verbose=0):
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0, base=10),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
class DataFrame(NDFrame):              other = other._convert(datetime=True, timedelta=True)              if not self.columns.equals(combined_columns):                  self = self.reindex(columns=combined_columns)         elif isinstance(other, list):             if not other:                 pass             elif not isinstance(other[0], DataFrame):                 other = DataFrame(other)                 if (self.columns.get_indexer(other.columns) >= 0).all():                     other = other.reindex(columns=self.columns)          from pandas.core.reshape.concat import concat
class _AxesBase(martist.Artist):              if right is None:                  right = old_right         if self.get_xscale() == 'log' and (left <= 0 or right <= 0):               old_left, old_right = self.get_xlim()              if left <= 0:                  cbook._warn_external(                      'Attempted to set non-positive left xlim on a '
def _unstack_multiple(data, clocs, fill_value=None):              result = data              for i in range(len(clocs)):                  val = clocs[i]                 result = result.unstack(val, fill_value=fill_value)                  clocs = [v if i > v else v - 1 for v in clocs]              return result
class PagedList(object):  def uppercase_escape(s):     unicode_escape = codecs.getdecoder('unicode_escape')      return re.sub(          r'\\U[0-9a-fA-F]{8}',         lambda m: unicode_escape(m.group(0))[0],         s)  try:      struct.pack(u'!I', 0)
def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return      mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)      if mobj:
class TestInsertIndexCoercion(CoercionBase):          )         msg = "cannot insert TimedeltaArray with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaArray with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, 1)
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()         mat = numeric_df.astype(float, copy=False).to_numpy()          if method == "pearson":             correl = libalgos.nancorr(mat, minp=min_periods)          elif method == "spearman":             correl = libalgos.nancorr_spearman(mat, minp=min_periods)          elif method == "kendall" or callable(method):              if min_periods is None:                  min_periods = 1             mat = mat.T              corrf = nanops.get_corr_func(method)              K = len(cols)              correl = np.empty((K, K), dtype=float)
