class Register(abc.ABCMeta):          reg = OrderedDict()          for cls in cls._reg:             if cls.run == NotImplemented:                 continue              name = cls.task_family              if name in reg and reg[name] != cls and \
def box_df_fail(request):      return request.param @pytest.fixture(     params=[         (pd.Index, False),         (pd.Series, False),         (pd.DataFrame, False),         pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),         (tm.to_array, False),     ],     ids=id_func, ) def box_transpose_fail(request):      return request.param    @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)  def box_with_array(request):
from twisted.python.failure import Failure  from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \      getAllReceivers, disconnect  from scrapy.xlib.pydispatch.robustapply import robustApply  logger = logging.getLogger(__name__)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class Line:          res = f'{first.prefix}{indent}{first.value}'          for leaf in leaves:              res += str(leaf)         for comment in self.comments.values():              res += str(comment)          return res + '\n'
from ..compat import (  )  from ..utils import (      int_or_none,      parse_filesize,  )
def _depth_first_search(set_tasks, current_task, visited):          for task in current_task._requires():              if task not in visited:                  _depth_first_search(set_tasks, task, visited)             if task in set_tasks["failed"] or task in set_tasks["upstream_failure"]:                  set_tasks["upstream_failure"].add(current_task)                  upstream_failure = True              if task in set_tasks["still_pending_ext"] or task in set_tasks["upstream_missing_dependency"]:
def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):          if dont_log is None or not isinstance(failure.value, dont_log):              logger.error("Error caught on signal handler: %(receiver)s",                           {'receiver': recv},                          extra={'spider': spider, 'failure': failure})          return failure      dont_log = named.pop('dont_log', None)
class Line:          return bool(self.leaves or self.comments) class UnformattedLines(Line):          The `preformatted` argument is ignored.          Keeps track of indentation `depth`, which is useful when the user         says `          `depth` is not used for indentation in this case.         raise NotImplementedError("Unformatted lines don't store comments separately.")      def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:         return False    @dataclass  class EmptyLineTracker:
class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):      for it.     _responseMatcher = re.compile('HTTP/1\.. 200')      def __init__(self, reactor, host, port, proxyConf, contextFactory,                   timeout=30, bindAddress=None):
def cartesian_product(X):          b = np.zeros_like(cumprodX)     return [         np.tile(             np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])         )         for i, x in enumerate(X)     ]
class FacebookGraphMixin(OAuth2Mixin):              future.set_exception(AuthError('Facebook auth error: %s' % str(response)))              return         args = escape.parse_qs_bytes(escape.native_str(response.body))          session = {              "access_token": args["access_token"][-1],              "expires": args.get("expires")
from datetime import datetime, timedelta import operator from typing import Any, Sequence, Type, Union, cast import warnings  import numpy as np from pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib from pandas._libs.tslibs.c_timestamp import integer_op_not_supported from pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds from pandas._libs.tslibs.timestamps import RoundTo, round_nsint64 from pandas._typing import DatetimeLikeScalar from pandas.compat import set_function_name  from pandas.compat.numpy import function as nv from pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning from pandas.util._decorators import Appender, Substitution from pandas.util._validators import validate_fillna_kwargs  from pandas.core.dtypes.common import (     is_categorical_dtype,      is_datetime64_any_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,     is_datetime_or_timedelta_dtype,      is_dtype_equal,     is_float_dtype,     is_integer_dtype,      is_list_like,      is_object_dtype,      is_period_dtype,     is_string_dtype,      is_timedelta64_dtype,     is_unsigned_integer_dtype,     pandas_dtype,  ) from pandas.core.dtypes.generic import ABCSeries from pandas.core.dtypes.inference import is_array_like from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna  from pandas.core import missing, nanops, ops from pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts from pandas.core.array_algos.transforms import shift from pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray from pandas.core.arrays.base import ExtensionArray, ExtensionOpsMixin import pandas.core.common as com from pandas.core.construction import array, extract_array from pandas.core.indexers import check_array_indexer from pandas.core.ops.common import unpack_zerodim_and_defer from pandas.core.ops.invalid import invalid_comparison, make_invalid_op  from pandas.tseries import frequencies from pandas.tseries.offsets import DateOffset, Tick   def _datetimelike_array_cmp(cls, op):     opname = f"__{op.__name__}__"     nat_result = opname == "__ne__"      class InvalidComparison(Exception):         pass      def _validate_comparison_value(self, other):         if isinstance(other, str):             try:                  other = self._scalar_from_string(other)             except ValueError:                  raise InvalidComparison(other)          if isinstance(other, self._recognized_scalars) or other is NaT:             other = self._scalar_type(other)             self._check_compatible_with(other)          elif not is_list_like(other):             raise InvalidComparison(other)          elif len(other) != len(self):             raise ValueError("Lengths must match")         else:             if isinstance(other, list):                  other = np.array(other)             if not isinstance(other, (np.ndarray, type(self))):                 raise InvalidComparison(other)             elif is_object_dtype(other.dtype):                 pass             elif not type(self)._is_recognized_dtype(other.dtype):                 raise InvalidComparison(other)             else:                   other = type(self)._from_sequence(other)                 self._check_compatible_with(other)         return other     @unpack_zerodim_and_defer(opname)     def wrapper(self, other):         try:             other = _validate_comparison_value(self, other)         except InvalidComparison:             return invalid_comparison(self, other, op)          dtype = getattr(other, "dtype", None)         if is_object_dtype(dtype):                with np.errstate(all="ignore"):                 result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)             return result         if isinstance(other, self._scalar_type) or other is NaT:             other_i8 = self._unbox_scalar(other)         else:              other_i8 = other.asi8         result = op(self.asi8, other_i8)         o_mask = isna(other)         if self._hasnans | np.any(o_mask):             result[self._isnan | o_mask] = nat_result          return result     return set_function_name(wrapper, opname, cls) class AttributesMixin:     _data: np.ndarray     @classmethod     def _simple_new(cls, values: np.ndarray, **kwargs):         raise AbstractMethodError(cls)     @property     def _scalar_type(self) -> Type[DatetimeLikeScalar]:         raise AbstractMethodError(self)      def _scalar_from_string(         self, value: str     ) -> Union[Period, Timestamp, Timedelta, NaTType]:         raise AbstractMethodError(self)      def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:         raise AbstractMethodError(self)      def _check_compatible_with(         self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False     ) -> None:         raise AbstractMethodError(self)   class DatelikeOps:      @Substitution(         URL="https://docs.python.org/3/library/datetime.html"         "     )     def strftime(self, date_format):         result = self._format_native_types(date_format=date_format, na_rep=np.nan)         return result.astype(object) class TimelikeOps:             - 'shift_forward' will shift the nonexistent time forward to the               closest existing time             - 'shift_backward' will shift the nonexistent time backward to the               closest existing time             - 'NaT' will return NaT where there are nonexistent times             - timedelta objects will shift nonexistent times by the timedelta             - 'raise' will raise an NonExistentTimeError if there are               nonexistent times.             .. versionadded:: 0.24.0         Returns         -------         DatetimeIndex, TimedeltaIndex, or Series             Index of the same type for a DatetimeIndex or TimedeltaIndex,             or a Series with the same index for a Series.         Raises         ------         ValueError if the `freq` cannot be converted.     def _round(self, freq, mode, ambiguous, nonexistent):          if is_datetime64tz_dtype(self):              naive = self.tz_localize(None)             result = naive._round(freq, mode, ambiguous, nonexistent)             aware = result.tz_localize(                 self.tz, ambiguous=ambiguous, nonexistent=nonexistent             )             return aware          values = self.view("i8")         result = round_nsint64(values, mode, freq)         result = self._maybe_mask_results(result, fill_value=NaT)         return self._simple_new(result, dtype=self.dtype)      @Appender((_round_doc + _round_example).format(op="round"))     def round(self, freq, ambiguous="raise", nonexistent="raise"):         return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)         Helper to set our freq in-place, returning self to allow method chaining.         Parameters         ----------         freq : DateOffset, None, or "infer"         Returns         -------         self     Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray      Assumes that __new__/__init__ defines:         _data         _freq      and that the inheriting class has methods:         _generate_range         box function to get object from internal representation         apply box func to passed values         Integer representation of the values.         Returns         -------         ndarray             An ndarray with int64 dtype.         Helper method for astype when converting to strings.          Returns          -------         ndarray[str]         return np.prod(self.shape)     def __len__(self) -> int:         return len(self._data)      def __getitem__(self, key):         if com.is_bool_indexer(key):               if is_object_dtype(key):                 key = np.asarray(key, dtype=bool)              key = check_array_indexer(self, key)             key = lib.maybe_booleans_to_slice(key.view(np.uint8))         elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):               pass          else:             key = check_array_indexer(self, key)         freq = self._get_getitem_freq(key)         result = self._data[key]         if lib.is_scalar(result):             return self._box_func(result)         return self._simple_new(result, dtype=self.dtype, freq=freq)     def _get_getitem_freq(self, key):         is_period = is_period_dtype(self.dtype)         if is_period:             freq = self.freq         else:             freq = None             if isinstance(key, slice):                 if self.freq is not None and key.step is not None:                     freq = key.step * self.freq                 else:                     freq = self.freq             elif key is Ellipsis:                   freq = self.freq         return freq      def __setitem__(         self,         key: Union[int, Sequence[int], Sequence[bool], slice],         value: Union[NaTType, Any, Sequence[Any]],     ) -> None:              if is_list_like(value):             is_slice = isinstance(key, slice)              if lib.is_scalar(key):                 raise ValueError("setting an array element with a sequence.")              if not is_slice:                 key = cast(Sequence, key)                 if len(key) != len(value) and not com.is_bool_indexer(key):                     msg = (                         f"shape mismatch: value array of length '{len(key)}' "                         "does not match indexing result of length "                         f"'{len(value)}'."                     )                     raise ValueError(msg)                 elif not len(key):                     return          value = self._validate_setitem_value(value)         key = check_array_indexer(self, key)         self._data[key] = value         self._maybe_clear_freq()      def _maybe_clear_freq(self):           pass      def astype(self, dtype, copy=True):             dtype = pandas_dtype(dtype)          if is_object_dtype(dtype):             return self._box_values(self.asi8.ravel()).reshape(self.shape)         elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):             return self._format_native_types()         elif is_integer_dtype(dtype):               values = self.asi8              if is_unsigned_integer_dtype(dtype):                  values = values.view("uint64")              if copy:                 values = values.copy()             return values         elif (             is_datetime_or_timedelta_dtype(dtype)             and not is_dtype_equal(self.dtype, dtype)         ) or is_float_dtype(dtype):               msg = f"Cannot cast {type(self).__name__} to dtype {dtype}"             raise TypeError(msg)         elif is_categorical_dtype(dtype):             arr_cls = dtype.construct_array_type()             return arr_cls(self, dtype=dtype)         else:             return np.asarray(self, dtype=dtype)      def view(self, dtype=None):         if dtype is None or dtype is self.dtype:             return type(self)(self._data, dtype=self.dtype)         return self._data.view(dtype=dtype)         def unique(self):         result = unique1d(self.asi8)         return type(self)(result, dtype=self.dtype)      @classmethod     def _concat_same_type(cls, to_concat, axis: int = 0):           dtypes = {str(x.dtype) for x in to_concat}         if len(dtypes) != 1:             raise ValueError("to_concat must have the same dtype (tz)", dtypes)          obj = to_concat[0]         dtype = obj.dtype          i8values = [x.asi8 for x in to_concat]         values = np.concatenate(i8values, axis=axis)          new_freq = None         if is_period_dtype(dtype):             new_freq = obj.freq         elif axis == 0:               to_concat = [x for x in to_concat if len(x)]              if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):                 pairs = zip(to_concat[:-1], to_concat[1:])                 if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):                     new_freq = obj.freq          return cls._simple_new(values, dtype=dtype, freq=new_freq)      def copy(self):         values = self.asi8.copy()         return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)      def _values_for_factorize(self):         return self.asi8, iNaT      @classmethod     def _from_factorized(cls, values, original):         return cls(values, dtype=original.dtype)      def _values_for_argsort(self):         return self._data      @Appender(ExtensionArray.shift.__doc__)     def shift(self, periods=1, fill_value=None, axis=0):         if not self.size or periods == 0:             return self.copy()          fill_value = self._validate_shift_value(fill_value)         new_values = shift(self._data, periods, axis, fill_value)         return type(self)._simple_new(new_values, dtype=self.dtype)        def _validate_fill_value(self, fill_value):          Parameters          ----------         fill_value : object          Returns          -------         fill_value : np.int64          Raises         ------         ValueError         Find indices where elements should be inserted to maintain order.          Find the indices into a sorted array `self` such that, if the         corresponding elements in `value` were inserted before the indices,         the order of `self` would be preserved.          Parameters          ----------         value : array_like             Values to insert into `self`.         side : {'left', 'right'}, optional             If 'left', the index of the first suitable location found is given.             If 'right', return the last such index.  If there is no suitable             index, return either 0 or N (where N is the length of `self`).         sorter : 1-D array_like, optional             Optional array of integer indices that sort `self` into ascending             order. They are typically the result of ``np.argsort``.          Returns          -------         indices : array of ints             Array of insertion points with the same shape as `value`.         Repeat elements of an array.         See Also         --------         numpy.ndarray.repeat         Return a Series containing counts of unique values.          Parameters          ----------         dropna : bool, default True             Don't include counts of NaT values.         Returns         -------         Series         return Index(self).map(mapper).array       def isna(self):         return self._isnan     @property     def _isnan(self):         return self.asi8 == iNaT     @property     def _hasnans(self):         return bool(self._isnan.any())     def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):         if self._hasnans:             if convert:                 result = result.astype(convert)             if fill_value is None:                 fill_value = np.nan             result[self._isnan] = fill_value         return result     def fillna(self, value=None, method=None, limit=None):            if isinstance(value, ABCSeries):             value = value.array          value, method = validate_fillna_kwargs(value, method)          mask = self.isna()          if is_array_like(value):             if len(value) != len(self):                 raise ValueError(                     f"Length of 'value' does not match. Got ({len(value)}) "                     f" expected {len(self)}"                 )             value = value[mask]          if mask.any():             if method is not None:                 if method == "pad":                     func = missing.pad_1d                 else:                     func = missing.backfill_1d                  values = self._data                 if not is_period_dtype(self):                        values = values.copy()                  new_values = func(values, limit=limit, mask=mask)                 if is_datetime64tz_dtype(self):                       new_values = new_values.view("i8")                 new_values = type(self)(new_values, dtype=self.dtype)             else:                  new_values = self.copy()                 new_values[mask] = value         else:             new_values = self.copy()         return new_values        @property      def freq(self):         return self._freq      @freq.setter     def freq(self, value):         if value is not None:             value = frequencies.to_offset(value)             self._validate_frequency(self, value)          self._freq = value      @property      def freqstr(self):
class tqdm(Comparable):                  else TqdmKeyError("Unknown argument(s): " + str(kwargs)))         if total is None and iterable is not None:             try:                 total = len(iterable)             except (TypeError, AttributeError):                 total = None           if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \ dynamic_ncols:              if dynamic_ncols:
class GroupBy(_GroupBy):                      axis=axis,                  )              )          filled = getattr(self, fill_method)(limit=limit)          fill_grp = filled.groupby(self.grouper.codes)          shifted = fill_grp.shift(periods=periods, freq=freq)
def ctc_batch_cost(y_true, y_pred, input_length, label_length):          Tensor with shape (samples,1) containing the              CTC loss of each element.     label_length = tf.to_int32(tf.squeeze(label_length))     input_length = tf.to_int32(tf.squeeze(input_length))      sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))      y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later.     if isinstance(line, UnformattedLines):          yield line          return      line_str = str(line).strip('\n')     if len(line_str) <= line_length and '\n' not in line_str:          yield line          return      if line.is_def:          split_funcs = [left_hand_split]      elif line.inside_brackets:         split_funcs = [delimiter_split]         if '\n' not in line_str:               split_funcs.append(right_hand_split)      else:          split_funcs = [right_hand_split]      for split_func in split_funcs:
__metaclass__ = type  from ansible.module_utils.six import string_types  from ansible.playbook.attribute import FieldAttribute  from ansible.utils.collection_loader import AnsibleCollectionLoader  def _ensure_default_collection(collection_list=None):
def _get_grouper( elif is_in_axis(gpr):              if gpr in obj:                  if validate:                     obj._check_label_or_level_ambiguity(gpr)                  in_axis, name, gpr = True, gpr, obj[gpr]                  exclusions.append(name)             elif obj._is_level_reference(gpr):                  in_axis, name, level, gpr = False, None, gpr, None              else:                  raise KeyError(gpr)
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response         location = safe_url_string(response.headers['location'])          redirected_url = urljoin(request.url, location)
class DatetimeTZDtype(PandasExtensionDtype):          if isinstance(string, str):              msg = "Could not construct DatetimeTZDtype from '{}'"             try:                 match = cls._match.match(string)                 if match:                     d = match.groupdict()                      return cls(unit=d["unit"], tz=d["tz"])             except Exception:                  pass              raise TypeError(msg.format(string))          raise TypeError("Could not construct DatetimeTZDtype")
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
from subprocess import Popen, PIPE  import os  from ..conf import settings from ..utils import DEVNULL, memoize, cache  from .generic import Generic
def hide_fmt_off(node: Node) -> bool:  def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:      container: Optional[LN] = container_of(leaf)     while container is not None:          for comment in list_comments(container.prefix, is_endmarker=False):              if comment.value in FMT_ON:                  return
def js_to_json(code):          '(?:[^'\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^'\\]*'|          /\*.*?\*/|,(?=\s*[\]}])|          [a-zA-Z_][.a-zA-Z_0-9]*|         (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|          [0-9]+(?=\s*:)
class CParserWrapper(ParserBase):             if isinstance(src, BufferedIOBase):                  src = TextIOWrapper(src, encoding=encoding, newline="")              kwds["encoding"] = "utf-8"
class S3CopyToTable(rdbms.CopyToTable):          if not (self.table):              raise Exception("table need to be specified")         path = self.s3_load_path()          connection = self.output().connect()          if not self.does_table_exist(connection):
class ExtractorError(Exception):              expected = True          if video_id is not None:              msg = video_id + ': ' + msg          if not expected:              msg = msg + u'; please report this issue on https://yt-dl.org/bug . Be sure to call youtube-dl with the --verbose flag and include its complete output. Make sure you are using the latest version; type  youtube-dl -U  to update.'          super(ExtractorError, self).__init__(msg)
class KeyEvent(LocationEvent):          self.key = key def _get_renderer(figure, print_method=None, *, draw_disabled=False):
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)          if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):              if not isinstance(other, DatetimeLikeArrayMixin):                  from pandas.core.arrays import DatetimeArray
def match(command):  @git_support  def get_new_command(command):     return replace_argument(command.script, 'push', 'push --force')  enabled_by_default = False
import warnings  from .. import backend as K  from .. import losses  from ..utils.generic_utils import to_list
class RadialLocator(mticker.Locator):          return self.base.refresh()      def view_limits(self, vmin, vmax):          vmin, vmax = self.base.view_limits(vmin, vmax)          if vmax > vmin:
from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.missing import isna  from pandas.core import algorithms  import pandas.core.common as com  from pandas.core.indexes.base import Index, InvalidIndexError, _index_shared_docs
class TimeseriesGenerator(Sequence):      def __getitem__(self, index):          if self.shuffle:              rows = np.random.randint(                 self.start_index, self.end_index, size=self.batch_size)          else:              i = self.start_index + self.batch_size * self.stride * index              rows = np.arange(i, min(i + self.batch_size *                                     self.stride, self.end_index), self.stride)          samples, targets = self._empty_batch(len(rows))          for j, row in enumerate(rows):
def match(command):  def _parse_operations(help_text_lines):        operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)      return operation_regex.findall(help_text_lines)
def generate_tokens(readline):      contline = None      indents = [0]      stashed = None      async_def = False
def _right_outer_join(x, y, max_groups):      return left_indexer, right_indexer def _factorize_keys(lk, rk, sort=True):      lk = extract_array(lk, extract_numpy=True)      rk = extract_array(rk, extract_numpy=True)
class Function(object):              callable_opts.fetch.append(x.name)          callable_opts.target.append(self.updates_op.name)          callable_fn = session._make_callable_from_options(callable_opts)
def test_multiprocessing_fit_error():          for i in range(good_batches):              yield (np.random.randint(batch_size, 256, (50, 2)),                    np.random.randint(batch_size, 2, 50))          raise RuntimeError      model = Sequential()
def to_pickle(obj, path, compression="infer", protocol=pickle.HIGHEST_PROTOCOL):          f.close()          for _f in fh:              _f.close() def read_pickle(path, compression="infer"):      Load pickled pandas object (or any object) from file.
def generate_tokens(readline):                          stashed = tok                          continue                     if token == 'def':                          if (stashed                                  and stashed[0] == NAME                                  and stashed[1] == 'async'):                             async_def = True                             async_def_indent = indents[-1]                              yield (ASYNC, stashed[1],                                     stashed[2], stashed[3],
def match(command):  def get_new_command(command):     return '{} -d {}'.format(command.script, _zip_file(command)[:-4])  def side_effect(old_cmd, command):
class BaseGrouper:                  pass              else:                  raise             return self._aggregate_series_pure_python(obj, func)      def _aggregate_series_fast(self, obj, func):          func = self._is_builtin_func(func)
import numpy as np  from pandas._libs import NaT, Period, Timestamp, index as libindex, lib, tslib as libts  from pandas._libs.tslibs import fields, parsing, timezones from pandas._typing import Label  from pandas.util._decorators import cache_readonly from pandas.core.dtypes.common import _NS_DTYPE, is_float, is_integer, is_scalar  from pandas.core.dtypes.missing import is_valid_nat_for_dtype  from pandas.core.arrays.datetimes import DatetimeArray, tz_to_dtype
class YAxis(Axis):      def get_minpos(self):          return self.axes.dataLim.minposy      def set_default_intervals(self):          ymin, ymax = 0., 1.
def parse_iso8601(date_str, delimiter='T'):          return None      m = re.search(         r'Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',          date_str)      if not m:          timezone = datetime.timedelta()
from .logs import debug  Command = namedtuple('Command', ('script', 'stdout', 'stderr')) CorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))   Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',                             'enabled_by_default', 'side_effect',                             'priority', 'requires_output'))
def text_to_word_sequence(text,      if lower:          text = text.lower()     if sys.version_info < (3,) and isinstance(text, unicode):         translate_map = dict((ord(c), unicode(split)) for c in filters)      else:         translate_map = maketrans(filters, split * len(filters))     text = text.translate(translate_map)      seq = text.split(split)      return [i for i in seq if i]
class Conv2DTranspose(Conv2D):                                                          stride_h,                                                          kernel_h,                                                          self.padding,                                                         out_pad_h)          output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],                                                          stride_w,                                                          kernel_w,                                                          self.padding,                                                         out_pad_w)          return tuple(output_shape)      def get_config(self):          config = super(Conv2DTranspose, self).get_config()         config.pop('dilation_rate')          config['output_padding'] = self.output_padding          return config
def crosstab(          **kwargs,      )      if normalize is not False:          table = _normalize(
def init_ndarray(values, index, columns, dtype=None, copy=False):          return arrays_to_mgr([values], columns, index, columns, dtype=dtype)      elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):          if columns is None:             columns = [0]         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)
class Axes(_AxesBase):      @_preprocess_data(replace_names=["x", "ymin", "ymax", "colors"],                        label_namer="x")     def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',                 label='', **kwargs):          Plot vertical lines.
def get_source_from_frame(frame):      if isinstance(source[0], bytes):         encoding = 'ascii'          for line in source[:2]:
class DataFrameGroupBy(GroupBy):                          result = type(block.values)._from_sequence(                              result.ravel(), dtype=block.values.dtype                          )                     except ValueError:                          result = result.reshape(1, -1)
import base64  import io  import itertools  import os from struct import unpack, pack  import time  import xml.etree.ElementTree as etree  from .common import FileDownloader  from .http import HttpFD  from ..utils import (      compat_urllib_request,      compat_urlparse,      format_bytes,
def parse_age_limit(s):  def strip_jsonp(code):     return re.sub(r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?\s*$', r'\1', code)  def js_to_json(code):
def _normalize(table, normalize, margins, margins_name="All"):              column_margin = column_margin / column_margin.sum()              table = concat([table, column_margin], axis=1)              table = table.fillna(0)          elif normalize == "index":              index_margin = index_margin / index_margin.sum()              table = table.append(index_margin)              table = table.fillna(0)          elif normalize == "all" or normalize is True:              column_margin = column_margin / column_margin.sum()
class TimeGrouper(Grouper):          binner = labels = date_range(              freq=self.freq,              start=first,              end=last,              tz=ax.tz,              name=ax.name,             ambiguous="infer",              nonexistent="shift_forward",          )
from twisted.python.failure import Failure  from scrapy.utils.defer import mustbe_deferred, defer_result  from scrapy.utils.request import request_fingerprint  from scrapy.utils.misc import arg_to_iter  logger = logging.getLogger(__name__)
class Parameter(object):          if dest is not None:              value = getattr(args, dest, None)              if value:                 self.set_global(self.parse_from_input(param_name, value)) else:                  self.reset_global()
class ArteTVPlus7IE(InfoExtractor):          info = self._download_json(json_url, video_id)          player_info = info['videoJsonPlayer']          info_dict = {              'id': player_info['VID'],              'title': player_info['VTI'],              'description': player_info.get('VDE'),             'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),              'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),          }
def cut(      x = _preprocess_for_cut(x)      x, dtype = _coerce_to_type(x)      if not np.iterable(bins):          if is_scalar(bins) and bins < 1:              raise ValueError("`bins` should be a positive integer.")
class Parser:                  if (new_data == data).all():                      data = new_data                      result = True             except (TypeError, ValueError):                  pass
def format_stdin_to_stdout(      finally:          if write_back == WriteBack.YES:             sys.stdout.write(dst)          elif write_back == WriteBack.DIFF:              src_name = "<stdin>  (original)"              dst_name = "<stdin>  (formatted)"             sys.stdout.write(diff(src, dst, src_name, dst_name))  def format_file_contents(
def parse_url(url, encoding=None):
default 'raise'          from pandas import DataFrame         sarray = fields.build_isocalendar_sarray(self.asi8)          iso_calendar_df = DataFrame(              sarray, columns=["year", "week", "day"], dtype="UInt32"          )
def fit_generator(model,                      cbk.validation_data = val_data          if workers > 0:             if is_sequence:                  enqueuer = OrderedEnqueuer(                      generator,                      use_multiprocessing=use_multiprocessing,
class Rolling(_Rolling_and_Expanding):      def count(self):         if self.is_freq_type:              window_func = self._get_roll_func("roll_count")              return self._apply(window_func, center=self.center, name="count")
def test_multiprocessing_fit_error():  def test_multiprocessing_evaluate_error():      batch_size = 10      good_batches = 3      def custom_generator():          for i in range(good_batches):              yield (np.random.randint(batch_size, 256, (50, 2)),                    np.random.randint(batch_size, 2, 50))          raise RuntimeError      model = Sequential()      model.add(Dense(1, input_shape=(2, )))      model.compile(loss='mse', optimizer='adadelta')     with pytest.raises(StopIteration):          model.evaluate_generator(             custom_generator(), good_batches + 1, 1,             workers=4, use_multiprocessing=True,          )     with pytest.raises(StopIteration):          model.evaluate_generator(              custom_generator(), good_batches + 1, 1,              use_multiprocessing=False,
class APIRouter(routing.Router):                      summary=route.summary,                      description=route.description,                      response_description=route.response_description,                     responses=responses,                      deprecated=route.deprecated,                      methods=route.methods,                      operation_id=route.operation_id,
class ReduceLROnPlateau(Callback):                  self.best = current                  self.wait = 0              elif not self.in_cooldown():                  if self.wait >= self.patience:                      old_lr = float(K.get_value(self.model.optimizer.lr))                      if old_lr > self.min_lr:
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              while steps_done < steps:                  generator_output = next(output_generator)
class Model(Container):                              ' and multiple workers may duplicate your data.'                              ' Please consider using the`keras.utils.Sequence'                              ' class.'))         if is_sequence:             steps = len(generator)          enqueuer = None          try:
def find_hook(hook_name, hooks_dir='hooks'):          logger.debug('No hooks/dir in template_dir')          return None      for hook_file in os.listdir(hooks_dir):          if valid_hook(hook_file, hook_name):             return os.path.abspath(os.path.join(hooks_dir, hook_file))     return None  def run_script(script_path, cwd='.'):
class Model(Container):                                           str(generator_output))                      batch_logs = {}                     if isinstance(x, list):                          batch_size = x[0].shape[0]                      elif isinstance(x, dict):                          batch_size = list(x.values())[0].shape[0]
class Conv2DTranspose(Conv2D):              strides=strides,              padding=padding,              data_format=data_format,              activation=activation,              use_bias=use_bias,              kernel_initializer=kernel_initializer,
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _get_combined_index(              index = index.sort_values()          except TypeError:              pass      return index
class Network(Layer):                  else:                      raise ValueError('Improperly formatted model config.')                  inbound_layer = created_layers[inbound_layer_name]                  if len(inbound_layer._inbound_nodes) <= inbound_node_index:                     add_unprocessed_node(layer, node_data)                     return                  inbound_node = inbound_layer._inbound_nodes[inbound_node_index]                  input_tensors.append(                      inbound_node.output_tensors[inbound_tensor_index])              if input_tensors:
import numpy as np  from pandas.core.dtypes.common import is_list_like import pandas.core.common as com   def cartesian_product(X):
class Parameter(object):          dest = self.parser_dest(param_name, task_name, glob=False)          if dest is not None:              value = getattr(args, dest, None)             params[param_name] = self.parse_from_input(param_name, value)      def set_global_from_args(self, param_name, task_name, args, is_without_section=False):
class WebSocketProtocol13(WebSocketProtocol):          self.write_ping(b"")          self.last_ping = now  class WebSocketClientConnection(simple_httpclient._HTTPConnection):
from pandas.core.dtypes.generic import ABCMultiIndex  from pandas.core.dtypes.missing import notna  from pandas.core.arrays import Categorical  from pandas.core.frame import DataFrame, _shared_docs  from pandas.core.indexes.base import Index  from pandas.core.reshape.concat import concat
import calendar  import contextlib  import ctypes  import datetime
CPU_INFO_TEST_SCENARIOS = [                  '7', 'POWER7 (architected), altivec supported'              ],              'processor_cores': 1,             'processor_count': 16,              'processor_threads_per_core': 1,             'processor_vcpus': 16          },      },      {
class Sequential(Model):                      first_layer = layer.layers[0]                      while isinstance(first_layer, (Model, Sequential)):                          first_layer = first_layer.layers[0]                     batch_shape = first_layer.batch_input_shape                     dtype = first_layer.dtype                  if hasattr(first_layer, 'batch_input_shape'):                      batch_shape = first_layer.batch_input_shape
def infer_dtype_from_scalar(val, pandas_dtype: bool = False):          if lib.is_period(val):              dtype = PeriodDtype(freq=val.freq)              val = val.ordinal      return dtype, val
from pandas.core.internals.construction import (      sanitize_index,      to_arrays,  )  from pandas.core.series import Series  from pandas.io.formats import console, format as fmt
def match(command, settings):  def get_new_command(command, settings):     return 'open http://' + command.script[5:]
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[1:3]          tm.assert_series_equal(result, expected)          result[:] = 1
class FacebookIE(InfoExtractor):              'duration': 38,              'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',          }      }, {          'url': 'https://www.facebook.com/video.php?v=10204634152394104',          'only_matching': True,
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class StackedRNNCells(Layer):         states = []         for cell_states in new_nested_states[::-1]:             states += cell_states         return inputs, states      def build(self, input_shape):          if isinstance(input_shape, list):
class PolarAxes(Axes):      @cbook._delete_parameter("3.3", "args")      @cbook._delete_parameter("3.3", "kwargs")      def draw(self, renderer, *args, **kwargs):          thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)          if thetamin > thetamax:              thetamin, thetamax = thetamax, thetamin
class _Rolling_and_Expanding(_Rolling):              raise ValueError("engine must be either 'numba' or 'cython'")           return self._apply(              apply_func,              center=False,              floor=0,              name=func,              use_numba_cache=engine == "numba",          )      def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):
class Conv2DTranspose(Conv2D):                   padding='valid',                   output_padding=None,                   data_format=None,                   activation=None,                   use_bias=True,                   kernel_initializer='glorot_uniform',
def assert_output(output, expected_entries, prefix=None):      any_mismatch = False      result = ''     template = '\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))      for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=""):          mismatch = not (expected_entry and expected_entry.check(line))          any_mismatch |= mismatch
import asyncio  from asyncio.base_events import BaseEventLoop  from concurrent.futures import Executor, ProcessPoolExecutor from functools import partial  import keyword  import os  from pathlib import Path  import tokenize  import sys  from typing import (     Dict, Generic, Iterable, Iterator, List, Optional, Set, Tuple, Type, TypeVar, Union  )  from attr import dataclass, Factory
def cli_option(params, command_option, param):  def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):      param = params.get(param)      assert isinstance(param, bool)      if separator:          return [command_option + separator + (true_value if param else false_value)]
def any_int_dtype(request):      return request.param  @pytest.fixture(params=ALL_REAL_DTYPES)  def any_real_dtype(request):
class APIRouter(routing.Router):          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,      ) -> None:         route = self.route_class(              path,              endpoint=endpoint,              response_model=response_model,
def serialize_response(              errors.extend(errors_)          if errors:              raise ValidationError(errors)          return jsonable_encoder(              value,              include=include,
class PamdRule(PamdLine):      valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']      def __init__(self, rule_type, rule_control, rule_path, rule_args=None):          self._control = None          self._args = None          self.rule_type = rule_type
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):      _data = None      _engine_type = libindex.PeriodEngine
from __future__ import unicode_literals import re   from .subtitles import SubtitlesInfoExtractor  from .common import ExtractorError  from ..utils import parse_iso8601
class DRTVIE(SubtitlesInfoExtractor):          title = data['Title']          description = data['Description']         timestamp = parse_iso8601(data['CreatedTime'][:-5])          thumbnail = None          duration = None
def rnn(step_function, inputs, initial_states,                  tiled_mask_t = tf.tile(mask_t,                                         tf.stack([1, tf.shape(output)[1]]))                  output = tf.where(tiled_mask_t, output, states[0])                 new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]                  output_ta_t = output_ta_t.write(time, output)                  return (time + 1, output_ta_t) + tuple(new_states)          else:
class inherits(object):          self.task_to_inherit = task_to_inherit      def __call__(self, task_that_inherits):          for param_name, param_obj in self.task_to_inherit.get_params():              if not hasattr(task_that_inherits, param_name):                  setattr(task_that_inherits, param_name, param_obj)          @task._task_wraps(task_that_inherits)         class Wrapped(task_that_inherits):             def clone_parent(_self, **args):                 return _self.clone(cls=self.task_to_inherit, **args)          return Wrapped  class requires(object):
class Series(base.IndexOpsMixin, generic.NDFrame):          if takeable:              return self._values[label]         return self.index.get_value(self, label)      def __setitem__(self, key, value):          key = com.apply_if_callable(key, self)
def _convert_listlike_datetimes(                  return DatetimeIndex(arg, tz=tz, name=name)              except ValueError:                  pass          return arg
class _AxesBase(martist.Artist):          bottom, top = sorted([bottom, top], reverse=bool(reverse))          self._viewLim.intervaly = (bottom, top)          if auto is not None:              self._autoscaleYon = bool(auto)
class NumericIndex(Index):      _is_numeric_dtype = True      def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):           if fastpath is not None:              warnings.warn(                  "The 'fastpath' keyword is deprecated, and will be "
class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):                  raise ValueError(                      "Cannot specify `categories` or `ordered` together with `dtype`."                  )          elif is_categorical(values):
class HttpProxyMiddleware(object):          proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))          if user:             user_pass = '%s:%s' % (unquote(user), unquote(password))              creds = base64.b64encode(user_pass).strip()          else:              creds = None
def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:      ):          for leaf in leaves:              result.append(leaf, preformatted=True)             comment_after = line.comments.get(id(leaf))             if comment_after:                  result.append(comment_after, preformatted=True)      bracket_split_succeeded_or_raise(head, body, tail)      for result in (head, body, tail):
def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,      padding = _preprocess_padding(padding)      if tf_data_format == 'NHWC':          spatial_start_dim = 1         strides = (1, 1) + strides + (1,)      else:          spatial_start_dim = 2         strides = (1, 1, 1) + strides      x = tf.expand_dims(x, spatial_start_dim)      depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)      pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)
def assert_equivalent(src: str, dst: str) -> None:      try:          src_ast = ast.parse(src)      except Exception as exc:         raise AssertionError(f"cannot parse source: {exc}") from None      try:          dst_ast = ast.parse(dst)
def read_pickle(path, compression="infer"):      Parameters      ----------     path : str         File path where the pickled object will be loaded.      compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'         For on-the-fly decompression of on-disk data. If 'infer', then use         gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',         or '.zip' respectively, and no decompression otherwise.         Set to None for no decompression.      Returns      -------
from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers  import pandas._libs.internals as libinternals  from pandas._libs.tslibs import Timedelta, conversion  from pandas._libs.tslibs.timezones import tz_compare  from pandas.util._validators import validate_bool_kwarg  from pandas.core.dtypes.cast import (
class Model(Container):      @interfaces.legacy_generator_methods_support      def fit_generator(self,                        generator,                       steps_per_epoch,                        epochs=1,                        verbose=1,                        callbacks=None,
def _get_comments(group_tasks):  _ORDERED_STATUSES = (      "already_done",      "completed",      "failed",      "scheduling_error",      "still_pending",
class DebugVisitor(Visitor[T]):              out(f" {node.value!r}", fg="blue", bold=False)      @classmethod     def show(cls, code: str) -> None:          v: DebugVisitor[None] = DebugVisitor()         list(v.visit(lib2to3_parse(code)))  KEYWORDS = set(keyword.kwlist)
def diff(arr, n: int, axis: int = 0):      dtype = arr.dtype      is_timedelta = False      if needs_i8_conversion(arr):          dtype = np.float64          arr = arr.view("i8")
from ansible.galaxy.role import GalaxyRole  from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel  from ansible.module_utils.ansible_release import __version__ as ansible_version  from ansible.module_utils._text import to_bytes, to_native, to_text  from ansible.parsing.yaml.loader import AnsibleLoader  from ansible.playbook.role.requirement import RoleRequirement  from ansible.utils.display import Display  from ansible.utils.plugin_docs import get_versioned_doclink  display = Display()  class GalaxyCLI(CLI):
import hmac  import time  import uuid from tornado.concurrent import TracebackFuture, return_future  from tornado import gen  from tornado import httpclient  from tornado import escape
class CombinedDatetimelikeProperties(          orig = data if is_categorical_dtype(data) else None          if orig is not None:             data = Series(orig.values.categories, name=orig.name, copy=False)          if is_datetime64_dtype(data.dtype):              return DatetimeProperties(data, orig)
class EventCollection(LineCollection):          .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py           segment = (lineoffset + linelength / 2.,                     lineoffset - linelength / 2.)         if positions is None or len(positions) == 0:              segments = []         elif hasattr(positions, 'ndim') and positions.ndim > 1:              raise ValueError('positions cannot be an array with more than '                               'one dimension.')          elif (orientation is None or orientation.lower() == 'none' or
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):      @Substitution(klass="PeriodIndex")      @Appender(_shared_docs["searchsorted"])      def searchsorted(self, value, side="left", sorter=None):         if isinstance(value, Period):             if value.freq != self.freq:                 raise raise_on_incompatible(self, value)             value = value.ordinal          elif isinstance(value, str):              try:                 value = Period(value, freq=self.freq).ordinal              except DateParseError:                  raise KeyError(f"Cannot interpret '{value}' as period")         return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)      @property      def is_full(self) -> bool:
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def load_model_from_path(model_path, meta=False, **overrides):      for name in pipeline:          if name not in disable:              config = meta.get("pipeline_args", {}).get(name, {})              factory = factories.get(name, name)              component = nlp.create_pipe(factory, config=config)              nlp.add_pipe(component, name=name)
class Index(IndexOpsMixin, PandasObject):         s = getattr(series, "_values", series)         if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):                   try:                 iloc = self.get_loc(key)                 return s[iloc]             except KeyError:                 if len(self) > 0 and (self.holds_integer() or self.is_boolean()):                     raise                 elif is_integer(key):                     return s[key]          s = com.values_from_object(series)          k = com.values_from_object(key)
def jsonable_encoder(      by_alias: bool = True,      skip_defaults: bool = None,      exclude_unset: bool = False,     include_none: bool = True,      custom_encoder: dict = {},      sqlalchemy_safe: bool = True,  ) -> Any:
class Schema(SchemaBase): not_: Optional[List[SchemaBase]] = PSchema(None, alias="not")      items: Optional[SchemaBase] = None      properties: Optional[Dict[str, SchemaBase]] = None     additionalProperties: Optional[Union[bool, SchemaBase]] = None  class Example(BaseModel):
def run_script_with_context(script_path, cwd, context):      ) as temp:          temp.write(Template(contents).render(**context))     return run_script(temp.name, cwd)  def run_hook(hook_name, project_dir, context):
def _arith_method_FRAME(cls, op, special):      @Appender(doc)      def f(self, other, axis=default_axis, level=None, fill_value=None):         if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):              return _frame_arith_method_with_reindex(self, other, op)          self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)
class Block(PandasObject):          mask = missing.mask_missing(values, to_replace)         if not mask.any():             if inplace:                 return [self]             return [self.copy()]           try:              blocks = self.putmask(mask, value, inplace=inplace)
def reformat_many(          )      finally:          shutdown(loop)         executor.shutdown()  async def schedule_formatting(
class KeyValueType(object):      def __init__(self, *separators):          self.separators = separators      def __call__(self, string):          found = {}          for sep in self.separators:             regex = '[^\\\\]' + sep             match = re.search(regex, string)             if match:                 found[match.start() + 1] = sep          if not found:
class APIRouter(routing.Router):      def add_api_websocket_route(          self, path: str, endpoint: Callable, name: str = None      ) -> None:         route = APIWebSocketRoute(path, endpoint=endpoint, name=name)          self.routes.append(route)      def websocket(self, path: str, name: str = None) -> Callable:
Wild         185.0                      result = coerce_to_dtypes(result, self.dtypes)          if constructor is not None:             result = Series(result, index=labels)          return result      def nunique(self, axis=0, dropna=True) -> Series:
class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):      def __len__(self) -> int:          return len(self._data)      def to_numpy(          self, dtype=None, copy=False, na_value: "Scalar" = lib.no_default,      ):
def iter_sequence_infinite(seq):      while True:          for item in seq:              yield item
class TestReshaping(BaseJSON, base.BaseReshapingTests):          return super().test_unstack(data, index)  class TestGetitem(BaseJSON, base.BaseGetitemTests):      pass
import traceback  from .variables import CommonVariable, Exploding, BaseVariable  from . import utils, pycompat  ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$') def get_local_reprs(frame, watch=()):      code = frame.f_code      vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())     result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]      result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))      result = collections.OrderedDict(result_items)
def _make_getset_interval(method_name, lim_name, attr_name):                  setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),                         ignore=True)              else:                 setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),                         ignore=True)          self.stale = True
class IntervalIndex(IntervalMixin, Index):              left_indexer = self.left.get_indexer(target_as_index.left)              right_indexer = self.right.get_indexer(target_as_index.right)              indexer = np.where(left_indexer == right_indexer, left_indexer, -1)          elif not is_object_dtype(target_as_index):              target_as_index = self._maybe_convert_i8(target_as_index)
class Sequential(Model):                  or (inputs, targets, sample_weights)              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.              max_queue_size: maximum size for the generator queue              workers: maximum number of processes to spin up              use_multiprocessing: if True, use process based threading.
def url_concat(url, args):      >>> url_concat("http://example.com/foo?a=b", [("c", "d"), ("c", "d2")])      'http://example.com/foo?a=b&c=d&c=d2'      parsed_url = urlparse(url)      if isinstance(args, dict):          parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
def conv2d(x, kernel, strides=(1, 1), padding='valid',  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None):
class Block(PandasObject):          transpose = self.ndim == 2          if value is None:              if self.is_numeric:
from six.moves.urllib import robotparser  from scrapy.exceptions import NotConfigured, IgnoreRequest  from scrapy.http import Request  from scrapy.utils.httpobj import urlparse_cached  logger = logging.getLogger(__name__)
class StackedRNNCells(Layer):                  output_dim = cell.state_size[0]              else:                  output_dim = cell.state_size             input_shape = (input_shape[0], input_shape[1], output_dim)          self.built = True      def get_config(self):
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _process_wp_text(article_title, article_text, wp_to_id):          return None, None     text_search = text_regex.search(article_text)      if text_search is None:          return None, None      text = text_search.group(0)
class Path:                  codes[i:i + len(path.codes)] = path.codes              i += len(path.vertices)          return cls(vertices, codes)      def __repr__(self):
from pandas.core.dtypes.cast import (  )  from pandas.core.dtypes.common import (      ensure_platform_int,      is_datetime64tz_dtype,      is_datetime_or_timedelta_dtype,      is_dtype_equal,
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):          raise raise_on_incompatible(self, None)
def test_check_mutually_exclusive_found(mutually_exclusive_terms):          'fox': 'red',          'socks': 'blue',      }     expected = "TypeError('parameters are mutually exclusive: string1|string2, box|fox|socks',)"      with pytest.raises(TypeError) as e:          check_mutually_exclusive(mutually_exclusive_terms, params)         assert e.value == expected  def test_check_mutually_exclusive_none():
class Categorical(ExtensionArray, PandasObject):          code_values = code_values[null_mask | (code_values >= 0)]          return algorithms.isin(self.codes, code_values)
class DataFrame(NDFrame):                      " or if the Series has a name"                  )             if other.name is None:                 index = None             else:                   index = Index([other.name], name=self.index.name)               idx_diff = other.index.difference(self.columns)              try:                  combined_columns = self.columns.append(idx_diff)              except TypeError:                  combined_columns = self.columns.astype(object).append(idx_diff)             other = other.reindex(combined_columns, copy=False)             other = DataFrame(                 other.values.reshape((1, len(other))),                 index=index,                 columns=combined_columns,              )             other = other._convert(datetime=True, timedelta=True)              if not self.columns.equals(combined_columns):                  self = self.reindex(columns=combined_columns)          elif isinstance(other, list):
def get_all_executables():      tf_entry_points = ['thefuck', 'fuck']      bins = [exe.name.decode('utf8') if six.PY2 else exe.name             for path in os.environ.get('PATH', '').split(':')              for exe in _safe(lambda: list(Path(path).iterdir()), [])              if not _safe(exe.is_dir, True)              and exe.name not in tf_entry_points]
class Index(IndexOpsMixin, PandasObject):              return other.equals(self)         try:             return array_equivalent(                 com.values_from_object(self), com.values_from_object(other)             )         except Exception:             return False      def identical(self, other):
import re  import numpy as np  import pytest from pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range  from pandas.core.indexes.base import InvalidIndexError  import pandas.util.testing as tm
class SymLogNorm(Normalize):          with np.errstate(invalid="ignore"):              masked = np.abs(a) > self.linthresh          sign = np.sign(a[masked])         log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))          log *= sign * self.linthresh          a[masked] = log          a[~masked] *= self._linscale_adj
def mask_zero_div_zero(x, y, result):          return result      if zmask.any():         shape = result.shape          zneg_mask = zmask & np.signbit(y)          zpos_mask = zmask & ~zneg_mask         nan_mask = (zmask & (x == 0)).ravel()          with np.errstate(invalid="ignore"):             neginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()             posinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()          if nan_mask.any() or neginf_mask.any() or posinf_mask.any():             result = result.astype("float64", copy=False).ravel()              np.putmask(result, nan_mask, np.nan)             np.putmask(result, posinf_mask, np.inf)             np.putmask(result, neginf_mask, -np.inf)             result = result.reshape(shape)      return result
def _match_one(filter_part, dct):      if m:          op = COMPARISON_OPERATORS[m.group('op')]          actual_value = dct.get(m.group('key'))         if (m.group('strval') is not None or
from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray  from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin  from pandas.core.base import IndexOpsMixin  import pandas.core.indexes.base as ibase from pandas.core.indexes.base import Index, _index_shared_docs  from pandas.core.indexes.extension import (      ExtensionIndex,      inherit_names,
class Line:      depth: int = 0      leaves: List[Leaf] = Factory(list)     comments: Dict[LeafID, Leaf] = Factory(dict)      bracket_tracker: BracketTracker = Factory(BracketTracker)      inside_brackets: bool = False      has_for: bool = False
commands:      - string  from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.connection import exec_command from ansible.module_utils.network.ios.ios import load_config  from ansible.module_utils.network.ios.ios import ios_argument_spec import re  def map_obj_to_commands(updates, module):
from pandas.core.dtypes.common import (      is_datetime64_dtype,      is_datetime64tz_dtype,      is_datetime_or_timedelta_dtype,      is_integer,      is_list_like,      is_scalar,      is_timedelta64_dtype,
class Line:          return False     def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:         if not (          if comment.type != token.COMMENT:              return False         try:             after = id(self.last_non_delimiter())         except LookupError:              comment.type = STANDALONE_COMMENT              comment.prefix = ''              return False          else:             if after in self.comments:                 self.comments[after].value += str(comment)             else:                 self.comments[after] = comment              return True     def last_non_delimiter(self) -> Leaf:         raise LookupError("No non-delimiters found")
class Zsh(Generic):      @memoize      def get_aliases(self):         raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\n')          return dict(self._parse_alias(alias)                      for alias in raw_aliases if alias and '=' in alias)
class Sequential(Model):      @interfaces.legacy_generator_methods_support      def fit_generator(self, generator,                       steps_per_epoch,                        epochs=1,                        verbose=1,                        callbacks=None,
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
import re  import os from thefuck.utils import memoize  from thefuck import shells  patterns = (          '^    at {file}:{line}:{col}',
def container_of(leaf: Leaf) -> LN:          if parent.children[0].prefix != same_prefix:              break          if parent.type in SURROUNDED_BY_BRACKETS:              break
def format_file_in_place(          if lock:              lock.acquire()          try:             sys.stdout.write(diff_contents)          finally:              if lock:                  lock.release()
def _coerce_to_type(x):      elif is_timedelta64_dtype(x):          x = to_timedelta(x)          dtype = np.dtype("timedelta64[ns]")      if dtype is not None:
class RangeIndex(Int64Index):     @staticmethod     def _validate_dtype(dtype):
class scheduler(Config):      disable_window = parameter.IntParameter(default=3600,                                              config_path=dict(section='scheduler', name='disable-window-seconds'))     disable_failures = parameter.IntParameter(default=None,                                                config_path=dict(section='scheduler', name='disable-num-failures'))     disable_hard_timeout = parameter.IntParameter(default=None,                                                    config_path=dict(section='scheduler', name='disable-hard-timeout'))      disable_persist = parameter.IntParameter(default=86400,                                               config_path=dict(section='scheduler', name='disable-persist-seconds'))
class Task(object):          exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)         positional_params = [(n, p) for n, p in params if p.significant]          for i, arg in enumerate(args):              if i >= len(positional_params):                  raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))
class RobotsTxtMiddleware(object):          if failure.type is not IgnoreRequest:              logger.error("Error downloading %(request)s: %(f_exception)s",                           {'request': request, 'f_exception': failure.value},                          extra={'spider': spider, 'failure': failure})      def _parse_robots(self, response):          rp = robotparser.RobotFileParser(response.url)
def evaluate_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
def dump_to_file(*output: str) -> str:      import tempfile      with tempfile.NamedTemporaryFile(         mode="w", prefix="blk_", suffix=".log", delete=False      ) as f:          for lines in output:              f.write(lines)
class MonthOffset(SingleConstructorOffset):      @apply_index_wraps      def apply_index(self, i):          shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)           return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)  class MonthEnd(MonthOffset):
class HTTP1Connection(httputil.HTTPConnection):          if content_length is not None:              return self._read_fixed_body(content_length, delegate)         if headers.get("Transfer-Encoding") == "chunked":              return self._read_chunked_body(delegate)          if self.is_client:              return self._read_body_until_close(delegate)
class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):      @staticmethod      def get_target_path(target):          if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):              return target.path          elif isinstance(target, bigquery.BigQueryTarget):             "{}:{}.{}".format(target.project_id, target.dataset_id, target.table_id)          else:             raise ValueError("Target not supported")
class Axes(_AxesBase):          if not np.iterable(xmax):              xmax = [xmax]         y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)           y = np.ravel(y)         xmin = np.resize(xmin, y.shape)         xmax = np.resize(xmax, y.shape)         verts = [((thisxmin, thisy), (thisxmax, thisy))                  for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]         lines = mcoll.LineCollection(verts, colors=colors,                                       linestyles=linestyles, label=label)          self.add_collection(lines, autolim=False)          lines.update(kwargs)
def _match_one(filter_part, dct):      m = operator_rex.search(filter_part)      if m:          op = COMPARISON_OPERATORS[m.group('op')]         if m.group('strval') is not None:              if m.group('op') not in ('=', '!='):                  raise ValueError(                      'Operator %s does not support string values!' % m.group('op'))             comparison_value = m.group('strval')          else:              try:                  comparison_value = int(m.group('intval'))
def update(x, new_x):          The variable `x` updated.     return tf_state_ops.assign(x, new_x)  @symbolic
class EarlyStopping(Callback):                   patience=0,                   verbose=0,                   mode='auto',                  baseline=None):          super(EarlyStopping, self).__init__()          self.monitor = monitor
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):          if isinstance(key, str):              try:                 return self._get_string_slice(key)             except (TypeError, KeyError, ValueError, OverflowError):                  pass              try:                  asdt, reso = parse_time_string(key, self.freq)                 key = asdt              except DateParseError:                  raise KeyError(f"Cannot interpret '{key}' as period")          elif is_integer(key):              raise KeyError(key)
class Base:          assert not indices.equals(np.array(indices))         if not isinstance(indices, RangeIndex):              same_values = Index(indices, dtype=object)              assert indices.equals(same_values)              assert same_values.equals(indices)
class HTTPBearer(HTTPBase):              else:                  return None          if scheme.lower() != "bearer":             raise HTTPException(                 status_code=HTTP_403_FORBIDDEN,                 detail="Invalid authentication credentials",             )          return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)
class Model(Container):              epoch_logs = {}              while epoch < epochs:                 for m in self.metrics:                     if isinstance(m, Layer) and m.stateful:                         m.reset_states()                  callbacks.on_epoch_begin(epoch)                  steps_done = 0                  batch_index = 0
class SimpleTaskState(object):      def get_necessary_tasks(self):          necessary_tasks = set()          for task in self.get_active_tasks():             if task.status not in (DONE, DISABLED) or \                     getattr(task, 'scheduler_disable_time', None) is not None:                  necessary_tasks.update(task.deps)                  necessary_tasks.add(task.id)          return necessary_tasks
class SortedCorrectedCommandsSequence(object):              return []          for command in self._commands:             if command.script != first.script or \                             command.side_effect != first.side_effect:                  return [first, command]          return [first]      def _remove_duplicates(self, corrected_commands):
class Tracer:              prefix='',              overwrite=False,              thread_info=False,      ):          self._write = get_write_function(output, overwrite)
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()         mat = numeric_df.values          if notna(mat).all():              if min_periods is not None and min_periods > len(mat):                 baseCov = np.empty((mat.shape[1], mat.shape[1]))                 baseCov.fill(np.nan)              else:                 baseCov = np.cov(mat.T)             baseCov = baseCov.reshape((len(cols), len(cols)))          else:             baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)         return self._constructor(baseCov, index=idx, columns=cols)      def corrwith(self, other, axis=0, drop=False, method="pearson") -> Series:
class SimpleTaskState(object):                  self.re_enable(task)             elif task.scheduler_disable_time is not None:                  return          if new_status == FAILED and task.can_disable() and task.status != DISABLED:
def _use_inf_as_na(key):  def _isna_ndarraylike(obj):     is_extension = is_extension_array_dtype(obj)      if not is_extension:           values = getattr(obj, "_values", obj)     else:         values = obj       dtype = values.dtype      if is_extension:         if isinstance(obj, (ABCIndexClass, ABCSeries)):             values = obj._values         else:             values = obj          result = values.isna()     elif isinstance(obj, ABCDatetimeArray):         return obj.isna()      elif is_string_dtype(dtype):          shape = values.shape          if is_string_like_dtype(dtype):              result = np.zeros(values.shape, dtype=bool)         else:              result = np.empty(shape, dtype=bool)             vec = libmissing.isnaobj(values.ravel())             result[...] = vec.reshape(shape)      elif needs_i8_conversion(dtype):
class Line:              return False          if closing.type == token.RBRACE:             self.leaves.pop()              return True          if closing.type == token.RSQB:              comma = self.leaves[-1]              if comma.parent and comma.parent.type == syms.listmaker:                 self.leaves.pop()                  return True
class Bidirectional(Wrapper):              return [output_shape] + state_shape + copy.copy(state_shape)          return output_shape      def call(self, inputs, training=None, mask=None, initial_state=None):          kwargs = {}          if has_arg(self.layer.call, 'training'):
class Axes(_AxesBase):      @_preprocess_data(replace_names=["y", "xmin", "xmax", "colors"],                        label_namer="y")     def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',                 label='', **kwargs):          Plot horizontal lines at each *y* from *xmin* to *xmax*.
class StackedRNNCells(Layer):                      cell.build([input_shape] + constants_shape)                  else:                      cell.build(input_shape)             if hasattr(cell.state_size, '__len__'):                  output_dim = cell.state_size[0]              else:                  output_dim = cell.state_size
class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass): rules = []      start_date = Timestamp(datetime(1970, 1, 1))     end_date = Timestamp(datetime(2030, 12, 31))      _cache = None      def __init__(self, name=None, rules=None):
class XAxis(Axis):      def get_minpos(self):          return self.axes.dataLim.minposx      def set_default_intervals(self):          xmin, xmax = 0., 1.
from thefuck.utils import eager  @git_support  def match(command):      return ("fatal: A branch named '" in command.output             and " already exists." in command.output)  @git_support  @eager  def get_new_command(command):      branch_name = re.findall(         r"fatal: A branch named '([^']*)' already exists.", command.output)[0]      new_command_templates = [['git branch -d {0}', 'git branch {0}'],                               ['git branch -d {0}', 'git checkout -b {0}'],                               ['git branch -D {0}', 'git branch {0}'],
def _isna_old(obj):          raise NotImplementedError("isna is not defined for MultiIndex")      elif isinstance(obj, type):          return False     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):          return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCGeneric):          return obj._constructor(obj._data.isna(func=_isna_old))
def _get_clickable(clickdata, form):      clickables = [          el for el in form.xpath(             'descendant::*[(self::input or self::button)'             ' and re:test(@type, "^submit$", "i")]'             '|descendant::button[not(@type)]',              namespaces={"re": "http://exslt.org/regular-expressions"})          ]      if not clickables:
class DataFrame(NDFrame):              return ops.dispatch_to_series(this, other, _arith_op)          else:             result = _arith_op(this.values, other.values)              return self._constructor(                  result, index=new_index, columns=new_columns, copy=False              )
def _recast_datetimelike_result(result: DataFrame) -> DataFrame:      result = result.copy()      obj_cols = [         idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])      ]
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):      @Appender(_index_shared_docs["repeat"] % _index_doc_kwargs)      def repeat(self, repeats, axis=None):          nv.validate_repeat(tuple(), dict(axis=axis))         freq = self.freq if is_period_dtype(self) else None         return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)      @Appender(_index_shared_docs["where"] % _index_doc_kwargs)      def where(self, cond, other=None):
def _make_getset_interval(method_name, lim_name, attr_name):                  setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),                         ignore=True)              else:                 setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),                         ignore=True)          self.stale = True
class IOLoop(Configurable):                  from tornado.process import cpu_count                  self._executor = ThreadPoolExecutor(max_workers=(cpu_count() * 5))              executor = self._executor          return executor.submit(func, *args)      def set_default_executor(self, executor):
def str_or_none(v, default=None):  def str_to_int(int_str):     if int_str is None:         return None      int_str = re.sub(r'[,\.\+]', '', int_str)      return int(int_str)
class SymLogNorm(Normalize):          masked = np.abs(a) > (self.linthresh * self._linscale_adj)          sign = np.sign(a[masked])         exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)          exp *= sign * self.linthresh          a[masked] = exp          a[~masked] /= self._linscale_adj
def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:          model_config=BaseConfig,          class_validators={},          alias="body",         schema=BodySchema(None),      )      return field
class Categorical(ExtensionArray, PandasObject):          inplace = validate_bool_kwarg(inplace, "inplace")          cat = self if inplace else self.copy()         if to_replace in cat.categories:             if isna(value):                 cat.remove_categories(to_replace, inplace=True)             else:                  categories = cat.categories.tolist()                 index = categories.index(to_replace)                 if value in cat.categories:                     value_index = categories.index(value)                      cat._codes[cat._codes == index] = value_index                     cat.remove_categories(to_replace, inplace=True)                  else:                     categories[index] = value                      cat.rename_categories(categories, inplace=True)          if not inplace:              return cat
def get_weighted_roll_func(cfunc: Callable) -> Callable:  def validate_baseindexer_support(func_name: Optional[str]) -> None:      BASEINDEXER_WHITELIST = {          "min",          "max",          "mean",
import re  import shlex  import pkgutil  import xml.etree.ElementTree as ET  from ansible.errors import AnsibleError  from ansible.module_utils._text import to_bytes, to_text
class tqdm(Comparable):          self.start_t = self.last_print_t      def __len__(self):          return self.total if self.iterable is None else \              (self.iterable.shape[0] if hasattr(self.iterable, "shape")
import warnings  import numpy as np from pandas._libs import NaT, algos as libalgos, lib, tslib, writers  from pandas._libs.index import convert_scalar  import pandas._libs.internals as libinternals  from pandas._libs.tslibs import Timedelta, conversion
class MediaPipeline(object):                      logger.error(                          '%(class)s found errors processing %(item)s',                          {'class': self.__class__.__name__, 'item': item},                         extra={'spider': info.spider, 'failure': value}                      )          return item
def _convert_by(by):  @Substitution("\ndata : DataFrame")  @Appender(_shared_docs["pivot"], indents=1)  def pivot(data: "DataFrame", index=None, columns=None, values=None) -> "DataFrame":      if values is None:          cols = [columns] if index is None else [index, columns]          append = index is None
class Driver(object):                      current_line = ""                      current_column = 0                      wait_for_nl = False             elif char == ' ':                  current_column += 1             elif char == '\t':                 current_column += 4              elif char == '\n':                  current_column = 0
from pandas.core.dtypes.common import (      is_float_dtype,      is_integer_dtype,      is_scalar,      needs_i8_conversion,      pandas_dtype,  )
class _AtIndexer(_ScalarAccessIndexer):          Require they keys to be the same type as the index. (so we don't          fallback)          if is_setter:              return list(key)
def run(cmdline_args=None, main_task_cls=None,      :param use_dynamic_argparse:      :param local_scheduler:      if use_dynamic_argparse:          interface = DynamicArgParseInterface()      else:
class Block(PandasObject):              values[indexer] = value            elif (             len(arr_value.shape)             and arr_value.shape[0] == values.shape[0]             and arr_value.size == values.size          ):              values[indexer] = value              try:                  values = values.astype(arr_value.dtype)              except ValueError:
class Text(Artist):          if self._renderer is None:              raise RuntimeError('Cannot get window extent w/o renderer')         bbox, info, descent = self._get_layout(self._renderer)         x, y = self.get_unitless_position()         x, y = self.get_transform().transform((x, y))         bbox = bbox.translated(x, y)         if dpi is not None:             self.figure.dpi = dpi_orig         return bbox      def set_backgroundcolor(self, color):
def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:      container: Optional[LN] = container_of(leaf)      while container is not None and container.type != token.ENDMARKER:         is_fmt_on = False         for comment in list_comments(container.prefix, is_endmarker=False):             if comment.value in FMT_ON:                 is_fmt_on = True             elif comment.value in FMT_OFF:                 is_fmt_on = False         if is_fmt_on:              return         yield container         container = container.next_sibling  def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:
class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):              arglist.append('{}@{}'.format(username, host))          else:              arglist = []             if not job.jar() or not os.path.exists(job.jar()):                  logger.error("Can't find jar: %s, full path %s", job.jar(), os.path.abspath(job.jar()))                  raise HadoopJarJobError("job jar does not exist")
def test_timedelta_cut_roundtrip():          ["0 days 23:57:07.200000", "2 days 00:00:00", "3 days 00:00:00"]      )      tm.assert_index_equal(result_bins, expected_bins)
class GalaxyCLI(CLI):          super(GalaxyCLI, self).init_parser(             desc="Perform various Role related operations.",          )
class TimedeltaIndex(          if self[0] <= other[0]:              left, right = self, other          else:              left, right = other, self
def gunzip(data):              if output or getattr(f, 'extrabuf', None):                  try:                     output += f.extrabuf                  finally:                      break              else:
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def bracket_split_build_line(          if leaves:              normalize_prefix(leaves[0], inside_brackets=True)               if original.is_import:                  for i in range(len(leaves) - 1, -1, -1):                      if leaves[i].type == STANDALONE_COMMENT:                          continue
class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):      def requestTunnel(self, protocol):         tunnelReq = 'CONNECT %s:%s HTTP/1.1\r\n' % (self._tunneledHost,                                                   self._tunneledPort)          if self._proxyAuthHeader:             tunnelReq += 'Proxy-Authorization: %s\r\n' % self._proxyAuthHeader         tunnelReq += '\r\n'          protocol.transport.write(tunnelReq)          self._protocolDataReceived = protocol.dataReceived          protocol.dataReceived = self.processProxyResponse
class Network(Layer):          while unprocessed_nodes:              for layer_data in config['layers']:                  layer = created_layers[layer_data['name']]                  if layer in unprocessed_nodes:                     for node_data in unprocessed_nodes.pop(layer):                         process_node(layer, node_data)          name = config.get('name')          input_tensors = []          output_tensors = []
from ..utils import (      compat_urllib_error,      compat_urllib_parse,      compat_urllib_request,      ExtractorError,  )
class DatetimeLikeArrayMixin(              return None          return self.freq.freqstr     @property     def inferred_freq(self):         if self.ndim != 1:             return None         try:             return frequencies.infer_freq(self)         except ValueError:             return None      @property     def _resolution(self):         return frequencies.Resolution.get_reso_from_freq(self.freqstr)      @property     def resolution(self):         return frequencies.Resolution.get_str(self._resolution)      @classmethod     def _validate_frequency(cls, index, freq, **kwargs):         if is_period_dtype(cls):              return None          inferred = index.inferred_freq         if index.size == 0 or inferred == freq.freqstr:             return None          try:             on_freq = cls._generate_range(                 start=index[0], end=None, periods=len(index), freq=freq, **kwargs             )             if not np.array_equal(index.asi8, on_freq.asi8):                 raise ValueError         except ValueError as e:             if "non-fixed" in str(e):                   raise e                  raise ValueError(                 f"Inferred frequency {inferred} from passed values "                 f"does not conform to passed frequency {freq.freqstr}"             ) from e         @property     def _is_monotonic_increasing(self):         return algos.is_monotonic(self.asi8, timelike=True)[0]      @property     def _is_monotonic_decreasing(self):         return algos.is_monotonic(self.asi8, timelike=True)[1]      @property     def _is_unique(self):         return len(unique1d(self.asi8)) == len(self)        _create_comparison_method = classmethod(_datetimelike_array_cmp)        __pow__ = make_invalid_op("__pow__")     __rpow__ = make_invalid_op("__rpow__")     __mul__ = make_invalid_op("__mul__")     __rmul__ = make_invalid_op("__rmul__")     __truediv__ = make_invalid_op("__truediv__")     __rtruediv__ = make_invalid_op("__rtruediv__")     __floordiv__ = make_invalid_op("__floordiv__")     __rfloordiv__ = make_invalid_op("__rfloordiv__")     __mod__ = make_invalid_op("__mod__")     __rmod__ = make_invalid_op("__rmod__")     __divmod__ = make_invalid_op("__divmod__")     __rdivmod__ = make_invalid_op("__rdivmod__")      def _add_datetimelike_scalar(self, other):          raise TypeError(f"cannot add {type(self).__name__} and {type(other).__name__}")      _add_datetime_arraylike = _add_datetimelike_scalar      def _sub_datetimelike_scalar(self, other):          assert other is not NaT         raise TypeError(f"cannot subtract a datelike from a {type(self).__name__}")      _sub_datetime_arraylike = _sub_datetimelike_scalar      def _sub_period(self, other):          raise TypeError(f"cannot subtract Period from a {type(self).__name__}")      def _add_offset(self, offset):         raise AbstractMethodError(self)      def _add_timedeltalike_scalar(self, other):         if isna(other):              new_values = np.empty(self.shape, dtype="i8")             new_values[:] = iNaT             return type(self)(new_values, dtype=self.dtype)          inc = delta_to_nanoseconds(other)         new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(             "i8"         )         new_values = self._maybe_mask_results(new_values)          new_freq = None         if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):              new_freq = self.freq          return type(self)(new_values, dtype=self.dtype, freq=new_freq)     def _add_timedelta_arraylike(self, other):           if len(self) != len(other):             raise ValueError("cannot add indices of unequal length")         if isinstance(other, np.ndarray):              from pandas.core.arrays import TimedeltaArray             other = TimedeltaArray._from_sequence(other)         self_i8 = self.asi8         other_i8 = other.asi8         new_values = checked_add_with_arr(             self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan         )         if self._hasnans or other._hasnans:             mask = (self._isnan) | (other._isnan)             new_values[mask] = iNaT         return type(self)(new_values, dtype=self.dtype)     def _add_nat(self):         if is_period_dtype(self):             raise TypeError(                 f"Cannot add {type(self).__name__} and {type(NaT).__name__}"             )            result = np.zeros(self.shape, dtype=np.int64)         result.fill(iNaT)         return type(self)(result, dtype=self.dtype, freq=None)     def _sub_nat(self):               result = np.zeros(self.shape, dtype=np.int64)         result.fill(iNaT)         return result.view("timedelta64[ns]")      def _sub_period_array(self, other):         if not is_period_dtype(self):             raise TypeError(                 f"cannot subtract {other.dtype}-dtype from {type(self).__name__}"             )         if self.freq != other.freq:             msg = DIFFERENT_FREQ.format(                 cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr             )             raise IncompatibleFrequency(msg)         new_values = checked_add_with_arr(             self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan         )         new_values = np.array([self.freq.base * x for x in new_values])         if self._hasnans or other._hasnans:             mask = (self._isnan) | (other._isnan)             new_values[mask] = NaT         return new_values     def _addsub_object_array(self, other: np.ndarray, op):         assert op in [operator.add, operator.sub]         if len(other) == 1:             return op(self, other[0])          warnings.warn(             "Adding/subtracting array of DateOffsets to "             f"{type(self).__name__} not vectorized",             PerformanceWarning,         )          assert self.shape == other.shape, (self.shape, other.shape)          res_values = op(self.astype("O"), np.array(other))         result = array(res_values.ravel())         result = extract_array(result, extract_numpy=True).reshape(self.shape)         return result     def _time_shift(self, periods, freq=None):         if freq is not None and freq != self.freq:             if isinstance(freq, str):                 freq = frequencies.to_offset(freq)             offset = periods * freq             result = self + offset              return result         if periods == 0:              return self.copy()         if self.freq is None:             raise NullFrequencyError("Cannot shift with no freq")         start = self[0] + periods * self.freq         end = self[-1] + periods * self.freq            return self._generate_range(start=start, end=end, periods=None, freq=self.freq)     @unpack_zerodim_and_defer("__add__")     def __add__(self, other):          if other is NaT:             result = self._add_nat()         elif isinstance(other, (Tick, timedelta, np.timedelta64)):             result = self._add_timedeltalike_scalar(other)         elif isinstance(other, DateOffset):              result = self._add_offset(other)         elif isinstance(other, (datetime, np.datetime64)):             result = self._add_datetimelike_scalar(other)         elif lib.is_integer(other):               if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._time_shift(other)           elif is_timedelta64_dtype(other):              result = self._add_timedelta_arraylike(other)         elif is_object_dtype(other):              result = self._addsub_object_array(other, operator.add)         elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):              return self._add_datetime_arraylike(other)         elif is_integer_dtype(other):             if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._addsub_int_array(other, operator.add)         else:                  return NotImplemented         if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):             from pandas.core.arrays import TimedeltaArray             return TimedeltaArray(result)         return result     def __radd__(self, other):          return self.__add__(other)     @unpack_zerodim_and_defer("__sub__")     def __sub__(self, other):          if other is NaT:             result = self._sub_nat()         elif isinstance(other, (Tick, timedelta, np.timedelta64)):             result = self._add_timedeltalike_scalar(-other)         elif isinstance(other, DateOffset):              result = self._add_offset(-other)         elif isinstance(other, (datetime, np.datetime64)):             result = self._sub_datetimelike_scalar(other)         elif lib.is_integer(other):               if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._time_shift(-other)          elif isinstance(other, Period):             result = self._sub_period(other)           elif is_timedelta64_dtype(other):              result = self._add_timedelta_arraylike(-other)         elif is_object_dtype(other):              result = self._addsub_object_array(other, operator.sub)         elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):              result = self._sub_datetime_arraylike(other)         elif is_period_dtype(other):              result = self._sub_period_array(other)         elif is_integer_dtype(other):             if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._addsub_int_array(other, operator.sub)          else:              return NotImplemented         if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):             from pandas.core.arrays import TimedeltaArray             return TimedeltaArray(result)         return result     def __rsub__(self, other):         if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):               if lib.is_scalar(other):                  return Timestamp(other) - self             if not isinstance(other, DatetimeLikeArrayMixin):                  from pandas.core.arrays import DatetimeArray                  other = DatetimeArray(other)             return other - self         elif (             is_datetime64_any_dtype(self.dtype)             and hasattr(other, "dtype")             and not is_datetime64_any_dtype(other.dtype)         ):               raise TypeError(                 f"cannot subtract {type(self).__name__} from {type(other).__name__}"             )         elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):              raise TypeError(f"cannot subtract {type(self).__name__} from {other.dtype}")         elif is_timedelta64_dtype(self.dtype):             if lib.is_integer(other) or is_integer_dtype(other):                   return -(self - other)              return (-self) + other          return -(self - other)      def __iadd__(self, other):         result = self + other         self[:] = result[:]          if not is_period_dtype(self):              self._freq = result._freq         return self      def __isub__(self, other):         result = self - other         self[:] = result[:]          if not is_period_dtype(self):              self._freq = result._freq         return self         def _reduce(self, name, axis=0, skipna=True, **kwargs):         op = getattr(self, name, None)         if op:             return op(skipna=skipna, **kwargs)          else:             return super()._reduce(name, skipna, **kwargs)     def min(self, axis=None, skipna=True, *args, **kwargs):         nv.validate_min(args, kwargs)         nv.validate_minmax_axis(axis)         Return the maximum value of the Array or maximum along         an axis.         See Also         --------         numpy.ndarray.max         Index.max : Return the maximum value in an Index.         Series.max : Return the maximum value in a Series.         Return the mean value of the Array.          .. versionadded:: 0.25.0          Parameters          ----------         skipna : bool, default True             Whether to ignore any NaT elements.          Returns          -------         scalar             Timestamp or Timedelta.          See Also         --------         numpy.ndarray.mean : Returns the average of array elements along a given axis.         Series.mean : Return the mean value in a Series.          Notes         -----         mean is only defined for Datetime and Timedelta dtypes, not for Period.     If a `periods` argument is passed to the Datetime/Timedelta Array/Index     constructor, cast it to an integer.      Parameters     ----------     periods : None, float, int      Returns     -------     periods : None or int      Raises     ------     TypeError         if periods is None, float, or int     Check that the `closed` argument is among [None, "left", "right"]      Parameters     ----------     closed : {None, "left", "right"}      Returns     -------     left_closed : bool     right_closed : bool      Raises     ------     ValueError : if argument is not among valid values     If the user passes a freq and another freq is inferred from passed data,     require that they match.      Parameters     ----------     freq : DateOffset or None     inferred_freq : DateOffset or None     freq_infer : bool      Returns     -------     freq : DateOffset or None     freq_infer : bool      Notes     -----     We assume at this point that `maybe_infer_freq` has been called, so     `freq` is either a DateOffset object or None. def maybe_infer_freq(freq):     freq_infer = False     if not isinstance(freq, DateOffset):          if freq != "infer":             freq = frequencies.to_offset(freq)         else:             freq_infer = True             freq = None     return freq, freq_infer
class YoutubeDL(object):                  format_spec = selector.selector                  def selector_function(formats):                      if format_spec == 'all':                          for f in formats:                              yield f
class DistributionFiles:          elif 'SteamOS' in data:              debian_facts['distribution'] = 'SteamOS'         elif path == '/etc/lsb-release' and 'Kali' in data:              debian_facts['distribution'] = 'Kali'              release = re.search('DISTRIB_RELEASE=(.*)', data)              if release:
def _get_combined_index(          calculate the union.      sort : bool, default False          Whether the result index should come out sorted or not.      Returns      -------
def makeMappingArray(N, data, gamma=1.0):      if (np.diff(x) < 0).any():          raise ValueError("data mapping points must have x in increasing order")     x = x * (N - 1)     xind = (N - 1) * np.linspace(0, 1, N) ** gamma     ind = np.searchsorted(x, xind)[1:-1]      distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])     lut = np.concatenate([         [y1[0]],         distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],         [y0[-1]],     ])      return np.clip(lut, 0.0, 1.0)
class FigureCanvasBase:                      renderer = _get_renderer(                          self.figure,                          functools.partial(                             print_method, orientation=orientation),                         draw_disabled=True)                     self.figure.draw(renderer)                      bbox_inches = self.figure.get_tightbbox(                          renderer, bbox_extra_artists=bbox_extra_artists)                      if pad_inches is None:
class ExtensionBlock(Block):          new_values = self.values if inplace else self.values.copy()         if isinstance(new, np.ndarray) and len(new) == len(mask):              new = new[mask]          mask = _safe_reshape(mask, new_values.shape)
class YearOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.month, self._day_opt, modby=12          )           return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )      def is_on_offset(self, dt: datetime) -> bool:          if self.normalize and not _is_normalized(dt):
class _LocIndexer(_LocationIndexer):                  new_key = []                  for i, component in enumerate(key):                     if isinstance(component, str) and labels.levels[i].is_all_dates:                          new_key.append(slice(component, component, None))                      else:                          new_key.append(component)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class StaticFileHandler(RequestHandler):          size = self.get_content_size()          if request_range:              start, end = request_range             if (start is not None and start >= size) or end == 0:  self.set_status(416)                  self.set_header("Content-Type", "text/plain")                  self.set_header("Content-Range", "bytes */%s" % (size,))                  return             if start is not None and start < 0:                 start += size              if end is not None and end > size:
class DatetimeIndexOpsMixin(ExtensionIndex):     def _get_addsub_freq(self, other) -> Optional[DateOffset]:          if is_period_dtype(self.dtype):              return self.freq          elif self.freq is None:              return None          elif lib.is_scalar(other) and isna(other):
def diff(arr, n: int, axis: int = 0):              result = res - lag              result[mask] = na              out_arr[res_indexer] = result          else:              out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]
class tqdm(object):          return self.total if self.iterable is None else \              (self.iterable.shape[0] if hasattr(self.iterable, "shape")               else len(self.iterable) if hasattr(self.iterable, "__len__")              else self.total)      def __enter__(self):          return self
Module contains tools for processing files into DataFrames or other objects  from collections import abc, defaultdict  import csv  import datetime from io import BufferedIOBase, StringIO, TextIOWrapper  import re  import sys  from textwrap import fill
def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):          vmin, vmax = vmax, vmin          swapped = True      maxabsvalue = max(abs(vmin), abs(vmax))      if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:          vmin = -expander
def test_multiprocessing_fit_error():      samples = batch_size * (good_batches + 1)     with pytest.raises(StopIteration):          model.fit_generator(              custom_generator(), samples, 1,              workers=4, use_multiprocessing=True,          )     with pytest.raises(StopIteration):          model.fit_generator(              custom_generator(), samples, 1,              use_multiprocessing=False,
class WebSocketProtocol13(WebSocketProtocol):      def accept_connection(self):          try:              self._handle_websocket_headers()              self._accept_connection()          except ValueError:              gen_log.debug("Malformed WebSocket request received",
class Model(Container):                      when using multiprocessing.              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Not used if using Sequence.              max_queue_size: maximum size for the generator queue              workers: maximum number of processes to spin up                  when using process based threading
class YoutubeDL(object):              force_properties = dict(                  (k, v) for k, v in ie_result.items() if v is not None)             for f in ('_type', 'url', 'ie_key'):                  if f in force_properties:                      del force_properties[f]              new_result = info.copy()
from thefuck.specific.git import git_support  @git_support  def match(command):     return ('did not match any file(s) known to git.' in command.stderr             and "Did you forget to 'git add'?" in command.stderr)  @git_support  def get_new_command(command):      missing_file = re.findall(             r"error: pathspec '([^']*)' "             r"did not match any file\(s\) known to git.", command.stderr)[0]      formatme = shell.and_('git add -- {}', '{}')      return formatme.format(missing_file, command.script)
__all__ = [  def get_objs_combined_axis(     objs, intersect: bool = False, axis=0, sort: bool = True  ) -> Index:      Extract combined index: return intersection or union (depending on the
class Model(Container):                      outs = [outs]                  outs_per_batch.append(outs)                 if isinstance(x, list):                      batch_size = x[0].shape[0]                  elif isinstance(x, dict):                      batch_size = list(x.values())[0].shape[0]
class Axis(martist.Artist):                  self._minor_tick_kw.update(kwtrans)                  for tick in self.minorTicks:                      tick._apply_params(**kwtrans)               if 'labelcolor' in kwtrans:                  self.offsetText.set_color(kwtrans['labelcolor'])
class tqdm(Comparable):          if unit_scale and unit_scale not in (True, 1):             total *= unit_scale              n *= unit_scale              if rate: rate *= unit_scale
class Scheduler(object):              if (best_task and batched_params and task.family == best_task.family and                      len(batched_tasks) < max_batch_size and task.is_batchable() and all(                     task.params.get(name) == value for name, value in unbatched_params.items())):                  for name, params in batched_params.items():                      params.append(task.params.get(name))                  batched_tasks.append(task)
from thefuck.utils import eager  @git_support  def match(command):     return ('branch' in command.script             and "fatal: A branch named '" in command.stderr              and " already exists." in command.stderr)
def assert_series_equal(                  f"is not equal to {right._values}."              )              raise AssertionError(msg)     elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):          assert_interval_array_equal(left.array, right.array)      elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):          _testing.assert_almost_equal(
def _period_array_cmp(cls, op):      @unpack_zerodim_and_defer(opname)      def wrapper(self, other):         ordinal_op = getattr(self.asi8, opname)          if isinstance(other, str):              try:
import sys  import tarfile  import threading  import time  import zipfile  from abc import abstractmethod  from multiprocessing.pool import ThreadPool
from pandas.util._decorators import Appender, cache_readonly  from pandas.core.dtypes.common import (      ensure_platform_int,      ensure_python_int,     is_int64_dtype,      is_integer,      is_integer_dtype,      is_list_like,
import numpy as np  from pandas._libs import NaT, iNaT, join as libjoin, lib  from pandas._libs.tslibs import timezones from pandas._typing import Label  from pandas.compat.numpy import function as nv  from pandas.errors import AbstractMethodError  from pandas.util._decorators import Appender, cache_readonly, doc  from pandas.core.dtypes.common import (      ensure_int64,      is_bool_dtype,      is_categorical_dtype,      is_dtype_equal,
class core(task.Config):  class _WorkerSchedulerFactory(object):      def create_local_scheduler(self):         return scheduler.CentralPlannerScheduler(prune_on_get_work=True)      def create_remote_scheduler(self, url):          return rpc.RemoteScheduler(url)
class TestPartialSetting:          df = orig.copy()         msg = "cannot insert DatetimeIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              df.loc[100.0, :] = df.iloc[0]
def format_str(      return dst_contents  GRAMMARS = [      pygram.python_grammar_no_print_statement_no_exec_statement,      pygram.python_grammar_no_print_statement,
This module implements the FormRequest class which is a more convenient class  See documentation in docs/topics/request-response.rst  from six.moves.urllib.parse import urljoin, urlencode  import lxml.html  from parsel.selector import create_root_node import six  from scrapy.http.request import Request  from scrapy.utils.python import to_bytes, is_listlike  from scrapy.utils.response import get_base_url
class FileWriter(object):          self.overwrite = overwrite      def write(self, s):         with open(self.path, 'w' if self.overwrite else 'a') as output_file:              output_file.write(s)          self.overwrite = False  thread_global = threading.local()   class Tracer:
async def func():                  self.async_inc, arange(8), batch_size=3              )          ]
class LocalFileSystem(FileSystem):              raise RuntimeError('Destination exists: %s' % new_path)          d = os.path.dirname(new_path)          if d and not os.path.exists(d):             self.fs.mkdir(d)          os.rename(old_path, new_path)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class ImagesPipeline(FilesPipeline):      MIN_WIDTH = 0      MIN_HEIGHT = 0     EXPIRES = 0      THUMBS = {}      DEFAULT_IMAGES_URLS_FIELD = 'image_urls'      DEFAULT_IMAGES_RESULT_FIELD = 'images'
def _get_empty_dtype_and_na(join_units):          dtype = upcast_classes["datetimetz"]          return dtype[0], tslibs.NaT      elif "datetime" in upcast_classes:         return np.dtype("M8[ns]"), tslibs.iNaT      elif "timedelta" in upcast_classes:          return np.dtype("m8[ns]"), np.timedelta64("NaT", "ns") else:
class Grouping:                  self._group_index = CategoricalIndex(                      Categorical.from_codes(                          codes=codes, categories=categories, ordered=self.grouper.ordered                     )                  )
def parse_duration(s):      m = re.match(
def format_str(src_contents: str, line_length: int) -> FileContent:      return dst_contents  def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement     drv = driver.Driver(grammar, pytree.convert)      if src_txt[-1] != '\n':          nl = '\r\n' if '\r\n' in src_txt[:1024] else '\n'          src_txt += nl     try:         result = drv.parse_string(src_txt, True)     except ParseError as pe:         lineno, column = pe.context[1]         lines = src_txt.splitlines()          try:             faulty_line = lines[lineno - 1]         except IndexError:             faulty_line = "<line number missing in source>"         raise ValueError(f"Cannot parse: {lineno}:{column}: {faulty_line}") from None      if isinstance(result, Leaf):          result = Node(syms.file_input, [result])
def get_grouper(              items = obj._data.items              try:                  items.get_loc(key)             except (KeyError, TypeError):                  return False
class GroupBy(_GroupBy):             order = np.roll(list(range(result.index.nlevels)), -1)             result = result.reorder_levels(order)             result = result.reindex(q, level=-1)              hi = len(q) * self.ngroups             arr = np.arange(0, hi, self.ngroups)             arrays = []             for i in range(self.ngroups):                 arr2 = arr + i                 arrays.append(arr2)             indices = np.concatenate(arrays)             assert len(indices) == len(result)              return result.take(indices)      @Substitution(name="groupby")
class Axes(_AxesBase):          if not np.iterable(ymax):              ymax = [ymax]         x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)           x = np.ravel(x)         ymin = np.resize(ymin, x.shape)         ymax = np.resize(ymax, x.shape)         verts = [((thisx, thisymin), (thisx, thisymax))                  for thisx, thisymin, thisymax in zip(x, ymin, ymax)]         lines = mcoll.LineCollection(verts, colors=colors,                                       linestyles=linestyles, label=label)          self.add_collection(lines, autolim=False)          lines.update(kwargs)
def _normalize(table, normalize, margins, margins_name="All"):          table = table.fillna(0)      elif margins is True:          column_margin = table.loc[:, margins_name].drop(margins_name)         index_margin = table.loc[margins_name, :].drop(margins_name)         table = table.drop(margins_name, axis=1).drop(margins_name)          table_index_names = table.index.names         table_columns_names = table.columns.names          table = _normalize(table, normalize=normalize, margins=False)
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1          if not good.all():             if skipna:                  pointer = self._codes[good].max()              else:                  return np.nan
class Bash(Generic):          return name, value      @memoize     @cache('.bashrc', '.bash_profile')      def get_aliases(self):         proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)         return dict(                 self._parse_alias(alias)                 for alias in proc.stdout.read().decode('utf-8').split('\n')                 if alias and '=' in alias)      def _get_history_file_name(self):          return os.environ.get("HISTFILE",
class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):      def __init__(self):          if not isinstance(self.dataflow_params, DataflowParamKeys):              raise ValueError("dataflow_params must be of type DataflowParamKeys")      @abstractmethod      def dataflow_executable(self):
class _AsOfMerge(_OrderedMerge):                  if self.tolerance < Timedelta(0):                      raise MergeError("tolerance must be positive")             elif is_int64_dtype(lt):                  if not is_integer(self.tolerance):                      raise MergeError(msg)                  if self.tolerance < 0:
class EarlyStopping(Callback):          baseline: Baseline value for the monitored quantity to reach.              Training will stop if the model doesn't show improvement              over the baseline.      def __init__(self,
client = TestClient(app)  def test_return_defaults():      response = client.get("/")      assert response.json() == {"sub": {}}
class PythonItemExporter(BaseItemExporter):              return dict(self._serialize_dict(value))          if is_listlike(value):              return [self._serialize_value(v) for v in value]         if self.binary:             return to_bytes(value, encoding=self.encoding)         else:             return to_unicode(value, encoding=self.encoding)      def _serialize_dict(self, value):          for key, val in six.iteritems(value):
def gunzip(data):                  raise      return output  def is_gzipped(response):      ctype = response.headers.get('Content-Type', b'')     return ctype in (b'application/x-gzip', b'application/gzip')
class IntegerArray(BaseMaskedArray):          ExtensionArray.argsort          data = self._data.copy()         data[self._mask] = data.min() - 1          return data      @classmethod
class SchemaBase(BaseModel): not_: Optional[List[Any]] = PSchema(None, alias="not")      items: Optional[Any] = None      properties: Optional[Dict[str, Any]] = None     additionalProperties: Optional[Union[bool, Any]] = None      description: Optional[str] = None      format: Optional[str] = None      default: Optional[Any] = None
def is_import(leaf: Leaf) -> bool:      )  def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:
def convert_to_index_sliceable(obj, key):         if idx.is_all_dates:              try:                  return idx._get_string_slice(key)              except (KeyError, ValueError, NotImplementedError):
class EarlyStopping(Callback):          self.min_delta = min_delta          self.wait = 0          self.stopped_epoch = 0          if mode not in ['auto', 'min', 'max']:              warnings.warn('EarlyStopping mode %s is unknown, '
def check_bool_indexer(index: Index, key) -> np.ndarray:          result = result.astype(bool)._values      else:          if is_sparse(result):             result = result.to_dense()          result = check_bool_array_indexer(index, result)      return result
class RequestHandler(object):          self._log()          self._finished = True          self.on_finish()          self.ui = None
class RNN(Layer):              state_size = self.cell.state_size          else:              state_size = [self.cell.state_size]         output_dim = state_size[0]          if self.return_sequences:              output_shape = (input_shape[0], input_shape[1], output_dim)
class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):                  dfxp_file = old_file                  srt_file = subtitles_filename(filename, lang, 'srt')                 with io.open(dfxp_file, 'rt', encoding='utf-8') as f:                      srt_data = dfxp2srt(f.read())                  with io.open(srt_file, 'wt', encoding='utf-8') as f:
from .generic import Generic  class Fish(Generic):      def _get_overridden_aliases(self):         overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()         if overridden_aliases:             return [alias.strip() for alias in overridden_aliases.split(',')]         else:             return ['cd', 'grep', 'ls', 'man', 'open']      def app_alias(self, fuck):
def _zip_file(command):     for c in command.script.split()[1:]:          if not c.startswith('-'):              if c.endswith('.zip'):                  return c
def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):          if is_object_dtype(dtype):              return tslib.ints_to_pydatetime(arr.view(np.int64))          elif dtype == np.int64:              return arr.view(dtype)
def split_line(          return      line_str = str(line).strip("\n")     if not line.should_explode and is_line_short_enough(         line, line_length=line_length, line_str=line_str      ):          yield line          return
async def serialize_response(  ) -> Any:      if field:          errors = []         if exclude_unset and isinstance(response_content, BaseModel):             if PYDANTIC_1:                 response_content = response_content.dict(exclude_unset=exclude_unset)             else:                 response_content = response_content.dict(                     skip_defaults=exclude_unset                 )          if is_coroutine:              value, errors_ = field.validate(response_content, {}, loc=("response",))          else:
def _make_ghost_gridspec_slots(fig, gs):              ax = fig.add_subplot(gs[nn])             ax.set_frame_on(False)             ax.set_xticks([])             ax.set_yticks([])             ax.set_facecolor((1, 0, 0, 0))  def _make_layout_margins(ax, renderer, h_pad, w_pad):
def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:  def normalize_fmt_off(node: Node) -> None:
class TestInsertIndexCoercion(CoercionBase):              with pytest.raises(TypeError, match=msg):                  obj.insert(1, pd.Timestamp("2012-01-01", tz="Asia/Tokyo"))         msg = "cannot insert DatetimeIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, 1)
class StaticFileHandler(RequestHandler):          .. versionadded:: 3.1         root = os.path.abspath(root)            if not (absolute_path + os.path.sep).startswith(root):              raise HTTPError(403, "%s is not in root static directory",                              self.path)
import warnings  import numpy as np from pandas._libs import NaT, Timestamp, lib, tslib  import pandas._libs.internals as libinternals  from pandas._libs.tslibs import Timedelta, conversion  from pandas._libs.tslibs.timezones import tz_compare
class Axes(_AxesBase):              Respective beginning and end of each line. If scalars are              provided, all lines will have same length.         colors : list of colors, default: 'k'          linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
class FilesPipeline(MediaPipeline):          dfd.addErrback(              lambda f:              logger.error(self.__class__.__name__ + '.store.stat_file',                          extra={'spider': info.spider, 'failure': f})          )          return dfd
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
Name: Max Speed, dtype: float64          if copy:              new_values = new_values.copy()         assert isinstance(self.index, PeriodIndex) new_index = self.index.to_timestamp(freq=freq, how=how)          return self._constructor(new_values, index=new_index).__finalize__(              self, method="to_timestamp"
class _LocIndexer(_LocationIndexer):              if isinstance(ax, MultiIndex):                  return False              if not ax.is_unique:                  return False
class IOLoop(Configurable):              if IOLoop.current(instance=False) is None:                  self.make_current()          elif make_current:             if IOLoop.current(instance=False) is None:                  raise RuntimeError("current IOLoop already exists")              self.make_current()
class DatetimeLikeBlockMixin:      def _holder(self):          return DatetimeArray      @property      def fill_value(self):          return np.datetime64("NaT", "ns")
class XmlItemExporter(BaseItemExporter):          elif is_listlike(serialized_value):              for value in serialized_value:                  self._export_xml_field('value', value)         else:              self._xg_characters(serialized_value)          self.xg.endElement(name)
class RNN(Layer):              self._num_constants = len(constants)              additional_specs += self.constants_spec         is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')          for tensor in additional_inputs:             if hasattr(tensor, '_keras_history') != is_keras_tensor:                  raise ValueError('The initial state or constants of an RNN'                                   ' layer cannot be specified with a mix of'                                  ' Keras tensors and non-Keras tensors')          if is_keras_tensor:
class DataFrame(NDFrame):     def _combine_frame(self, other, func, fill_value=None, level=None):          if fill_value is None:
def format_file_in_place(      if src.suffix == ".pyi":          mode |= FileMode.PYI     with tokenize.open(src) as src_buffer:         src_contents = src_buffer.read()      try:          dst_contents = format_file_contents(              src_contents, line_length=line_length, fast=fast, mode=mode
import functools  import inspect  import opcode  import sys  import re  import collections
def test_join_multi_wrong_order():      midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=["a", "b"])      midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=["b", "a"])     join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)      exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)      tm.assert_index_equal(midx1, join_idx)      assert lidx is None      tm.assert_numpy_array_equal(ridx, exp_ridx)
class OrderedEnqueuer(SequenceEnqueuer):                      yield inputs          except Exception as e:              self.stop()             raise StopIteration(e)      def _send_sequence(self):
class _AsOfMerge(_OrderedMerge):                  )              )             if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):                  if not isinstance(self.tolerance, Timedelta):                      raise MergeError(msg)                  if self.tolerance < Timedelta(0):
class BracketTracker:      bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)      delimiters: Dict[LeafID, Priority] = Factory(dict)      previous: Optional[Leaf] = None     _for_loop_variable: int = 0     _lambda_arguments: int = 0      def mark(self, leaf: Leaf) -> None:
class _Numeric:          raise ValueError     def __gt__(self, other):         return not self.__lt__(other)       def __le__(self, other):          return self.__lt__(other) or self.__eq__(other)      def __ge__(self, other):         return self.__gt__(other) or self.__eq__(other)  class SemanticVersion(Version):
import os  from shutil import rmtree  import string  import tempfile from typing import List, Optional, Union, cast  import warnings  import zipfile
class IOLoop(Configurable):      _current = threading.local()     _ioloop_for_asyncio = weakref.WeakKeyDictionary()      @classmethod      def configure(cls, impl, **kwargs):
def na_logical_op(x: np.ndarray, y, op):              assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))              x = ensure_object(x)              y = ensure_object(y)             result = libops.vec_binop(x, y, op)          else:              assert lib.is_scalar(y)
default 'raise'                      "You must pass a freq argument as current index has none."                  )             freq = get_period_alias(freq)          return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)
class ShellModule(ShellBase):          return ""      def join_path(self, *args):         parts = []         for arg in args:             arg = self._unquote(arg).replace('/', '\\')             parts.extend([a for a in arg.split('\\') if a])         path = '\\'.join(parts)         if path.startswith('~'):             return path         return path      def get_remote_filename(self, pathname):
def update_sub(x, decrement):          The variable `x` updated.     return tf_state_ops.assign_sub(x, decrement)  @symbolic
class JSInterpreter(object):              return opfunc(x, y)          m = re.match(             r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]+)\)$' % _NAME_RE, expr)          if m:              fname = m.group('func')              argvals = tuple([                  int(v) if v.isdigit() else local_vars[v]                 for v in m.group('args').split(',')])              if fname not in self._functions:                  self._functions[fname] = self.extract_function(fname)              return self._functions[fname](argvals)
class Index(IndexOpsMixin, PandasObject):                  return self._constructor(values, **attributes)              except (TypeError, ValueError):                  pass          return Index(values, **attributes)      def _update_inplace(self, result, **kwargs):
def read_batch_urls(batch_fd):      with contextlib.closing(batch_fd) as fd:          return [url for url in map(fixup, fd) if url]
class Model(Container):                              ' and multiple workers may duplicate your data.'                              ' Please consider using the`keras.utils.Sequence'                              ' class.'))         if is_sequence:             steps = len(generator)          enqueuer = None          try:
class _AtIndexer(_ScalarAccessIndexer):                          "can only have integer indexers"                      )              else:                 if is_integer(i) and not ax.holds_integer():                      raise ValueError(                          "At based indexing on an non-integer "                          "index can only have non-integer "
def assert_series_equal(      if check_categorical:          if is_categorical_dtype(left) or is_categorical_dtype(right):             assert_categorical_equal(left.values, right.values, obj=f"{obj} category")
class NPOIE(InfoExtractor):              'http://e.omroep.nl/metadata/aflevering/%s' % video_id,              video_id,             transform_source=lambda j: re.sub(r'parseMetadata\((.*?)\);\n//.*$', r'\1', j)          )          token_page = self._download_webpage(              'http://ida.omroep.nl/npoplayer/i.js',
class Axes(_AxesBase):          zdelta = 0.1         def line_props_with_rcdefaults(subkey, explicit, zdelta=0):              d = {k.split('.')[-1]: v for k, v in rcParams.items()                   if k.startswith(f'boxplot.{subkey}')}              d['zorder'] = zorder + zdelta              if explicit is not None:                  d.update(                      cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))
from __future__ import unicode_literals  u'hello'  U"hello"
class GroupBy(_GroupBy):          mask = self._cumcount_array(ascending=False) < n          return self._selected_obj[mask]     def _reindex_output(self, output):          If we have categorical groupers, then we might want to make sure that          we have a fully re-indexed output to the levels. This means expanding
from pandas._config.localization import (  )  import pandas._libs.testing as _testing from pandas._typing import FrameOrSeries  from pandas.compat import _get_lzma_file, _import_lzma  from pandas.core.dtypes.common import (
def conv2d(x, kernel, strides=(1, 1), padding='valid',  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None):
class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):          start = right[0]          if end < start:             return type(self)(data=[])          else:              lslice = slice(*left.slice_locs(start, end))             left_chunk = left.values[lslice]              return self._shallow_copy(left_chunk)      def _can_fast_union(self, other) -> bool:
Depth = int  NodeType = int  LeafID = int  Priority = int  LN = Union[Leaf, Node]  out = partial(click.secho, bold=True, err=True)  err = partial(click.secho, fg='red', err=True)
class TaskProcess(AbstractTaskProcess):              self.task.trigger_event(Event.START, self.task)              t0 = time.time()              status = None             try:                 new_deps = self._run_get_new_deps()                 if new_deps is None:                     status = RUNNING                 else:                     status = SUSPENDED                     logger.info(                         '[pid %s] Worker %s new requirements      %s',                         os.getpid(), self.worker_id, self.task.task_id)                     return             finally:                 if status != SUSPENDED:                     self.task.trigger_event(                         Event.PROCESSING_TIME, self.task, time.time() - t0)                     error_message = json.dumps(self.task.on_success())                     logger.info('[pid %s] Worker %s done      %s', os.getpid(),                                 self.worker_id, self.task.task_id)                     self.task.trigger_event(Event.SUCCESS, self.task)                     status = DONE          except KeyboardInterrupt:              raise
class SparkSubmitTask(luigi.Task):          command = []          if value and isinstance(value, dict):              for prop, value in value.items():                 command += [name, '"{0}={1}"'.format(prop, value)]          return command      def _flag_arg(self, name, value):
class Function(object):          self.fetches = [tf.identity(x) for x in self.fetches]         self.session_kwargs = session_kwargs          if session_kwargs:              raise ValueError('Some keys in session_kwargs are not '                               'supported at this '
def get_write_function(output):              stderr.write(s)      elif isinstance(output, (pycompat.PathLike, str)):          def write(s):             with open(output_path, 'a') as output_file:                  output_file.write(s)      else:          assert isinstance(output, utils.WritableStream)
async def request_body_to_args(          for field in required_params:              value: Any = None              if received_body is not None:                 if field.shape in sequence_shapes and isinstance(                     received_body, FormData                 ):                      value = received_body.getlist(field.alias)                  else:                      value = received_body.get(field.alias)
class LineGenerator(Visitor[Line]):      current_line: Line = Factory(Line)      remove_u_prefix: bool = False     def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:          If the line is empty, only emit if it makes sense.
def get_renderer(fig):              return canvas.get_renderer()          else:              from . import backend_bases             return backend_bases._get_renderer(fig, draw_disabled=True)  def get_subplotspec_list(axes_list, grid_spec=None):
class DataFrame(NDFrame):              )          return result     def transpose(self, *args, **kwargs):          Transpose index and columns.
import pandas.core.indexes.base as ibase  from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name  from pandas.core.indexes.extension import ExtensionIndex, inherit_names  import pandas.core.missing as missing  _index_doc_kwargs = dict(ibase._index_doc_kwargs)  _index_doc_kwargs.update(dict(target_klass="CategoricalIndex"))
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0]
from thefuck.utils import replace_argument, for_app  @for_app('php')  def match(command):     return "php -s" in command.script  def get_new_command(command):      return replace_argument(command.script, "-s", "-S")   requires_output = False
def js_to_json(code):          "(?:[^"\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^"\\]*"|          '(?:[^'\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^'\\]*'|          {comment}|,(?={skip}[\]}}])|         [a-zA-Z_][.a-zA-Z_0-9]*|          \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|          [0-9]+(?={skip}:)
def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None              )  def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:
class ObjectBlock(Block):      def _can_hold_element(self, element: Any) -> bool:          return True     def should_store(self, value) -> bool:          return not (              issubclass(                  value.dtype.type,
def get_handle(          from io import TextIOWrapper          g = TextIOWrapper(f, encoding=encoding, newline="")         if not isinstance(f, BufferedIOBase):              handles.append(g)          f = g
def get_response(args, config_dir):      requests_session = get_requests_session()      if not args.session and not args.session_read_only:          kwargs = get_requests_kwargs(args)
class Line:                      break          if commas > 1:             self.leaves.pop()              return True          return False
def hist2d(  @_copy_docstring_and_deprecators(Axes.hlines)  def hlines(         y, xmin, xmax, colors='k', linestyles='solid', label='', *,          data=None, **kwargs):      return gca().hlines(          y, xmin, xmax, colors=colors, linestyles=linestyles,
def test_aggregate_mixed_types():      tm.assert_frame_equal(result, expected)  class TestLambdaMangling:      def test_basic(self):          df = pd.DataFrame({"A": [0, 0, 1, 1], "B": [1, 2, 3, 4]})
def read_pickle(path, compression="infer"):          f.close()          for _f in fh:              _f.close()
class CmdlineTest(unittest.TestCase):      def test_cmdline_ambiguous_class(self, logger):          self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])     @mock.patch("logging.getLogger")     @mock.patch("warnings.warn")     def test_cmdline_non_ambiguous_class(self, warn, logger):         luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])         self.assertTrue(NonAmbiguousClass.has_run)       @mock.patch("logging.getLogger")      @mock.patch("logging.StreamHandler")      def test_setup_interface_logging(self, handler, logger):
class MediaPipeline(object):          dfd.addCallback(self._check_media_to_download, request, info)          dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)          dfd.addErrback(lambda f: logger.error(             f.value, extra={'spider': info.spider, 'failure': f})          ) return dfd.addBoth(lambda _: wad)
def _get_collection_info(dep_map, existing_collections, collection, requirement,      if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):          display.vvvv("Collection requirement '%s' is a tar artifact" % to_text(collection))          b_tar_path = to_bytes(collection, errors='surrogate_or_strict')     elif urlparse(collection).scheme:          display.vvvv("Collection requirement '%s' is a URL to a tar artifact" % collection)         b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)      if b_tar_path:          req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
from pandas.core.dtypes.common import (  from pandas.core.dtypes.dtypes import IntervalDtype  from pandas.core.dtypes.generic import (      ABCDatetimeIndex,      ABCIndexClass,      ABCInterval,      ABCIntervalIndex,
class SeriesGroupBy(GroupBy):                      periods=periods, fill_method=fill_method, limit=limit, freq=freq                  )              )          filled = getattr(self, fill_method)(limit=limit)          fill_grp = filled.groupby(self.grouper.codes)          shifted = fill_grp.shift(periods=periods, freq=freq)
async def func():                  self.async_inc, arange(8), batch_size=3              )          ]
from pandas.core.dtypes.cast import (      validate_numeric_casting,  )  from pandas.core.dtypes.common import (     ensure_float64,      ensure_int64,      ensure_platform_int,      infer_dtype_from_object,
class DatetimeIndex(DatetimeTimedeltaMixin):              return Timestamp(value).asm8          raise ValueError("Passed item and index have different timezone")
class TestPeriodIndex:          p2 = pd.Period("2014-01-04", freq=freq)          assert pidx.searchsorted(p2) == 3         msg = "Input has different freq=H from PeriodIndex"          with pytest.raises(IncompatibleFrequency, match=msg):              pidx.searchsorted(pd.Period("2014-01-01", freq="H"))         msg = "Input has different freq=5D from PeriodIndex"          with pytest.raises(IncompatibleFrequency, match=msg):              pidx.searchsorted(pd.Period("2014-01-01", freq="5D"))  class TestPeriodIndexConversion:      def test_tolist(self):
class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):          super().test_merge_on_extension_array_duplicates(data)  class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):      @skip_nested
def standardize_weights(y,                               ' The classes %s exist in the data but not in '                               '`class_weight`.'                               % (existing_classes - existing_class_weight))         return weights      else:         if sample_weight_mode is None:             return np.ones((y.shape[0],), dtype=K.floatx())         else:             return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())  def check_num_samples(ins,
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):          if not is_list:              start = self.index[0]              if isinstance(self.index, PeriodIndex):                 where = Period(where, freq=self.index.freq).ordinal                 start = start.ordinal              if where < start:                  if not is_series:
class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):              return res          return CategoricalIndex(res, name=self.name)  CategoricalIndex._add_numeric_methods_add_sub_disabled()  CategoricalIndex._add_numeric_methods_disabled()
def js_to_json(code):              }.get(m.group(0), m.group(0)), v[1:-1])          INTEGER_TABLE = (             (r'^0[xX][0-9a-fA-F]+', 16),             (r'^0+[0-7]+', 8),          )          for regex, base in INTEGER_TABLE:              im = re.match(regex, v)              if im:                 i = int(im.group(0), base)                  return '"%d":' % i if v.endswith(':') else '%d' % i          return '"%s"' % v
import numpy as np  import time  import json  import warnings  from collections import deque  from collections import OrderedDict
def generate_commands(vlan_id, to_set, to_remove):      if "vlan_id" in to_remove:          return ["no vlan {0}".format(vlan_id)]      for key, value in to_set.items():          if key == "vlan_id" or value is None:              continue          commands.append("{0} {1}".format(key, value))     for key in to_remove:         commands.append("no {0}".format(key))       if commands:          commands.insert(0, "vlan {0}".format(vlan_id))       return commands
def _ensure_default_collection(collection_list=None):  class CollectionSearch:     _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)      def _load_collections(self, attr, ds):
from blib2to3 import pygram, pytree  from blib2to3.pgen2 import driver, token  from blib2to3.pgen2.grammar import Grammar  from blib2to3.pgen2.parse import ParseError  __version__ = "19.3b0"
class DataFrame(NDFrame):          Parameters          ----------         *args, **kwargs             Additional arguments and keywords have no effect but might be             accepted for compatibility with numpy.          Returns          -------
def month_by_name(name):          return None def fix_xml_all_ampersand(xml_str):     return xml_str.replace(u'&', u'&amp;')  def setproctitle(title):
class Model(Container):                          val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)                          validation_generator = val_enqueuer.get()                      else:                         validation_generator = validation_data                  else:                      if len(validation_data) == 2:                          val_x, val_y = validation_data
def melt(          else:              id_vars = list(id_vars)             missing = Index(np.ravel(id_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'id_vars' are not present"
LOGGING_CONFIG_DEFAULTS = dict(      version=1,      disable_existing_loggers=False,      loggers={         "root": {"level": "INFO", "handlers": ["console"]},          "sanic.error": {              "level": "INFO",              "handlers": ["error_console"],
class GroupBy(_GroupBy):          output = output.drop(labels=list(g_names), axis=1)         output = output.set_index(self.grouper.result_index).reindex(index, copy=False)
def unified_strdate(date_str):          '%d/%m/%Y',          '%d/%m/%y',          '%Y/%m/%d %H:%M:%S',          '%Y-%m-%d %H:%M:%S',          '%d.%m.%Y %H:%M',          '%d.%m.%Y %H.%M',
class GeneratorEnqueuer(SequenceEnqueuer):                  else:                      thread.join(timeout)         if self._use_multiprocessing:             if self.queue is not None:                 self.queue.close()          self._threads = []          self._stop_event = None
class Categorical(NDArrayBackedExtensionArray, PandasObject):          Returns True if `key` is in this Categorical.         if is_scalar(key) and isna(key):              return self.isna().any()          return contains(self, key, container=self._codes)
class Function(object):                          'supported with sparse inputs.')                  return self._legacy_call(inputs)              return self._call(inputs)          else:              if py_any(is_tensor(x) for x in inputs):
class DatetimeIndexOpsMixin(ExtensionIndex):              return (lhs_mask & rhs_mask).nonzero()[0]      __add__ = make_wrapped_arith_op("__add__")
def map_obj_to_commands(updates, module):  def map_config_to_obj(module):     rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])     if rc == 0:         output = out     else:         rc, out, err = exec_command(module,                                     'show running-config | begin banner %s'                                     % module.params['banner'])         if out:             output = re.search(r'\^C(.*?)\^C', out, re.S).group(1).strip()          else:              output = None      obj = {'banner': module.params['banner'], 'state': 'absent'}      if output:          obj['text'] = output
class DataFrame(NDFrame):          scalar          if takeable:             series = self._iget_item_cache(col)             return com.maybe_box_datetimelike(series._values[index])          series = self._get_item_cache(col)          engine = self.index._engine
def predict_generator(model, generator,      try:          if workers > 0:             if is_sequence:                  enqueuer = OrderedEnqueuer(                      generator,                      use_multiprocessing=use_multiprocessing)
def _unstack_multiple(data, clocs, fill_value=None):      comp_ids, obs_ids = compress_group_index(group_index, sort=False)      recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)     if rlocs == []:          dummy_index = Index(obs_ids, name="__placeholder__")      else:
class TimedeltaIndex(                  "represent unambiguous timedelta values durations."              )         if isinstance(data, TimedeltaArray):              if copy:                  data = data.copy()              return cls._simple_new(data, name=name, freq=freq)
def _add_margins(      row_names = result.index.names      try:          for dtype in set(result.dtypes):              cols = result.select_dtypes([dtype]).columns             margin_dummy[cols] = margin_dummy[cols].astype(dtype)          result = result.append(margin_dummy)      except TypeError:
def crosstab(          kwargs = {"aggfunc": aggfunc}      table = df.pivot_table(         "__dummy__",          index=rownames,          columns=colnames,          margins=margins,
class Categorical(ExtensionArray, PandasObject):                      raise ValueError("fill value must be in categories")                  values_codes = _get_codes_for_values(value, self.categories)                 indexer = np.where(values_codes != -1)                 codes[indexer] = values_codes[values_codes != -1]              elif is_hashable(value):
optional.          if bin_range is not None:              bin_range = self.convert_xunits(bin_range)          if weights is not None:              w = cbook._reshape_2D(weights, 'weights')
def evaluate_generator(model, generator,      try:          if workers > 0:             if is_sequence:                  enqueuer = OrderedEnqueuer(                      generator,                      use_multiprocessing=use_multiprocessing)
def l2_normalize(x, axis=-1):      return x / np.sqrt(y)  def binary_crossentropy(target, output, from_logits=False):      if not from_logits:          output = np.clip(output, 1e-7, 1 - 1e-7)
class NDFrame(PandasObject, SelectionMixin):              data = self.fillna(method=fill_method, limit=limit, axis=axis)          rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1         rs = rs.loc[~rs.index.duplicated()]         rs = rs.reindex_like(data)         if freq is None:             mask = isna(com.values_from_object(data))             np.putmask(rs.values, mask, np.nan)          return rs      def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):
import operator  import numpy as np  import pytest import pandas.util._test_decorators as td  import pandas as pd  import pandas._testing as tm from pandas.arrays import BooleanArray from pandas.core.arrays.boolean import coerce_to_array from pandas.tests.extension.base import BaseOpsUtil  def make_data():
def get_future_imports(node: Node) -> Set[str]:              module_name = first_child.children[1]              if not isinstance(module_name, Leaf) or module_name.value != "__future__":                  break             for import_from_child in first_child.children[3:]:                 if isinstance(import_from_child, Leaf):                     if import_from_child.type == token.NAME:                         imports.add(import_from_child.value)                 else:                     assert import_from_child.type == syms.import_as_names                     for leaf in import_from_child.children:                         if isinstance(leaf, Leaf) and leaf.type == token.NAME:                             imports.add(leaf.value)          else:              break      return imports
class Axes(_AxesBase):              except ValueError: pass              else:                 if c.size == xsize:                      c = c.ravel()                      c_is_mapped = True else:
default: :rc:`scatter.edgecolors`              marker_obj.get_transform())          if not marker_obj.is_filled():              edgecolors = 'face'             linewidths = rcParams['lines.linewidth']          offsets = np.ma.column_stack([x, y])
from __future__ import print_function  import warnings  import numpy as np  from .training_utils import iter_sequence_infinite  from .. import backend as K  from ..utils.data_utils import Sequence
class GroupBy(_GroupBy):          base_func = getattr(libgroupby, how)          for name, obj in self._iterate_slices():              if aggregate:                  result_sz = ngroups              else:                 result_sz = len(obj.values)              if not cython_dtype:                 cython_dtype = obj.values.dtype              result = np.zeros(result_sz, dtype=cython_dtype)              func = partial(base_func, result, labels)              inferences = None              if needs_values:                 vals = obj.values                  if pre_processing:                      vals, inferences = pre_processing(vals)                  func = partial(func, vals)              if needs_mask:                 mask = isna(obj.values).view(np.uint8)                  func = partial(func, mask)              if needs_ngroups:
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def standardize_weights(y,                               ' for an input with shape ' +                               str(y.shape) + '. '                               'sample_weight cannot be broadcast.')         return sample_weight     elif isinstance(class_weight, dict):          if len(y.shape) > 2:              raise ValueError('`class_weight` not supported for '                               '3+ dimensional targets.')         if y.shape[1] > 1:             y_classes = np.argmax(y, axis=1)         elif y.shape[1] == 1:             y_classes = np.reshape(y, y.shape[0])          else:              y_classes = y         weights = np.asarray([class_weight[cls] for cls in y_classes                               if cls in class_weight])         if len(weights) != len(y_classes):              existing_classes = set(y_classes)              existing_class_weight = set(class_weight.keys())
class _LocationIndexer(_NDFrameIndexerBase):          if self.axis is not None:              return self._convert_tuple(key, is_setter=True)
from twisted.internet import defer  from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback  from scrapy.utils.spider import iterate_spider_output  from scrapy.utils.misc import load_object from scrapy.utils.log import logformatter_adapter  from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest  from scrapy import signals  from scrapy.http import Request, Response
def verify_collections(collections, search_paths, apis, validate_certs, ignore_e                      for search_path in search_paths:                          b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')                          if os.path.isdir(b_search_path):                              local_collection = CollectionRequirement.from_path(b_search_path, False)                              break                      if local_collection is None:
def _use_inf_as_na(key):          globals()["_isna"] = _isna_new def _isna_ndarraylike(obj):     values = getattr(obj, "_values", obj)     dtype = values.dtype      if is_extension_array_dtype(dtype):         result = values.isna()     elif is_string_dtype(dtype):         result = _isna_string_dtype(values, dtype, old=False)      elif needs_i8_conversion(dtype):          result = values.view("i8") == iNaT     else:         result = np.isnan(values)       if isinstance(obj, ABCSeries):         result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)      return result      values = getattr(obj, "_values", obj)      dtype = values.dtype     if is_string_dtype(dtype):         result = _isna_string_dtype(values, dtype, old=True)       elif needs_i8_conversion(dtype):          result = values.view("i8") == iNaT      else:         result = ~np.isfinite(values)      if isinstance(obj, ABCSeries):
import logging  import warnings  from twisted.python.failure import Failure  from scrapy.exceptions import ScrapyDeprecationWarning  logger = logging.getLogger(__name__) warnings.warn("Module `scrapy.log` has been deprecated, Scrapy now relies on "               "the builtin Python library for logging. Read the updated "               "logging entry in the documentation to learn more.",               ScrapyDeprecationWarning, stacklevel=2) def msg(message=None, _level=logging.INFO, **kw):     warnings.warn('log.msg has been deprecated, create a python logger and '                   'log through it instead',                   ScrapyDeprecationWarning, stacklevel=2)
class Warnings(object):              "loaded. (Shape: {shape})")      W021 = ("Unexpected hash collision in PhraseMatcher. Matches may be "              "incorrect. Modify PhraseMatcher._terminal_hash to fix.")  @add_codes
from __future__ import division  import os  import re  import sys  import mimetypes  import threading  from time import sleep, time
def na_logical_op(x: np.ndarray, y, op):                      f"and scalar of type [{typ}]"                  )     return result  def logical_op(
def make_data():  @pytest.fixture  def dtype():     return pd.BooleanDtype()  @pytest.fixture
class Task(object):          self.failures.add_failure()      def has_excessive_failures(self):         if (self.failures.first_failure_time is not None and                 self.disable_hard_timeout):              if (time.time() >= self.failures.first_failure_time +                      self.disable_hard_timeout):                  return True
class RNN(Layer):      @property      def non_trainable_weights(self):          if isinstance(self.cell, Layer):              return self.cell.non_trainable_weights          return []
def unified_strdate(date_str):      upload_date = None     date_str = date_str.replace(',',' ')     date_str = re.sub(r' ?(\+|-)[0-9:]*$', '', date_str)      format_expressions = [          '%d %B %Y',          '%B %d %Y',
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class TestGetIndexer:          expected = np.array([0] * size, dtype="intp")          tm.assert_numpy_array_equal(result, expected)      @pytest.mark.parametrize(          "tuples, closed",          [
def run_hook(hook_name, project_dir, context):      :param project_dir: The directory to execute the script from.      :param context: Cookiecutter project context.     script = find_hook(hook_name)     if script is None:          logger.debug('No %s hook found', hook_name)          return      logger.debug('Running hook %s', hook_name)     run_script_with_context(script, project_dir, context)
from pandas.core.dtypes.common import (      is_bool,      is_bool_dtype,      is_categorical_dtype,     is_datetime64_dtype,      is_datetime64tz_dtype,      is_datetimelike,      is_dtype_equal,
class Sequential(Model):                  at the end of every epoch. It should typically                  be equal to the number of samples of your                  validation dataset divided by the batch size.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Maximum size for the generator queue
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                                                          kshp=kernel_shape,                                                          subsample=strides,                                                          border_mode=th_padding,                                                         filter_flip=not flip_filters)      conv_out = op(kernel, x, output_shape[2:])      conv_out = _postprocess_conv2d_output(conv_out, x, padding,                                            kernel_shape, strides, data_format)
class StringMethods(NoNewAttributesMixin):      _doc_args["istitle"] = dict(type="titlecase", method="istitle")      _doc_args["isnumeric"] = dict(type="numeric", method="isnumeric")      _doc_args["isdecimal"] = dict(type="decimal", method="isdecimal")      isalnum = _noarg_wrapper(          lambda x: x.isalnum(),          name="isalnum",          docstring=_shared_docs["ismethods"] % _doc_args["isalnum"],          returns_string=False,      )      isalpha = _noarg_wrapper(          lambda x: x.isalpha(),          name="isalpha",          docstring=_shared_docs["ismethods"] % _doc_args["isalpha"],          returns_string=False,      )      isdigit = _noarg_wrapper(          lambda x: x.isdigit(),          name="isdigit",          docstring=_shared_docs["ismethods"] % _doc_args["isdigit"],          returns_string=False,      )      isspace = _noarg_wrapper(          lambda x: x.isspace(),          name="isspace",          docstring=_shared_docs["ismethods"] % _doc_args["isspace"],          returns_string=False,      )      islower = _noarg_wrapper(          lambda x: x.islower(),          name="islower",          docstring=_shared_docs["ismethods"] % _doc_args["islower"],          returns_string=False,      )      isupper = _noarg_wrapper(          lambda x: x.isupper(),          name="isupper",          docstring=_shared_docs["ismethods"] % _doc_args["isupper"],          returns_string=False,      )      istitle = _noarg_wrapper(          lambda x: x.istitle(),          name="istitle",          docstring=_shared_docs["ismethods"] % _doc_args["istitle"],          returns_string=False,      )      isnumeric = _noarg_wrapper(          lambda x: x.isnumeric(),          name="isnumeric",          docstring=_shared_docs["ismethods"] % _doc_args["isnumeric"],          returns_string=False,      )      isdecimal = _noarg_wrapper(          lambda x: x.isdecimal(),          name="isdecimal",          docstring=_shared_docs["ismethods"] % _doc_args["isdecimal"],          returns_string=False,      )      @classmethod
def categorical_accuracy(y_true, y_pred):  def sparse_categorical_accuracy(y_true, y_pred):     return K.cast(K.equal(K.max(y_true, axis=-1),                            K.cast(K.argmax(y_pred, axis=-1), K.floatx())),                    K.floatx())
from asyncio.base_events import BaseEventLoop  from concurrent.futures import Executor, ProcessPoolExecutor  from enum import Enum, Flag  from functools import partial, wraps  import keyword  import logging  from multiprocessing import Manager
def hide_fmt_off(node: Node) -> bool:                  hidden_value = (                      comment.value + "\n" + "".join(str(n) for n in ignored_nodes)                  )                  first_idx = None                  for ignored in ignored_nodes:                      index = ignored.remove()
class StringMethods(NoNewAttributesMixin):          if isinstance(others, ABCSeries):              return [others]          elif isinstance(others, ABCIndexClass):             return [Series(others._values, index=others)]          elif isinstance(others, ABCDataFrame):              return [others[x] for x in others]          elif isinstance(others, np.ndarray) and others.ndim == 2:
class TestBackend(object):                             np.asarray([-5., -4., 0., 4., 9.],                                        dtype=np.float32))     @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),                         reason='This test is for tensorflow parallelism.')     def test_tensorflow_session_parallelism_settings(self, monkeypatch):         for threads in [1, 2]:             K.clear_session()             monkeypatch.setenv('OMP_NUM_THREADS', str(threads))             cfg = K.get_session()._config             assert cfg.intra_op_parallelism_threads == threads             assert cfg.inter_op_parallelism_threads == threads   if __name__ == '__main__':      pytest.main([__file__])
from . import grammar, parse, token, tokenize, pgen  class Driver(object):     def __init__(self, grammar, convert=None, logger=None):          self.grammar = grammar          if logger is None:              logger = logging.getLogger(__name__)          self.logger = logger          self.convert = convert      def parse_tokens(self, tokens, debug=False):
class DatetimeLikeBlockMixin:          return self.array_values()  class DatetimeBlock(DatetimeLikeBlockMixin, Block):      __slots__ = ()
class IntervalArray(IntervalMixin, ExtensionArray):          return self.left.size      def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):          Take elements from the IntervalArray.
class DataFrame(NDFrame):              if can_concat:                  if how == "left":                     res = concat(frames, axis=1, join="outer", verify_integrity=True)                      return res.reindex(self.index, copy=False)                  else:                     return concat(frames, axis=1, join=how, verify_integrity=True)              joined = frames[0]
class GeneratorEnqueuer(SequenceEnqueuer):          while self.is_running():              if not self.queue.empty():                 inputs = self.queue.get()                 if inputs is not None:                     yield inputs              else:                  all_finished = all([not thread.is_alive() for thread in self._threads])                  if all_finished and self.queue.empty():                      raise StopIteration()                  else:                      time.sleep(self.wait_time)
class GalaxyCLI(CLI):                  if not os.path.exists(b_dir_path):                      os.makedirs(b_dir_path)         display.display("- %s was created successfully" % obj_name)      def execute_info(self):
async def serialize_response(      exclude: Union[SetIntStr, DictIntStrAny] = set(),      by_alias: bool = True,      exclude_unset: bool = False,      is_coroutine: bool = True,  ) -> Any:      if field:          errors = []          response_content = _prepare_response_content(             response_content, by_alias=by_alias, exclude_unset=exclude_unset          )          if is_coroutine:              value, errors_ = field.validate(response_content, {}, loc=("response",))
from pandas.core.dtypes.common import (  from pandas.core.dtypes.dtypes import CategoricalDtype  from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries  from pandas.core.dtypes.inference import is_hashable from pandas.core.dtypes.missing import isna, notna  from pandas.core import ops  from pandas.core.accessor import PandasDelegate, delegate_names
class Sequential(Model):      def __init__(self, layers=None, name=None):          super(Sequential, self).__init__(name=name)          if layers:
class TestTimeSeries(TestData):          )          tm.assert_index_equal(expected.index, result.index)     def test_diff(self):          self.ts.diff()           a = 10000000000000000         b = a + 1         s = Series([a, b])          rs = s.diff()         assert rs[1] == 1           rs = self.ts.diff(-1)         xp = self.ts - self.ts.shift(-1)         assert_series_equal(rs, xp)           rs = self.ts.diff(0)         xp = self.ts - self.ts         assert_series_equal(rs, xp)           s = Series(date_range("20130102", periods=5))         rs = s - s.shift(1)         xp = s.diff()         assert_series_equal(rs, xp)           nrs = rs - rs.shift(1)         nxp = xp.diff()         assert_series_equal(nrs, nxp)           s = Series(             date_range("2000-01-01 09:00:00", periods=5, tz="US/Eastern"), name="foo"         )         result = s.diff()         assert_series_equal(             result, Series(TimedeltaIndex(["NaT"] + ["1 days"] * 4), name="foo")         )       def test_pct_change(self):          rs = self.ts.pct_change(fill_method=None)          assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)
class LSTMCell(Layer):          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.implementation = implementation          self.state_size = (self.units, self.units)          self._dropout_mask = None          self._recurrent_dropout_mask = None
from six.moves.urllib.parse import urlunparse  from scrapy.utils.httpobj import urlparse_cached  from scrapy.exceptions import NotConfigured   class HttpProxyMiddleware(object):
def main():                  module.fail_json(msg='Unable to parse pool_ids option.')              pool_id, quantity = list(value.items())[0]          else:             pool_id, quantity = value, 1         pool_ids[pool_id] = str(quantity)      consumer_type = module.params["consumer_type"]      consumer_name = module.params["consumer_name"]      consumer_id = module.params["consumer_id"]
import pytest  import pandas.util._test_decorators as td  import pandas as pd from pandas import DataFrame, MultiIndex, Series  import pandas._testing as tm
class CmdlineTest(unittest.TestCase):          self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'      def _execute(self, *new_args, **kwargs):          args = (sys.executable, '-m', 'scrapy.cmdline') + new_args          proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)         comm = proc.communicate()         return comm[0].strip()      def test_default_settings(self):          self.assertEqual(self._execute('settings', '--get', 'TEST1'), \
class _LocationIndexer(_NDFrameIndexerBase):                  raise              raise IndexingError(key) from e      def __setitem__(self, key, value):          if isinstance(key, tuple):              key = tuple(com.apply_if_callable(x, self.obj) for x in key)
def to_pickle(obj, path, compression="infer", protocol=pickle.HIGHEST_PROTOCOL):      >>> import os      >>> os.remove("./dummy.pkl")     path = stringify_path(path)     f, fh = get_handle(path, "wb", compression=compression, is_text=False)      if protocol < 0:          protocol = pickle.HIGHEST_PROTOCOL      try:
class DictParameter(Parameter):          return json.loads(s, object_pairs_hook=_FrozenOrderedDict)      def serialize(self, x):         return json.dumps(x, cls=DictParameter._DictParamEncoder)  class ListParameter(Parameter):
def generate_tokens(readline):                          yield (STRING, token, spos, epos, line) elif initial.isidentifier():                      if token in ('async', 'await'):                         if async_def:                              yield (ASYNC if token == 'async' else AWAIT,                                     token, spos, epos, line)                              continue
class MarkerStyle:          self._snap_threshold = None          self._joinstyle = 'round'          self._capstyle = 'butt'         self._filled = True          self._marker_function()      def __bool__(self):
class Model(Container):              return averages      @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10,                            workers=1,                            use_multiprocessing=False,
class ExtensionBlock(Block):      def setitem(self, indexer, value):         Set the value inplace, returning a same-typed block.          This differs from Block.setitem by not allowing setitem to change          the dtype of the Block.
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)      def __sub__(self, other):          other = lib.item_from_zerodim(other)         if isinstance(other, (ABCSeries, ABCDataFrame)):              return NotImplemented
class FacebookIE(InfoExtractor):              'timezone': '-60',              'trynum': '1',              }         request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))          request.add_header('Content-Type', 'application/x-www-form-urlencoded')          try:             login_results = compat_urllib_request.urlopen(request).read()              if re.search(r'<form(.*)name="login"(.*)</form>', login_results) is not None:                  self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')                  return              check_form = {                 'fb_dtsg': self._search_regex(r'"fb_dtsg":"(.*?)"', login_results, 'fb_dtsg'),                  'nh': self._search_regex(r'name="nh" value="(\w*?)"', login_results, 'nh'),                  'name_action_selected': 'dont_save',                 'submit[Continue]': self._search_regex(r'<input value="(.*?)" name="submit\[Continue\]"', login_results, 'continue'),              }             check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))              check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')             check_response = compat_urllib_request.urlopen(check_req).read()              if re.search(r'id="checkpointSubmitButton"', check_response) is not None:                  self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')          except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)      def __add__(self, other):          other = lib.item_from_zerodim(other)         if isinstance(other, (ABCSeries, ABCDataFrame)):              return NotImplemented
class GroupBy(_GroupBy):          if isinstance(self.obj, Series):              result.name = self.obj.name         return result      @classmethod      def _add_numeric_operations(cls):
def jsonable_encoder(          data,          by_alias=by_alias,          exclude_unset=exclude_unset,         include_none=include_none,          custom_encoder=custom_encoder,          sqlalchemy_safe=sqlalchemy_safe,      )
class TestCartesianProduct:          tm.assert_index_equal(result1, expected1)          tm.assert_index_equal(result2, expected2)      def test_empty(self):          X = [[], [0, 1], []]
VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {          Feature.NUMERIC_UNDERSCORES,          Feature.TRAILING_COMMA_IN_CALL,          Feature.TRAILING_COMMA_IN_DEF,      },      TargetVersion.PY38: {          Feature.UNICODE_LITERALS,
class FigureCanvasBase:          Returns          -------         axes: topmost axes containing the point, or None if no axes.           axes_list = [a for a in self.figure.get_axes()                      if a.patch.contains_point(xy)]           if axes_list:              axes = cbook._topmost_artist(axes_list)          else:
def match(command):  def get_new_command(command):     broken_cmd = re.findall(r'ERROR: unknown command \"([a-z]+)\"',                              command.output)[0]     new_cmd = re.findall(r'maybe you meant \"([a-z]+)\"', command.output)[0]      return replace_argument(command.script, broken_cmd, new_cmd)
class Text(Artist):          if not self.get_visible():              return Bbox.unit()         if dpi is not None:             dpi_orig = self.figure.dpi             self.figure.dpi = dpi          if self.get_text() == '':             tx, ty = self._get_xy_display()             return Bbox.from_bounds(tx, ty, 0, 0)          if renderer is not None:              self._renderer = renderer
class APIRouter(routing.Router):                      include_in_schema=route.include_in_schema,                      name=route.name,                  )      def get(          self,
class TestBackend(object):          assert output == [21.]          assert K.get_session().run(fetches=[x, y]) == [30., 40.]     @pytest.mark.skipif(K.backend() != 'tensorflow',                          reason='Uses the `options` and `run_metadata` arguments.')      def test_function_tf_run_options_with_run_metadata(self):          from tensorflow.core.protobuf import config_pb2
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries  from pandas.core import algorithms  from pandas.core.accessor import PandasDelegate from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin  from pandas.core.arrays.datetimelike import (      DatetimeLikeArrayMixin,      _ensure_datetimelike_to_i8,
class _Rolling_and_Expanding(_Rolling):      )      def count(self):         if isinstance(self.window, BaseIndexer):             validate_baseindexer_support("count")          blocks, obj = self._create_blocks()          results = []
class TruncatedNormal(Initializer):          self.seed = seed      def __call__(self, shape, dtype=None):         return K.truncated_normal(shape, self.mean, self.stddev,                                   dtype=dtype, seed=self.seed)      def get_config(self):          return {
class TimedeltaIndex(          this, other = self, other          if this._can_fast_union(other):             return this._fast_union(other)          else:              result = Index._union(this, other, sort=sort)              if isinstance(result, TimedeltaIndex):
class Model(Container):                          if isinstance(metric_fn, Layer) and metric_fn.stateful:                              self.stateful_metric_names.append(metric_name)                              self.metrics_updates += metric_fn.updates                  handle_metrics(output_metrics)
def filename_from_url(url, content_type):      return fn  def get_unique_filename(filename, exists=os.path.exists):      attempt = 0      while True:          suffix = '-' + str(attempt) if attempt > 0 else ''         if not exists(filename + suffix):             return filename + suffix          attempt += 1
class AsyncHTTPClient(Configurable):              return          self._closed = True          if self._instance_cache is not None:             if self._instance_cache.get(self.io_loop) is not self:                  raise RuntimeError("inconsistent AsyncHTTPClient cache")             del self._instance_cache[self.io_loop]      def fetch(          self,
def assert_series_equal(      check_exact=False,      check_datetimelike_compat=False,      check_categorical=True,      obj="Series",  ):
class UnformattedLines(Line):         return False   @dataclass  class EmptyLineTracker:
def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:      if not target_versions:          return GRAMMARS      elif all(not version.is_python2() for version in target_versions):           return [              pygram.python_grammar_no_print_statement_no_exec_statement,              pygram.python_grammar_no_print_statement,          ]      else:         return [pygram.python_grammar]  def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
class LookupModule(LookupBase):          ret = []          for term in terms:              var = term.split()[0]             ret.append(os.getenv(var, ''))          return ret
def read_json(          dtype = True      if convert_axes is None and orient != "table":          convert_axes = True      compression = _infer_compression(path_or_buf, compression)      filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(
default 'raise'          DatetimeIndex(['2018-03-01 09:00:00-05:00',                         '2018-03-02 09:00:00-05:00',                         '2018-03-03 09:00:00-05:00'],                       dtype='datetime64[ns, US/Eastern]', freq='D')          With the ``tz=None``, we can remove the time zone information          while keeping the local time (not converted to UTC):
class Index(IndexOpsMixin, PandasObject):          if pself is not self or ptarget is not target:              return pself.get_indexer_non_unique(ptarget)         if is_categorical(target):              tgt_values = np.asarray(target)         elif self.is_all_dates and target.is_all_dates:             tgt_values = target.asi8          else:              tgt_values = target._get_engine_target()
def jsonable_encoder(                      or (not isinstance(key, str))                      or (not key.startswith("_sa"))                  )                 and (value is not None or include_none)                  and ((include and key in include) or key not in exclude)              ):                  encoded_key = jsonable_encoder(                      key,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
import copy  import datetime  from functools import partial  import string from typing import TYPE_CHECKING, Optional, Tuple, Union  import warnings  import numpy as np  from pandas._libs import Timedelta, hashtable as libhashtable, lib  import pandas._libs.join as libjoin from pandas._typing import FrameOrSeries  from pandas.errors import MergeError  from pandas.util._decorators import Appender, Substitution
class RNN(Layer):      @property      def trainable_weights(self):          if isinstance(self.cell, Layer):              return self.cell.trainable_weights          return []
class CSVLogger(Callback):              if os.path.exists(self.filename):                  with open(self.filename, 'r' + self.file_flags) as f:                      self.append_header = not bool(len(f.readline()))             self.csv_file = open(self.filename, 'a' + self.file_flags)          else:             self.csv_file = open(self.filename, 'w' + self.file_flags)      def on_epoch_end(self, epoch, logs=None):          logs = logs or {}
class Model(Container):          return self.history      @interfaces.legacy_generator_methods_support     def evaluate_generator(self, generator, steps,                             max_queue_size=10,                             workers=1,                             use_multiprocessing=False):
class MinhatecaIE(InfoExtractor):          filesize_approx = parse_filesize(self._html_search_regex(              r'<p class="fileSize">(.*?)</p>',              webpage, 'file size approximation', fatal=False))         duration = int_or_none(self._html_search_regex(             r'(?s)<p class="fileLeng[ht][th]">.*?([0-9]+)\s*s',              webpage, 'duration', fatal=False))          view_count = int_or_none(self._html_search_regex(              r'<p class="downloadsCounter">([0-9]+)</p>',
from datetime import datetime, timedelta import operator from typing import Any, Sequence, Type, Union, cast import warnings  import numpy as np from pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib from pandas._libs.tslibs.c_timestamp import integer_op_not_supported from pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds from pandas._libs.tslibs.timestamps import RoundTo, round_nsint64 from pandas._typing import DatetimeLikeScalar from pandas.compat import set_function_name  from pandas.compat.numpy import function as nv from pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning from pandas.util._decorators import Appender, Substitution from pandas.util._validators import validate_fillna_kwargs  from pandas.core.dtypes.common import (     is_categorical_dtype,      is_datetime64_any_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,     is_datetime_or_timedelta_dtype,      is_dtype_equal,     is_float_dtype,     is_integer_dtype,      is_list_like,      is_object_dtype,      is_period_dtype,     is_string_dtype,      is_timedelta64_dtype,     is_unsigned_integer_dtype,     pandas_dtype,  ) from pandas.core.dtypes.generic import ABCSeries from pandas.core.dtypes.inference import is_array_like from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna  from pandas.core import missing, nanops, ops from pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts from pandas.core.array_algos.transforms import shift from pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray from pandas.core.arrays.base import ExtensionArray, ExtensionOpsMixin import pandas.core.common as com from pandas.core.construction import array, extract_array from pandas.core.indexers import check_array_indexer from pandas.core.ops.common import unpack_zerodim_and_defer from pandas.core.ops.invalid import invalid_comparison, make_invalid_op  from pandas.tseries import frequencies from pandas.tseries.offsets import DateOffset, Tick   def _datetimelike_array_cmp(cls, op):     opname = f"__{op.__name__}__"     nat_result = opname == "__ne__"      class InvalidComparison(Exception):         pass      def _validate_comparison_value(self, other):         if isinstance(other, str):             try:                  other = self._scalar_from_string(other)             except ValueError:                  raise InvalidComparison(other)         if isinstance(other, self._recognized_scalars) or other is NaT:             other = self._scalar_type(other)             self._check_compatible_with(other)         elif not is_list_like(other):             raise InvalidComparison(other)         elif len(other) != len(self):             raise ValueError("Lengths must match")         else:             if isinstance(other, list):                  other = np.array(other)             if not isinstance(other, (np.ndarray, type(self))):                 raise InvalidComparison(other)             elif is_object_dtype(other.dtype):                 pass             elif not type(self)._is_recognized_dtype(other.dtype):                 raise InvalidComparison(other)             else:                   other = type(self)._from_sequence(other)                 self._check_compatible_with(other)         return other     @unpack_zerodim_and_defer(opname)     def wrapper(self, other):         try:             other = _validate_comparison_value(self, other)         except InvalidComparison:             return invalid_comparison(self, other, op)          dtype = getattr(other, "dtype", None)         if is_object_dtype(dtype):                with np.errstate(all="ignore"):                 result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)             return result         if isinstance(other, self._scalar_type) or other is NaT:             other_i8 = self._unbox_scalar(other)         else:              other_i8 = other.asi8         result = op(self.asi8, other_i8)         o_mask = isna(other)         if self._hasnans | np.any(o_mask):             result[self._isnan | o_mask] = nat_result         return result     return set_function_name(wrapper, opname, cls) class AttributesMixin:     _data: np.ndarray     @classmethod     def _simple_new(cls, values: np.ndarray, **kwargs):         raise AbstractMethodError(cls)      @property     def _scalar_type(self) -> Type[DatetimeLikeScalar]:         raise AbstractMethodError(self)      def _scalar_from_string(         self, value: str     ) -> Union[Period, Timestamp, Timedelta, NaTType]:         raise AbstractMethodError(self)      def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:         raise AbstractMethodError(self)     def _check_compatible_with(         self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False     ) -> None:         raise AbstractMethodError(self)   class DatelikeOps:      @Substitution(         URL="https://docs.python.org/3/library/datetime.html"         "     )     def strftime(self, date_format):         result = self._format_native_types(date_format=date_format, na_rep=np.nan)         return result.astype(object) class TimelikeOps:             - 'shift_forward' will shift the nonexistent time forward to the               closest existing time             - 'shift_backward' will shift the nonexistent time backward to the               closest existing time             - 'NaT' will return NaT where there are nonexistent times             - timedelta objects will shift nonexistent times by the timedelta             - 'raise' will raise an NonExistentTimeError if there are               nonexistent times.             .. versionadded:: 0.24.0         Returns         -------         DatetimeIndex, TimedeltaIndex, or Series             Index of the same type for a DatetimeIndex or TimedeltaIndex,             or a Series with the same index for a Series.         Raises         ------         ValueError if the `freq` cannot be converted.     def _round(self, freq, mode, ambiguous, nonexistent):          if is_datetime64tz_dtype(self):              naive = self.tz_localize(None)             result = naive._round(freq, mode, ambiguous, nonexistent)             aware = result.tz_localize(                 self.tz, ambiguous=ambiguous, nonexistent=nonexistent             )             return aware          values = self.view("i8")         result = round_nsint64(values, mode, freq)         result = self._maybe_mask_results(result, fill_value=NaT)         return self._simple_new(result, dtype=self.dtype)      @Appender((_round_doc + _round_example).format(op="round"))     def round(self, freq, ambiguous="raise", nonexistent="raise"):         return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)         Helper to set our freq in-place, returning self to allow method chaining.         Parameters         ----------         freq : DateOffset, None, or "infer"         Returns         -------         self     Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray      Assumes that __new__/__init__ defines:         _data         _freq      and that the inheriting class has methods:         _generate_range         box function to get object from internal representation         apply box func to passed values         Integer representation of the values.          Returns         -------         ndarray             An ndarray with int64 dtype.         Helper method for astype when converting to strings.          Returns          -------         ndarray[str]         This getitem defers to the underlying array, which by-definition can         only handle list-likes, slices, and integer scalars         Find the `freq` attribute to assign to the result of a __getitem__ lookup.         If a fill_value is passed to `take` convert it to an i8 representation,         raising ValueError if this is not possible.         if is_valid_nat_for_dtype(fill_value, self.dtype):             fill_value = iNaT         elif isinstance(fill_value, self._recognized_scalars):             self._check_compatible_with(fill_value)             fill_value = self._scalar_type(fill_value)             fill_value = self._unbox_scalar(fill_value)         else:             raise ValueError(                 f"'fill_value' should be a {self._scalar_type}. "                 f"Got '{str(fill_value)}'."             )         return fill_value      def _validate_shift_value(self, fill_value):          if is_valid_nat_for_dtype(fill_value, self.dtype):             fill_value = NaT         elif not isinstance(fill_value, self._recognized_scalars):              if self._scalar_type is Period and lib.is_integer(fill_value):                  new_fill = Period._from_ordinal(fill_value, freq=self.freq)             else:                 new_fill = self._scalar_type(fill_value)                warnings.warn(                 f"Passing {type(fill_value)} to shift is deprecated and "                 "will raise in a future version, pass "                 f"{self._scalar_type.__name__} instead.",                 FutureWarning,                 stacklevel=10,             )             fill_value = new_fill          fill_value = self._unbox_scalar(fill_value)         return fill_value     def _validate_searchsorted_value(self, value):         if isinstance(value, str):              try:                 value = self._scalar_from_string(value)             except ValueError as err:                 raise TypeError(                     "searchsorted requires compatible dtype or scalar"                 ) from err          elif is_valid_nat_for_dtype(value, self.dtype):             value = NaT          elif isinstance(value, self._recognized_scalars):             value = self._scalar_type(value)          elif is_list_like(value) and not isinstance(value, type(self)):             value = array(value)              if not type(self)._is_recognized_dtype(value):                 raise TypeError(                     "searchsorted requires compatible dtype or scalar, "                     f"not {type(value).__name__}"                 )          if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):             raise TypeError(f"Unexpected type for 'value': {type(value)}")          if isinstance(value, type(self)):             self._check_compatible_with(value)             value = value.asi8         else:             value = self._unbox_scalar(value)          return value      def _validate_setitem_value(self, value):         if lib.is_scalar(value) and not isna(value):             value = com.maybe_box_datetimelike(value)          if is_list_like(value):             value = type(self)._from_sequence(value, dtype=self.dtype)             self._check_compatible_with(value, setitem=True)             value = value.asi8         elif isinstance(value, self._scalar_type):             self._check_compatible_with(value, setitem=True)             value = self._unbox_scalar(value)         elif is_valid_nat_for_dtype(value, self.dtype):             value = iNaT         else:             msg = (                 f"'value' should be a '{self._scalar_type.__name__}', 'NaT', "                 f"or array of those. Got '{type(value).__name__}' instead."             )             raise TypeError(msg)          return value      def _validate_insert_value(self, value):         if isinstance(value, self._recognized_scalars):             value = self._scalar_type(value)             self._check_compatible_with(value, setitem=True)           elif is_valid_nat_for_dtype(value, self.dtype):              value = NaT         else:             raise TypeError(                 f"cannot insert {type(self).__name__} with incompatible label"             )          return value     def _validate_where_value(self, other):         if is_valid_nat_for_dtype(other, self.dtype):             other = NaT         elif isinstance(other, self._recognized_scalars):             other = self._scalar_type(other)             self._check_compatible_with(other, setitem=True)         elif not is_list_like(other):             raise TypeError(f"Where requires matching dtype, not {type(other)}")         else:               other = array(other)             other = extract_array(other, extract_numpy=True)              if is_categorical_dtype(other.dtype):                  if is_dtype_equal(other.categories.dtype, self.dtype):                     other = other._internal_get_values()              if not type(self)._is_recognized_dtype(other.dtype):                 raise TypeError(f"Where requires matching dtype, not {other.dtype}")             self._check_compatible_with(other, setitem=True)          if lib.is_scalar(other):             other = self._unbox_scalar(other)         else:             other = other.view("i8")         return other         def searchsorted(self, value, side="left", sorter=None):         value = self._validate_searchsorted_value(value)          return self.asi8.searchsorted(value, side=side, sorter=sorter)     def value_counts(self, dropna=False):         from pandas import Series, Index         if dropna:             values = self[~self.isna()]._data         else:             values = self._data         cls = type(self)         result = value_counts(values, sort=False, dropna=dropna)         index = Index(             cls(result.index.view("i8"), dtype=self.dtype), name=result.index.name         )         return Series(result._values, index=index, name=result.name)         return if each value is nan         return if I have any nans; enables various perf speedups         Parameters         ----------         result : a ndarray         fill_value : object, default iNaT         convert : str, dtype or None         Returns         -------         result : ndarray with values replace by the fill_value         mask the result if needed, convert to the provided dtype if its not         None         This is an internal routine.         Return the frequency object if it is set, otherwise None.
class Categorical(ExtensionArray, PandasObject):              )              raise ValueError(msg)         codes = np.asarray(codes)          if len(codes) and not is_integer_dtype(codes):              raise ValueError("codes need to be array-like integers")
def request_httprepr(request):      parsed = urlparse_cached(request)      path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))      s = to_bytes(request.method) + b" " + to_bytes(path) + b" HTTP/1.1\r\n"     s += b"Host: " + to_bytes(parsed.hostname) + b"\r\n"      if request.headers:          s += request.headers.to_string() + b"\r\n"      s += b"\r\n"
import bz2  from collections import abc  import gzip from io import BufferedIOBase, BytesIO  import mmap  import os  import pathlib
from keras.utils import Sequence  from keras import backend as K  pytestmark = pytest.mark.skipif(     K.backend() == 'tensorflow',      reason='Temporarily disabled until the use_multiprocessing problem is solved')  STEPS_PER_EPOCH = 100
def unified_strdate(date_str, day_first=True):          timetuple = email.utils.parsedate_tz(date_str)          if timetuple:              upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')     return compat_str(upload_date)  def determine_ext(url, default_ext='unknown_video'):
from pandas.core.dtypes.missing import isna, notna  from pandas.core import nanops, ops  from pandas.core.algorithms import take  from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin  from pandas.core.ops.common import unpack_zerodim_and_defer  from pandas.core.tools.numeric import to_numeric
class GroupBy(_GroupBy):          Parameters          ----------         output: Series or DataFrame              Object resulting from grouping and applying an operation.          Returns          -------
def to_pickle(obj, path, compression="infer", protocol=pickle.HIGHEST_PROTOCOL):      ----------      obj : any object          Any python object.     path : str         File path where the pickled object will be stored.      compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'         A string representing the compression to use in the output file. By         default, infers from the file extension in specified path.      protocol : int          Int which indicates which protocol should be used by the pickler,          default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible
class APIRouter(routing.Router):                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,                  )              elif isinstance(route, routing.Route):                  self.add_route(
class Model(Container):          for epoch in range(initial_epoch, epochs):             for m in self.metrics:                 if isinstance(m, Layer) and m.stateful:                     m.reset_states()              callbacks.on_epoch_begin(epoch)              epoch_logs = {}              if steps_per_epoch is not None:
from pandas.core.dtypes.common import (  )  from pandas.core.dtypes.missing import isna, notna  from pandas.core import nanops  import pandas.core.algorithms as algorithms  from pandas.core.arrays import Categorical, try_cast_to_ea
class RNN(Layer):              input_shape = input_shape[0]          if hasattr(self.cell.state_size, '__len__'):             output_dim = self.cell.state_size[0]          else:             output_dim = self.cell.state_size          if self.return_sequences:              output_shape = (input_shape[0], input_shape[1], output_dim)
class Selector(_ParselSelector, object_ref):      selectorlist_cls = SelectorList      def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):          st = _st(response, type or self._default_type)          if _root is not None:
class HTTP1Connection(httputil.HTTPConnection):              self._chunking_output = (                  start_line.method in ("POST", "PUT", "PATCH")                  and "Content-Length" not in headers                 and "Transfer-Encoding" not in headers              )          else:              assert isinstance(start_line, httputil.ResponseStartLine)
from scrapy.pipelines.media import MediaPipeline  from scrapy.exceptions import NotConfigured, IgnoreRequest  from scrapy.http import Request  from scrapy.utils.misc import md5sum  logger = logging.getLogger(__name__)
class Index(IndexOpsMixin, PandasObject):              return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)         elif (             is_interval_dtype(data) or is_interval_dtype(dtype)         ) and not is_object_dtype(dtype):             closed = kwargs.get("closed", None)             return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)          elif (              is_datetime64_any_dtype(data)
class Scraper(object):          logger.error(              "Spider error processing %(request)s (referer: %(referer)s)",              {'request': request, 'referer': referer},             extra={'spider': spider, 'failure': _failure}          )          self.signals.send_catch_log(              signal=signals.spider_error,
from thefuck.specific.git import git_support  @git_support  def match(command):      return ('push' in command.script_parts             and 'set-upstream' in command.output)  def _get_upstream_option_index(command_parts):
class FlvReader(io.BytesIO):      def read_unsigned_long_long(self):         return unpack('!Q', self.read(8))[0]      def read_unsigned_int(self):         return unpack('!I', self.read(4))[0]      def read_unsigned_char(self):         return unpack('!B', self.read(1))[0]      def read_string(self):          res = b''
class RNN(Layer):                  the size of the recurrent state                  (which should be the same as the size of the cell output).                  This can also be a list/tuple of integers                 (one size per state). In this case, the first entry                 (`state_size[0]`) should be the same as                 the size of the cell output.              It is also possible for `cell` to be a list of RNN cell instances,              in which cases the cells get stacked on after the other in the RNN,              implementing an efficient stacked RNN.
class Index(IndexOpsMixin, PandasObject):                              pass                         return Float64Index(data, copy=copy, dtype=dtype, name=name)                      elif inferred == "string":                          pass
def _search(stderr):  def match(command, settings):     return 'EDITOR' in os.environ and _search(command.stderr)  def get_new_command(command, settings):
class FileWriter(object):          self.overwrite = overwrite      def write(self, s):         with open(self.path, 'w' if self.overwrite else 'a') as output_file:              output_file.write(s)          self.overwrite = False
def map_obj_to_commands(updates, module):          if want['text'] and (want['text'] != have.get('text')):              banner_cmd = 'banner %s' % module.params['banner']              banner_cmd += ' @\n'             banner_cmd += want['text'].strip()              banner_cmd += '\n@'              commands.append(banner_cmd)
def weighted_masked_objective(fn):              score_array *= mask             score_array /= K.mean(mask)          if weights is not None:
class Bash(Generic):      def app_alias(self, fuck):          alias = "TF_ALIAS={0}" \                  " alias {0}='PYTHONIOENCODING=utf-8" \                 " TF_CMD=$(thefuck $(fc -ln -1)) && " \                  " eval $TF_CMD".format(fuck)          if settings.alter_history:
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def fit_generator(model,                  val_enqueuer_gen = val_enqueuer.get()              elif val_gen:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer_gen = iter_sequence_infinite(val_data)                      validation_steps = validation_steps or len(val_data)                  else:
class ItemMeta(ABCMeta):          new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))          _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)         fields = {}          new_attrs = {}          for n in dir(_class):              v = getattr(_class, n)
class Index(IndexOpsMixin, PandasObject):                      "unicode",                      "mixed",                  ]:                     return self._invalid_indexer("label", key)              elif kind in ["loc"] and is_integer(key):                  if not self.holds_integer():                     return self._invalid_indexer("label", key)          return key
class LightSource:                                   .format(lookup.keys)) from err         if hasattr(intensity, 'mask'):              mask = intensity.mask[..., 0]              for i in range(3):                  blend[..., i][mask] = rgb[..., i][mask]
class RNN(Layer):              output_shape = (input_shape[0], output_dim)          if self.return_state:             state_shape = [(input_shape[0], output_dim) for _ in self.states]              return [output_shape] + state_shape          else:              return output_shape
class TestScalar2:          result = ser.loc["a"]          assert result == 1         msg = (             "cannot do label indexing on Index "             r"with these indexers \[0\] of type int"         )         with pytest.raises(TypeError, match=msg):              ser.at[0]         with pytest.raises(TypeError, match=msg):              ser.loc[0]     def test_frame_raises_type_error(self):          df = DataFrame({"A": [1, 2, 3]}, index=list("abc"))          result = df.at["a", "A"]
async def schedule_formatting(      mode: Mode,      report: "Report",      loop: asyncio.AbstractEventLoop,     executor: Executor,  ) -> None:
class Progbar(object):          info = ' - %.0fs' % (now - self.start)          if self.verbose == 1:              if (not force and (now - self.last_update) < self.interval and                     current < self.target):                  return              prev_total_width = self.total_width
class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):          -------          loc : int          if is_valid_nat_for_dtype(key, self.dtype):              key = NaT
class Sanic:                  "Endpoint with name `{}` was not found".format(view_name)              )          if view_name == "static" or view_name.endswith(".static"):              filename = kwargs.pop("filename", None)
class Sequence(object):  _SHARED_SEQUENCES = {}
from .generic import Generic  class Zsh(Generic):      def app_alias(self, alias_name):         alias = "alias {0}='TF_ALIAS={0}" \                  " PYTHONIOENCODING=utf-8" \                 ' TF_SHELL_ALIASES=$(alias)' \                 " TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&" \                  " eval $TF_CMD".format(alias_name)          if settings.alter_history:
def _recursively_freeze(value):      Parameter whose value is a ``dict``.
def _get_join_indexers(     lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)      kwargs = copy.copy(kwargs)      if how == "left":
class MetricsHandler(tornado.web.RequestHandler):          self._scheduler = scheduler      def get(self):         metrics = self._scheduler._state._metrics_collector.generate_latest()          if metrics:             metrics.configure_http_handler(self)              self.write(metrics)
class Rolling(_Rolling_and_Expanding):      def _on(self):          if self.on is None:             return self.obj.index          elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:              return Index(self.obj[self.on])          else:
from typing import (      Callable,      Collection,      Dict,      Generic,      Iterable,      Iterator,
def to_categorical(y, num_classes=None):      y = np.array(y, dtype='int')      input_shape = y.shape      y = y.ravel()      if not num_classes:          num_classes = np.max(y) + 1
GRAMMARS = [  def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement     if src_txt[-1] != "\n":          src_txt += "\n"      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)
class Field(dict):  class ItemMeta(ABCMeta):      def __new__(mcs, class_name, bases, attrs):          new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))          _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)
def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -      if src_txt[-1:] != "\n":          src_txt += "\n"     for grammar in get_grammars(set(target_versions)):         drv = driver.Driver(grammar, pytree.convert)          try:              result = drv.parse_string(src_txt, True)              break
class YoutubeDL(object):                  comparison_value = m.group('value')                  str_op = STR_OPERATORS[m.group('op')]                  if m.group('negation'):                     op = lambda attr, value: not str_op                  else:                      op = str_op
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def standardize_weights(y,                               'sample-wise weights, make sure your '                               'sample_weight array is 1D.')     if sample_weight is not None and class_weight is not None:         warnings.warn('Found both `sample_weight` and `class_weight`: '                       '`class_weight` argument will be ignored.')       if sample_weight is not None:          if len(sample_weight.shape) > len(y.shape):              raise ValueError('Found a sample_weight with shape' +
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),              False,              padding,              padding],         output_shape=output_shape)      return _postprocess_conv2d_output(x, data_format)
https://support.sas.com/techsup/technote/ts140.pdf  from collections import abc  from datetime import datetime from io import BytesIO  import struct  import warnings
class Orthogonal(Initializer):          rng = np.random          if self.seed is not None:              rng = np.random.RandomState(self.seed)          a = rng.normal(0.0, 1.0, flat_shape)          u, _, v = np.linalg.svd(a, full_matrices=False)
class _AxesBase(martist.Artist):              return          dL = self.dataLim         x0, x1 = map(x_trf.inverted().transform, dL.intervalx)         y0, y1 = map(y_trf.inverted().transform, dL.intervaly)          xr = 1.05 * (x1 - x0)          yr = 1.05 * (y1 - y0)
except ImportError: from pydantic.fields import Field as ModelField  async def serialize_response(      *,      field: ModelField = None,
class SortedCorrectedCommandsSequence(object):      def _realise(self):         commands = self._remove_duplicates(self._commands)         self._cached = [self._cached[0]] + sorted(             commands, key=lambda corrected_command: corrected_command.priority)          self._realised = True          debug('SortedCommandsSequence was realised with: {}, after: {}'.format(              self._cached, '\n'.join(format_stack())), self._settings)
def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:              trailing_comma_safe = trailing_comma_safe and py36          leaf_priority = delimiters.get(id(leaf))          if leaf_priority == delimiter_priority:             normalize_prefix(current_line.leaves[0], inside_brackets=True)              yield current_line              current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
def _can_use_numexpr(op, op_str, a, b, dtype_check):          if np.prod(a.shape) > _MIN_ELEMENTS:               dtypes = set()              for o in [a, b]:                 if hasattr(o, "dtypes"):                      s = o.dtypes.value_counts()                      if len(s) > 1:                          return False                      dtypes |= set(s.index.astype(str))                 elif isinstance(o, np.ndarray):                      dtypes |= {o.dtype.name}
class ResponseTypes(object):      def from_content_disposition(self, content_disposition):          try:             filename = to_native_str(content_disposition).split(';')[1].split('=')[1]              filename = filename.strip('"\'')              return self.from_filename(filename)          except IndexError:
class Driver(object):      def parse_stream_raw(self, stream, debug=False):         tokens = tokenize.generate_tokens(stream.readline)          return self.parse_tokens(tokens, debug)      def parse_stream(self, stream, debug=False):
class Feature(Enum):      NUMERIC_UNDERSCORES = 3      TRAILING_COMMA_IN_CALL = 4      TRAILING_COMMA_IN_DEF = 5  VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {     TargetVersion.PY27: set(),     TargetVersion.PY33: {Feature.UNICODE_LITERALS},     TargetVersion.PY34: {Feature.UNICODE_LITERALS},     TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},      TargetVersion.PY36: {          Feature.UNICODE_LITERALS,          Feature.F_STRINGS,          Feature.NUMERIC_UNDERSCORES,          Feature.TRAILING_COMMA_IN_CALL,          Feature.TRAILING_COMMA_IN_DEF,      },      TargetVersion.PY37: {          Feature.UNICODE_LITERALS,
class ImagesPipeline(FilesPipeline):              background = Image.new('RGBA', image.size, (255, 255, 255))              background.paste(image, image)              image = background.convert('RGB')          elif image.mode != 'RGB':              image = image.convert('RGB')
def generate_context(      context = OrderedDict([])      try:         with open(context_file) as file_handle:              obj = json.load(file_handle, object_pairs_hook=OrderedDict)      except ValueError as e:
def run_hook(hook_name, project_dir, context):      script = find_hooks().get(hook_name)      if script is None:          logging.debug('No hooks found')         return EXIT_SUCCESS     return run_script_with_context(script, project_dir, context)
import tempfile  from jinja2 import Template  from cookiecutter import utils  _HOOKS = [
def _match_one(filter_part, dct):          \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*          (?:              (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|              (?P<strval>(?![0-9.])[a-z0-9A-Z]*)          )          \s*$
def match(command, settings):     return 'ls' in command.script and not ('ls -' in command.script)  def get_new_command(command, settings):
def unified_timestamp(date_str, day_first=True):      for expression in date_formats(day_first):          try:             dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta              return calendar.timegm(dt.timetuple())          except ValueError:              pass      timetuple = email.utils.parsedate_tz(date_str)      if timetuple:         return calendar.timegm(timetuple.timetuple())  def determine_ext(url, default_ext='unknown_video'):
class CategoricalBlock(ExtensionBlock):              )          return result
class Scheduler(object):                  for batch_task in self._state.get_batch_running_tasks(task.batch_id):                      batch_task.expl = expl         if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:               if status == PENDING or status != task.status:
class IntegerArray(BaseMaskedArray):              if incompatible type with an IntegerDtype, equivalent of same_kind              casting          if isinstance(dtype, _IntegerDtype):              result = self._data.astype(dtype.numpy_dtype, copy=False)              return type(self)(result, mask=self._mask, copy=False)          if is_float_dtype(dtype):
class Model(Container):          nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)          self.metrics_updates = []          self.stateful_metric_names = []          with K.name_scope('metrics'):              for i in range(len(self.outputs)):                  if i in skip_target_indices:
class DatetimeBlock(DatetimeLikeBlockMixin, Block):          ).reshape(i8values.shape)          return np.atleast_2d(result)     def should_store(self, value) -> bool:         return is_datetime64_dtype(value.dtype)       def set(self, locs, values):          values = conversion.ensure_datetime64ns(values, copy=False)
class FacebookIE(InfoExtractor):              video_title = self._html_search_regex(                  r'(?s)<span class="fbPhotosPhotoCaption".*?id="fbPhotoPageCaption"><span class="hasCaption">(.*?)</span>',                  webpage, 'alternative title', default=None)             if len(video_title) > 80 + 3:                 video_title = video_title[:80] + '...'          if not video_title: video_title = 'Facebook video
class TimeGrouper(Grouper):          rng += freq_mult          rng -= bin_shift         bins = memb.searchsorted(rng, side="left")          if nat_count > 0:
def predict_generator(model, generator,      steps_done = 0      all_outs = []     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator'
class BaseAsyncIOLoop(IOLoop):          self.readers = set()          self.writers = set()          self.closing = False          IOLoop._ioloop_for_asyncio[asyncio_loop] = self          super(BaseAsyncIOLoop, self).initialize(**kwargs)
class Tracer:          @pysnooper.snoop(thread_info=True)      def __init__(              self,
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class Index(IndexOpsMixin, PandasObject):          left_indexer = self.get_indexer(target, "pad", limit=limit)          right_indexer = self.get_indexer(target, "backfill", limit=limit)         target = np.asarray(target)         left_distances = abs(self.values[left_indexer] - target)         right_distances = abs(self.values[right_indexer] - target)          op = operator.lt if self.is_monotonic_increasing else operator.le          indexer = np.where(
class GalaxyAPI:              data = self._call_galaxy(url)              results = data['results']              done = (data.get('next_link', None) is None)              while not done:                 url = _urljoin(self.api_server, data['next_link'])                  data = self._call_galaxy(url)                  results += data['results']                  done = (data.get('next_link', None) is None)          except Exception as e:             display.vvvv("Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s"                          % (role_id, related, to_text(e)))          return results      @g_connect(['v1'])
class Parser:          for date_unit in date_units:              try:                  new_data = to_datetime(new_data, errors="raise", unit=date_unit)             except (ValueError, OverflowError):                  continue              return new_data, True          return data, False
def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:      else:          BodySchema = params.Body      field = Field(          name="body",          type_=BodyModel,
class Bidirectional(Wrapper):              return self.forward_layer.updates + self.backward_layer.updates          return []      @property      def losses(self):          if hasattr(self.forward_layer, 'losses'):              return self.forward_layer.losses + self.backward_layer.losses          return []      @property      def constraints(self):          constraints = {}
class SeriesGroupBy(GroupBy):          minlength = ngroups or 0          out = np.bincount(ids[mask], minlength=minlength)         return Series(              out,              index=self.grouper.result_index,              name=self._selection_name,              dtype="int64",          )      def _apply_to_column_groupbys(self, func):
class Sanic:          netloc = kwargs.pop("_server", None)          if netloc is None and external:             netloc = self.config.get("SERVER_NAME", "")          if external:              if not scheme:
class DataFrame(NDFrame):          if is_transposed:              data = data.T          result = data._data.quantile(              qs=q, axis=1, interpolation=interpolation, transposed=is_transposed          )
from pandas._libs.lib import infer_dtype  from pandas.core.dtypes.common import (      _NS_DTYPE,      ensure_int64,      is_categorical_dtype,      is_datetime64_dtype,      is_datetime64tz_dtype,
class Categorical(ExtensionArray, PandasObject):          max : the maximum of this `Categorical`          self.check_for_ordered("max")          good = self._codes != -1          if not good.all():              if skipna:
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              res._is_copy = self._is_copy          return res     def _iget_item_cache(self, item):          ax = self._info_axis          if ax.is_unique:              lower = self._get_item_cache(ax[item])          else:             lower = self._take_with_is_copy(item, axis=self._info_axis_number)          return lower      def _box_item_values(self, key, values):
def jsonable_encoder(      custom_encoder: dict = {},  ) -> Any:      if isinstance(obj, BaseModel):         if not obj.Config.json_encoders:             return jsonable_encoder(                 obj.dict(include=include, exclude=exclude, by_alias=by_alias),                 include_none=include_none,             )         else:             return jsonable_encoder(                 obj.dict(include=include, exclude=exclude, by_alias=by_alias),                 include_none=include_none,                 custom_encoder=obj.Config.json_encoders,             )      if isinstance(obj, Enum):          return obj.value      if isinstance(obj, (str, int, float, type(None))):
def url_has_any_extension(url, extensions):  def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):      return (          to_native_str(parts.scheme),         to_native_str(parts.netloc.encode('idna')),          quote(to_bytes(parts.path, path_encoding), _safe_chars),
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):          vmin, vmax = vmax, vmin          swapped = True      maxabsvalue = max(abs(vmin), abs(vmax))      if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:          vmin = -expander
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def sanitize_array(          arr = np.arange(data.start, data.stop, data.step, dtype="int64")          subarr = _try_cast(arr, dtype, copy, raise_cast_failure)      else:          subarr = _try_cast(data, dtype, copy, raise_cast_failure)
class GeneratorEnqueuer(SequenceEnqueuer):                  try:                      if self._use_multiprocessing or self.queue.qsize() < max_queue_size:                          generator_output = next(self._generator)                         self.queue.put(generator_output)                      else:                          time.sleep(self.wait_time)                  except StopIteration:                      break                 except Exception:                      self._stop_event.set()                     raise          try:              if self._use_multiprocessing:                 self.queue = multiprocessing.Queue(maxsize=max_queue_size)                  self._stop_event = multiprocessing.Event()              else:                  self.queue = queue.Queue()
__credits__ = \  import re  from codecs import BOM_UTF8, lookup  from blib2to3.pgen2.token import *  from . import token
class tqdm(Comparable):                  if ncols else 10,                  charset=Bar.BLANK)              res = bar_format.format(bar=full_bar, **format_dict)             if ncols:                 return disp_trim(res, ncols)          else:              return ((prefix + ": ") if prefix else '') + \
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z1,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
def _iter_command_classes(module_name):      for module in walk_modules(module_name):         for obj in vars(module).itervalues():              if inspect.isclass(obj) and \                issubclass(obj, ScrapyCommand) and \                obj.__module__ == module.__name__:                  yield obj  def _get_commands_from_module(module, inproject):
def _htmlentity_transform(entity):              numstr = '0%s' % numstr          else:              base = 10         return compat_chr(int(numstr, base))      return ('&%s;' % entity)
class IntervalArray(IntervalMixin, ExtensionArray):                  msg = f"'value' should be an interval type, got {type(value)} instead."                  raise TypeError(msg) from err          key = check_array_indexer(self, key)          left = self.left.copy(deep=True)         if needs_float_conversion:             left = left.astype("float")         left.values[key] = value_left          self._left = left          right = self.right.copy(deep=True)         if needs_float_conversion:             right = right.astype("float")         right.values[key] = value_right          self._right = right      def __eq__(self, other):
class SimpleTaskState(object):              elif task.scheduler_disable_time is not None:                  return         if new_status == FAILED and task.can_disable():              task.add_failure()              if task.has_excessive_failures():                  task.scheduler_disable_time = time.time()
class TestBackend(object):      def test_log(self):          check_single_tensor_operation('log', (4, 2), WITH_NP)      @pytest.mark.skipif(K.backend() == 'theano',                          reason='theano returns tuples for update ops')      def test_update_add(self):         x = np.random.randn(3, 4)          x_var = K.variable(x)         increment = np.random.randn(3, 4)         x += increment         K.eval(K.update_add(x_var, increment))         assert_allclose(x, K.eval(x_var), atol=1e-05)      @pytest.mark.skipif(K.backend() == 'theano',                          reason='theano returns tuples for update ops')      def test_update_sub(self):         x = np.random.randn(3, 4)          x_var = K.variable(x)         decrement = np.random.randn(3, 4)         x -= decrement         K.eval(K.update_sub(x_var, decrement))         assert_allclose(x, K.eval(x_var), atol=1e-05)      @pytest.mark.skipif(K.backend() == 'cntk',                          reason='cntk doesn\'t support gradient in this way.')
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
import pandas.core.algorithms as algos  from pandas.core.arrays import ExtensionArray  from pandas.core.base import IndexOpsMixin, PandasObject  import pandas.core.common as com  from pandas.core.indexers import maybe_convert_indices  from pandas.core.indexes.frozen import FrozenList  import pandas.core.missing as missing
class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):          logger.info("Inserting file: %s", f)          colnames = ''         if len(self.columns) > 0:              colnames = ",".join([x[0] for x in self.columns])              colnames = '({})'.format(colnames)
class GroupBy(_GroupBy):              if not self.observed and isinstance(result_index, CategoricalIndex):                  out = out.reindex(result_index)              return out.sort_index() if self.sort else out
class RangeIndex(Int64Index):      @Appender(_index_shared_docs["get_indexer"])      def get_indexer(self, target, method=None, limit=None, tolerance=None):         if not (method is None and tolerance is None and is_list_like(target)):             return super().get_indexer(target, method=method, tolerance=tolerance)          if self.step > 0:              start, stop, step = self.start, self.stop, self.step
class Line:              and self.leaves[0].value == 'yield'          )          if not (
def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr      pandas object          The original object that was pickled and then re-read.     if path is None:         path = f"__{rands(10)}__.pickle"     with ensure_clean(path) as path:         pd.to_pickle(obj, path)         return pd.read_pickle(path)  def round_trip_pathlib(writer, reader, path: Optional[str] = None):
class DirectoryIterator(Iterator):              fname = self.filenames[j]              img = load_img(os.path.join(self.directory, fname),                             grayscale=grayscale,                            target_size=self.target_size,                             interpolation=self.interpolation)              x = img_to_array(img, data_format=self.data_format)              x = self.image_data_generator.random_transform(x)              x = self.image_data_generator.standardize(x)
class tqdm(Comparable):              if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):                  bar_format = _unicode(bar_format)              res = bar_format.format(bar=full_bar, **format_dict)             if ncols:                 return disp_trim(res, ncols)          elif bar_format:
class TestScalar2:          result = df.loc["a", "A"]          assert result == 1         msg = (             "cannot do label indexing on Index "             r"with these indexers \[0\] of type int"         )         with pytest.raises(TypeError, match=msg):              df.at["a", 0]         with pytest.raises(TypeError, match=msg):              df.loc["a", 0]      def test_series_at_raises_key_error(self):
class LinuxHardware(Hardware):      MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*"')      def populate(self, collected_facts=None):          hardware_facts = {}          self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):                  new_index = self.index[loc]          if is_scalar(loc):             new_values = self._data.fast_xs(loc)                  if not is_list_like(new_values) or self.ndim == 1:                 return com.maybe_box_datetimelike(new_values)              result = self._constructor_sliced(                  new_values,
def main(model="en_core_web_sm"):  def filter_spans(spans):     get_sort_key = lambda span: (span.end - span.start, span.start)      sorted_spans = sorted(spans, key=get_sort_key, reverse=True)      result = []      seen_tokens = set()      for span in sorted_spans:          if span.start not in seen_tokens and span.end - 1 not in seen_tokens:              result.append(span)             seen_tokens.update(range(span.start, span.end))      return result
from pandas._libs import NaT, Timedelta, index as libindex from pandas._typing import Label  from pandas.util._decorators import Appender  from pandas.core.dtypes.common import (
from datetime import datetime, timedelta  import operator from typing import Any, Sequence, Type, Union, cast import warnings  import numpy as np from pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib from pandas._libs.tslibs.c_timestamp import integer_op_not_supported from pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds from pandas._libs.tslibs.timestamps import RoundTo, round_nsint64 from pandas._typing import DatetimeLikeScalar from pandas.compat import set_function_name  from pandas.compat.numpy import function as nv from pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning from pandas.util._decorators import Appender, Substitution from pandas.util._validators import validate_fillna_kwargs  from pandas.core.dtypes.common import (      is_categorical_dtype,     is_datetime64_any_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,     is_datetime_or_timedelta_dtype,      is_dtype_equal,     is_float_dtype,     is_integer_dtype,      is_list_like,     is_object_dtype,      is_period_dtype,     is_string_dtype,     is_timedelta64_dtype,     is_unsigned_integer_dtype,     pandas_dtype,  ) from pandas.core.dtypes.generic import ABCIndexClass, ABCPeriodArray, ABCSeries from pandas.core.dtypes.inference import is_array_like from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna  from pandas.core import missing, nanops, ops from pandas.core.algorithms import checked_add_with_arr, take, unique1d, value_counts import pandas.core.common as com from pandas.core.indexers import check_bool_array_indexer from pandas.core.ops.common import unpack_zerodim_and_defer from pandas.core.ops.invalid import invalid_comparison, make_invalid_op  from pandas.tseries import frequencies from pandas.tseries.offsets import DateOffset, Tick from .base import ExtensionArray, ExtensionOpsMixin def _datetimelike_array_cmp(cls, op):     opname = f"__{op.__name__}__"     nat_result = opname == "__ne__"     @unpack_zerodim_and_defer(opname)     def wrapper(self, other):          if isinstance(other, str):             try:                  other = self._scalar_from_string(other)             except ValueError:                  return invalid_comparison(self, other, op)         if isinstance(other, self._recognized_scalars) or other is NaT:             other = self._scalar_type(other)             self._check_compatible_with(other)             other_i8 = self._unbox_scalar(other)             result = op(self.view("i8"), other_i8)             if isna(other):                 result.fill(nat_result)          elif not is_list_like(other):             return invalid_comparison(self, other, op)          elif len(other) != len(self):             raise ValueError("Lengths must match")          else:             if isinstance(other, list):                  other = np.array(other)              if not isinstance(other, (np.ndarray, type(self))):                 return invalid_comparison(self, other, op)              if is_object_dtype(other):                    with np.errstate(all="ignore"):                     result = ops.comp_method_OBJECT_ARRAY(                         op, self.astype(object), other                     )                 o_mask = isna(other)              elif not type(self)._is_recognized_dtype(other.dtype):                 return invalid_comparison(self, other, op)              else:                  other = type(self)._from_sequence(other)                 self._check_compatible_with(other)                 result = op(self.view("i8"), other.view("i8"))                 o_mask = other._isnan              if o_mask.any():                 result[o_mask] = nat_result          if self._hasnans:             result[self._isnan] = nat_result          return result      return set_function_name(wrapper, opname, cls) class AttributesMixin:     _data: np.ndarray     @classmethod     def _simple_new(cls, values, **kwargs):         raise AbstractMethodError(cls)      @property     def _scalar_type(self) -> Type[DatetimeLikeScalar]:         raise AbstractMethodError(self)      def _scalar_from_string(         self, value: str     ) -> Union[Period, Timestamp, Timedelta, NaTType]:         raise AbstractMethodError(self)      def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:         raise AbstractMethodError(self)      def _check_compatible_with(         self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False     ) -> None:         raise AbstractMethodError(self) class DatelikeOps:     @Substitution(         URL="https://docs.python.org/3/library/datetime.html"         "     )     def strftime(self, date_format):         result = self._format_native_types(date_format=date_format, na_rep=np.nan)         return result.astype(object)   class TimelikeOps:              - 'shift_forward' will shift the nonexistent time forward to the               closest existing time             - 'shift_backward' will shift the nonexistent time backward to the               closest existing time             - 'NaT' will return NaT where there are nonexistent times             - timedelta objects will shift nonexistent times by the timedelta             - 'raise' will raise an NonExistentTimeError if there are               nonexistent times.             .. versionadded:: 0.24.0         Returns         -------         DatetimeIndex, TimedeltaIndex, or Series             Index of the same type for a DatetimeIndex or TimedeltaIndex,             or a Series with the same index for a Series.         Raises         ------         ValueError if the `freq` cannot be converted.     def _round(self, freq, mode, ambiguous, nonexistent):          values = _ensure_datetimelike_to_i8(self)         result = round_nsint64(values, mode, freq)         result = self._maybe_mask_results(result, fill_value=NaT)          dtype = self.dtype         if is_datetime64tz_dtype(self):             dtype = None         return self._ensure_localized(             self._simple_new(result, dtype=dtype), ambiguous, nonexistent         )      @Appender((_round_doc + _round_example).format(op="round"))     def round(self, freq, ambiguous="raise", nonexistent="raise"):         return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)      @Appender((_round_doc + _floor_example).format(op="floor"))     def floor(self, freq, ambiguous="raise", nonexistent="raise"):         return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)      @Appender((_round_doc + _ceil_example).format(op="ceil"))     def ceil(self, freq, ambiguous="raise", nonexistent="raise"):         return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)     Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray     Assumes that __new__/__init__ defines:         _data         _freq     @property     def ndim(self) -> int:         return self._data.ndim         box function to get object from internal representation         apply box func to passed values         Integer representation of the values.         Returns         -------         ndarray             An ndarray with int64 dtype.         Helper method for astype when converting to strings.          Returns          -------         ndarray[str]         return np.prod(self.shape)     def __len__(self) -> int:         return len(self._data)     def __getitem__(self, key):         is_int = lib.is_integer(key)         if lib.is_scalar(key) and not is_int:             raise IndexError(                 "only integers, slices (`:`), ellipsis (`...`), "                 "numpy.newaxis (`None`) and integer or boolean "                 "arrays are valid indices"             )         getitem = self._data.__getitem__         if is_int:             val = getitem(key)             if lib.is_scalar(val):                  return self._box_func(val)             return type(self)(val, dtype=self.dtype)          if com.is_bool_indexer(key):             key = check_bool_array_indexer(self, key)             if key.all():                 key = slice(0, None, None)             else:                 key = lib.maybe_booleans_to_slice(key.view(np.uint8))         is_period = is_period_dtype(self)         if is_period:             freq = self.freq         else:             freq = None             if isinstance(key, slice):                 if self.freq is not None and key.step is not None:                     freq = key.step * self.freq                 else:                     freq = self.freq             elif key is Ellipsis:                   freq = self.freq          result = getitem(key)         if result.ndim > 1:               if is_period:                 return self._simple_new(result, dtype=self.dtype, freq=freq)             return result         return self._simple_new(result, dtype=self.dtype, freq=freq)      def __setitem__(         self,         key: Union[int, Sequence[int], Sequence[bool], slice],         value: Union[NaTType, Any, Sequence[Any]],     ) -> None:              if lib.is_scalar(value) and not isna(value):             value = com.maybe_box_datetimelike(value)          if is_list_like(value):             is_slice = isinstance(key, slice)              if lib.is_scalar(key):                 raise ValueError("setting an array element with a sequence.")              if not is_slice:                 key = cast(Sequence, key)                 if len(key) != len(value) and not com.is_bool_indexer(key):                     msg = (                         f"shape mismatch: value array of length '{len(key)}' "                         "does not match indexing result of length "                         f"'{len(value)}'."                     )                     raise ValueError(msg)                 elif not len(key):                     return              value = type(self)._from_sequence(value, dtype=self.dtype)             self._check_compatible_with(value, setitem=True)             value = value.asi8         elif isinstance(value, self._scalar_type):             self._check_compatible_with(value, setitem=True)             value = self._unbox_scalar(value)         elif is_valid_nat_for_dtype(value, self.dtype):             value = iNaT          else:             msg = (                 f"'value' should be a '{self._scalar_type.__name__}', 'NaT', "                 f"or array of those. Got '{type(value).__name__}' instead."             )             raise TypeError(msg)         self._data[key] = value         self._maybe_clear_freq()      def _maybe_clear_freq(self):           pass     def astype(self, dtype, copy=True):             from pandas import Categorical          dtype = pandas_dtype(dtype)          if is_object_dtype(dtype):             return self._box_values(self.asi8)         elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):             return self._format_native_types()         elif is_integer_dtype(dtype):               values = self.asi8              if is_unsigned_integer_dtype(dtype):                  values = values.view("uint64")              if copy:                 values = values.copy()             return values         elif (             is_datetime_or_timedelta_dtype(dtype)             and not is_dtype_equal(self.dtype, dtype)         ) or is_float_dtype(dtype):               msg = f"Cannot cast {type(self).__name__} to dtype {dtype}"             raise TypeError(msg)         elif is_categorical_dtype(dtype):             return Categorical(self, dtype=dtype)         else:             return np.asarray(self, dtype=dtype)     def view(self, dtype=None):         if dtype is None or dtype is self.dtype:             return type(self)(self._data, dtype=self.dtype)         return self._data.view(dtype=dtype)       def unique(self):         result = unique1d(self.asi8)         return type(self)(result, dtype=self.dtype)     def _validate_fill_value(self, fill_value):         if isna(fill_value):             fill_value = iNaT         elif isinstance(fill_value, self._recognized_scalars):             self._check_compatible_with(fill_value)             fill_value = self._scalar_type(fill_value)             fill_value = self._unbox_scalar(fill_value)          else:             raise ValueError(                 f"'fill_value' should be a {self._scalar_type}. Got '{fill_value}'."             )         return fill_value      def take(self, indices, allow_fill=False, fill_value=None):         if allow_fill:             fill_value = self._validate_fill_value(fill_value)         new_values = take(             self.asi8, indices, allow_fill=allow_fill, fill_value=fill_value         )         return type(self)(new_values, dtype=self.dtype)     @classmethod     def _concat_same_type(cls, to_concat):         dtypes = {x.dtype for x in to_concat}         assert len(dtypes) == 1         dtype = list(dtypes)[0]         values = np.concatenate([x.asi8 for x in to_concat])         return cls(values, dtype=dtype)     def copy(self):         values = self.asi8.copy()         return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)     def _values_for_factorize(self):         return self.asi8, iNaT     @classmethod     def _from_factorized(cls, values, original):         return cls(values, dtype=original.dtype)     def _values_for_argsort(self):         return self._data         def searchsorted(self, value, side="left", sorter=None):         if isinstance(value, str):             value = self._scalar_from_string(value)         if not (isinstance(value, (self._scalar_type, type(self))) or isna(value)):             raise ValueError(f"Unexpected type for 'value': {type(value)}")          self._check_compatible_with(value)         if isinstance(value, type(self)):             value = value.asi8         else:             value = self._unbox_scalar(value)          return self.asi8.searchsorted(value, side=side, sorter=sorter)     def repeat(self, repeats, *args, **kwargs):         nv.validate_repeat(args, kwargs)         values = self._data.repeat(repeats)         return type(self)(values.view("i8"), dtype=self.dtype)      def value_counts(self, dropna=False):         from pandas import Series, Index         if dropna:             values = self[~self.isna()]._data          else:             values = self._data          cls = type(self)          result = value_counts(values, sort=False, dropna=dropna)         index = Index(             cls(result.index.view("i8"), dtype=self.dtype), name=result.index.name         )         return Series(result.values, index=index, name=result.name)     def map(self, mapper):              from pandas import Index         return Index(self).map(mapper).array       def isna(self):         return self._isnan     @property     def _isnan(self):         return self.asi8 == iNaT     @property     def _hasnans(self):         return bool(self._isnan.any())      def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):          if self._hasnans:             if convert:                 result = result.astype(convert)             if fill_value is None:                 fill_value = np.nan             result[self._isnan] = fill_value         return result      def fillna(self, value=None, method=None, limit=None):            if isinstance(value, ABCSeries):             value = value.array          value, method = validate_fillna_kwargs(value, method)          mask = self.isna()          if is_array_like(value):             if len(value) != len(self):                 raise ValueError(                     f"Length of 'value' does not match. Got ({len(value)}) "                     f" expected {len(self)}"                 )             value = value[mask]          if mask.any():             if method is not None:                 if method == "pad":                     func = missing.pad_1d                 else:                     func = missing.backfill_1d                  values = self._data                 if not is_period_dtype(self):                        values = values.copy()                  new_values = func(values, limit=limit, mask=mask)                 if is_datetime64tz_dtype(self):                       new_values = new_values.view("i8")                 new_values = type(self)(new_values, dtype=self.dtype)             else:                  new_values = self.copy()                 new_values[mask] = value         Return the frequency object if it is set, otherwise None.         Return the frequency object as a string if its set, otherwise None         Tryies to return a string representing a frequency guess,         generated by infer_freq.  Returns None if it can't autodetect the         frequency.         Returns day, hour, minute, second, millisecond or microsecond         Validate that a frequency is compatible with the values of a given         Datetime Array/Index or Timedelta Array/Index          Parameters          ----------         index : DatetimeIndex or TimedeltaIndex             The index on which to determine if the given frequency is valid         freq : DateOffset             The frequency to validate         Add a timedelta-like, Tick or TimedeltaIndex-like object         to self, yielding an int64 numpy array         Parameters         ----------         delta : {timedelta, np.timedelta64, Tick,                  TimedeltaIndex, ndarray[timedelta64]}          Returns          -------         result : ndarray[int64]          Notes         -----         The result's name is set outside of _add_delta by the calling         method (__add__ or __sub__), if necessary (i.e. for Indexes).         Add a delta of a timedeltalike         return the i8 result view         Add a delta of a TimedeltaIndex         return the i8 result view         Add pd.NaT to self         Subtract pd.NaT from self         Subtract a Period Array/Index from self.  This is only valid if self         is itself a Period Array/Index, raises otherwise.  Both objects must         have the same frequency.         Parameters         ----------         other : PeriodIndex or PeriodArray         Returns         -------         result : np.ndarray[object]             Array of DateOffset objects; nulls represented by NaT.         Add or subtract array-like of DateOffset objects         Parameters         ----------         other : np.ndarray[object]         op : {operator.add, operator.sub}         Returns         -------         result : same class as self         Shift each value by `periods`.          Note this is different from ExtensionArray.shift, which         shifts the *position* of each element, padding the end with         missing values.          Parameters         ----------         periods : int             Number of periods to shift by.         freq : pandas.DateOffset, pandas.Timedelta, or str             Frequency increment to shift by.         Ensure that we are re-localized.         This is for compat as we can then call this on all datetimelike         arrays generally (ignored for Period/Timedelta)          Parameters         ----------         arg : Union[DatetimeLikeArray, DatetimeIndexOpsMixin, ndarray]         ambiguous : str, bool, or bool-ndarray, default 'raise'         nonexistent : str, default 'raise'         from_utc : bool, default False             If True, localize the i8 ndarray to UTC first before converting to             the appropriate tz. If False, localize directly to the tz.          Returns         -------         localized array         Return the minimum value of the Array or minimum along         an axis.          See Also         --------         numpy.ndarray.min         Index.min : Return the minimum value in an Index.         Series.min : Return the minimum value in a Series.         Return the maximum value of the Array or maximum along         an axis.         See Also         --------         numpy.ndarray.max         Index.max : Return the maximum value in an Index.         Series.max : Return the maximum value in a Series.         Return the mean value of the Array.          .. versionadded:: 0.25.0          Parameters         ----------         skipna : bool, default True             Whether to ignore any NaT elements.         Returns         -------         scalar             Timestamp or Timedelta.         See Also         --------         numpy.ndarray.mean : Returns the average of array elements along a given axis.         Series.mean : Return the mean value in a Series.         Notes         -----         mean is only defined for Datetime and Timedelta dtypes, not for Period.     If a `periods` argument is passed to the Datetime/Timedelta Array/Index     constructor, cast it to an integer.      Parameters     ----------     periods : None, float, int      Returns     -------     periods : None or int      Raises     ------     TypeError         if periods is None, float, or int     Check that the `closed` argument is among [None, "left", "right"]     Parameters     ----------     closed : {None, "left", "right"}      Returns     -------     left_closed : bool     right_closed : bool      Raises     ------     ValueError : if argument is not among valid values     If the user passes a freq and another freq is inferred from passed data,     require that they match.      Parameters     ----------     freq : DateOffset or None     inferred_freq : DateOffset or None     freq_infer : bool      Returns     -------     freq : DateOffset or None     freq_infer : bool      Notes     -----     We assume at this point that `maybe_infer_freq` has been called, so     `freq` is either a DateOffset object or None.     Comparing a DateOffset to the string "infer" raises, so we need to     be careful about comparisons.  Make a dummy variable `freq_infer` to     signify the case where the given freq is "infer" and set freq to None     to avoid comparison trouble later on.      Parameters     ----------     freq : {DateOffset, None, str}      Returns     -------     freq : {DateOffset, None}     freq_infer : bool     Helper for coercing an input scalar or array to i8.      Parameters     ----------     other : 1d array     to_utc : bool, default False         If True, convert the values to UTC before extracting the i8 values         If False, extract the i8 values directly.      Returns     -------     i8 1d array
async def request_validation_exception_handler(      request: Request, exc: RequestValidationError  ) -> JSONResponse:      return JSONResponse(         status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={"detail": exc.errors()}      )
def _preprocess_conv1d_input(x, data_format):      return x, tf_data_format def _preprocess_conv2d_input(x, data_format):          x: input tensor.          data_format: string, `"channels_last"` or `"channels_first"`.          A tensor.
class SymLogNorm(Normalize):          linscale : float, default: 1              This allows the linear range (-*linthresh* to *linthresh*) to be              stretched relative to the logarithmic range. Its value is the             number of decades to use for each half of the linear range. For             example, when *linscale* == 1.0 (the default), the space used for             the positive and negative halves of the linear range will be equal             to one decade in the logarithmic range.          Normalize.__init__(self, vmin, vmax, clip)          self.linthresh = float(linthresh)         self._linscale_adj = (linscale / (1.0 - np.e ** -1))          if vmin is not None and vmax is not None:              self._transform_vmin_vmax()
class TensorBoard(Callback):                          tf.summary.image(mapped_weight_name, w_img)                  if hasattr(layer, 'output'):                     tf.summary.histogram('{}_out'.format(layer.name),                                          layer.output)          self.merged = tf.summary.merge_all()          if self.write_graph:
class _ScalarAccessIndexer(_NDFrameIndexerBase):          if not isinstance(key, tuple):              key = _tuplify(self.ndim, key)          if len(key) != self.ndim:              raise ValueError("Not enough indexers for scalar access (setting)!")         key = list(self._convert_key(key, is_setter=True))          self.obj._set_value(*key, value=value, takeable=self._takeable)
class InputLayer(Layer):          self.trainable = False          self.built = True          self.sparse = sparse          if input_shape and batch_input_shape:              raise ValueError('Only provide the input_shape OR '
Ur"hello" from __future__ import unicode_literals  "hello"  "hello"
class Model(Container):          if do_validation:              self._make_test_function()          val_gen = (hasattr(validation_data, 'next') or                     hasattr(validation_data, '__next__') or                     isinstance(validation_data, Sequence))         if val_gen and not validation_steps:             raise ValueError('When using a generator for validation data, '                              'you must specify a value for '                              '`validation_steps`.')          out_labels = self._get_deduped_metrics_names()
def match(command, settings):  @utils.git_support  def get_new_command(command, settings):     return '{} --staged'.format(command.script)
def untokenize(iterable):      ut = Untokenizer()      return ut.untokenize(iterable) def generate_tokens(readline):      The generate_tokens() generator requires one argument, readline, which      must be a callable object which provides the same interface as the
def match(command, settings):  @sudo_support  def get_new_command(command, settings):     return re.sub('^mkdir (.*)', 'mkdir -p \\1', command.script)
class TimeseriesGenerator(Sequence):          self.reverse = reverse          self.batch_size = batch_size      def __len__(self):          return int(np.ceil(             (self.end_index - self.start_index) /              (self.batch_size * self.stride)))      def _empty_batch(self, num_rows):
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      def get_indexer_non_unique(self, target):          target = ensure_index(target)         if isinstance(target, PeriodIndex):             if target.freq != self.freq:                 no_matches = -1 * np.ones(self.shape, dtype=np.intp)                 return no_matches, no_matches             target = target.asi8          indexer, missing = self._int64index.get_indexer_non_unique(target)          return ensure_platform_int(indexer), missing
class Task(object):          return False     def can_disable(self):         return (self.disable_failures is not None or                 self.disable_hard_timeout is not None)       @property      def pretty_id(self):          param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())
class HttpProxyMiddleware(object):          creds, proxy = self.proxies[scheme]          request.meta['proxy'] = proxy          if creds:             request.headers['Proxy-Authorization'] = 'Basic ' + creds
class EarlyStopping(Callback):          if self.monitor_op(current - self.min_delta, self.best):              self.best = current              self.wait = 0          else:              self.wait += 1              if self.wait >= self.patience:                  self.stopped_epoch = epoch                  self.model.stop_training = True      def on_train_end(self, logs=None):          if self.stopped_epoch > 0 and self.verbose > 0:
def test_multiprocessing_predict_error():      model.add(Dense(1, input_shape=(5,)))      model.compile(loss='mse', optimizer='adadelta')     with pytest.raises(StopIteration):          model.predict_generator(              custom_generator(), good_batches * workers + 1, 1,              workers=workers, use_multiprocessing=True,          )     with pytest.raises(StopIteration):          model.predict_generator(              custom_generator(), good_batches + 1, 1,              use_multiprocessing=False,
def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:      current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)      lowest_depth = sys.maxsize      trailing_comma_safe = True      for leaf in line.leaves:         current_line.append(leaf, preformatted=True)         comment_after = line.comments.get(id(leaf))         if comment_after:             current_line.append(comment_after, preformatted=True)          lowest_depth = min(lowest_depth, leaf.bracket_depth)          if (              leaf.bracket_depth == lowest_depth
def get_source_from_frame(frame):      if isinstance(source[0], bytes):         encoding = 'ascii'          for line in source[:2]:
class RFPDupeFilter(BaseDupeFilter):          self.logger = logging.getLogger(__name__)          if path:              self.file = open(os.path.join(path, 'requests.seen'), 'a+')              self.fingerprints.update(x.rstrip() for x in self.file)      @classmethod
class SparseDataFrame(DataFrame):          new_data = {}          for col in left.columns:             new_data[col] = func(left[col], float(right[col]))          return self._constructor(              new_data,
class Worker(object):      Structure for tracking worker activity and keeping their references.     def __init__(self, worker_id, last_active=None):          self.id = worker_id self.reference = None self.last_active = last_active
def _get_distinct_objs(objs: List[Index]) -> List[Index]:  def _get_combined_index(     indexes: List[Index], intersect: bool = False, sort: bool = False  ) -> Index:      Return the union or intersection of indexes.
class _Window(PandasObject, ShallowMixin, SelectionMixin):                  def calc(x):                      x = np.concatenate((x, additional_nans))                     if not isinstance(window, BaseIndexer):                          min_periods = calculate_min_periods(                              window, self.min_periods, len(x), require_min_periods, floor                          )                      else:                          min_periods = calculate_min_periods(                             self.min_periods or 1,                              self.min_periods,                              len(x),                              require_min_periods,
class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):          Fast lookup of value from 1-dimensional ndarray. Only use this if you          know what you're doing          if isinstance(key, (datetime, np.datetime64)):              return self.get_value_maybe_box(series, key)
class MTVServicesInfoExtractor(InfoExtractor):          video_id = self._id_from_uri(uri)          data = compat_urllib_parse.urlencode({'uri': uri})         def fix_ampersand(s):             return s.replace(u'& ', '&amp; ')          idoc = self._download_xml(              self._FEED_URL + '?' + data, video_id,             u'Downloading info', transform_source=fix_ampersand)          return [self._get_video_info(item) for item in idoc.findall('.//item')]
class requires(object):      def __call__(self, task_that_requires):          task_that_requires = self.inherit_decorator(task_that_requires)          @task._task_wraps(task_that_requires)         class Wrapped(task_that_requires):             def requires(_self):                 return _self.clone_parent()          return Wrapped  class copies(object):
class WaitIterator(object):          the inputs.          self._running_future = TracebackFuture()          if self._finished:              self._return_result(self._finished.popleft())
missing_value = StataMissingValue(um)                      loc = missing_loc[umissing_loc == j]                      replacement.iloc[loc] = missing_value else:                  dtype = series.dtype
def map_config_to_obj(module):  def map_params_to_obj(module):      text = module.params['text']     if text:         text = str(text).strip()       return {          'banner': module.params['banner'],          'text': text,
def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):          if is_object_dtype(dtype):              return tslibs.ints_to_pytimedelta(arr.view(np.int64))          elif dtype == np.int64:              return arr.view(dtype)          if dtype not in [_INT64_DTYPE, _TD_DTYPE]:
class Bidirectional(Wrapper):              kwargs['mask'] = mask          if initial_state is not None and has_arg(self.layer.call, 'initial_state'):             if not isinstance(initial_state, list):                 raise ValueError(                     'When passing `initial_state` to a Bidirectional RNN, the state '                     'should be a list containing the states of the underlying RNNs. '                     'Found: ' + str(initial_state))              forward_state = initial_state[:len(initial_state) // 2]              backward_state = initial_state[len(initial_state) // 2:]              y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)
class FastAPI(Starlette):                  response_model_exclude_unset=bool(                      response_model_exclude_unset or response_model_skip_defaults                  ),                  include_in_schema=include_in_schema,                  response_class=response_class or self.default_response_class,                  name=name,
class FormRequest(Request):  def _get_form_url(form, url):      if url is None:         return form.action or form.base_url      return urljoin(form.base_url, url)
def cache(*depends_on):              return fn(*args, **kwargs)          cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')          key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])          etag = '.'.join(_get_mtime(name) for name in depends_on)         with shelve.open(cache_path) as db:              if db.get(key, {}).get('etag') == etag:                  return db[key]['value']              else:
class ClipsyndicateIE(InfoExtractor):          pdoc = self._download_xml(              'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,              video_id, u'Downloading video info',             transform_source=fix_xml_all_ampersand)           track_doc = pdoc.find('trackList/track')          def find_param(name):
def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,          if isinstance(iterable, np.ndarray):              return tqdm_class(np.ndenumerate(iterable),                                total=total or len(iterable), **tqdm_kwargs)     return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))  def _tzip(iter1, *iter2plus, **tqdm_kwargs):
import platform  import re  import ssl  import socket  import subprocess  import sys  import traceback
def add_special_arithmetic_methods(cls):          def f(self, other):              result = method(self, other)               self._update_inplace(
class Sequential(Model):              for layer in self._layers:                  x = layer(x)              self.outputs = [x]             if self._layers:                 self._layers[0].batch_input_shape = batch_shape          if self.inputs:              self._init_graph_network(self.inputs,
class TimedeltaIndex(      @Appender(_shared_docs["searchsorted"])      def searchsorted(self, value, side="left", sorter=None):          if isinstance(value, (np.ndarray, Index)):             value = np.array(value, dtype=_TD_DTYPE, copy=False)         else:             value = Timedelta(value).asm8.view(_TD_DTYPE)         return self.values.searchsorted(value, side=side, sorter=sorter)      def is_type_compatible(self, typ) -> bool:          return typ == self.inferred_type or typ == "timedelta"
def get_meta_refresh(response):  def response_status_message(status):     return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))  def response_httprepr(response):
def get_new_command(command):      if '2' in command.script:          return command.script.replace("2", "3")      split_cmd2 = command.script_parts      split_cmd3 = split_cmd2[:]      split_cmd2.insert(1, ' 2 ')      split_cmd3.insert(1, ' 3 ')     last_arg = command.script_parts[-1]       return [         last_arg + ' --help',          "".join(split_cmd3),          "".join(split_cmd2),      ]
class Response(object_ref):          if isinstance(url, Link):              url = url.url          url = self.urljoin(url)          return Request(url, callback,                         method=method,
class BoolBlock(NumericBlock):              return issubclass(tipo.type, np.bool_)          return isinstance(element, (bool, np.bool_))     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(              value          )
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class ExcelFormatter:                  raise KeyError("Not all names specified in 'columns' are found")             self.df = df          self.columns = self.df.columns          self.float_format = float_format
class Model(Container):          stateful_metric_indices = []          if hasattr(self, 'metrics'):             for i, m in enumerate(self.metrics):                 if isinstance(m, Layer) and m.stateful:                     m.reset_states()              stateful_metric_indices = [                  i for i, name in enumerate(self.metrics_names)                  if str(name) in self.stateful_metric_names]
def jsonable_encoder(                  exclude=exclude,                  by_alias=by_alias,                  exclude_unset=bool(exclude_unset or skip_defaults),              ) else:              obj_dict = obj.dict(                  include=include,                  exclude=exclude,
from pandas.core.arrays.datetimes import (      validate_tz_from_dtype,  )  import pandas.core.common as com from pandas.core.indexes.base import Index, maybe_extract_name  from pandas.core.indexes.datetimelike import (      DatetimelikeDelegateMixin,      DatetimeTimedeltaMixin,
class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):              other = TimedeltaIndex(other)          return self, other      def get_loc(self, key, method=None, tolerance=None):
class InvalidModeException(CookiecutterException):      Raised when cookiecutter is called with both `no_input==True` and      `replay==True` at the same time.
def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:              and trailing_comma_safe          ):              current_line.append(Leaf(token.COMMA, ','))         normalize_prefix(current_line.leaves[0], inside_brackets=True)          yield current_line
def get_grouper(              return False          try:              return gpr is obj[gpr.name]         except (KeyError, IndexError):              return False      for i, (gpr, level) in enumerate(zip(keys, levels)):
class Series(base.IndexOpsMixin, generic.NDFrame):                  self[:] = value              else:                  self.loc[key] = value          except TypeError as e:              if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):
def gen_python_files_in_dir(      assert root.is_absolute(), f"INTERNAL ERROR: `root` must be absolute but is {root}"      for child in path.iterdir():         normalized_path = "/" + child.resolve().relative_to(root).as_posix()          if child.is_dir():              normalized_path += "/"          exclude_match = exclude.search(normalized_path)
from ..utils import (      compat_urllib_parse,      compat_urllib_request,      urlencode_postdata,       ExtractorError,  )
def generate_files(repo_dir, context=None, output_dir='.',      with work_in(repo_dir):         if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:              logging.error("Stopping generation because pre_gen_project"                            " hook script didn't exit sucessfully")              return
class TestDataFrameUnaryOperators:          tm.assert_frame_equal(-(df < 0), ~(df < 0))      @pytest.mark.parametrize(          "df",          [
class Axes(_AxesBase):              Respective beginning and end of each line. If scalars are              provided, all lines will have same length.         colors : list of colors, default: 'k'          linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
class DataFrame(NDFrame):                  for k1, k2 in zip(key, value.columns):                      self[k1] = value[k2]              else:                  indexer = self.loc._get_listlike_indexer(                      key, axis=1, raise_missing=False                  )[1]
class Line:              self.bracket_tracker.mark(leaf)              self.maybe_remove_trailing_comma(leaf)              self.maybe_increment_for_loop_variable(leaf)             if self.maybe_adapt_standalone_comment(leaf):                 return          if not self.append_comment(leaf):              self.leaves.append(leaf)      @property      def is_comment(self) -> bool:
class RobotsTxtMiddleware(object):          rp_dfd.callback(rp)      def _robots_error(self, failure, netloc):         self._parsers.pop(netloc).callback(None)
from matplotlib._pylab_helpers import Gcf  from matplotlib.backend_managers import ToolManager  from matplotlib.transforms import Affine2D  from matplotlib.path import Path  _log = logging.getLogger(__name__)
from pandas.core.dtypes.common import (      is_array_like,      is_bool,      is_bool_dtype,      is_categorical_dtype,      is_datetime64tz_dtype,      is_dtype_equal,
patterns = ['permission denied',  def match(command):      for pattern in patterns:          if pattern.lower() in command.stderr.lower()\                  or pattern.lower() in command.stdout.lower():
class TupleParameter(Parameter):          try:             return tuple(tuple(x) for x in json.loads(x))          except ValueError: return literal_eval(x)     def serialize(self, x):         return json.dumps(x)   class NumericalParameter(Parameter):
def violinplot(  @_copy_docstring_and_deprecators(Axes.vlines)  def vlines(         x, ymin, ymax, colors='k', linestyles='solid', label='', *,          data=None, **kwargs):      return gca().vlines(          x, ymin, ymax, colors=colors, linestyles=linestyles,
class Request:          :rtype: str         if "//" in self.app.config.SERVER_NAME:             return self.app.url_for(view_name, _external=True, **kwargs)          scheme = self.scheme          host = self.server_name
class CrawlerProcess(CrawlerRunner):      def __init__(self, settings):          super(CrawlerProcess, self).__init__(settings)          install_shutdown_handlers(self._signal_shutdown)         configure_logging(settings)         log_scrapy_info(settings)      def _signal_shutdown(self, signum, _):          install_shutdown_handlers(self._signal_kill)
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class TestTimedelta64ArithmeticUnsorted:          tm.assert_index_equal(result1, result4)          tm.assert_index_equal(result2, result3)  class TestAddSubNaTMasking:
class BaseSettings(MutableMapping):          if basename in self:              warnings.warn('_BASE settings are deprecated.',                            category=ScrapyDeprecationWarning)             compsett = BaseSettings(self[name + "_BASE"], priority='default')             compsett.update(self[name])              return compsett         else:             return self[name]      def getpriority(self, name):
class DataFrame(NDFrame):              new_data = ops.dispatch_to_series(self, other, func)          else:              with np.errstate(all="ignore"):                 new_data = func(self.values.T, other.values).T          return new_data      def _construct_result(self, result) -> "DataFrame":
class Tracer:          old_local_reprs = self.frame_to_local_reprs.get(frame, {})          self.frame_to_local_reprs[frame] = local_reprs = \                                        get_local_reprs(frame, watch=self.watch)          newish_string = ('Starting var:.. ' if event == 'call' else                                                              'New var:....... ')
class NDFrame(PandasObject, SelectionMixin):              data = self.fillna(method=fill_method, limit=limit, axis=axis)          rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1          rs = rs.reindex_like(data)          if freq is None:              mask = isna(com.values_from_object(data))
class Text(Artist):      def update(self, kwargs):  sentinel = object()          bbox = kwargs.pop("bbox", sentinel)          super().update(kwargs)          if bbox is not sentinel:
from pandas.core.algorithms import (  )  from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs  import pandas.core.common as com from pandas.core.construction import extract_array, sanitize_array  from pandas.core.missing import interpolate_2d  from pandas.core.sorting import nargsort
class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):      )      _engine_type = libindex.DatetimeEngine      _tz = None      _freq = None
class ReduceLROnPlateau(Callback):              monitored has stopped increasing; in `auto`              mode, the direction is automatically inferred              from the name of the monitored quantity.         epsilon: threshold for measuring the new optimum,              to only focus on significant changes.          cooldown: number of epochs to wait before resuming              normal operation after lr has been reduced.
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):          padding: One of `"same"`, `"valid"`, `"full"`.          output_padding: Integer, amount of padding along the output dimension,              Can be set to `None` in which case the output length is inferred.          The output length (integer).
class Index(IndexOpsMixin, PandasObject):                      self._invalid_indexer("label", key)              elif kind == "loc" and is_integer(key):                 if not self.holds_integer():                      self._invalid_indexer("label", key)          return key
Name: Max Speed, dtype: float64          if copy:              new_values = new_values.copy()         assert isinstance(self.index, DatetimeIndex) new_index = self.index.to_period(freq=freq)          return self._constructor(new_values, index=new_index).__finalize__(              self, method="to_period"
def get_openapi_path(              operation_parameters = get_openapi_operation_parameters(all_route_params)              parameters.extend(operation_parameters)              if parameters:                 operation["parameters"] = parameters              if method in METHODS_WITH_BODY:                  request_body_oai = get_openapi_operation_request_body(                      body_field=route.body_field, model_name_map=model_name_map
class ReduceLROnPlateau(Callback):                                    'rate to %s.' % (epoch + 1, new_lr))                          self.cooldown_counter = self.cooldown                          self.wait = 0                 self.wait += 1      def in_cooldown(self):          return self.cooldown_counter > 0
def _align_method_FRAME(  def _should_reindex_frame_op(     left: "DataFrame", right, axis, default_axis: int, fill_value, level  ) -> bool:      assert isinstance(left, ABCDataFrame)      if not isinstance(right, ABCDataFrame):          return False
class CentralPlannerScheduler(Scheduler):          worker_id = kwargs['worker']          self.update(worker_id, {'host': host})
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or not np.isnan(right_value):                      return False              else:                 if np.any(left_value != right_value):                     return False          return True
def reset_display_options():      pd.reset_option("^display.", silent=True) def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:      Pickle an object and then read it again.      Parameters      ----------     obj : pandas object          The object to pickle and then re-read.     path : str, default None          The path where the pickled object is written and then read.      Returns
default: :rc:`scatter.edgecolors`          path = marker_obj.get_path().transformed(              marker_obj.get_transform())          if not marker_obj.is_filled():             edgecolors = 'face'              if linewidths is None:                  linewidths = rcParams['lines.linewidth']              elif np.iterable(linewidths):
def fit_generator(model,      val_gen = (hasattr(validation_data, 'next') or                 hasattr(validation_data, '__next__') or                isinstance(validation_data, Sequence))     if (val_gen and not isinstance(validation_data, Sequence) and              not validation_steps):          raise ValueError('`validation_steps=None` is only valid for a'                           ' generator based on the `keras.utils.Sequence`'
def test_resample_integerarray():      result = ts.resample("3T").mean()      expected = Series(         [1, 4, 7], index=pd.date_range("1/1/2000", periods=3, freq="3T"), dtype="Int64"      )      tm.assert_series_equal(result, expected)
def _period_array_cmp(cls, op):              except ValueError:                  return invalid_comparison(self, other, op)         elif isinstance(other, int):               other = Period(other, freq=self.freq)             result = ordinal_op(other.ordinal)          if isinstance(other, self._recognized_scalars) or other is NaT:              other = self._scalar_type(other)
class EmptyLineTracker:          lines (two on module-level).          before, after = self._maybe_empty_lines(current_line)         before -= self.previous_after          self.previous_after = after          self.previous_line = current_line          return before, after
class CollectionSearch: if not ds:              return None          return ds
def posix_pipe(fin, fout, delim='\n', buf_size=256,  RE_OPTS = re.compile(r'\n {8}(\S+)\s{2,}:\s*([^,]+)') RE_SHLEX = re.compile(r'\s*--?([^\s=]+)(?:\s*|=|$)')  UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')
class TestTimedeltaIndex(DatetimeLike):      def test_pickle_compat_construction(self):          pass      def test_isin(self):          index = tm.makeTimedeltaIndex(4)
class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):      _can_hold_element = DatetimeBlock._can_hold_element      to_native_types = DatetimeBlock.to_native_types      fill_value = np.datetime64("NaT", "ns")      @property      def _holder(self):
class _Alpha:          raise ValueError     def __gt__(self, other):         return not self.__lt__(other)       def __le__(self, other):          return self.__lt__(other) or self.__eq__(other)      def __ge__(self, other):         return self.__gt__(other) or self.__eq__(other)  class _Numeric:
def _clone_functional_model(model, input_tensors=None):                              kwargs['mask'] = computed_mask                      output_tensors = to_list(                          layer(computed_tensor, **kwargs))                     output_masks = to_list(                         layer.compute_mask(computed_tensor,                                            computed_mask))                      computed_tensors = [computed_tensor]                      computed_masks = [computed_mask]                  else:
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):          t1, t2 = self._parsed_string_to_bounds(reso, parsed)          return slice(             self.searchsorted(t1.ordinal, side="left"),             self.searchsorted(t2.ordinal, side="right"),          )      def _convert_tolerance(self, tolerance, target):
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class Bidirectional(Wrapper):          self.supports_masking = True          self._trainable = True          super(Bidirectional, self).__init__(layer, **kwargs)      @property      def trainable(self):
class Index(IndexOpsMixin, PandasObject):          is_null_slicer = start is None and stop is None          is_index_slice = is_int(start) and is_int(stop)         is_positional = is_index_slice and not self.is_integer()          if kind == "getitem":
def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:      ):          for leaf in leaves:              result.append(leaf, preformatted=True)             comment_after = line.comments.get(id(leaf))             if comment_after:                  result.append(comment_after, preformatted=True)      bracket_split_succeeded_or_raise(head, body, tail)      for result in (head, body, tail):
from .common import (      is_unsigned_integer_dtype,      pandas_dtype,  ) from .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype  from .generic import (      ABCDataFrame,      ABCDatetimeArray,
from ..utils import (      unified_strdate,      parse_duration,      qualities,      url_basename,  )
class SimpleTaskState(object):              elif task.scheduler_disable_time is not None and new_status != DISABLED:                  return         if new_status == FAILED and task.can_disable() and task.status != DISABLED:              task.add_failure()              if task.has_excessive_failures():                  task.scheduler_disable_time = time.time()
class MultiIndex(Index):          if len(uniques) < len(level_index):              level_index = level_index.take(uniques)          if len(level_index):              grouper = level_index.take(codes)
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:          return tiow.read(), encoding, newline def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:      if not target_versions:          return [             pygram.python_grammar_no_print_statement_no_exec_statement,             pygram.python_grammar_no_print_statement,             pygram.python_grammar,          ]      elif all(version.is_python2() for version in target_versions):         return [pygram.python_grammar_no_print_statement, pygram.python_grammar]      else:         return [pygram.python_grammar_no_print_statement_no_exec_statement]  def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
class FacebookGraphMixin(OAuth2Mixin):             Added the ability to override ``self._FACEBOOK_BASE_URL``.          url = self._FACEBOOK_BASE_URL + path         return self.oauth2_request(url, callback, access_token,                                    post_args, **args)  def _oauth_signature(consumer_token, method, url, parameters={}, token=None):
def _match_one(filter_part, dct):                      raise ValueError(                          'Invalid integer value %r in filter part %r' % (                              m.group('intval'), filter_part))         actual_value = dct.get(m.group('key'))          if actual_value is None:              return m.group('none_inclusive')          return op(actual_value, comparison_value)
class Worker(object):      def __init__(self, worker_id, last_active=None):          self.id = worker_id self.reference = None         self.last_active = last_active self.started = time.time() self.tasks = set()          self.info = {}
class GalaxyCLI(CLI):              else:                  requirements = []                  for collection_input in collections:                     name, dummy, requirement = collection_input.partition(':')                      requirements.append((name, requirement or '*', None))              output_path = GalaxyCLI._resolve_path(output_path)
class ReduceLROnPlateau(Callback):      def __init__(self, monitor='val_loss', factor=0.1, patience=10,                  verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):          super(ReduceLROnPlateau, self).__init__()          self.monitor = monitor          if factor >= 1.0:              raise ValueError('ReduceLROnPlateau '                               'does not support a factor >= 1.0.')          self.factor = factor          self.min_lr = min_lr         self.epsilon = epsilon          self.patience = patience          self.verbose = verbose          self.cooldown = cooldown
def _match_one(filter_part, dct):              if m.group('op') not in ('=', '!='):                  raise ValueError(                      'Operator %s does not support string values!' % m.group('op'))             comparison_value = m.group('strval') or m.group('intval')          else:              try:                  comparison_value = int(m.group('intval'))
class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):          result = np.asarray(result)           if self.orig is not None:             result = take_1d(result, self.orig.cat.codes)              index = self.orig.index          else:              index = self._parent.index
class LinuxHardware(Hardware):          if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):              i = processor_occurence
from pandas.core.dtypes.common import (      is_scalar,  )  from pandas.core.dtypes.dtypes import CategoricalDtype from pandas.core.dtypes.missing import isna  from pandas.core import accessor  from pandas.core.algorithms import take_1d
class NDFrame(PandasObject, SelectionMixin):          inplace = validate_bool_kwarg(inplace, "inplace")          if axis == 0:              ax = self._info_axis_name              _maybe_transposed_self = self          elif axis == 1:              _maybe_transposed_self = self.T              ax = 1         else:             _maybe_transposed_self = self          ax = _maybe_transposed_self._get_axis_number(ax)          if _maybe_transposed_self.ndim == 2:
default 'raise'          >>> tz_aware.tz_localize(None)          DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',                         '2018-03-03 09:00:00'],                       dtype='datetime64[ns]', freq='D')          Be careful with DST changes. When there is sequential data, pandas can          infer the DST time:
class WrappedResponse(object):      def get_all(self, name, default=None):         return [to_native_str(v) for v in self.response.headers.getlist(name)]      getheaders = get_all
class FigureCanvasBase:      def inaxes(self, xy):         Check if a point is in an axes.          Parameters          ----------
except AttributeError:          if ret:              raise subprocess.CalledProcessError(ret, p.args, output=output)          return output
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class Operation(BaseModel):      operationId: Optional[str] = None      parameters: Optional[List[Union[Parameter, Reference]]] = None      requestBody: Optional[Union[RequestBody, Reference]] = None     responses: Union[Responses, Dict[Union[str], Response]]      callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None      deprecated: Optional[bool] = None
class LineGenerator(Visitor[Line]):          yield from self.visit_default(leaf)          yield from self.line()     def visit_unformatted(self, node: LN) -> Iterator[Line]:
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def _make_wrapped_arith_op_with_freq(opname: str):          if result is NotImplemented:              return NotImplemented         new_freq = self._get_addsub_freq(other)          result._freq = new_freq          return result
def map_obj_to_commands(updates, module, warnings):          else:              add('protocol unix-socket')     if needs_update('state') and not needs_update('vrf'):          if want['state'] == 'stopped':              add('shutdown')          elif want['state'] == 'started':
class Scraper(object):              if download_failure.frames:                  logger.error('Error downloading %(request)s',                               {'request': request},                              extra={'spider': spider, 'failure': download_failure})              else:                  errmsg = download_failure.getErrorMessage()                  if errmsg:
def get_openapi(      if components:          output["components"] = components      output["paths"] = paths     return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)
from fastapi.exceptions import RequestValidationError  from starlette.exceptions import HTTPException  from starlette.requests import Request
class DatetimeIndexOpsMixin(ExtensionIndex):      def is_all_dates(self) -> bool:          return True
from .interface import BaseInterfaceTests from .io import BaseParsingTests from .methods import BaseMethodsTests from .missing import BaseMissingTests from .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil from .printing import BasePrintingTests from .reduce import (      BaseBooleanReduceTests,
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class APIRouter(routing.Router):              assert not prefix.endswith(                  "/"              ), "A path prefix must not end with '/', as the routes will start with '/'"          for route in router.routes:              if isinstance(route, APIRoute):                 if responses is None:                     responses = {}                 responses = {**responses, **route.responses}                  self.add_api_route(                      prefix + route.path,                      route.endpoint,
def parse_dfxp_time_expr(time_expr):      if mobj:          return float(mobj.group('time_offset'))     mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:\.\d+)?)$', time_expr)      if mobj:         return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))  def srt_subtitles_timecode(seconds):
class YoutubeDL(object):                      elif string == '(':                          if current_selector:                              raise syntax_error('Unexpected "("', start)                         current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])                      elif string == '+':                          video_selector = current_selector                         audio_selector = _parse_format_selection(tokens, [','])                         current_selector = None                         selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))                      else:                          raise syntax_error('Operator not recognized: "{0}"'.format(string), start)                  elif type == tokenize.ENDMARKER:
def get_variable_shape(x):      return int_shape(x)  def print_tensor(x, message=''):
class RadialLocator(mticker.Locator):          return self.base.refresh()      def view_limits(self, vmin, vmax):          vmin, vmax = self.base.view_limits(vmin, vmax)          if vmax > vmin:
def _summary_format(set_tasks, worker):          str_output += 'Did not run any tasks'      smiley = ""      reason = ""     if set_tasks["failed"]:         smiley = ":("         reason = "there were failed tasks"         if set_tasks["scheduling_error"]:             reason += " and tasks whose scheduling failed"      elif set_tasks["scheduling_error"]:          smiley = ":("          reason = "there were tasks whose scheduling failed"
class YoutubeDL(object):                      elif string == '/':                          first_choice = current_selector                          second_choice = _parse_format_selection(tokens, inside_choice=True)                         current_selector = None                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))                      elif string == '[':                          if not current_selector:                              current_selector = FormatSelector(SINGLE, 'best', [])
from pandas.util._decorators import Appender, Substitution, cache_readonly, doc  from pandas.core.dtypes.cast import maybe_cast_result  from pandas.core.dtypes.common import (      ensure_float,      is_datetime64_dtype,      is_integer_dtype,      is_numeric_dtype,      is_object_dtype,
class APIRoute(routing.Route):          response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),          response_model_by_alias: bool = True,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Optional[Type[Response]] = None,          dependency_overrides_provider: Any = None,
class WebSocketProtocol(abc.ABC):      async def _receive_frame_loop(self) -> None:          raise NotImplementedError()  class _PerMessageDeflateCompressor(object):      def __init__(
def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:      check_lpar = False      for index, child in enumerate(list(node.children)):          if check_lpar:              if child.type == syms.atom:                  if maybe_make_parens_invisible_in_atom(child, parent=node):
class IntegerArray(ExtensionArray, ExtensionOpsMixin):              with warnings.catch_warnings():                  warnings.filterwarnings("ignore", "elementwise", FutureWarning)                  with np.errstate(all="ignore"):                     result = op(self._data, other)              if mask is None:
class FrameApply:          from pandas import Series          if not should_reduce:              EMPTY_SERIES = Series([])              try:                 r = self.f(EMPTY_SERIES, *self.args, **self.kwds)              except Exception:                  pass              else:                  should_reduce = not isinstance(r, Series)          if should_reduce:             return self.obj._constructor_sliced(np.nan, index=self.agg_axis)          else:              return self.obj.copy()
class Block(PandasObject):      def setitem(self, indexer, value):         Set the value inplace, returning a a maybe different typed block.          Parameters          ----------
class StringArray(PandasArray):              if copy:                  return self.copy()              return self          return super().astype(dtype, copy)      def _reduce(self, name, skipna=True, **kwargs):
def js_to_json(code):          v = m.group(0)          if v in ('true', 'false', 'null'):              return v         elif v.startswith('/*') or v == ',':              return ""          if v[0] in ("'", '"'):
def _match_one(filter_part, dct):          return op(actual_value, comparison_value)      UNARY_OPERATORS = {         '': lambda v: v is not None,         '!': lambda v: v is None,      }          (?P<op>%s)\s*(?P<key>[a-z_]+)
class RangeIndex(Int64Index):          if self.step > 0:              start, stop, step = self.start, self.stop, self.step          else:              start, stop, step = (self.stop - self.step, self.start + 1, -self.step)          target_array = np.asarray(target)          if not (is_integer_dtype(target_array) and target_array.ndim == 1):
def format_file_in_place(          with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:         src_name = f"{src.name}  (original)"         dst_name = f"{src.name}  (formatted)"          diff_contents = diff(src_contents, dst_contents, src_name, dst_name)          if lock:              lock.acquire()
class Block(PandasObject):          mask = isna(values)          if not self.is_object and not quoting:             values = values.astype(str)          else:              values = np.array(values, dtype="object")
class DatetimeTZDtype(PandasExtensionDtype):              tz = timezones.tz_standardize(tz)          elif tz is not None:              raise pytz.UnknownTimeZoneError(tz)         elif tz is None:              raise TypeError("A 'tz' is required.")          self._unit = unit
class GeneratorEnqueuer(SequenceEnqueuer):          self._use_multiprocessing = use_multiprocessing          self._threads = []          self._stop_event = None          self.queue = None          self.seed = seed
def func_no_args():    for i in range(10):      print(i)      continue    return None async def coroutine(arg):   "Single-line docstring. Multiline is harder to reformat."   async with some_connection() as conn:       await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
class WrappedRequest(object):          return self.request.meta.get('is_unverifiable', False)       @property      def unverifiable(self):          return self.is_unverifiable()     def get_origin_req_host(self):         return urlparse_cached(self.request).hostname      def has_header(self, name):          return name in self.request.headers
def _convert_listlike_datetimes(      elif unit is not None:          if format is not None:              raise ValueError("cannot specify both format and unit")         arg = getattr(arg, "values", arg)         result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)          if errors == "ignore":              from pandas import Index
class SparseDataFrame(DataFrame):          this, other = self.align(other, join="outer", axis=0, level=level, copy=False)          new_data = {}         for col, series in this.items():             new_data[col] = func(series.values, other.values)          fill_value = self._get_op_result_fill_value(other, func)
class Line:              bracket_depth = leaf.bracket_depth              if bracket_depth == depth and leaf.type == token.COMMA:                  commas += 1                 if leaf.parent and leaf.parent.type == syms.arglist:                      commas += 1                      break
class Series(base.IndexOpsMixin, generic.NDFrame):              else:                  return self.iloc[key]         if isinstance(key, list):              return self.loc[key]          return self.reindex(key)      def _get_values_tuple(self, key):
class Sequential(Model):              return (proba > 0.5).astype('int32')      def get_config(self):         config = []          for layer in self.layers:             config.append({                  'class_name': layer.__class__.__name__,                  'config': layer.get_config()              })         return copy.deepcopy(config)      @classmethod      def from_config(cls, config, custom_objects=None):         model = cls()         for conf in config:              layer = layer_module.deserialize(conf,                                               custom_objects=custom_objects)              model.add(layer)          return model
class NDFrame(PandasObject, SelectionMixin):          self._data.set_axis(axis, labels)          self._clear_item_cache()     def transpose(self, *args, **kwargs):           axes, kwargs = self._construct_axes_from_arguments(             args, kwargs, require_all=True         )         axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)         axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)           if len(axes) != len(set(axes)):             raise ValueError(f"Must specify {self._AXIS_LEN} unique axes")          new_axes = self._construct_axes_dict_from(             self, [self._get_axis(x) for x in axes_names]         )         new_values = self.values.transpose(axes_numbers)         if kwargs.pop("copy", None) or (len(args) and args[-1]):             new_values = new_values.copy()          nv.validate_transpose(tuple(), kwargs)         return self._constructor(new_values, **new_axes).__finalize__(self)       def swapaxes(self, axis1, axis2, copy=True):
class Index(IndexOpsMixin, PandasObject):          if is_categorical(target):              tgt_values = np.asarray(target)         elif self.is_all_dates:              tgt_values = target.asi8          else:              tgt_values = target._ndarray_values
class TestPeriodIndexArithmetic:          with pytest.raises(TypeError):              other - obj  class TestPeriodSeriesArithmetic:      def test_ops_series_timedelta(self):
class YoutubeIE(YoutubeBaseInfoExtractor):              })          return chapters      def _real_extract(self, url):          url, smuggled_data = unsmuggle_url(url, {})
class APIRoute(routing.Route):              response_model_exclude=self.response_model_exclude,              response_model_by_alias=self.response_model_by_alias,              response_model_exclude_unset=self.response_model_exclude_unset,              dependency_overrides_provider=self.dependency_overrides_provider,          )
CPU_INFO_TEST_SCENARIOS = [                  '23', 'POWER8 (architected), altivec supported',              ],              'processor_cores': 1,             'processor_count': 48,              'processor_threads_per_core': 1,             'processor_vcpus': 48          },      },      {
def diff(arr, n: int, axis: int = 0):      elif is_bool_dtype(dtype):          dtype = np.object_      elif is_integer_dtype(dtype):          dtype = np.float64
from binaryornot.check import is_binary  from .exceptions import (      NonTemplatedInputDirException,      ContextDecodingException,      OutputDirExistsException  )  from .find import find_template  from .utils import make_sure_path_exists, work_in from .hooks import run_hook, EXIT_SUCCESS  def copy_without_render(path, context):
class WrappedRequest(object):          return name in self.request.headers      def get_header(self, name, default=None):         return to_native_str(self.request.headers.get(name, default))      def header_items(self):          return [             (to_native_str(k), [to_native_str(x) for x in v])              for k, v in self.request.headers.items()          ]
class FacebookIE(InfoExtractor):          login_page_req = compat_urllib_request.Request(self._LOGIN_URL)          login_page_req.add_header('Cookie', 'locale=en_US')         self.report_login()         login_page = self._download_webpage(login_page_req, None, note=False,              errnote='Unable to download login page')          lsd = self._search_regex(              r'<input type="hidden" name="lsd" value="([^"]*)"',
class BaseMethodsTests(BaseExtensionTests):          expected = empty          self.assert_extension_array_equal(result, expected)      def test_shift_fill_value(self, data):          arr = data[:4]          fill_value = data[0]
from pandas.core.dtypes.inference import is_array_like  from pandas import compat  from pandas.core import ops from pandas.core.arrays import PandasArray  from pandas.core.construction import extract_array  from pandas.core.indexers import check_array_indexer  from pandas.core.missing import isna
def _isna_new(obj):          raise NotImplementedError("isna is not defined for MultiIndex")      elif isinstance(obj, type):          return False     elif isinstance(         obj,         (             ABCSeries,             np.ndarray,             ABCIndexClass,             ABCExtensionArray,             ABCDatetimeArray,             ABCTimedeltaArray,         ),     ):          return _isna_ndarraylike(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()
class PeriodicCallback(object):              self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)      def _update_next(self, current_time):          if self._next_timeout <= current_time:             callback_time_sec = self.callback_time / 1000.0              self._next_timeout += (math.floor((current_time - self._next_timeout) /                                                callback_time_sec) + 1) * callback_time_sec
def _factorize_keys(lk, rk, sort=True):              np.putmask(rlab, rmask, count)          count += 1      return llab, rlab, count
class TestGetItem:      def test_dti_custom_getitem(self):          rng = pd.bdate_range(START, END, freq="C")          smaller = rng[:5]         exp = DatetimeIndex(rng.view(np.ndarray)[:5])          tm.assert_index_equal(smaller, exp)          assert smaller.freq == rng.freq          sliced = rng[::5]
class YoutubeIE(YoutubeBaseInfoExtractor):                      errnote='Unable to download video annotations', fatal=False,                      data=urlencode_postdata({xsrf_field_name: xsrf_token}))         chapters = self._extract_chapters(description_original, video_duration)          if self._downloader.params.get('youtube_include_dash_manifest', True):
class EmptyLineTracker:          This is for separating `def`, `async def` and `class` with extra empty          lines (two on module-level).         if isinstance(current_line, UnformattedLines):             return 0, 0           before, after = self._maybe_empty_lines(current_line)          before -= self.previous_after          self.previous_after = after
class Parameter(object):              return [str(v) for v in x]          return str(x)     def parse_from_input(self, param_name, x):          Parses the parameter value from input ``x``, handling defaults and is_list.
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a minimum!          Raises          ------          TypeError
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          data_format: string, `"channels_last"` or `"channels_first"`.              Whether to use Theano or TensorFlow/CNTK data format              for inputs/kernels/outputs.          A tensor, result of transposed 2D convolution.
def _normalize(table, normalize, margins, margins_name="All"):              table = table.append(index_margin)              table = table.fillna(0)          else:              raise ValueError("Not a valid normalize argument")         table.index.names = table_index_names         table.columns.names = table_columns_names       else:          raise ValueError("Not a valid margins argument")
default: 'top'          if renderer is None:              renderer = get_renderer(self)         kwargs = get_tight_layout_figure(             self, self.axes, subplotspec_list, renderer,             pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)          if kwargs:              self.subplots_adjust(**kwargs)
from difflib import get_close_matches  from thefuck.utils import get_all_executables, \     get_valid_history_without_current, get_closest  from thefuck.specific.sudo import sudo_support  @sudo_support  def match(command):     return (command.script_parts              and 'not found' in command.stderr              and bool(get_close_matches(command.script_parts[0],                                         get_all_executables())))
class APIRouter(routing.Router):                  response_model_exclude_unset=bool(                      response_model_exclude_unset or response_model_skip_defaults                  ),                  include_in_schema=include_in_schema,                  response_class=response_class or self.default_response_class,                  name=name,
to the w3lib.url module. Always import those from there instead.  import posixpath  import re  from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,                                      urlparse, parse_qsl, urlencode,                                     unquote)  from w3lib.url import *  from w3lib.url import _safe_chars from scrapy.utils.python import to_native_str  def url_is_from_any_domain(url, domains):
class VarianceScaling(Initializer):          if self.distribution == 'normal':              stddev = np.sqrt(scale) / .87962566103423978             return K.truncated_normal(shape, 0., stddev,                                       dtype=dtype, seed=self.seed)          else:              limit = np.sqrt(3. * scale)             return K.random_uniform(shape, -limit, limit,                                     dtype=dtype, seed=self.seed)      def get_config(self):          return {
class Index(IndexOpsMixin, PandasObject):              else:                  return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)         elif is_period_dtype(data) and not is_object_dtype(dtype):             return PeriodIndex(data, copy=copy, name=name, **kwargs)          elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class ComplexBlock(FloatOrComplexBlock):              element, (float, int, complex, np.float_, np.int_)          ) and not isinstance(element, (bool, np.bool_))     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.complexfloating)
import traceback  from .variables import CommonVariable, Exploding, BaseVariable  from . import utils, pycompat  ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')
class CentralPlannerScheduler(Scheduler):          for task in self._state.get_active_tasks():              self._state.fail_dead_worker_task(task, self._config, assistant_ids)             if task.id not in necessary_tasks and self._state.prune(task, self._config):                  remove_tasks.append(task.id)          self._state.inactivate_tasks(remove_tasks)
class CSVLogger(Callback):          self.writer = None          self.keys = None          self.append_header = True         self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''          super(CSVLogger, self).__init__()      def on_train_begin(self, logs=None):
def bracket_split_build_line(          if leaves:              normalize_prefix(leaves[0], inside_brackets=True)               if original.is_import:                 if leaves[-1].type != token.COMMA:                     leaves.append(Leaf(token.COMMA, ","))      for leaf in leaves:          result.append(leaf, preformatted=True)
class Tracer:              thread_global.depth -= 1              if not ended_by_exception:                 return_value_repr = utils.get_shortish_repr(arg)                  self.write('{indent}Return value:.. {return_value_repr}'.                             format(**locals()))
import re  import shutil  import subprocess  import socket  import sys  import time  import tokenize
class Model(Container):                  to yield from `generator` before declaring one epoch                  finished and starting the next epoch. It should typically                  be equal to the number of samples of your dataset                 divided by the batch size. Not used if using `Sequence`.              epochs: Integer, total number of iterations on the data.              verbose: Verbosity mode, 0, 1, or 2.              callbacks: List of callbacks to be called during training.
from parsel.selector import create_root_node  import six  from scrapy.http.request import Request  from scrapy.utils.python import to_bytes, is_listlike  class FormRequest(Request):
class BracketTracker:         if self._for_loop_variable and leaf.type == token.NAME and leaf.value == "in":              self.depth -= 1             self._for_loop_variable -= 1              return True          return False
class Settings(dict):          return self.get(item)      def update(self, **kwargs):
class Model(Container):          if hasattr(self, 'metrics'):             for m in self.metrics:                 if isinstance(m, Layer) and m.stateful:                     m.reset_states()              stateful_metric_indices = [                  i for i, name in enumerate(self.metrics_names)                  if str(name) in self.stateful_metric_names]
VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {          Feature.NUMERIC_UNDERSCORES,          Feature.TRAILING_COMMA_IN_CALL,          Feature.TRAILING_COMMA_IN_DEF,      },  }
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
from pandas.errors import AbstractMethodError  from pandas import DataFrame, get_option from pandas.io.common import get_filepath_or_buffer, is_s3_url  def get_engine(engine):
def _isna_ndarraylike_old(obj):      dtype = values.dtype      if is_string_dtype(dtype):          shape = values.shape          if is_string_like_dtype(dtype):             result = np.zeros(values.shape, dtype=bool)         else:             result = np.empty(shape, dtype=bool)             vec = libmissing.isnaobj_old(values.ravel())             result[:] = vec.reshape(shape)     elif is_datetime64_dtype(dtype):          result = values.view("i8") == iNaT      else:
class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):          self._data._freq = freq
class YoutubeDL(object):                  else:                      filter_parts.append(string)         def _parse_format_selection(tokens, endwith=[]):              selectors = []              current_selector = None              for type, string, start, _, _ in tokens:
class _Concatenator:      def _get_comb_axis(self, i: int) -> Index:          data_axis = self.objs[0]._get_block_manager_axis(i)          return get_objs_combined_axis(             self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort          )      def _get_concat_axis(self) -> Index:
class MetacriticIE(InfoExtractor):          webpage = self._download_webpage(url, video_id)          info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,             video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)          clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)          formats = []
import logging from six.moves.urllib.parse import urljoin  from w3lib.url import safe_url_string
class SeriesGroupBy(GroupBy):              res, out = np.zeros(len(ri), dtype=out.dtype), res              res[ids[idx]] = out         return Series(res, index=ri, name=self._selection_name)      @Appender(Series.describe.__doc__)      def describe(self, **kwargs):
class Sequential(Model):              generator: generator yielding batches of input samples.              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.              max_queue_size: maximum size for the generator queue              workers: maximum number of processes to spin up              use_multiprocessing: if True, use process based threading.
class _iLocIndexer(_LocationIndexer):              if not is_integer(k):                  return False             ax = self.obj.axes[i]             if not ax.is_unique:                 return False           return True      def _validate_integer(self, key: int, axis: int) -> None:
from pandas.core.dtypes.common import (      is_scalar,      pandas_dtype,  )  from pandas.core.arrays.period import (      PeriodArray,
class Sequential(Model):                                          initial_epoch=initial_epoch)      @interfaces.legacy_generator_methods_support     def evaluate_generator(self, generator, steps,                             max_queue_size=10, workers=1,                             use_multiprocessing=False):
class core(task.Config):  class WorkerSchedulerFactory(object):      def create_local_scheduler(self):         return scheduler.CentralPlannerScheduler()      def create_remote_scheduler(self, host, port):          return rpc.RemoteScheduler(host=host, port=port)
def _isna_new(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):         return _isna_ndarraylike(np.asarray(obj))      else:          return False
class tqdm(object):          self.n = 0      def __len__(self):         return len(self.iterable)      def __iter__(self):
class DRTVIE(SubtitlesInfoExtractor):      }      def _real_extract(self, url):         mobj = re.match(self._VALID_URL, url)         video_id = mobj.group('id')          programcard = self._download_json(              'http://www.dr.dk/mu/programcard/expanded/%s' % video_id, video_id, 'Downloading video JSON')
class _Rolling_and_Expanding(_Rolling):              pairwise = True if pairwise is None else pairwise          other = self._shallow_copy(other)         window = self._get_window(other)          def _get_corr(a, b):              a = a.rolling(
def _get_join_indexers(      mapped = (         _factorize_keys(left_keys[n], right_keys[n], sort=sort)          for n in range(len(left_keys))      )      zipped = zip(*mapped)
def get_request_handler(      response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),      response_model_by_alias: bool = True,      response_model_exclude_unset: bool = False,      dependency_overrides_provider: Any = None,  ) -> Callable:      assert dependant.call is not None, "dependant.call must be a function"
class AmbiguousClass(luigi.Task):      pass class NonAmbiguousClass(luigi.ExternalTask):     pass   class NonAmbiguousClass(luigi.Task):      def run(self):         NonAmbiguousClass.has_run = True    class TaskWithSameName(luigi.Task):      def run(self):
def _factorize_keys(lk, rk, sort=True):          rk, _ = rk._values_for_factorize()      elif (         is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)      ):          if lk.categories.equals(rk.categories):              rk = rk.codes
import inspect  import sys  PY3 = (sys.version_info[0] == 3)  if hasattr(abc, 'ABC'):      ABC = abc.ABC
class YoutubeDL(object):                  elif type in [tokenize.NAME, tokenize.NUMBER]:                      current_selector = FormatSelector(SINGLE, string, [])                  elif type == tokenize.OP:                     if string in endwith:                          break                     elif string == ')':                           tokens.restore_last_token()                          break                     if string == ',':                          selectors.append(current_selector)                          current_selector = None                      elif string == '/':                          first_choice = current_selector                         second_choice = _parse_format_selection(tokens, [','])                          current_selector = None                          selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))                      elif string == '[':
class ExecutionEngine(object):          def log_failure(msg):              def errback(failure):                 logger.error(msg, extra={'spider': spider, 'failure': failure})              return errback          dfd.addBoth(lambda _: self.downloader.close())
def _get_functions(overridden):  def _get_aliases(overridden):      aliases = {}      proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)     alias_out = proc.stdout.read().decode('utf-8').strip().split('\n')     for alias in alias_out:         name, value = alias.replace('alias ', '', 1).split(' ', 1)          if name not in overridden:              aliases[name] = value      return aliases
class FormRequest(Request):  def _get_form_url(form, url):      if url is None:         return urljoin(form.base_url, form.action)      return urljoin(form.base_url, url)
def _preprocess_numpy_input(x, data_format, mode):          Preprocessed Numpy array.      if mode == 'tf':          x /= 127.5          x -= 1.
def _get_operations():      proc = subprocess.Popen(["dnf", '--help'],                              stdout=subprocess.PIPE,                              stderr=subprocess.PIPE)     lines = proc.stdout.read()      return _parse_operations(lines)
class IntBlock(NumericBlock):              )          return is_integer(element)     def should_store(self, value) -> bool:          return is_integer_dtype(value) and value.dtype == self.dtype
class CategoricalIndex(Index, accessor.PandasDelegate):      take_nd = take      def map(self, mapper):          Map values using input correspondence (a dict, Series, or function).
class TupleParameter(ListParameter):          try:              return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))         except ValueError:             return literal_eval(x)  class NumericalParameter(Parameter):
class RandomNormal(Initializer):          self.seed = seed      def __call__(self, shape, dtype=None):         return K.random_normal(shape, self.mean, self.stddev,                                dtype=dtype, seed=self.seed)      def get_config(self):          return {
def interpolate_1d(                  inds = lib.maybe_convert_objects(inds)          else:              inds = xvalues         result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])          result[preserve_nans] = np.nan          return result
def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:                  lpar = Leaf(token.LPAR, "")                  rpar = Leaf(token.RPAR, "")                  index = child.remove() or 0                 node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))          check_lpar = isinstance(child, Leaf) and child.value in parens_after
class Errors(object):      E168 = ("Unknown field: {field}")      E169 = ("Can't find module: {module}")      E170 = ("Cannot apply transition {name}: invalid for the current state.")  @add_codes
class GRUCell(Layer):          self.implementation = implementation          self.reset_after = reset_after          self.state_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
class Sequential(Model):                  finished and starting the next epoch. It should typically                  be equal to the number of samples of your dataset                  divided by the batch size.              epochs: Integer, total number of iterations on the data.                  Note that in conjunction with initial_epoch, the parameter                  epochs is to be understood as "final epoch". The model is
def read_user_choice(var_name, options):      ))      user_choice = click.prompt(         prompt, type=click.Choice(choices), default=default      )      return choice_map[user_choice]
class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)          return result      def __rsub__(self, other):         if is_datetime64_dtype(other) and is_timedelta64_dtype(self):              if not isinstance(other, DatetimeLikeArrayMixin):
def fit_generator(model,      if do_validation:          model._make_test_function()     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps_per_epoch is None:         if is_sequence:              steps_per_epoch = len(generator)          else:              raise ValueError('`steps_per_epoch=None` is only valid for a'
class _AtIndexer(_ScalarAccessIndexer):          if is_setter:              return list(key)         for ax, i in zip(self.obj.axes, key):             if ax.is_integer():                 if not is_integer(i):                     raise ValueError(                         "At based indexing on an integer index "                         "can only have integer indexers"                     )             else:                 if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):                     raise ValueError(                         "At based indexing on an non-integer "                         "index can only have non-integer "                         "indexers"                     )         return key  @Appender(IndexingMixin.iat.__doc__)
class Parameter(object):              description.append('for all instances of class %s' % task_name)          elif self.description:              description.append(self.description)         if self.has_value:             description.append(" [default: %s]" % (self.value,))          if self.is_list:              action = "append"
class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):              )          return super().fillna(value, **kwargs)     def should_store(self, value) -> bool:         return is_timedelta64_dtype(value.dtype)       def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):          values = self.values
def assert_series_equal(              check_dtype=check_dtype,              obj=str(obj),          )     elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):          assert_extension_array_equal(left._values, right._values)      elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):
import os  import zipfile  from thefuck.utils import for_app  def _is_bad_zip(file):
def split_line(          result: List[Line] = []          try:             for l in split_func(line, py36=py36):                  if str(l).strip('\n') == line_str:                      raise CannotSplit("Split function returned an unchanged result")
def id_func(x):  @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])
from pandas.core.dtypes.common import (      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,     is_int64_dtype,      is_integer,      is_integer_dtype,      is_list_like,
def format_sizeof(num, suffix=''):          Number with Order of Magnitude SI unit postfix.      for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:         if abs(num) < 1000.0:             if abs(num) < 100.0:                 if abs(num) < 10.0:                      return '{0:1.2f}'.format(num) + unit + suffix                  return '{0:2.1f}'.format(num) + unit + suffix              return '{0:3.0f}'.format(num) + unit + suffix
def str_repeat(arr, repeats):      else:          def rep(x, r):              try:                  return bytes.__mul__(x, r)              except TypeError:
def _get_renderer(figure, print_method=None, *, draw_disabled=False):          except Done as exc:              renderer, = figure._cachedRenderer, = exc.args     if draw_disabled:         for meth_name in dir(RendererBase):             if (meth_name.startswith("draw_")                     or meth_name in ["open_group", "close_group"]):                 setattr(renderer, meth_name, lambda *args, **kwargs: None)       return renderer
def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:      for f in flat_dependant.body_params:          BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)      required = any(True for f in flat_dependant.body_params if f.required)      if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):          BodySchema: Type[params.Body] = params.File      elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):
def url_has_any_extension(url, extensions):      return posixpath.splitext(parse_url(url).path)[1].lower() in extensions  def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,                       encoding=None):     scheme, netloc, path, params, query, fragment = parse_url(url)     keyvals = parse_qsl(query, keep_blank_values)      keyvals.sort()      query = urlencode(keyvals)        path = safe_url_string(_unquotepath(path)) or '/'      fragment = '' if not keep_fragments else fragment      return urlunparse((scheme, netloc.lower(), path, params, query, fragment))  def _unquotepath(path):      for reserved in ('2f', '2F', '3f', '3F'):          path = path.replace('%' + reserved, '%25' + reserved.upper())     return unquote(path)  def parse_url(url, encoding=None):
class Model(Container):                      when using multiprocessing.              steps: Total number of steps (batches of samples)                  to yield from `generator` before stopping.                 Not used if using Sequence.              max_queue_size: Maximum size for the generator queue.              workers: Maximum number of processes to spin up                  when using process based threading
class PamdService(object):              if current_line.matches(rule_type, rule_control, rule_path):                  if current_line.prev is not None:                      current_line.prev.next = current_line.next                     current_line.next.prev = current_line.prev                  else:                      self._head = current_line.next                      current_line.next.prev = None
class Rhsm(RegistrationBase):          for pool_id, quantity in sorted(pool_ids.items()):              if pool_id in available_pool_ids:                 args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]                  rc, stderr, stdout = self.module.run_command(args, check_rc=True)              else:                  self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
from scrapy.exceptions import DontCloseSpider  from scrapy.http import Response, Request  from scrapy.utils.misc import load_object  from scrapy.utils.reactor import CallLaterOnce from scrapy.utils.log import logformatter_adapter  logger = logging.getLogger(__name__)
class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):          if not lib.is_scalar(value):              value = np.asarray(value)         values = self._ndarray         t = np.result_type(value, values)         if t != self._ndarray.dtype:             values = values.astype(t, casting="safe")             values[key] = value             self._dtype = PandasDtype(t)             self._ndarray = values         else:             self._ndarray[key] = value      def __len__(self) -> int:          return len(self._ndarray)
class Index(IndexOpsMixin, PandasObject):      _infer_as_myclass = False      _engine_type = libindex.ObjectEngine      _accessors = {"str"}
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def fit_generator(model,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
from thefuck.specific.git import git_support  @git_support  def match(command):     return (command.script.split()[1] == 'stash'             and 'usage:' in command.stderr)  stash_commands = (
from ansible.module_utils.urls import open_url  from ansible.utils.display import Display  from ansible.utils.hashing import secure_hash_s  display = Display()
class RandomUniform(Initializer):          self.seed = seed      def __call__(self, shape, dtype=None):         return K.random_uniform(shape, self.minval, self.maxval,                                 dtype=dtype, seed=self.seed)      def get_config(self):          return {
class TestBackend(object):          assert_allclose(y1, y2, atol=1e-05)      def test_random_normal(self):           for mean, std in [(0., 1.), (-10., 5.)]:             rand = K.eval(K.random_normal((300, 200),                                           mean=mean, stddev=std, seed=1337))             assert rand.shape == (300, 200)              assert np.abs(np.mean(rand) - mean) < std * 0.015              assert np.abs(np.std(rand) - std) < std * 0.015               r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)             samples = np.array([K.eval(r) for _ in range(200)])             assert np.abs(np.mean(samples) - mean) < std * 0.015             assert np.abs(np.std(samples) - std) < std * 0.015       def test_random_uniform(self):          min_val = -1.          max_val = 1.         rand = K.eval(K.random_uniform((200, 100), min_val, max_val))         assert rand.shape == (200, 100)          assert np.abs(np.mean(rand)) < 0.015          assert max_val - 0.015 < np.max(rand) <= max_val          assert min_val + 0.015 > np.min(rand) >= min_val         r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)         samples = np.array([K.eval(r) for _ in range(200)])         assert np.abs(np.mean(samples)) < 0.015         assert max_val - 0.015 < np.max(samples) <= max_val         assert min_val + 0.015 > np.min(samples) >= min_val       def test_random_binomial(self):          p = 0.5         rand = K.eval(K.random_binomial((200, 100), p))         assert rand.shape == (200, 100)          assert np.abs(np.mean(rand) - p) < 0.015          assert np.max(rand) == 1          assert np.min(rand) == 0         r = K.random_binomial((10, 10), p)         samples = np.array([K.eval(r) for _ in range(200)])         assert np.abs(np.mean(samples) - p) < 0.015         assert np.max(samples) == 1         assert np.min(samples) == 0       def test_truncated_normal(self):          mean = 0.          std = 1.          min_val = -2.          max_val = 2.         rand = K.eval(K.truncated_normal((300, 200),                                          mean=mean, stddev=std, seed=1337))         assert rand.shape == (300, 200)          assert np.abs(np.mean(rand) - mean) < 0.015          assert np.max(rand) <= max_val          assert np.min(rand) >= min_val
def _preprocess_conv2d_input(x, data_format):          x = tf.cast(x, 'float32')      tf_data_format = 'NHWC'      if data_format == 'channels_first':         if not _has_nchw_support(): x = tf.transpose(x, (0, 2, 3, 1))          else:              tf_data_format = 'NCHW'
class ExtensionBlock(Block):                  raise IndexError(f"{self} only contains one item")              return self.values     def should_store(self, value):          return isinstance(value, self._holder)     def set(self, locs, values, check=False):          assert locs.tolist() == [0]         self.values = values      def putmask(          self, mask, new, align=True, inplace=False, axis=0, transpose=False,
class SitemapSpider(Spider):      def _parse_sitemap(self, response):          if response.url.endswith('/robots.txt'):             for url in sitemap_urls_from_robots(response.body):                  yield Request(url, callback=self._parse_sitemap)          else:              body = self._get_sitemap_body(response)
def rnn(step_function, inputs, initial_states,              for o, p in zip(new_states, place_holders):                  n_s.append(o.replace_placeholders({p: o.output}))              if len(n_s) > 0:                 new_output = n_s[0]              return new_output, n_s          final_output, final_states = _recurrence(rnn_inputs, states, mask)
def dfxp2srt(dfxp_data):          for ns in v:              dfxp_data = dfxp_data.replace(ns, k)     dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))      out = []      paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')
def get_new_command(command):      branch_name = re.findall(          r"fatal: A branch named '([^']*)' already exists.", command.stderr)[0]      new_command_templates = [['git branch -d {0}', 'git branch {0}'],                               ['git branch -D {0}', 'git branch {0}'],                               ['git checkout {0}']]      for new_command_template in new_command_templates:          yield shell.and_(*new_command_template).format(branch_name)
from typing import (      Sequence,      Set,      Tuple,     Type,      TypeVar,      Union,      cast,
class BlockManager(PandasObject):          if len(self.blocks) != len(other.blocks):              return False            def canonicalize(block):             return (block.dtype.name, block.mgr_locs.as_array.tolist())          self_blocks = sorted(self.blocks, key=canonicalize)          other_blocks = sorted(other.blocks, key=canonicalize)
from pandas._libs.tslibs import (      timezones,      tzconversion,  )  from pandas.errors import PerformanceWarning  from pandas.core.dtypes.common import (
def filter_spans(spans):      spans (iterable): The spans to filter.      RETURNS (list): The filtered spans.     get_sort_key = lambda span: (span.end - span.start, span.start)      sorted_spans = sorted(spans, key=get_sort_key, reverse=True)      result = []      seen_tokens = set()
class GroupBy(_GroupBy[FrameOrSeries]):                  )              inference = None             if is_integer_dtype(vals):                  inference = np.int64             elif is_datetime64_dtype(vals):                  inference = "datetime64[ns]"                  vals = np.asarray(vals).astype(np.float)
class Session(BaseConfigDict):          for name, value in request_headers.items():              value = value.decode('utf8')              if name == 'User-Agent' and value.startswith('HTTPie/'):                  continue
def js_to_json(code):          "(?:[^"\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^"\\]*"|          '(?:[^'\\]*(?:\\\\|\\['"nurtbfx/\n]))*[^'\\]*'|         /\*.*?\*/|,(?=\s*[\]}])|          [a-zA-Z_][.a-zA-Z_0-9]*|          \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|          [0-9]+(?=\s*:)
cls = dtype.construct_array_type()                 result = try_cast_to_ea(cls, result, dtype=dtype)              elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:                  result = maybe_downcast_to_dtype(result, dtype)
from pandas._libs.lib import no_default  from pandas._libs.tslibs import frequencies as libfrequencies, resolution  from pandas._libs.tslibs.parsing import parse_time_string  from pandas._libs.tslibs.period import Period from pandas._typing import Label  from pandas.util._decorators import Appender, cache_readonly  from pandas.core.dtypes.common import (
class DatetimeLikeArrayMixin(              return None          return self.freq.freqstr     @property     def inferred_freq(self):         if self.ndim != 1:             return None         try:             return frequencies.infer_freq(self)         except ValueError:             return None      @property     def _resolution(self):         return frequencies.Resolution.get_reso_from_freq(self.freqstr)      @property     def resolution(self):         return frequencies.Resolution.get_str(self._resolution)      @classmethod     def _validate_frequency(cls, index, freq, **kwargs):         if is_period_dtype(cls):              return None          inferred = index.inferred_freq         if index.size == 0 or inferred == freq.freqstr:             return None          try:             on_freq = cls._generate_range(                 start=index[0], end=None, periods=len(index), freq=freq, **kwargs             )             if not np.array_equal(index.asi8, on_freq.asi8):                 raise ValueError         except ValueError as e:             if "non-fixed" in str(e):                   raise e                  raise ValueError(                 f"Inferred frequency {inferred} from passed values "                 f"does not conform to passed frequency {freq.freqstr}"             ) from e         @property     def _is_monotonic_increasing(self):         return algos.is_monotonic(self.asi8, timelike=True)[0]      @property     def _is_monotonic_decreasing(self):         return algos.is_monotonic(self.asi8, timelike=True)[1]      @property     def _is_unique(self):         return len(unique1d(self.asi8)) == len(self)        _create_comparison_method = classmethod(_datetimelike_array_cmp)        __pow__ = make_invalid_op("__pow__")     __rpow__ = make_invalid_op("__rpow__")     __mul__ = make_invalid_op("__mul__")     __rmul__ = make_invalid_op("__rmul__")     __truediv__ = make_invalid_op("__truediv__")     __rtruediv__ = make_invalid_op("__rtruediv__")     __floordiv__ = make_invalid_op("__floordiv__")     __rfloordiv__ = make_invalid_op("__rfloordiv__")     __mod__ = make_invalid_op("__mod__")     __rmod__ = make_invalid_op("__rmod__")     __divmod__ = make_invalid_op("__divmod__")     __rdivmod__ = make_invalid_op("__rdivmod__")      def _add_datetimelike_scalar(self, other):          raise TypeError(f"cannot add {type(self).__name__} and {type(other).__name__}")      _add_datetime_arraylike = _add_datetimelike_scalar      def _sub_datetimelike_scalar(self, other):          assert other is not NaT         raise TypeError(f"cannot subtract a datelike from a {type(self).__name__}")      _sub_datetime_arraylike = _sub_datetimelike_scalar      def _sub_period(self, other):          raise TypeError(f"cannot subtract Period from a {type(self).__name__}")      def _add_offset(self, offset):         raise AbstractMethodError(self)      def _add_timedeltalike_scalar(self, other):         if isna(other):              new_values = np.empty(self.shape, dtype="i8")             new_values[:] = iNaT             return type(self)(new_values, dtype=self.dtype)          inc = delta_to_nanoseconds(other)         new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(             "i8"         )         new_values = self._maybe_mask_results(new_values)          new_freq = None         if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):              new_freq = self.freq          return type(self)(new_values, dtype=self.dtype, freq=new_freq)     def _add_timedelta_arraylike(self, other):          if len(self) != len(other):             raise ValueError("cannot add indices of unequal length")         if isinstance(other, np.ndarray):              from pandas.core.arrays import TimedeltaArray             other = TimedeltaArray._from_sequence(other)         self_i8 = self.asi8         other_i8 = other.asi8         new_values = checked_add_with_arr(             self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan         )         if self._hasnans or other._hasnans:             mask = (self._isnan) | (other._isnan)             new_values[mask] = iNaT         return type(self)(new_values, dtype=self.dtype)     def _add_nat(self):         if is_period_dtype(self):             raise TypeError(                 f"Cannot add {type(self).__name__} and {type(NaT).__name__}"             )           result = np.zeros(self.shape, dtype=np.int64)         result.fill(iNaT)         return type(self)(result, dtype=self.dtype, freq=None)      def _sub_nat(self):               result = np.zeros(self.shape, dtype=np.int64)         result.fill(iNaT)         return result.view("timedelta64[ns]")      def _sub_period_array(self, other):         if not is_period_dtype(self):             raise TypeError(                 f"cannot subtract {other.dtype}-dtype from {type(self).__name__}"             )         if self.freq != other.freq:             msg = DIFFERENT_FREQ.format(                 cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr             )             raise IncompatibleFrequency(msg)         new_values = checked_add_with_arr(             self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan         )         new_values = np.array([self.freq.base * x for x in new_values])         if self._hasnans or other._hasnans:             mask = (self._isnan) | (other._isnan)             new_values[mask] = NaT         return new_values     def _addsub_object_array(self, other: np.ndarray, op):         assert op in [operator.add, operator.sub]         if len(other) == 1:             return op(self, other[0])          warnings.warn(             "Adding/subtracting array of DateOffsets to "             f"{type(self).__name__} not vectorized",             PerformanceWarning,         )          assert self.shape == other.shape, (self.shape, other.shape)         res_values = op(self.astype("O"), np.array(other))         result = array(res_values.ravel())         result = extract_array(result, extract_numpy=True).reshape(self.shape)         return result      def _time_shift(self, periods, freq=None):         if freq is not None and freq != self.freq:             if isinstance(freq, str):                 freq = frequencies.to_offset(freq)             offset = periods * freq             result = self + offset              return result         if periods == 0:              return self.copy()         if self.freq is None:             raise NullFrequencyError("Cannot shift with no freq")         start = self[0] + periods * self.freq         end = self[-1] + periods * self.freq            return self._generate_range(start=start, end=end, periods=None, freq=self.freq)     @unpack_zerodim_and_defer("__add__")     def __add__(self, other):          if other is NaT:             result = self._add_nat()         elif isinstance(other, (Tick, timedelta, np.timedelta64)):             result = self._add_timedeltalike_scalar(other)         elif isinstance(other, DateOffset):              result = self._add_offset(other)         elif isinstance(other, (datetime, np.datetime64)):             result = self._add_datetimelike_scalar(other)         elif lib.is_integer(other):               if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._time_shift(other)           elif is_timedelta64_dtype(other):              result = self._add_timedelta_arraylike(other)         elif is_object_dtype(other):              result = self._addsub_object_array(other, operator.add)         elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):              return self._add_datetime_arraylike(other)         elif is_integer_dtype(other):             if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._addsub_int_array(other, operator.add)         else:                  return NotImplemented         if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):             from pandas.core.arrays import TimedeltaArray             return TimedeltaArray(result)         return result     def __radd__(self, other):          return self.__add__(other)     @unpack_zerodim_and_defer("__sub__")     def __sub__(self, other):          if other is NaT:             result = self._sub_nat()         elif isinstance(other, (Tick, timedelta, np.timedelta64)):             result = self._add_timedeltalike_scalar(-other)         elif isinstance(other, DateOffset):              result = self._add_offset(-other)         elif isinstance(other, (datetime, np.datetime64)):             result = self._sub_datetimelike_scalar(other)         elif lib.is_integer(other):               if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._time_shift(-other)          elif isinstance(other, Period):             result = self._sub_period(other)           elif is_timedelta64_dtype(other):              result = self._add_timedelta_arraylike(-other)         elif is_object_dtype(other):              result = self._addsub_object_array(other, operator.sub)         elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):              result = self._sub_datetime_arraylike(other)         elif is_period_dtype(other):              result = self._sub_period_array(other)         elif is_integer_dtype(other):             if not is_period_dtype(self):                 raise integer_op_not_supported(self)             result = self._addsub_int_array(other, operator.sub)          else:              return NotImplemented         if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):             from pandas.core.arrays import TimedeltaArray             return TimedeltaArray(result)         return result     def __rsub__(self, other):         if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):               if lib.is_scalar(other):                  return Timestamp(other) - self             if not isinstance(other, DatetimeLikeArrayMixin):                  from pandas.core.arrays import DatetimeArray                  other = DatetimeArray(other)             return other - self         elif (             is_datetime64_any_dtype(self.dtype)             and hasattr(other, "dtype")             and not is_datetime64_any_dtype(other.dtype)         ):               raise TypeError(                 f"cannot subtract {type(self).__name__} from {type(other).__name__}"             )         elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):              raise TypeError(f"cannot subtract {type(self).__name__} from {other.dtype}")         elif is_timedelta64_dtype(self.dtype):             if lib.is_integer(other) or is_integer_dtype(other):                   return -(self - other)              return (-self) + other          return -(self - other)      def __iadd__(self, other):         result = self + other         self[:] = result[:]          if not is_period_dtype(self):              self._freq = result._freq         return self      def __isub__(self, other):         result = self - other         self[:] = result[:]          if not is_period_dtype(self):              self._freq = result._freq         return self         def _reduce(self, name, axis=0, skipna=True, **kwargs):         op = getattr(self, name, None)         if op:             return op(skipna=skipna, **kwargs)          else:             return super()._reduce(name, skipna, **kwargs)     def min(self, axis=None, skipna=True, *args, **kwargs):         nv.validate_min(args, kwargs)         nv.validate_minmax_axis(axis)         Return the maximum value of the Array or maximum along         an axis.         See Also         --------         numpy.ndarray.max         Index.max : Return the maximum value in an Index.         Series.max : Return the maximum value in a Series.         Return the mean value of the Array.          .. versionadded:: 0.25.0          Parameters          ----------         skipna : bool, default True             Whether to ignore any NaT elements.          Returns          -------         scalar             Timestamp or Timedelta.          See Also         --------         numpy.ndarray.mean : Returns the average of array elements along a given axis.         Series.mean : Return the mean value in a Series.          Notes         -----         mean is only defined for Datetime and Timedelta dtypes, not for Period.     If a `periods` argument is passed to the Datetime/Timedelta Array/Index     constructor, cast it to an integer.      Parameters     ----------     periods : None, float, int      Returns     -------     periods : None or int      Raises     ------     TypeError         if periods is None, float, or int     Check that the `closed` argument is among [None, "left", "right"]      Parameters     ----------     closed : {None, "left", "right"}      Returns     -------     left_closed : bool     right_closed : bool      Raises     ------     ValueError : if argument is not among valid values     If the user passes a freq and another freq is inferred from passed data,     require that they match.      Parameters     ----------     freq : DateOffset or None     inferred_freq : DateOffset or None     freq_infer : bool      Returns     -------     freq : DateOffset or None     freq_infer : bool      Notes     -----     We assume at this point that `maybe_infer_freq` has been called, so     `freq` is either a DateOffset object or None. def maybe_infer_freq(freq):     freq_infer = False     if not isinstance(freq, DateOffset):          if freq != "infer":             freq = frequencies.to_offset(freq)         else:             freq_infer = True             freq = None     return freq, freq_infer
class Driver(object):      def parse_string(self, text, debug=False):         tokens = tokenize.generate_tokens(io.StringIO(text).readline)          return self.parse_tokens(tokens, debug)      def _partially_consume_prefix(self, prefix, column):
from pandas.errors import AbstractMethodError  from pandas.util._decorators import Appender  from pandas.core.dtypes.common import (      is_integer,      is_iterator,      is_list_like,
def generate_comments(leaf: LN) -> Iterator[Leaf]:      for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):          yield Leaf(pc.type, pc.value, prefix="\n" * pc.newlines)         if pc.value in FMT_ON:             raise FormatOn(pc.consumed)          if pc.value in FMT_OFF:             if pc.type == STANDALONE_COMMENT:                 raise FormatOff(pc.consumed)              prev = preceding_leaf(leaf)             if not prev or prev.type in WHITESPACE:                 raise FormatOff(pc.consumed)  @dataclass
class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):      @doc(Index.__contains__)      def __contains__(self, key: Any) -> bool:         if is_scalar(key) and isna(key):              return self.hasnans         hash(key)          return contains(self, key, container=self._engine)      @doc(Index.astype)
from pandas.core.dtypes.common import (  from pandas.core.dtypes.generic import ABCSeries  from pandas.core.accessor import PandasDelegate, delegate_names from pandas.core.algorithms import take_1d  from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray  from pandas.core.base import NoNewAttributesMixin, PandasObject  from pandas.core.indexes.datetimes import DatetimeIndex
def get_handle(      try:          from s3fs import S3File         need_text_wrapping = (BufferedIOBase, S3File)      except ImportError:         need_text_wrapping = BufferedIOBase      handles: List[IO] = list()      f = path_or_buf
def func_no_args():          print(i)          continue      return None async def coroutine(arg):      "Single-line docstring. Multiline is harder to reformat."      async with some_connection() as conn:          await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
class LineGenerator(Visitor[Line]):          If any lines were generated, set up a new current_line.          Yields :class:`Line` objects.          if isinstance(node, Leaf):              any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()             try:                 for comment in generate_comments(node):                     if any_open_brackets:                          self.current_line.append(comment)                     elif comment.type == token.COMMENT:                          self.current_line.append(comment)                         yield from self.line()                      else:                          yield from self.line()                          self.current_line.append(comment)                         yield from self.line()              except FormatOff as f_off:                 f_off.trim_prefix(node)                 yield from self.line(type=UnformattedLines)                 yield from self.visit(node)              except FormatOn as f_on:                   f_on.trim_prefix(node)                 yield from self.visit_default(node)             else:                 normalize_prefix(node, inside_brackets=any_open_brackets)                 if self.normalize_strings and node.type == token.STRING:                     normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)                     normalize_string_quotes(node)                 if node.type not in WHITESPACE:                     self.current_line.append(node)          yield from super().visit_default(node)      def visit_INDENT(self, node: Node) -> Iterator[Line]:
class Scraper(object):          dfd.addErrback(              lambda f: logger.error('Scraper bug processing %(request)s',                                     {'request': request},                                    extra={'spider': spider, 'failure': f}))          self._scrape_next(spider, slot)          return dfd
def dfxp2srt(dfxp_data):          raise ValueError('Invalid dfxp/TTML subtitle')      for para, index in zip(paras, itertools.count(1)):         begin_time = parse_dfxp_time_expr(para.attrib['begin'])          end_time = parse_dfxp_time_expr(para.attrib.get('end'))          if not end_time:             end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])          out.append('%d\n%s --> %s\n%s\n\n' % (              index,              srt_subtitles_timecode(begin_time),
class TestGetItem:      def test_dti_business_getitem(self):          rng = pd.bdate_range(START, END)          smaller = rng[:5]         exp = DatetimeIndex(rng.view(np.ndarray)[:5])          tm.assert_index_equal(smaller, exp)          assert smaller.freq == rng.freq
def top_k_categorical_accuracy(y_true, y_pred, k=5):  def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):     return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),                    axis=-1)
python_symbols = Symbols(python_grammar)  python_grammar_no_print_statement = python_grammar.copy()  del python_grammar_no_print_statement.keywords["print"]  pattern_grammar = driver.load_packaged_grammar("blib2to3", _PATTERN_GRAMMAR_FILE)  pattern_symbols = Symbols(pattern_grammar)
class Axes(_AxesBase):                      cbook.normalize_kwargs(                          boxprops, mpatches.PathPatch._alias_map))          else:             final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)          final_whiskerprops = line_props_with_rcdefaults(             'whiskerprops', whiskerprops)          final_capprops = line_props_with_rcdefaults(             'capprops', capprops)          final_flierprops = line_props_with_rcdefaults(              'flierprops', flierprops)          final_medianprops = line_props_with_rcdefaults(             'medianprops', medianprops, zdelta)          final_meanprops = line_props_with_rcdefaults(              'meanprops', meanprops, zdelta)          removed_prop = 'marker' if meanline else 'linestyle'
def urljoin(base, path):          path = path.decode('utf-8')      if not isinstance(path, compat_str) or not path:          return None     if re.match(r'^(?:https?:)?//', path):          return path      if isinstance(base, bytes):          base = base.decode('utf-8')
class SeriesGroupBy(GroupBy):          val = self.obj._internal_get_values()           val[isna(val)] = np.datetime64("NaT")          try:             sorter = np.lexsort((val, ids))         except TypeError:             msg = f"val.dtype must be object, got {val.dtype}"             assert val.dtype == object, msg             val, _ = algorithms.factorize(val, sort=False)             sorter = np.lexsort((val, ids))             _isna = lambda a: a == -1         else:             _isna = isna          ids, val = ids[sorter], val[sorter]          idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]         inc = np.r_[1, val[1:] != val[:-1]]         mask = _isna(val)          if dropna:              inc[idx] = 1              inc[mask] = 0
import os   from ansible.plugins.lookup import LookupBase  class LookupModule(LookupBase):
def _clone_functional_model(model, input_tensors=None):                              kwargs['mask'] = computed_masks                      output_tensors = to_list(                          layer(computed_tensors, **kwargs))                     output_masks = to_list(                         layer.compute_mask(computed_tensors,                                            computed_masks))                  for x, y, mask in zip(reference_output_tensors,                                        output_tensors,
class Errors(object): "details: https://spacy.io/api/lemmatizer      E174 = ("Architecture '{name}' not found in registry. Available "              "names: {names}")  @add_codes
class GroupBy(_GroupBy):          ).sortlevel()          if self.as_index:             d = {self.obj._get_axis_name(self.axis): index, "copy": False}              return output.reindex(**d)
class HiveCommandClient(HiveClient):          if partition is None:              stdout = run_hive_cmd('use {0}; show tables like "{1}";'.format(database, table))             return stdout and table in stdout          else:
class BinGrouper(BaseGrouper):              ngroups,          )      @cache_readonly      def result_index(self):          if len(self.binlabels) != 0 and isna(self.binlabels[0]):
class DataFrame(NDFrame):          dtype: object          nv.validate_transpose(args, dict())         return super().transpose(1, 0, **kwargs)      T = property(transpose)
from difflib import get_close_matches  from functools import wraps  import shelve  from decorator import decorator  import tempfile  import os
def target_version_option_callback(  @click.option(      "--config",      type=click.Path(         exists=False, file_okay=True, dir_okay=False, readable=True, allow_dash=False      ),      is_eager=True,      callback=read_pyproject_toml,
class Sanic:                  if _rn not in self.named_response_middleware:                      self.named_response_middleware[_rn] = deque()                  if middleware not in self.named_response_middleware[_rn]:                     self.named_response_middleware[_rn].append(middleware)      def middleware(self, middleware_or_request):
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              if verbose == 1:                  progbar = Progbar(target=steps)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      else:          strides = (1, 1) + strides     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,                                padding=padding,                                data_format=tf_data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC': x = tf.transpose(x, (0, 3, 1, 2))      return x
class XportReader(abc.Iterator):          if isinstance(filepath_or_buffer, (str, bytes)):              self.filepath_or_buffer = open(filepath_or_buffer, "rb")          else:              contents = filepath_or_buffer.read()             try:                 contents = contents.encode(self._encoding)             except UnicodeEncodeError:                 pass             self.filepath_or_buffer = BytesIO(contents)          self._read_header()
class LocalCache(collections.OrderedDict):          self.limit = limit      def __setitem__(self, key, value):         while len(self) >= self.limit:             self.popitem(last=False)          super(LocalCache, self).__setitem__(key, value)
dependency tree to find the noun phrase they are referring to  for example:  $9.4 million --> Net income.  Compatible with: spaCy v2.0.0+ Last tested with: v2.1.0  from __future__ import unicode_literals, print_function
class AsyncioServer:              task = asyncio.ensure_future(coro, loop=self.loop)              return task      def __await__(self):          task = asyncio.ensure_future(self.serve_coro)
def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):      if dim_size is None:          return None      if output_padding is None:          if padding == 'valid':
def fit_generator(model,              elif val_gen:                  val_data = validation_data                  if isinstance(val_data, Sequence):                     val_enqueuer_gen = iter_sequence_infinite(generator)                  else:                      val_enqueuer_gen = val_data              else:
class FastParquetImpl(BaseImpl):          if partition_cols is not None:              kwargs["file_scheme"] = "hive"         if is_s3_url(path):               path, _, _, _ = get_filepath_or_buffer(path, mode="wb")               kwargs["open_with"] = lambda path, _: path          else:              path, _, _, _ = get_filepath_or_buffer(path)
class NumpyArrayIterator(Iterator):                             dtype=K.floatx())          for i, j in enumerate(index_array):              x = self.x[j]              x = self.image_data_generator.random_transform(x.astype(K.floatx()))              x = self.image_data_generator.standardize(x)              batch_x[i] = x
class Series(base.IndexOpsMixin, generic.NDFrame):          from pandas.core.reshape.concat import concat          if isinstance(to_append, (list, tuple)):             to_concat = [self] + to_append          else:              to_concat = [self, to_append]          return concat(
class HTTPRequest(HTTPMessage):          )          headers = dict(self._orig.headers)          if 'Host' not in headers:              headers['Host'] = url.netloc.split('@')[-1]          headers = ['%s: %s' % (name, value)
class TestProcessProtocol(protocol.ProcessProtocol):      def __init__(self):          self.deferred = defer.Deferred()         self.out = ''         self.err = ''          self.exitcode = None      def outReceived(self, data):
class _Window(PandasObject, SelectionMixin):              except (ValueError, TypeError):                  raise TypeError("cannot handle this type -> {0}".format(values.dtype))          values[np.isinf(values)] = np.NaN          return values
class BaseComparisonOpsTests(BaseOpsUtil):              assert result is NotImplemented          else:              raise pytest.skip(f"{type(data).__name__} does not implement __eq__")
class CentralPlannerScheduler(Scheduler):          tasks.sort(key=self._rank(), reverse=True)          for task in tasks:             in_workers = assistant or worker in task.workers              if task.status == 'RUNNING' and in_workers:
def jsonable_encoder(                      exclude=exclude,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
def unified_timestamp(date_str, day_first=True):      date_str = date_str.replace(',', ' ')     pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)      timezone, date_str = extract_timezone(date_str)
class ContractsManager(object):          def eb_wrapper(failure):              case = _create_testcase(method, 'errback')             exc_info = failure.value, failure.type, failure.getTracebackObject()              results.addError(case, exc_info)          request.callback = cb_wrapper
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def format_file_in_place(          return False      if write_back == write_back.YES:         with open(src, "w", encoding=src_buffer.encoding) as f:              f.write(dst_contents)      elif write_back == write_back.DIFF:          src_name = f"{src}  (original)"
def should_series_dispatch(left, right, op):          return True     if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):           return True      return False
def run_script(script_path, cwd='.'):          shell=run_thru_shell,          cwd=cwd      )     return proc.wait()  def run_script_with_context(script_path, cwd, context):
class ExecutionEngine(object):          d = self.scraper.enqueue_scrape(response, request, spider)          d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',                                             extra={'spider': spider, 'failure': f}))          return d      def spider_is_idle(self, spider):
from pandas.core.frame import DataFrame  from pandas.core.groupby import ops  from pandas.core.groupby.categorical import recode_for_groupby, recode_from_groupby  from pandas.core.indexes.api import CategoricalIndex, Index, MultiIndex  from pandas.core.series import Series  from pandas.io.formats.printing import pprint_thing
class InfoExtractor(object):                                      f['url'] = initialization_url                                  f['fragments'].append({location_key(initialization_url): initialization_url})                              f['fragments'].extend(representation_ms_info['fragments'])                         try:                             existing_format = next(                                 fo for fo in formats                                 if fo['format_id'] == representation_id)                         except StopIteration:                             full_info = formats_dict.get(representation_id, {}).copy()                             full_info.update(f)                             formats.append(full_info)                         else:                             existing_format.update(f)                      else:                          self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)          return formats
class APIRouter(routing.Router):                      response_model_exclude=route.response_model_exclude,                      response_model_by_alias=route.response_model_by_alias,                      response_model_exclude_unset=route.response_model_exclude_unset,                      include_in_schema=route.include_in_schema,                      response_class=route.response_class or default_response_class,                      name=route.name,
def validate_baseindexer_support(func_name: Optional[str]) -> None:          "median",          "std",          "var",          "kurt",          "quantile",      }
class BarPlot(MPLPlot):      def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):          ax.set_xlim((start_edge, end_edge))         ax.set_xticks(self.tick_pos)         ax.set_xticklabels(ticklabels)          if name is not None and self.use_index:              ax.set_xlabel(name)
from .common import InfoExtractor  from ..utils import (      compat_urllib_parse,      ExtractorError,  )  def _media_xml_tag(tag):
def match(command, settings):  def get_new_command(command, settings):      cmds = command.script.split(' ')     machine = ""      if len(cmds) >= 3:          machine = cmds[2]     return shells.and_("vagrant up " +  machine, command.script)
class Worker(object):              return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],                                      self.tasks)          else:             return state.get_pending_tasks()      def is_trivial_worker(self, state):
def pivot_table(                  agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)      table = agged     if table.index.nlevels > 1:
def create_cloned_field(field: ModelField) -> ModelField:              original_type.__name__, __config__=original_type.__config__          )          for f in original_type.__fields__.values():             use_type.__fields__[f.name] = f          use_type.__validators__ = original_type.__validators__      if PYDANTIC_1:          new_field = ModelField(
def read_conllx(input_data, use_morphology=False, n=0):                      continue                  try:                      id_ = int(id_) - 1                     head = (int(head) - 1) if head != "0" else id_                      dep = "ROOT" if dep == "root" else dep                      tag = pos if tag == "_" else tag                      tag = tag + "__" + morph if use_morphology else tag
class BaseReshapingTests(BaseExtensionTests):          result[0] = result[1]          assert data[0] == data[1]
class CorrectedCommand(object):              compatibility_call(self.side_effect, old_cmd, self.script)          logs.debug(u'PYTHONIOENCODING: {}'.format(             os.environ.get('PYTHONIOENCODING', '>-not-set-<')))          print(self.script)
def add_codes(err_cls):      class ErrorsWithCodes(object):          def __getattribute__(self, code):             msg = getattr(err_cls, code)             return "[{code}] {msg}".format(code=code, msg=msg)      return ErrorsWithCodes()
class GroupBy(_GroupBy): func(**kwargs)              if result_is_index:                 result = algorithms.take_nd(obj.values, result)              if post_processing:                  result = post_processing(result, inferences)
def _isna_ndarraylike_old(obj):      return result  def notna(obj):      Detect non-missing values for an array-like object.
class _LocIndexer(_LocationIndexer):          if isinstance(labels, MultiIndex):             if isinstance(key, str) and labels.levels[0].is_all_dates:                  key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))
class BracketTracker:         if self._lambda_arguments and leaf.type == token.COLON:              self.depth -= 1             self._lambda_arguments -= 1              return True          return False
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):          if isinstance(maybe_slice, slice):              return self[maybe_slice]         taken = ExtensionIndex.take(              self, indices, axis, allow_fill, fill_value, **kwargs          )          freq = self.freq if is_period_dtype(self) else None         assert taken.freq == freq, (taken.freq, freq, taken)         return self._shallow_copy(taken, freq=freq)       _can_hold_na = True      _na_value = NaT
def _isna_new(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike(np.asarray(obj))      else:         return obj is None  def _isna_old(obj):
class TestSeriesComparison:          dti = dti.tz_localize("US/Central")          ser = Series(dti).rename(names[1])          result = op(ser, dti)          assert result.name == names[2]
def js_to_json(code):          if v in ('true', 'false', 'null'):              return v          if v.startswith('"'):             return v         if v.startswith("'"):              v = v[1:-1]              v = re.sub(r"\\\\|\\'|\"", lambda m: {                  '\\\\': '\\\\',
class Model(BaseModel):  class ModelSubclass(Model):      y: int  @app.get("/", response_model=Model, response_model_exclude_unset=True)  def get() -> ModelSubclass:     return ModelSubclass(sub={}, y=1)  client = TestClient(app)
default 'raise'              )          new_dates = new_dates.view(DT64NS_DTYPE)          dtype = tz_to_dtype(tz)         return self._simple_new(new_dates, dtype=dtype, freq=self.freq)
class ReduceLROnPlateau(Callback):              self.mode = 'auto'          if (self.mode == 'min' or             (self.mode == 'auto' and 'acc' not in self.monitor)):             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)              self.best = np.Inf          else:             self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)              self.best = -np.Inf          self.cooldown_counter = 0          self.wait = 0
def get_objs_combined_axis(          The axis to extract indexes from.      sort : bool, default True          Whether the result index should come out sorted or not.      Returns      -------      Index      obs_idxes = [obj._get_axis(axis) for obj in objs]     return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)  def _get_distinct_objs(objs: List[Index]) -> List[Index]:
class QuarterOffset(DateOffset):          shifted = liboffsets.shift_quarters(              dtindex.asi8, self.n, self.startingMonth, self._day_opt          )           return type(dtindex)._simple_new(             shifted, freq=dtindex.freq, dtype=dtindex.dtype         )  class BQuarterEnd(QuarterOffset):
class ImageDataGenerator(object):              The inputs, normalized.         if self.preprocessing_function:             x = self.preprocessing_function(x)          if self.rescale:              x *= self.rescale          if self.samplewise_center:
def _cat_compare_op(op):              mask = (self._codes == -1) | (other_codes == -1)              if mask.any():                 ret[mask] = False              return ret          if is_scalar(other):
def match(command, settings):      return _search(command.stderr) or _search(command.stdout)  def get_new_command(command, settings):      m = _search(command.stderr) or _search(command.stdout)      editor_call = '{} {} +{}'.format(os.environ['EDITOR'],                                      m.group('file'),                                      m.group('line'))      return shells.and_(editor_call, command.script)
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),          padding: string, "same" or "valid".          data_format: "channels_last" or "channels_first".              Whether to use Theano or TensorFlow data format         in inputs/kernels/outputs.          ValueError: if using an even kernel size with padding 'same'.
class Spider(object_ref):          crawler.signals.connect(self.close, signals.spider_closed)      def start_requests(self):         if self.make_requests_from_url is not Spider.make_requests_from_url:              warnings.warn(                 "Spider.make_requests_from_url method is deprecated; "                 "it won't be called in future Scrapy releases. "                 "Please override start_requests method instead."              )              for url in self.start_urls:                  yield self.make_requests_from_url(url)
def reformat_many(      if sys.platform == "win32":          worker_count = min(worker_count, 61)     executor = ProcessPoolExecutor(max_workers=worker_count)      try:          loop.run_until_complete(              schedule_formatting(
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):              try:                  loc = self._get_string_slice(key)                  return series[loc]             except (TypeError, ValueError):                  pass              asdt, reso = parse_time_string(key, self.freq)
def base_url(url):  def urljoin(base, path):      if not isinstance(path, compat_str) or not path:          return None      if re.match(r'^(?:https?:)?//', path):          return path     if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):          return None      return compat_urlparse.urljoin(base, path)
class _LocIndexer(_LocationIndexer):              return self._getbool_axis(key, axis=axis)          elif is_list_like_indexer(key):                 if isinstance(labels, ABCMultiIndex):                  if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:                       key = list(key)                 elif isinstance(key, ABCDataFrame):                      raise NotImplementedError(                         "Indexing a MultiIndex with a "                         "DataFrame key is not "                         "implemented"                     )                 elif hasattr(key, "ndim") and key.ndim > 1:                     raise NotImplementedError(                         "Indexing a MultiIndex with a "                         "multidimensional key is not "                         "implemented"                     )                  if (                     not isinstance(key, tuple)                     and len(key)                     and not isinstance(key[0], tuple)                 ):                     key = tuple([key])               if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):
def print_tensor(x, message=''):          The same tensor `x`, unchanged.      return tf.Print(x, [x], message)
class Block(PandasObject):          check_setitem_lengths(indexer, value, values)           if is_empty_indexer(indexer, arr_value):              pass
class RedirectMiddleware(BaseRedirectMiddleware):          if 'Location' not in response.headers or response.status not in allowed_status:              return response          location = to_native_str(response.headers['location'].decode('latin1'))          redirected_url = urljoin(request.url, location)
except ImportError:  def _prepare_response_content(     res: Any, *, by_alias: bool = True, exclude_unset: bool  ) -> Any:      if isinstance(res, BaseModel):          if PYDANTIC_1:             return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)          else:              return res.dict(                 by_alias=by_alias, skip_defaults=exclude_unset )      elif isinstance(res, list):          return [             _prepare_response_content(item, exclude_unset=exclude_unset) for item in res          ]      elif isinstance(res, dict):          return {             k: _prepare_response_content(v, exclude_unset=exclude_unset)              for k, v in res.items()          }      return res
def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde          for hlevel, level in zip(zipped, levels):              to_concat = []              for key, index in zip(hlevel, indexes):                 try:                     i = level.get_loc(key)                 except KeyError as err:                     raise ValueError(f"Key {key} not in level {level}") from err                  to_concat.append(np.repeat(i, len(index)))              codes_list.append(np.concatenate(to_concat))
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class tqdm(Comparable):          if disable is None and hasattr(file, "isatty") and not file.isatty():              disable = True          if disable:              self.iterable = iterable              self.disable = disable              self.pos = self._get_free_pos(self)              self._instances.remove(self)              self.n = initial              return          if kwargs:
import re  from .common import InfoExtractor  from ..utils import (      find_xpath_attr,     fix_xml_all_ampersand,  )
class Index(IndexOpsMixin, PandasObject):              multi_join_idx = multi_join_idx.remove_unused_levels()             return multi_join_idx, lidx, ridx          jl = list(overlap)[0]
else:  from twisted.internet import defer, reactor, ssl  logger = logging.getLogger(__name__)
class CategoricalBlock(ExtensionBlock):      def _holder(self):          return Categorical      def to_native_types(self, slicer=None, na_rep="", quoting=None, **kwargs):          values = self.values
class Model(Container):                  val_data += [0.]              for cbk in callbacks:                  cbk.validation_data = val_data         is_sequence = isinstance(generator, Sequence)         if not is_sequence and use_multiprocessing and workers > 1:             warnings.warn(                 UserWarning('Using a generator with `use_multiprocessing=True`'                             ' and multiple workers may duplicate your data.'                             ' Please consider using the`keras.utils.Sequence'                             ' class.'))         if is_sequence:             steps_per_epoch = len(generator)          enqueuer = None          try:
def melt(          else:              value_vars = list(value_vars)             missing = Index(np.ravel(value_vars)).difference(cols)              if not missing.empty:                  raise KeyError(                      "The following 'value_vars' are not present in"
class TFOptimizer(Optimizer):      @interfaces.legacy_get_updates_support      def get_updates(self, loss, params):         grads = self.optimizer.compute_gradients(loss, params)          self.updates = [K.update_add(self.iterations, 1)]          opt_update = self.optimizer.apply_gradients(              grads, global_step=self.iterations)
class YoutubeIE(YoutubeBaseInfoExtractor):          video_id = mobj.group(2)          return video_id      @staticmethod     def _extract_chapters(description, duration):          if not description:              return None          chapter_lines = re.findall(
class ItemMeta(ABCMeta):          new_attrs['fields'] = fields          new_attrs['_class'] = _class          return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
class ListParameter(Parameter):         Ensure that list parameter is converted to a tuple so it can be hashed.          :param str x: the value to parse.          :return: the normalized (hashable/immutable) value.
class _MergeOperation:                      )                  ]              else:                 left_keys = [self.left.index.values]          if left_drop:              self.left = self.left._drop_labels_or_levels(left_drop)
class NumericIndex(Index):              name = data.name          return cls._simple_new(subarr, name=name)      @Appender(_index_shared_docs["_maybe_cast_slice_bound"])      def _maybe_cast_slice_bound(self, label, side, kind):          assert kind in ["ix", "loc", "getitem", None]
def _urlencode(seq, enc):  def _get_form(response, formname, formid, formnumber, formxpath):      text = response.body_as_unicode()     root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)      forms = root.xpath('//form')      if not forms:          raise ValueError("No <form> element found in %s" % response)
class Request(object_ref):          s = safe_url_string(url, self.encoding)          self._url = escape_ajax(s)         if ':' not in self._url:              raise ValueError('Missing scheme in request url: %s' % self._url)      url = property(_get_url, obsolete_setter(_set_url, 'url'))
class EmptyLineTracker:                  return 0, 0              newlines = 2              if current_line.depth:                  newlines -= 1
class DataFrame(NDFrame):          return new_data     def _combine_match_index(self, other, func):          if ops.should_series_dispatch(self, other, func):
def disp_trim(data, length):      if len(data) == disp_len(data):          return data[:length] while disp_len(data) > length:          data = data[:-1]     if RE_ANSI.search(data):         return data + "\033[0m"      return data
except ImportError:  from gzip import GzipFile  import six
def _unstack_multiple(data, clocs, fill_value=None):              for i in range(len(clocs)):                  val = clocs[i]                  result = result.unstack(val, fill_value=fill_value)                 clocs = [v if i > v else v - 1 for v in clocs]              return result
def jsonable_encoder(                      value,                      by_alias=by_alias,                      exclude_unset=exclude_unset,                     include_none=include_none,                      custom_encoder=custom_encoder,                      sqlalchemy_safe=sqlalchemy_safe,                  )
class CollectionRequirement:              manifest = info['manifest_file']['collection_info']              namespace = manifest['namespace']              name = manifest['name']             version = manifest['version']              dependencies = manifest['dependencies']          else:              display.warning("Collection at '%s' does not have a MANIFEST.json file, cannot detect version."
from tensorflow.python.ops import functional_ops  from tensorflow.python.ops import ctc_ops as ctc  from .common import floatx, epsilon, image_data_format  import functools  import threading
class LinuxHardware(Hardware):              mtab_entries.append(fields)          return mtab_entries      def get_mount_info(self, mount, device, uuids):          mount_size = get_mount_size(mount)
class Categorical(ExtensionArray, PandasObject):          min : the minimum of this `Categorical`          self.check_for_ordered("min")          good = self._codes != -1          if not good.all():              if skipna:
def _get_spider_loader(settings):              'Please use SPIDER_LOADER_CLASS.',              category=ScrapyDeprecationWarning, stacklevel=2          )     cls_path = settings.get('SPIDER_LOADER_CLASS',                             settings.get('SPIDER_MANAGER_CLASS'))      loader_cls = load_object(cls_path)      verifyClass(ISpiderLoader, loader_cls)      return loader_cls.from_settings(settings.frozencopy())
logger = logging.getLogger(__name__)  title_regex = re.compile(r"(?<=<title>).*(?=</title>)")  id_regex = re.compile(r"(?<=<id>)\d*(?=</id>)") text_regex = re.compile(r"(?<=<text xml:space=\"preserve\">).*(?=</text)")  info_regex = re.compile(r"{[^{]*?}")  html_regex = re.compile(r"&lt;!--[^-]*--&gt;") ref_regex = re.compile(r"&lt;ref.*?&gt;")
async def serialize_response(              exclude=exclude,              by_alias=by_alias,              exclude_unset=exclude_unset,          )      else:          return jsonable_encoder(response_content)
class WebSocketHandler(tornado.web.RequestHandler):          .. versionadded:: 3.1         assert self.stream is not None         self.stream.set_nodelay(value)      def on_connection_close(self) -> None:          if self.ws_connection:
from pandas.core.dtypes.common import (      ensure_object,      is_bool_dtype,      is_complex_dtype,     is_datetime64_dtype,     is_datetime64tz_dtype,      is_datetimelike_v_numeric,      is_dtype_equal,      is_extension_array_dtype,      is_float_dtype,      is_integer_dtype,      is_object_dtype,     is_period_dtype,      is_scalar,      is_string_dtype,      is_string_like_dtype,     is_timedelta64_dtype,      needs_i8_conversion,      pandas_dtype,  )  from pandas.core.dtypes.generic import (      ABCDataFrame,     ABCDatetimeArray,      ABCExtensionArray,      ABCIndexClass,      ABCMultiIndex,      ABCSeries,     ABCTimedeltaArray,  )  from pandas.core.dtypes.inference import is_list_like
def get_file(fname,          Path to the downloaded file      if cache_dir is None:         cache_dir = os.path.join(os.path.expanduser('~'), '.keras')      if md5_hash is not None and file_hash is None:          file_hash = md5_hash          hash_algorithm = 'md5'
def get_flat_dependant(dependant: Dependant) -> Dependant:  def is_scalar_field(field: Field) -> bool:     return (          field.shape == Shape.SINGLETON          and not lenient_issubclass(field.type_, BaseModel)          and not lenient_issubclass(field.type_, sequence_types + (dict,))          and not isinstance(field.schema, params.Body)     )  def is_scalar_sequence_field(field: Field) -> bool:
def _partition_tasks(worker):      set_tasks["completed"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}      set_tasks["already_done"] = {task for (task, status, ext) in task_history                                   if status == 'DONE' and task not in pending_tasks and task not in set_tasks["completed"]}     set_tasks["failed"] = {task for (task, status, ext) in task_history if status == 'FAILED'}      set_tasks["scheduling_error"] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}      set_tasks["still_pending_ext"] = {task for (task, status, ext) in task_history                                       if status == 'PENDING' and task not in set_tasks["failed"] and task not in set_tasks["completed"] and not ext}      set_tasks["still_pending_not_ext"] = {task for (task, status, ext) in task_history                                           if status == 'PENDING' and task not in set_tasks["failed"] and task not in set_tasks["completed"] and ext}      set_tasks["run_by_other_worker"] = set()      set_tasks["upstream_failure"] = set()      set_tasks["upstream_missing_dependency"] = set()
and Index.__new__.  These should not depend on core.internals.  from typing import TYPE_CHECKING, Any, Optional, Sequence, Union, cast  import numpy as np
class Parameter(object):          :raises MissingParameterException: if x is false-y and no default is specified.          if not x:             if self.has_value:                 return self.value              elif self.is_bool:                  return False              elif self.is_list:
class FloatBlock(FloatOrComplexBlock):          )          return formatter.get_result_as_array()     def should_store(self, value) -> bool:          return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
class Categorical(ExtensionArray, PandasObject):          good = self._codes != -1          if not good.all():             if skipna:                  pointer = self._codes[good].min()              else:                  return np.nan
class RedirectMiddleware(BaseRedirectMiddleware):      def process_response(self, request, response, spider):          if (request.meta.get('dont_redirect', False) or                response.status in getattr(spider, 'handle_httpstatus_list', [])):              return response          if request.method == 'HEAD':
def get_new_command(command):          pass      if upstream_option_index is not -1:          command.script_parts.pop(upstream_option_index)         command.script_parts.pop(upstream_option_index)      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]      return replace_argument(" ".join(command.script_parts), 'push', push_upstream)
from pandas.core.dtypes.common import (      ensure_platform_int,      is_bool,      is_bool_dtype,     is_categorical,      is_categorical_dtype,      is_datetime64_any_dtype,      is_dtype_equal,
class TestPeriodIndex(DatetimeLike):          idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq="A-JUN")          ts = Series(np.random.randn(len(idx)), index=idx)         result = ts[2007]          expected = ts[idx == "2007"]          tm.assert_series_equal(result, expected)
from pandas.core.dtypes.generic import (  )  from pandas.core.dtypes.missing import notna  from pandas.core import algorithms  from pandas.core.algorithms import unique
def evaluate_generator(model, generator,      steps_done = 0      outs_per_batch = []      batch_sizes = []     is_sequence = isinstance(generator, Sequence)     if not is_sequence and use_multiprocessing and workers > 1:          warnings.warn(              UserWarning('Using a generator with `use_multiprocessing=True`'                          ' and multiple workers may duplicate your data.'                          ' Please consider using the`keras.utils.Sequence'                          ' class.'))      if steps is None:         if is_sequence:              steps = len(generator)          else:              raise ValueError('`steps=None` is only valid for a generator'
class Series(base.IndexOpsMixin, generic.NDFrame):          kwargs["inplace"] = validate_bool_kwarg(kwargs.get("inplace", False), "inplace")         non_mapping = is_scalar(index) or (             is_list_like(index) and not is_dict_like(index)         )         if non_mapping:              return self._set_name(index, inplace=kwargs.get("inplace"))         return super().rename(index=index, **kwargs)      @Substitution(**_shared_doc_kwargs)      @Appender(generic.NDFrame.reindex.__doc__)
class BaseAsyncIOLoop(IOLoop):              if all_fds:                  self.close_fd(fileobj)          self.asyncio_loop.close()      def add_handler(self, fd, handler, events):          fd, fileobj = self.split_fd(fd)
class ObjectBlock(Block):              if convert:                  block = [b.convert(numeric=False, copy=True) for b in block]              return block          return self
class Categorical(ExtensionArray, PandasObject):          Only ordered `Categoricals` have a maximum!          Raises          ------          TypeError
def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L          security_definition = jsonable_encoder(              security_requirement.security_scheme.model,              by_alias=True,             include_none=False,          )          security_name = security_requirement.security_scheme.scheme_name          security_definitions[security_name] = security_definition
def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),      if isinstance(output_shape, (tuple, list)):          output_shape = tf.stack(output_shape)     x, tf_data_format = _preprocess_conv2d_input(x, data_format)      if data_format == 'channels_first' and tf_data_format == 'NHWC':          output_shape = (output_shape[0],
def _get_collection_info(dep_map, existing_collections, collection, requirement,      existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]      if existing and not collection_info.force:         existing[0].add_requirement(to_text(collection_info), requirement)          collection_info = existing[0]      dep_map[to_text(collection_info)] = collection_info
class Model(Container):                  enqueuer.start(workers=workers, max_queue_size=max_queue_size)                  output_generator = enqueuer.get()              else:                 output_generator = generator              callback_model.stop_training = False
class HTTP1Connection(httputil.HTTPConnection):              return connection_header != "close"          elif ("Content-Length" in headers                or headers.get("Transfer-Encoding", "").lower() == "chunked"               or start_line.method in ("HEAD", "GET")):              return connection_header == "keep-alive"          return False
class TimedeltaIndex(                      result._set_freq("infer")              return result     def _fast_union(self, other):          if len(other) == 0:              return self.view(type(self))
def whitespace(leaf: Leaf) -> str:          ):              return NO      elif prev.type in OPENING_BRACKETS:          return NO
def check_required_arguments(argument_spec, module_parameters):              missing.append(k)      if missing:         msg = "missing required arguments: %s" % ", ".join(missing)          raise TypeError(to_native(msg))      return missing
single_quoted = (  tabsize = 8  class TokenError(Exception): pass  class StopTokenizing(Exception): pass
def assert_series_equal(          Compare datetime-like which is comparable ignoring dtype.      check_categorical : bool, default True          Whether to compare internal Categorical exactly.      obj : str, default 'Series'          Specify object name being compared, internally used to show appropriate          assertion message.
class LSTMCell(Layer):                  inputs_f = inputs                  inputs_c = inputs                  inputs_o = inputs             x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i             x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f             x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c             x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o              if 0 < self.recurrent_dropout < 1.:                  h_tm1_i = h_tm1 * rec_dp_mask[0]
class Conv2DTranspose(Conv2D):          out_height = conv_utils.deconv_length(height,                                                stride_h, kernel_h,                                                self.padding,                                               out_pad_h)          out_width = conv_utils.deconv_length(width,                                               stride_w, kernel_w,                                               self.padding,                                              out_pad_w)          if self.data_format == 'channels_first':              output_shape = (batch_size, self.filters, out_height, out_width)          else:
class TestSeriesAnalytics:          assert s.is_monotonic is False          assert s.is_monotonic_decreasing is True     def test_unstack(self):          index = MultiIndex(             levels=[["bar", "foo"], ["one", "three", "two"]],             codes=[[1, 1, 0, 0], [0, 1, 0, 2]],         )          s = Series(np.arange(4.0), index=index)         unstacked = s.unstack()          expected = DataFrame(             [[2.0, np.nan, 3.0], [0.0, 1.0, np.nan]],             index=["bar", "foo"],             columns=["one", "three", "two"],         )          tm.assert_frame_equal(unstacked, expected)          unstacked = s.unstack(level=0)         tm.assert_frame_equal(unstacked, expected.T)          index = MultiIndex(             levels=[["bar"], ["one", "two", "three"], [0, 1]],             codes=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],         )         s = Series(np.random.randn(6), index=index)         exp_index = MultiIndex(             levels=[["one", "two", "three"], [0, 1]],             codes=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],         )         expected = DataFrame({"bar": s.values}, index=exp_index).sort_index(level=0)         unstacked = s.unstack(0).sort_index()         tm.assert_frame_equal(unstacked, expected)           idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])         ts = pd.Series([1, 2], index=idx)         left = ts.unstack()         right = DataFrame(             [[np.nan, 1], [2, np.nan]], index=[101, 102], columns=[np.nan, 3.5]         )         tm.assert_frame_equal(left, right)          idx = pd.MultiIndex.from_arrays(             [                 ["cat", "cat", "cat", "dog", "dog"],                 ["a", "a", "b", "a", "b"],                 [1, 2, 1, 1, np.nan],             ]         )         ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)         right = DataFrame(             [[1.0, 1.3], [1.1, np.nan], [np.nan, 1.4], [1.2, np.nan]],             columns=["cat", "dog"],         )         tpls = [("a", 1), ("a", 2), ("b", np.nan), ("b", 1)]         right.index = pd.MultiIndex.from_tuples(tpls)         tm.assert_frame_equal(ts.unstack(level=0), right)       @pytest.mark.parametrize("func", [np.any, np.all])      @pytest.mark.parametrize("kwargs", [dict(keepdims=True), dict(out=object())])      @td.skip_if_np_lt("1.15")
class Task(object):          params_str = {}          params = dict(self.get_params())          for param_name, param_value in six.iteritems(self.param_kwargs):             if params[param_name].significant:                 params_str[param_name] = params[param_name].serialize(param_value)          return params_str
def unescapeHTML(s):      assert type(s) == compat_str      return re.sub(         r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)  def get_subprocess_encoding():
def in_top_k(predictions, targets, k):  def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),                      padding='valid', data_format=None):      data_format = normalize_data_format(data_format)      x = _preprocess_conv2d_input(x, data_format)
class Function(object):                                  feed_symbols,                                  symbol_vals,                                  session)         fetched = self._callable_fn(*array_vals)          return fetched[:len(self.outputs)]      def _legacy_call(self, inputs):
class Fish(Generic):      def info(self):         proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],                       stdout=PIPE, stderr=DEVNULL)         version = proc.stdout.read().decode('utf-8').strip()          return u'Fish Shell {}'.format(version)      def put_to_history(self, command):
default: 'top'          from .tight_layout import (              get_renderer, get_subplotspec_list, get_tight_layout_figure)          subplotspec_list = get_subplotspec_list(self.axes)          if None in subplotspec_list:
class BaseGrouper:              if mask.any():                  result = result.astype("float64")                  result[mask] = np.nan          if kind == "aggregate" and self._filter_empty_groups and not counts.all():              assert result.ndim != 2
class Series(base.IndexOpsMixin, generic.NDFrame):                  indexer = self.index.get_indexer_for(key)                  return self.iloc[indexer]              else:                 return self._get_values(key)          if isinstance(key, (list, tuple)):
except ImportError:      from ordereddict import OrderedDict  from luigi import six  class TaskClassException(Exception):
from keras.utils.data_utils import validate_file  from keras import backend as K  pytestmark = pytest.mark.skipif(     K.backend() == 'tensorflow',      reason='Temporarily disabled until the use_multiprocessing problem is solved')  if sys.version_info < (3,):
class Scraper(object):                      spider=spider, exception=output.value)              else:                  logger.error('Error processing %(item)s', {'item': item},                              extra={'spider': spider, 'failure': output})          else:              logkws = self.logformatter.scraped(output, response, spider)              logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
import logging  from six.moves.urllib.parse import urljoin  from scrapy.http import HtmlResponse  from scrapy.utils.response import get_meta_refresh from scrapy.utils.python import to_native_str  from scrapy.exceptions import IgnoreRequest, NotConfigured  logger = logging.getLogger(__name__)
class GalaxyCLI(CLI):          obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]          inject_data = dict(             description='your description',              ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),          )          if galaxy_type == 'role':
class YoutubeDL(object):                          FORMAT_RE.format(numeric_field),                          r'%({0})s'.format(numeric_field), outtmpl)             filename = expand_path(outtmpl % template_dict)
class MailSender(object):              msg = MIMEMultipart()          else:              msg = MIMENonMultipart(*mimetype.split('/', 1))          msg['From'] = self.mailfrom          msg['To'] = COMMASPACE.join(to)          msg['Date'] = formatdate(localtime=True)
def data(dtype):      return pd.array(make_data(), dtype=dtype) def test_boolean_array_constructor():     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")      result = BooleanArray(values, mask)     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)      with pytest.raises(TypeError, match="values should be boolean numpy array"):         BooleanArray(values.tolist(), mask)      with pytest.raises(TypeError, match="mask should be boolean numpy array"):         BooleanArray(values, mask.tolist())      with pytest.raises(TypeError, match="values should be boolean numpy array"):         BooleanArray(values.astype(int), mask)      with pytest.raises(TypeError, match="mask should be boolean numpy array"):         BooleanArray(values, None)     with pytest.raises(ValueError, match="values must be a 1D array"):         BooleanArray(values.reshape(1, -1), mask)      with pytest.raises(ValueError, match="mask must be a 1D array"):         BooleanArray(values, mask.reshape(1, -1))   def test_boolean_array_constructor_copy():     values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(values, mask)     assert result._data is values     assert result._mask is mask      result = BooleanArray(values, mask, copy=True)     assert result._data is not values     assert result._mask is not mask   def test_to_boolean_array():     expected = BooleanArray(         np.array([True, False, True]), np.array([False, False, False])     )     result = pd.array([True, False, True], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([True, False, True]), dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([True, False, True], dtype=object), dtype="boolean")     tm.assert_extension_array_equal(result, expected)       expected = BooleanArray(         np.array([True, False, True]), np.array([False, False, True])     )      result = pd.array([True, False, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([True, False, None], dtype=object), dtype="boolean")     tm.assert_extension_array_equal(result, expected)   def test_to_boolean_array_all_none():     expected = BooleanArray(np.array([True, True, True]), np.array([True, True, True]))      result = pd.array([None, None, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)     result = pd.array(np.array([None, None, None], dtype=object), dtype="boolean")     tm.assert_extension_array_equal(result, expected)   @pytest.mark.parametrize(     "a, b",     [         ([True, False, None, np.nan, pd.NA], [True, False, None, None, None]),         ([True, np.nan], [True, None]),         ([True, pd.NA], [True, None]),         ([np.nan, np.nan], [None, None]),         (np.array([np.nan, np.nan], dtype=float), [None, None]),     ], ) def test_to_boolean_array_missing_indicators(a, b):     result = pd.array(a, dtype="boolean")     expected = pd.array(b, dtype="boolean")     tm.assert_extension_array_equal(result, expected)   @pytest.mark.parametrize(     "values",     [         ["foo", "bar"],         ["1", "2"],          [1, 2],         [1.0, 2.0],         pd.date_range("20130101", periods=2),         np.array(["foo"]),         np.array([1, 2]),         np.array([1.0, 2.0]),         [np.nan, {"a": 1}],     ], ) def test_to_boolean_array_error(values):      with pytest.raises(TypeError):         pd.array(values, dtype="boolean")   def test_to_boolean_array_from_integer_array():     result = pd.array(np.array([1, 0, 1, 0]), dtype="boolean")     expected = pd.array([True, False, True, False], dtype="boolean")     tm.assert_extension_array_equal(result, expected)       result = pd.array(np.array([1, 0, 1, None]), dtype="boolean")     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)   def test_to_boolean_array_from_float_array():     result = pd.array(np.array([1.0, 0.0, 1.0, 0.0]), dtype="boolean")     expected = pd.array([True, False, True, False], dtype="boolean")     tm.assert_extension_array_equal(result, expected)       result = pd.array(np.array([1.0, 0.0, 1.0, np.nan]), dtype="boolean")     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)   def test_to_boolean_array_integer_like():      result = pd.array([1, 0, 1, 0], dtype="boolean")     expected = pd.array([True, False, True, False], dtype="boolean")     tm.assert_extension_array_equal(result, expected)       result = pd.array([1, 0, 1, None], dtype="boolean")     expected = pd.array([True, False, True, None], dtype="boolean")     tm.assert_extension_array_equal(result, expected)   def test_coerce_to_array():      values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(*coerce_to_array(values, mask=mask))     expected = BooleanArray(values, mask)     tm.assert_extension_array_equal(result, expected)     assert result._data is values     assert result._mask is mask     result = BooleanArray(*coerce_to_array(values, mask=mask, copy=True))     expected = BooleanArray(values, mask)     tm.assert_extension_array_equal(result, expected)     assert result._data is not values     assert result._mask is not mask       values = [True, False, None, False]     mask = np.array([False, False, False, True], dtype="bool")     result = BooleanArray(*coerce_to_array(values, mask=mask))     expected = BooleanArray(         np.array([True, False, True, True]), np.array([False, False, True, True])     )     tm.assert_extension_array_equal(result, expected)     result = BooleanArray(*coerce_to_array(np.array(values, dtype=object), mask=mask))     tm.assert_extension_array_equal(result, expected)     result = BooleanArray(*coerce_to_array(values, mask=mask.tolist()))     tm.assert_extension_array_equal(result, expected)       values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")      with pytest.raises(ValueError, match="values must be a 1D list-like"):         coerce_to_array(values.reshape(1, -1))      with pytest.raises(ValueError, match="mask must be a 1D list-like"):         coerce_to_array(values, mask=mask.reshape(1, -1))   def test_coerce_to_array_from_boolean_array():      values = np.array([True, False, True, False], dtype="bool")     mask = np.array([False, False, False, True], dtype="bool")     arr = BooleanArray(values, mask)     result = BooleanArray(*coerce_to_array(arr))     tm.assert_extension_array_equal(result, arr)      assert result._data is arr._data     assert result._mask is arr._mask      result = BooleanArray(*coerce_to_array(arr), copy=True)     tm.assert_extension_array_equal(result, arr)     assert result._data is not arr._data     assert result._mask is not arr._mask      with pytest.raises(ValueError, match="cannot pass mask for BooleanArray input"):         coerce_to_array(arr, mask=mask)   def test_coerce_to_numpy_array():      arr = pd.array([True, False, None], dtype="boolean")     result = np.array(arr)     expected = np.array([True, False, pd.NA], dtype="object")     tm.assert_numpy_array_equal(result, expected)       arr = pd.array([True, False, True], dtype="boolean")     result = np.array(arr)     expected = np.array([True, False, True], dtype="object")     tm.assert_numpy_array_equal(result, expected)       result = np.array(arr, dtype="bool")     expected = np.array([True, False, True], dtype="bool")     tm.assert_numpy_array_equal(result, expected)      arr = pd.array([True, False, None], dtype="boolean")     with pytest.raises(ValueError):         np.array(arr, dtype="bool")   def test_to_boolean_array_from_strings():     result = BooleanArray._from_sequence_of_strings(         np.array(["True", "False", np.nan], dtype=object)     )     expected = BooleanArray(         np.array([True, False, False]), np.array([False, False, True])     )      tm.assert_extension_array_equal(result, expected)   def test_to_boolean_array_from_strings_invalid_string():     with pytest.raises(ValueError, match="cannot be cast"):         BooleanArray._from_sequence_of_strings(["donkey"])   def test_repr():     df = pd.DataFrame({"A": pd.array([True, False, None], dtype="boolean")})     expected = "       A\n0   True\n1  False\n2   <NA>"     assert repr(df) == expected      expected = "0     True\n1    False\n2     <NA>\nName: A, dtype: boolean"     assert repr(df.A) == expected      expected = "<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean"     assert repr(df.A.array) == expected   @pytest.mark.parametrize("box", [True, False], ids=["series", "array"]) def test_to_numpy(box):     con = pd.Series if box else pd.array      arr = con([True, False, True], dtype="boolean")     result = arr.to_numpy()     expected = np.array([True, False, True], dtype="object")     tm.assert_numpy_array_equal(result, expected)      arr = con([True, False, None], dtype="boolean")     result = arr.to_numpy()     expected = np.array([True, False, pd.NA], dtype="object")     tm.assert_numpy_array_equal(result, expected)      arr = con([True, False, None], dtype="boolean")     result = arr.to_numpy(dtype="str")     expected = np.array([True, False, pd.NA], dtype="<U5")     tm.assert_numpy_array_equal(result, expected)       arr = con([True, False, True], dtype="boolean")     result = arr.to_numpy(dtype="bool")     expected = np.array([True, False, True], dtype="bool")     tm.assert_numpy_array_equal(result, expected)     arr = con([True, False, None], dtype="boolean")     with pytest.raises(ValueError, match="cannot convert to 'bool'-dtype"):         result = arr.to_numpy(dtype="bool")       arr = con([True, False, None], dtype="boolean")     result = arr.to_numpy(dtype=object, na_value=None)     expected = np.array([True, False, None], dtype="object")     tm.assert_numpy_array_equal(result, expected)      result = arr.to_numpy(dtype=bool, na_value=False)     expected = np.array([True, False, False], dtype="bool")     tm.assert_numpy_array_equal(result, expected)      result = arr.to_numpy(dtype="int64", na_value=-99)     expected = np.array([1, 0, -99], dtype="int64")     tm.assert_numpy_array_equal(result, expected)      result = arr.to_numpy(dtype="float64", na_value=np.nan)     expected = np.array([1, 0, np.nan], dtype="float64")     tm.assert_numpy_array_equal(result, expected)       with pytest.raises(ValueError, match="cannot convert to 'int64'-dtype"):         arr.to_numpy(dtype="int64")     with pytest.raises(ValueError, match="cannot convert to 'float64'-dtype"):         arr.to_numpy(dtype="float64")   def test_to_numpy_copy():      arr = pd.array([True, False, True], dtype="boolean")     result = arr.to_numpy(dtype=bool)     result[0] = False     tm.assert_extension_array_equal(         arr, pd.array([False, False, True], dtype="boolean")     )      arr = pd.array([True, False, True], dtype="boolean")     result = arr.to_numpy(dtype=bool, copy=True)     result[0] = False     tm.assert_extension_array_equal(arr, pd.array([True, False, True], dtype="boolean"))   def test_astype():      arr = pd.array([True, False, None], dtype="boolean")      with pytest.raises(ValueError, match="cannot convert NA to integer"):         arr.astype("int64")      with pytest.raises(ValueError, match="cannot convert float NaN to"):         arr.astype("bool")      result = arr.astype("float64")     expected = np.array([1, 0, np.nan], dtype="float64")     tm.assert_numpy_array_equal(result, expected)      result = arr.astype("str")     expected = np.array(["True", "False", "<NA>"], dtype="object")     tm.assert_numpy_array_equal(result, expected)       arr = pd.array([True, False, True], dtype="boolean")     result = arr.astype("int64")     expected = np.array([1, 0, 1], dtype="int64")     tm.assert_numpy_array_equal(result, expected)      result = arr.astype("bool")     expected = np.array([True, False, True], dtype="bool")     tm.assert_numpy_array_equal(result, expected)   def test_astype_to_boolean_array():      arr = pd.array([True, False, None], dtype="boolean")      result = arr.astype("boolean")     tm.assert_extension_array_equal(result, arr)     result = arr.astype(pd.BooleanDtype())     tm.assert_extension_array_equal(result, arr)   def test_astype_to_integer_array():      arr = pd.array([True, False, None], dtype="boolean")      result = arr.astype("Int64")     expected = pd.array([1, 0, None], dtype="Int64")     tm.assert_extension_array_equal(result, expected)   @pytest.mark.parametrize("na", [None, np.nan, pd.NA]) def test_setitem_missing_values(na):     arr = pd.array([True, False, None], dtype="boolean")     expected = pd.array([True, None, None], dtype="boolean")     arr[1] = na     tm.assert_extension_array_equal(arr, expected)   @pytest.mark.parametrize(     "ufunc", [np.add, np.logical_or, np.logical_and, np.logical_xor] ) def test_ufuncs_binary(ufunc):      a = pd.array([True, False, None], dtype="boolean")     result = ufunc(a, a)     expected = pd.array(ufunc(a._data, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)      s = pd.Series(a)     result = ufunc(s, a)     expected = pd.Series(ufunc(a._data, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_series_equal(result, expected)       arr = np.array([True, True, False])     result = ufunc(a, arr)     expected = pd.array(ufunc(a._data, arr), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)      result = ufunc(arr, a)     expected = pd.array(ufunc(arr, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)       result = ufunc(a, True)     expected = pd.array(ufunc(a._data, True), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)      result = ufunc(True, a)     expected = pd.array(ufunc(True, a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)       with pytest.raises(TypeError):         ufunc(a, "test")   @pytest.mark.parametrize("ufunc", [np.logical_not]) def test_ufuncs_unary(ufunc):     a = pd.array([True, False, None], dtype="boolean")     result = ufunc(a)     expected = pd.array(ufunc(a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_extension_array_equal(result, expected)      s = pd.Series(a)     result = ufunc(s)     expected = pd.Series(ufunc(a._data), dtype="boolean")     expected[a._mask] = np.nan     tm.assert_series_equal(result, expected)   @pytest.mark.parametrize("values", [[True, False], [True, None]]) def test_ufunc_reduce_raises(values):     a = pd.array(values, dtype="boolean")     with pytest.raises(NotImplementedError):         np.add.reduce(a)   class TestLogicalOps(BaseOpsUtil):     def test_numpy_scalars_ok(self, all_logical_operators):         a = pd.array([True, False, None], dtype="boolean")         op = getattr(a, all_logical_operators)          tm.assert_extension_array_equal(op(True), op(np.bool(True)))         tm.assert_extension_array_equal(op(False), op(np.bool(False)))      def get_op_from_name(self, op_name):         short_opname = op_name.strip("_")         short_opname = short_opname if "xor" in short_opname else short_opname + "_"         try:             op = getattr(operator, short_opname)         except AttributeError:              rop = getattr(operator, short_opname[1:])             op = lambda x, y: rop(y, x)          return op      def test_empty_ok(self, all_logical_operators):         a = pd.array([], dtype="boolean")         op_name = all_logical_operators         result = getattr(a, op_name)(True)         tm.assert_extension_array_equal(a, result)          result = getattr(a, op_name)(False)         tm.assert_extension_array_equal(a, result)          def test_logical_length_mismatch_raises(self, all_logical_operators):         op_name = all_logical_operators         a = pd.array([True, False, None], dtype="boolean")         msg = "Lengths must match to compare"          with pytest.raises(ValueError, match=msg):             getattr(a, op_name)([True, False])          with pytest.raises(ValueError, match=msg):             getattr(a, op_name)(np.array([True, False]))          with pytest.raises(ValueError, match=msg):             getattr(a, op_name)(pd.array([True, False], dtype="boolean"))      def test_logical_nan_raises(self, all_logical_operators):         op_name = all_logical_operators         a = pd.array([True, False, None], dtype="boolean")         msg = "Got float instead"          with pytest.raises(TypeError, match=msg):             getattr(a, op_name)(np.nan)      @pytest.mark.parametrize("other", ["a", 1])     def test_non_bool_or_na_other_raises(self, other, all_logical_operators):         a = pd.array([True, False], dtype="boolean")         with pytest.raises(TypeError, match=str(type(other).__name__)):             getattr(a, all_logical_operators)(other)      def test_kleene_or(self):          a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = a | b         expected = pd.array(             [True, True, True, True, False, None, True, None, None], dtype="boolean"         )         tm.assert_extension_array_equal(result, expected)         result = b | a         tm.assert_extension_array_equal(result, expected)          tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")         )     @pytest.mark.parametrize(         "other, expected",         [             (pd.NA, [True, None, None]),             (True, [True, True, True]),             (np.bool_(True), [True, True, True]),             (False, [True, False, None]),             (np.bool_(False), [True, False, None]),         ],     )     def test_kleene_or_scalar(self, other, expected):          a = pd.array([True, False, None], dtype="boolean")         result = a | other         expected = pd.array(expected, dtype="boolean")         tm.assert_extension_array_equal(result, expected)          result = other | a         tm.assert_extension_array_equal(result, expected)           tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")         )     def test_kleene_and(self):          a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = a & b         expected = pd.array(             [True, False, None, False, False, False, None, False, None], dtype="boolean"         )         tm.assert_extension_array_equal(result, expected)         result = b & a         tm.assert_extension_array_equal(result, expected)          tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")         )     @pytest.mark.parametrize(         "other, expected",         [             (pd.NA, [None, False, None]),             (True, [True, False, None]),             (False, [False, False, False]),             (np.bool_(True), [True, False, None]),             (np.bool_(False), [False, False, False]),         ],     )     def test_kleene_and_scalar(self, other, expected):         a = pd.array([True, False, None], dtype="boolean")         result = a & other         expected = pd.array(expected, dtype="boolean")         tm.assert_extension_array_equal(result, expected)          result = other & a         tm.assert_extension_array_equal(result, expected)           tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")         )     def test_kleene_xor(self):         a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = a ^ b         expected = pd.array(             [False, True, None, True, False, None, None, None, None], dtype="boolean"         )         tm.assert_extension_array_equal(result, expected)         result = b ^ a         tm.assert_extension_array_equal(result, expected)          tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")         )     @pytest.mark.parametrize(         "other, expected",         [             (pd.NA, [None, None, None]),             (True, [False, True, None]),             (np.bool_(True), [False, True, None]),             (np.bool_(False), [True, False, None]),         ],     )     def test_kleene_xor_scalar(self, other, expected):         a = pd.array([True, False, None], dtype="boolean")         result = a ^ other         expected = pd.array(expected, dtype="boolean")         tm.assert_extension_array_equal(result, expected)          result = other ^ a         tm.assert_extension_array_equal(result, expected)           tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")         )     @pytest.mark.parametrize(         "other", [True, False, pd.NA, [True, False, None] * 3],     )     def test_no_masked_assumptions(self, other, all_logical_operators):          a = pd.arrays.BooleanArray(             np.array([True, True, True, False, False, False, True, False, True]),             np.array([False] * 6 + [True, True, True]),         )         b = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         if isinstance(other, list):             other = pd.array(other, dtype="boolean")         result = getattr(a, all_logical_operators)(other)         expected = getattr(b, all_logical_operators)(other)         tm.assert_extension_array_equal(result, expected)         if isinstance(other, BooleanArray):             other._data[other._mask] = True             a._data[a._mask] = False             result = getattr(a, all_logical_operators)(other)             expected = getattr(b, all_logical_operators)(other)             tm.assert_extension_array_equal(result, expected) class TestComparisonOps(BaseOpsUtil):     def _compare_other(self, data, op_name, other):         op = self.get_op_from_name(op_name)          result = pd.Series(op(data, other))         expected = pd.Series(op(data._data, other), dtype="boolean")          expected[data._mask] = pd.NA         tm.assert_series_equal(result, expected)          s = pd.Series(data)         result = op(s, other)         expected = pd.Series(data._data)         expected = op(expected, other)         expected = expected.astype("boolean")          expected[data._mask] = pd.NA         tm.assert_series_equal(result, expected)      def test_compare_scalar(self, data, all_compare_operators):         op_name = all_compare_operators         self._compare_other(data, op_name, True)      def test_compare_array(self, data, all_compare_operators):         op_name = all_compare_operators         other = pd.array([True] * len(data), dtype="boolean")         self._compare_other(data, op_name, other)         other = np.array([True] * len(data))         self._compare_other(data, op_name, other)         other = pd.Series([True] * len(data))         self._compare_other(data, op_name, other)      @pytest.mark.parametrize("other", [True, False, pd.NA])     def test_scalar(self, other, all_compare_operators):         op = self.get_op_from_name(all_compare_operators)         a = pd.array([True, False, None], dtype="boolean")          result = op(a, other)          if other is pd.NA:             expected = pd.array([None, None, None], dtype="boolean")         else:             values = op(a._data, other)             expected = BooleanArray(values, a._mask, copy=True)         tm.assert_extension_array_equal(result, expected)           result[0] = None         tm.assert_extension_array_equal(             a, pd.array([True, False, None], dtype="boolean")          )     def test_array(self, all_compare_operators):         op = self.get_op_from_name(all_compare_operators)         a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")         b = pd.array([True, False, None] * 3, dtype="boolean")         result = op(a, b)         values = op(a._data, b._data)         mask = a._mask | b._mask         expected = BooleanArray(values, mask)         tm.assert_extension_array_equal(result, expected)          result[0] = None         tm.assert_extension_array_equal(             a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype="boolean")          )         tm.assert_extension_array_equal(             b, pd.array([True, False, None] * 3, dtype="boolean")          ) class TestArithmeticOps(BaseOpsUtil):     def test_error(self, data, all_arithmetic_operators):           op = all_arithmetic_operators         s = pd.Series(data)         ops = getattr(s, op)         opa = getattr(data, op)           with pytest.raises(TypeError):             ops("foo")         with pytest.raises(TypeError):             ops(pd.Timestamp("20180101"))           if op not in ("__mul__", "__rmul__"):              with pytest.raises(TypeError):                 ops(pd.Series("foo", index=s.index))           result = opa(pd.DataFrame({"A": s}))         assert result is NotImplemented          with pytest.raises(NotImplementedError):             opa(np.arange(len(s)).reshape(-1, len(s)))   @pytest.mark.parametrize("dropna", [True, False]) def test_reductions_return_types(dropna, data, all_numeric_reductions):     op = all_numeric_reductions     s = pd.Series(data)     if dropna:         s = s.dropna()      if op in ("sum", "prod"):         assert isinstance(getattr(s, op)(), np.int64)     elif op in ("min", "max"):         assert isinstance(getattr(s, op)(), np.bool_)     else:          assert isinstance(getattr(s, op)(), np.float64)   @pytest.mark.parametrize(     "values, exp_any, exp_all, exp_any_noskip, exp_all_noskip",     [         ([True, pd.NA], True, True, True, pd.NA),         ([False, pd.NA], False, False, pd.NA, False),         ([pd.NA], False, True, pd.NA, pd.NA),         ([], False, True, False, True),     ], ) def test_any_all(values, exp_any, exp_all, exp_any_noskip, exp_all_noskip):      exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)     exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)     exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)     exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)      for con in [pd.array, pd.Series]:         a = con(values, dtype="boolean")         assert a.any() is exp_any         assert a.all() is exp_all         assert a.any(skipna=False) is exp_any_noskip         assert a.all(skipna=False) is exp_all_noskip          assert np.any(a.any()) is exp_any         assert np.all(a.all()) is exp_all                  @td.skip_if_no("pyarrow", min_version="0.15.0") def test_arrow_array(data):      import pyarrow as pa      arr = pa.array(data)       data_object = np.array(data, dtype=object)     data_object[data.isna()] = None     expected = pa.array(data_object, type=pa.bool_(), from_pandas=True)     assert arr.equals(expected)   @td.skip_if_no("pyarrow", min_version="0.15.1.dev") def test_arrow_roundtrip():      import pyarrow as pa      data = pd.array([True, False, None], dtype="boolean")     df = pd.DataFrame({"a": data})     table = pa.table(df)     assert table.field("a").type == "bool"     result = table.to_pandas()     assert isinstance(result["a"].dtype, pd.BooleanDtype)     tm.assert_frame_equal(result, df)   def test_value_counts_na():     arr = pd.array([True, False, pd.NA], dtype="boolean")     result = arr.value_counts(dropna=False)     expected = pd.Series([1, 1, 1], index=[True, False, pd.NA], dtype="Int64")     tm.assert_series_equal(result, expected)      result = arr.value_counts(dropna=True)     expected = pd.Series([1, 1], index=[True, False], dtype="Int64")     tm.assert_series_equal(result, expected)   def test_diff():     a = pd.array(         [True, True, False, False, True, None, True, None, False], dtype="boolean"     )     result = pd.core.algorithms.diff(a, 1)     expected = pd.array(         [None, False, True, False, True, None, None, None, None], dtype="boolean"     )     tm.assert_extension_array_equal(result, expected)      s = pd.Series(a)     result = s.diff()     expected = pd.Series(expected)     tm.assert_series_equal(result, expected)
def predict_generator(model, generator,              enqueuer.start(workers=workers, max_queue_size=max_queue_size)              output_generator = enqueuer.get()          else:             if is_sequence:                  output_generator = iter_sequence_infinite(generator)              else:                  output_generator = generator
def js_to_json(code):              ([{,]\s*)              ("[^"]*"|\'[^\']*\'|[a-z0-9A-Z]+)              (:\s*)             ([0-9.]+|true|false|"[^"]*"|\'[^\']*\'|\[|\{)      res = re.sub(r',(\s*\])', lambda m: m.group(1), res)      return res
class BlockManager(PandasObject):                          convert=convert,                          regex=regex,                      )                     if m.any():                          new_rb = _extend_blocks(result, new_rb)                      else:                          new_rb.append(b)
def uppercase_escape(s):      return re.sub(          r'\\U([0-9a-fA-F]{8})',          lambda m: compat_chr(int(m.group(1), base=16)), s)
class APIRoute(routing.Route):          self.response_model_exclude = response_model_exclude          self.response_model_by_alias = response_model_by_alias          self.response_model_exclude_unset = response_model_exclude_unset          self.include_in_schema = include_in_schema          self.response_class = response_class
class Language(object):              kwargs = component_cfg.get(name, {})              kwargs.setdefault("batch_size", batch_size)              if not hasattr(pipe, "pipe"):                 docs = _pipe(pipe, docs, kwargs)              else:                  docs = pipe.pipe(docs, **kwargs)          for doc, gold in zip(docs, golds):
def dispatch_to_series(left, right, func, str_rep=None, axis=None):          assert right.index.equals(left.columns)         def column_op(a, b):             return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}      elif isinstance(right, ABCSeries): assert right.index.equals(left.index)
class CSVLogger(Callback):          if not self.writer:              class CustomDialect(csv.excel):                  delimiter = self.sep               self.writer = csv.DictWriter(self.csv_file,                                          fieldnames=['epoch'] + self.keys, dialect=CustomDialect)              if self.append_header:                  self.writer.writeheader()
class DictParameter(Parameter):      tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related      values (like a database connection config).         JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.          Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.
class CollectionRequirement:                  requirement = req                  op = operator.eq                   if parent and version == '*' and requirement != '*':                     break                 elif requirement == '*' or version == '*':                     continue              if not op(LooseVersion(version), LooseVersion(requirement)):                  break
class MultiIndex(Index):                      indexer = self._get_level_indexer(key, level=level)                      new_index = maybe_mi_droplevels(indexer, [0], drop_level)                      return indexer, new_index             except TypeError:                  pass              if not any(isinstance(k, slice) for k in key):
def srt_subtitles_timecode(seconds):  def dfxp2srt(dfxp_data):      LEGACY_NAMESPACES = (         ('http://www.w3.org/ns/ttml', [             'http://www.w3.org/2004/11/ttaf1',             'http://www.w3.org/2006/04/ttaf1',             'http://www.w3.org/2006/10/ttaf1',          ]),         ('http://www.w3.org/ns/ttml             'http://www.w3.org/ns/ttml          ]),      )
def _isna_old(obj):      elif hasattr(obj, "__array__"):          return _isna_ndarraylike_old(np.asarray(obj))      else:         return obj is None  _isna = _isna_new
class Tracer:          self._write(s)      def __enter__(self):          calling_frame = inspect.currentframe().f_back          if not self._is_internal_frame(calling_frame):              calling_frame.f_trace = self.trace              self.target_frames.add(calling_frame)         stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])          stack.append(sys.gettrace())          sys.settrace(self.trace)      def __exit__(self, exc_type, exc_value, exc_traceback):          stack = self.thread_local.original_trace_functions          sys.settrace(stack.pop())          calling_frame = inspect.currentframe().f_back
def standardize_weights(y,      Everything gets normalized to a single sample-wise (or timestep-wise)     weight array.          y: Numpy array of model targets to be weighted.
class SimpleRNNCell(Layer):          self.dropout = min(1., max(0., dropout))          self.recurrent_dropout = min(1., max(0., recurrent_dropout))          self.state_size = self.units          self._dropout_mask = None          self._recurrent_dropout_mask = None
def write_flv_header(stream, metadata):      stream.write(b'\x12')     stream.write(pack('!L', len(metadata))[1:])      stream.write(b'\x00\x00\x00\x00\x00\x00\x00')      stream.write(metadata)
import re  from .common import InfoExtractor  from ..utils import (     fix_xml_all_ampersand,  )
class tqdm(object):                      l_bar_user, r_bar_user = bar_format.split('{bar}')                     l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)                  else:                      return bar_format.format(**bar_args)
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):      @cache_readonly      def _engine(self):          period = weakref.ref(self)          return self._engine_type(period, len(self))      @doc(Index.__contains__)
from pandas.core.dtypes.common import (      is_list_like,      is_object_dtype,      is_scalar,  )  from pandas.core.dtypes.dtypes import register_extension_dtype  from pandas.core.dtypes.missing import isna
def lib2to3_parse(src_txt: str) -> Node:      grammar = pygram.python_grammar_no_print_statement      if src_txt[-1] != "\n":         nl = "\r\n" if "\r\n" in src_txt[:1024] else "\n"         src_txt += nl      for grammar in GRAMMARS:          drv = driver.Driver(grammar, pytree.convert)          try:
class WebSocketHandler(tornado.web.RequestHandler):          if not self._on_close_called:              self._on_close_called = True              self.on_close()      def send_error(self, *args, **kwargs):          if self.stream is None:
class BusinessHourMixin(BusinessMixin):              if bd != 0:                 skip_bd = BusinessDay(n=bd)                  if not self.next_bday.is_on_offset(other):                      prev_open = self._prev_opening_time(other)
def array_equivalent(left, right, strict_nan=False):                  if not isinstance(right_value, float) or not np.isnan(right_value):                      return False              else:                 if left_value != right_value:                      return False          return True
class CannotSplit(Exception):      It holds the number of bytes of the prefix consumed before the format     control comment appeared.         unformatted_prefix = leaf.prefix[: self.consumed]         return Leaf(token.NEWLINE, unformatted_prefix)   class FormatOn(FormatError):    class WriteBack(Enum):      NO = 0      YES = 1
patterns = (          '^lua: {file}:{line}:',         '^{file} \(line {line}\):',          '^{file}: line {line}: ',          '^{file}:{line}:',          '^{file}:{line}:{col}',          'at {file} line {line}',      )
import threading  import time  import traceback  import math import weakref from tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback  from tornado.log import app_log, gen_log
class Conv2DTranspose(Conv2D):              output_shape,              self.strides,              padding=self.padding,             data_format=self.data_format)          if self.use_bias:              outputs = K.bias_add(
def format_stdin_to_stdout(      `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to      :func:`format_file_contents`.     src = sys.stdin.read()      dst = src      try:          dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
def get_request_handler(                  exclude=response_model_exclude,                  by_alias=response_model_by_alias,                  exclude_unset=response_model_exclude_unset,                  is_coroutine=is_coroutine,              )              response = response_class(
def get_elements_by_attribute(attribute, value, html, escape_value=True):      retlist = []          <([a-zA-Z0-9:._-]+)          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?           \s+%s=['"]?%s['"]?          (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|="[^"]*"|='[^']*'))*?          \s*>          (?P<content>.*?)          </\1>
class StackedRNNCells(Layer):                                   '`state_size` attribute. '                                   'received cells:', cells)          self.cells = cells          super(StackedRNNCells, self).__init__(**kwargs)      @property      def state_size(self):                state_size = []         for cell in self.cells[::-1]:              if hasattr(cell.state_size, '__len__'):                  state_size += list(cell.state_size)              else:                  state_size.append(cell.state_size)          return tuple(state_size)      def call(self, inputs, states, constants=None, **kwargs):          nested_states = []         for cell in self.cells[::-1]:              if hasattr(cell.state_size, '__len__'):                  nested_states.append(states[:len(cell.state_size)])                  states = states[len(cell.state_size):]              else:                  nested_states.append([states[0]])                  states = states[1:]         nested_states = nested_states[::-1]          new_nested_states = []
class Model(Container):              validation_steps: Only relevant if `validation_data`                  is a generator. Total number of steps (batches of samples)                  to yield from `generator` before stopping.              class_weight: Dictionary mapping class indices to a weight                  for the class.              max_queue_size: Integer. Maximum size for the generator queue.
class RetcodesTest(LuigiTestCase):          with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):              self.run_and_expect('RequiringTask', 0)              self.run_and_expect('RequiringTask --retcode-not-run 5', 5)
def count_leading_spaces(s):  def process_list_block(docstring, starting_point, section_end,                         leading_spaces, marker):      ending_point = docstring.find('\n\n', starting_point)     block = docstring[starting_point:(None if ending_point == -1 else                                       ending_point - 1)]      docstring_slice = docstring[starting_point:section_end].replace(block, marker)      docstring = (docstring[:starting_point]
default: :rc:`scatter.edgecolors`          collection = mcoll.PathCollection(                  (path,), scales,                 facecolors=colors,                 edgecolors=edgecolors,                  linewidths=linewidths,                  offsets=offsets,                  transOffset=kwargs.pop('transform', self.transData),
def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:      srcbuf = io.BytesIO(src)      encoding, lines = tokenize.detect_encoding(srcbuf.readline)      newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"      srcbuf.seek(0)      with io.TextIOWrapper(srcbuf, encoding) as tiow:
def create_instance(objcls, settings, crawler, *args, **kwargs):      ``*args`` and ``**kwargs`` are forwarded to the constructors.      Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.      if settings is None:          if crawler is None:              raise ValueError("Specify at least one of settings and crawler.")          settings = crawler.settings      if crawler and hasattr(objcls, 'from_crawler'):         return objcls.from_crawler(crawler, *args, **kwargs)      elif hasattr(objcls, 'from_settings'):         return objcls.from_settings(settings, *args, **kwargs)      else:         return objcls(*args, **kwargs)  @contextmanager
class LinuxHardware(Hardware):          pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))          maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT          for fields in mtab_entries:              device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]
def crosstab(      from pandas import DataFrame      df = DataFrame(data, index=common_idx)      if values is None:          df["__dummy__"] = 0          kwargs = {"aggfunc": len, "fill_value": 0}
class S3CopyToTable(rdbms.CopyToTable):          if '.' in self.table:              query = ("select 1 as table_exists "                       "from information_schema.tables "                      "where table_schema = %s and table_name = %s limit 1")          else:              query = ("select 1 as table_exists "                       "from pg_table_def "                      "where tablename = %s limit 1")          cursor = connection.cursor()          try:              cursor.execute(query, tuple(self.table.split('.')))
TEST_MODULES = [      'tornado.test.curl_httpclient_test',      'tornado.test.escape_test',      'tornado.test.gen_test',      'tornado.test.httpclient_test',      'tornado.test.httpserver_test',      'tornado.test.httputil_test',
class CategoricalIndex(Index, accessor.PandasDelegate):      @Appender(_index_shared_docs["_convert_scalar_indexer"])      def _convert_scalar_indexer(self, key, kind=None):         if self.categories._defer_to_indexing:             return self.categories._convert_scalar_indexer(key, kind=kind)           return super()._convert_scalar_indexer(key, kind=kind)      @Appender(_index_shared_docs["_convert_list_indexer"])
def read_pickle(path, compression="infer"):      >>> import os      >>> os.remove("./dummy.pkl")     path = stringify_path(path)     f, fh = get_handle(path, "rb", compression=compression, is_text=False)
def na_value_for_dtype(dtype, compat: bool = True):      if is_extension_array_dtype(dtype):          return dtype.na_value     if (         is_datetime64_dtype(dtype)         or is_datetime64tz_dtype(dtype)         or is_timedelta64_dtype(dtype)         or is_period_dtype(dtype)     ):          return NaT      elif is_float_dtype(dtype):          return np.nan
def split_line(      If `py36` is True, splitting may generate syntax that is only compatible      with Python 3.6 and later.     if isinstance(line, UnformattedLines) or line.is_comment:          yield line          return
class _AxesBase(martist.Artist):          left, right = sorted([left, right], reverse=bool(reverse))          self._viewLim.intervalx = (left, right)          if auto is not None:              self._autoscaleXon = bool(auto)
def match(command):  @git_support  def get_new_command(command):      push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]     return replace_argument(command.script, 'push', push_upstream)
def fit_generator(model,              if val_gen and workers > 0:                  val_data = validation_data                 if isinstance(val_data, Sequence):                      val_enqueuer = OrderedEnqueuer(                          val_data,                          use_multiprocessing=use_multiprocessing)
class FeedExporter(object):          d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,                                              extra={'spider': spider}))          d.addErrback(lambda f: logger.error(logfmt % "Error storing", log_args,                                             extra={'spider': spider, 'failure': f}))          return d      def item_scraped(self, item, spider):
setup(              "options_test.cfg",              "static/robots.txt",              "static/dir/index.html",              "templates/utf8.html",              "test.crt",              "test.key",
def update_add(x, increment):          The variable `x` updated.     return tf_state_ops.assign_add(x, increment)  @symbolic
from pandas.core.dtypes.generic import ABCSeries  from pandas.core.dtypes.missing import isna  from pandas._typing import AnyArrayLike  from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs  import pandas.core.common as com  import pandas.core.indexes.base as ibase
class TestBackend(object):          else:              assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)     def test_print_tensor(self):          check_single_tensor_operation('print_tensor', (), WITH_NP)          check_single_tensor_operation('print_tensor', (2,), WITH_NP)         check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)         check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)      def test_elementwise_operations(self):          check_single_tensor_operation('max', (4, 2), WITH_NP)
def _unstack_multiple(data, clocs, fill_value=None):      index = data.index      clocs = [index._get_level_number(i) for i in clocs]      rlocs = [i for i in range(index.nlevels) if i not in clocs]
def is_string_dtype(arr_or_dtype) -> bool:         is_excluded_checks = (is_period_dtype, is_interval_dtype)          return any(is_excluded(dtype) for is_excluded in is_excluded_checks)      return _is_dtype(arr_or_dtype, condition)
def conv_input_length(output_length, filter_size, padding, stride):      return (output_length - 1) * stride - 2 * pad + filter_size def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):
def test_resample_categorical_data_with_timedeltaindex():          index=pd.to_timedelta([0, 10], unit="s"),      )      expected = expected.reindex(["Group_obj", "Group"], axis=1)     expected["Group"] = expected["Group_obj"].astype("category")      tm.assert_frame_equal(result, expected)
class FastAPI(Starlette):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
class FastAPI(Starlette):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
def jsonable_encoder(              )          return jsonable_encoder(              obj_dict,             include_none=include_none,              custom_encoder=encoder,              sqlalchemy_safe=sqlalchemy_safe,          )
class APIRouter(routing.Router):          response_model_by_alias: bool = True,          response_model_skip_defaults: bool = None,          response_model_exclude_unset: bool = False,          include_in_schema: bool = True,          response_class: Type[Response] = None,          name: str = None,
def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf  def get_future_imports(node: Node) -> Set[str]:     imports = set()      for child in node.children:          if child.type != syms.simple_stmt:              break
def _isna_old(obj):      elif isinstance(obj, type):          return False      elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):         return _isna_ndarraylike_old(obj)      elif isinstance(obj, ABCDataFrame):          return obj.isna()      elif isinstance(obj, list):         return _isna_ndarraylike_old(np.asarray(obj, dtype=object))      elif hasattr(obj, "__array__"):         return _isna_ndarraylike_old(np.asarray(obj))      else:          return False
def test_check_mutually_exclusive_none():  def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):      with pytest.raises(TypeError) as te:          check_mutually_exclusive(mutually_exclusive_terms, None)         assert "TypeError: 'NoneType' object is not iterable" in to_native(te.error)
class ColorbarBase(_ColorbarMappableDummy):      def set_label(self, label, **kw):         self._label = str(label)          self._labelkw = kw          self._set_label()
from .generic import Generic  class Bash(Generic):      def app_alias(self, fuck):         alias = "TF_ALIAS={0}" \                 " alias {0}='PYTHONIOENCODING=utf-8" \                 " TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && " \                  " eval $TF_CMD".format(fuck)          if settings.alter_history:
class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):              return self         arr = operator.inv(com.values_from_object(self))         return self.__array_wrap__(arr)      def __nonzero__(self):          raise ValueError(
class OffsiteMiddleware(object):          if not allowed_domains: return re.compile('')          url_pattern = re.compile("^https?://.*$")          for domain in allowed_domains:             if url_pattern.match(domain):                  message = ("allowed_domains accepts only domains, not URLs. "                             "Ignoring URL entry %s in allowed_domains." % domain)                  warnings.warn(message, URLWarning)         domains = [re.escape(d) for d in allowed_domains if d is not None]          regex = r'^(.*\.)?(%s)$' % '|'.join(domains)          return re.compile(regex)
class scheduler(Config):      visualization_graph = parameter.Parameter(default="svg", config_path=dict(section='scheduler', name='visualization-graph'))  def fix_time(x):
from scrapy.utils.ftp import ftp_makedirs_cwd  from scrapy.exceptions import NotConfigured  from scrapy.utils.misc import load_object  from scrapy.utils.python import get_func_args  logger = logging.getLogger(__name__)
class APIRouter(routing.Router):              response_model_exclude_unset=bool(                  response_model_exclude_unset or response_model_skip_defaults              ),              include_in_schema=include_in_schema,              response_class=response_class or self.default_response_class,              name=name,
class Categorical(ExtensionArray, PandasObject):              if dtype == self.dtype:                  return self              return self._set_dtype(dtype)          if is_integer_dtype(dtype) and self.isna().any():              msg = "Cannot convert float NaN to integer"              raise ValueError(msg)
default: :rc:`scatter.edgecolors`              - 'none': No patch boundary will be drawn.              - A color or sequence of colors.             For non-filled markers, the *edgecolors* kwarg is ignored and             forced to 'face' internally.          plotnonfinite : bool, default: False              Set to plot points with nonfinite *c*, in conjunction with
class Tracer:          self.target_codes = set()          self.target_frames = set()          self.thread_local = threading.local()      def __call__(self, function):          self.target_codes.add(function.__code__)          @functools.wraps(function)
import time  import traceback  import math from tornado.concurrent import TracebackFuture, is_future  from tornado.log import app_log, gen_log  from tornado.platform.auto import set_close_exec, Waker  from tornado import stack_context
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
class ExecutionEngine(object):          d = self._download(request, spider)          d.addBoth(self._handle_downloader_output, request, spider)          d.addErrback(lambda f: logger.info('Error while handling downloader output',                                            extra={'spider': spider, 'failure': f}))          d.addBoth(lambda _: slot.remove_request(request))          d.addErrback(lambda f: logger.info('Error while removing request from slot',                                            extra={'spider': spider, 'failure': f}))          d.addBoth(lambda _: slot.nextcall.schedule())          d.addErrback(lambda f: logger.info('Error while scheduling new request',                                            extra={'spider': spider, 'failure': f}))          return d      def _handle_downloader_output(self, response, request, spider):
class Sequential(Model):                                               use_multiprocessing=use_multiprocessing)      @interfaces.legacy_generator_methods_support     def predict_generator(self, generator, steps,                            max_queue_size=10, workers=1,                            use_multiprocessing=False, verbose=0):
fig, ax = plt.subplots(2, 1)  pcm = ax[0].pcolormesh(X, Y, Z,                         norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,                                               vmin=-1.0, vmax=1.0),                         cmap='RdBu_r')  fig.colorbar(pcm, ax=ax[0], extend='both')
class DataFrame(NDFrame):              other = other._convert(datetime=True, timedelta=True)              if not self.columns.equals(combined_columns):                  self = self.reindex(columns=combined_columns)         elif isinstance(other, list) and not isinstance(other[0], DataFrame):             other = DataFrame(other)             if (self.columns.get_indexer(other.columns) >= 0).all():                 other = other.reindex(columns=self.columns)          from pandas.core.reshape.concat import concat
class _AxesBase(martist.Artist):              if right is None:                  right = old_right         if self.get_xscale() == 'log':              if left <= 0:                  cbook._warn_external(                      'Attempted to set non-positive left xlim on a '
def _unstack_multiple(data, clocs, fill_value=None):              result = data              for i in range(len(clocs)):                  val = clocs[i]                 result = result.unstack(val)                  clocs = [v if i > v else v - 1 for v in clocs]              return result
class PagedList(object):  def uppercase_escape(s):      return re.sub(          r'\\U[0-9a-fA-F]{8}',         lambda m: m.group(0).decode('unicode-escape'), s)  try:      struct.pack(u'!I', 0)
def match_filter_func(filter_str):  def parse_dfxp_time_expr(time_expr):      if not time_expr:         return 0.0      mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)      if mobj:
class TestInsertIndexCoercion(CoercionBase):          )         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, pd.Timestamp("2012-01-01"))         msg = "cannot insert TimedeltaIndex with incompatible label"          with pytest.raises(TypeError, match=msg):              obj.insert(1, 1)
Wild         185.0          numeric_df = self._get_numeric_data()          cols = numeric_df.columns          idx = cols.copy()         mat = numeric_df.values          if method == "pearson":             correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)          elif method == "spearman":             correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)          elif method == "kendall" or callable(method):              if min_periods is None:                  min_periods = 1             mat = ensure_float64(mat).T              corrf = nanops.get_corr_func(method)              K = len(cols)              correl = np.empty((K, K), dtype=float)
