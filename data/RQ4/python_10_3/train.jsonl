{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of decades to use for each half of the linear range. For\n            example, when *linscale* == 1.0 (the default), the space used for\n            the positive and negative halves of the linear range will be equal\n            to one decade in the logarithmic range.\n         Normalize.__init__(self, vmin, vmax, clip)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - np.e ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()", "fixed": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of powers of *base* (decades for base 10) to use for each\n            half of the linear range. For example, when *linscale* == 1.0\n            (the default), the space used for the positive and negative halves\n            of the linear range will be equal to a decade in the logarithmic\n            range if ``base=10``.\n        base : float, default: None\n            For v3.2 the default is the old value of ``np.e``, but that is\n            deprecated for v3.3 when base will default to 10.  During the\n            transition, specify the *base* kwarg to avoid a deprecation\n            warning.\n         Normalize.__init__(self, vmin, vmax, clip)\n        if base is None:\n            self._base = np.e\n            cbook.warn_deprecated(\"3.3\", message=\"default base will change \"\n                \"from np.e to 10.  To suppress this warning specify the base \"\n                \"kwarg.\")\n        else:\n            self._base = base\n        self._log_base = np.log(self._base)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - self._base ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()"}
{"id": "tornado_1", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "pandas_38", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n     comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n    if rlocs == []:\n         dummy_index = Index(obs_ids, name=\"__placeholder__\")\n     else:", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n     comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n    if not rlocs:\n         dummy_index = Index(obs_ids, name=\"__placeholder__\")\n     else:"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
