{"id": "scrapy_7", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n     return urljoin(form.base_url, url)"}
{"id": "pandas_20", "problem": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):", "fixed": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "black_5", "problem": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n            if original.is_import:\n                 for i in range(len(leaves) - 1, -1, -1):\n                     if leaves[i].type == STANDALONE_COMMENT:\n                         continue", "fixed": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n            no_commas = original.is_def and not any(\n                l.type == token.COMMA for l in leaves\n            )\n            if original.is_import or no_commas:\n                 for i in range(len(leaves) - 1, -1, -1):\n                     if leaves[i].type == STANDALONE_COMMENT:\n                         continue"}
{"id": "keras_20", "problem": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support():\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'", "fixed": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'"}
{"id": "keras_39", "problem": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    current < self.target):\n                 return\n             prev_total_width = self.total_width", "fixed": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    (self.target is not None and current < self.target)):\n                 return\n             prev_total_width = self.total_width"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "black_14", "problem": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            for import_from_child in first_child.children[3:]:\n                if isinstance(import_from_child, Leaf):\n                    if import_from_child.type == token.NAME:\n                        imports.add(import_from_child.value)\n                else:\n                    assert import_from_child.type == syms.import_as_names\n                    for leaf in import_from_child.children:\n                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:\n                            imports.add(leaf.value)\n         else:\n             break\n     return imports", "fixed": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            imports |= set(get_imports_from_children(first_child.children[3:]))\n         else:\n             break\n     return imports"}
{"id": "pandas_80", "problem": " def make_data():\n @pytest.fixture\n def dtype():\n    return pd.BooleanDtype()\n @pytest.fixture", "fixed": " def make_data():\n @pytest.fixture\n def dtype():\n    return BooleanDtype()\n @pytest.fixture"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n        attributes.pop(\"tz\", None)\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj +\n               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj"}
{"id": "youtube-dl_39", "problem": " from ..utils import (\n     compat_urllib_parse,\n     compat_urllib_request,\n     urlencode_postdata,\n     ExtractorError,\n )", "fixed": " from ..utils import (\n     compat_urllib_parse,\n     compat_urllib_request,\n     urlencode_postdata,\n     ExtractorError,\n    limit_length,\n )"}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "PySnooper_1", "problem": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')", "fixed": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\nif pycompat.PY2:\n    from io import open\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):"}
{"id": "pandas_138", "problem": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n     if dtype is not None:", "fixed": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        x = x.astype(np.int64)\n     if dtype is not None:"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')\n            quote = m.group('quote')\n            if quote is not None:\n                comparison_value = comparison_value.replace(r'\\%s' % quote, quote)\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "youtube-dl_19", "problem": " import re\n import shutil\n import subprocess\n import socket\n import sys\n import time\n import tokenize", "fixed": " import re\n import shutil\n import subprocess\n import socket\nimport string\n import sys\n import time\n import tokenize"}
{"id": "pandas_167", "problem": " def convert_to_index_sliceable(obj, key):\n        if idx.is_all_dates:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):", "fixed": " def convert_to_index_sliceable(obj, key):\n        if idx._supports_partial_string_indexing:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, endwith=[]):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:", "fixed": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n     @property\n     def trainable(self):", "fixed": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n        self.input_spec = layer.input_spec\n     @property\n     def trainable(self):"}
{"id": "fastapi_1", "problem": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)", "fixed": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_41", "problem": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )", "fixed": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +", "fixed": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +"}
