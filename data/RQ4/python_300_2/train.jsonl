{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "pandas_75", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         if isinstance(key, str):\n             try:\n                return self._get_string_slice(key)\n            except (TypeError, KeyError, ValueError, OverflowError):\n                 pass\n             try:\n                 asdt, reso = parse_time_string(key, self.freq)\n                key = asdt\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{key}' as period\")\n         elif is_integer(key):\n             raise KeyError(key)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         if isinstance(key, str):\n             try:\n                loc = self._get_string_slice(key)\n                return loc\n            except (TypeError, ValueError):\n                 pass\n             try:\n                 asdt, reso = parse_time_string(key, self.freq)\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{key}' as period\")\n            grp = resolution.Resolution.get_freq_group(reso)\n            freqn = resolution.get_freq_group(self.freq)\n            assert grp >= freqn\n            if grp == freqn:\n                key = Period(asdt, freq=self.freq)\n                loc = self.get_loc(key, method=method, tolerance=tolerance)\n                return loc\n            elif method is None:\n                raise KeyError(key)\n            else:\n                key = asdt\n         elif is_integer(key):\n             raise KeyError(key)"}
{"id": "fastapi_2", "problem": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:", "fixed": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "matplotlib_5", "problem": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            linewidths = rcParams['lines.linewidth']\n         offsets = np.ma.column_stack([x, y])", "fixed": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            if linewidths is None:\n                linewidths = rcParams['lines.linewidth']\n            elif np.iterable(linewidths):\n                linewidths = [\n                    lw if lw is not None else rcParams['lines.linewidth']\n                    for lw in linewidths]\n         offsets = np.ma.column_stack([x, y])"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):"}
{"id": "pandas_135", "problem": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n            return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)", "fixed": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n        except TypeError as err:\n            if \"ndarray\" in str(err):\n                pass\n            else:\n                raise\n        return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)"}
{"id": "tqdm_6", "problem": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else self.total)\n     def __enter__(self):\n         return self", "fixed": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else getattr(self, \"total\", None))\n     def __enter__(self):\n         return self"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$", "fixed": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n            (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n             False,\n             padding,\n             padding],\n        output_shape=output_shape)\n     return _postprocess_conv2d_output(x, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n             False,\n             padding,\n             padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n     return _postprocess_conv2d_output(x, data_format)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output: Series or DataFrame\n             Object resulting from grouping and applying an operation.\n         Returns\n         -------", "fixed": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output : Series or DataFrame\n             Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n         Returns\n         -------"}
{"id": "pandas_123", "problem": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n                     elif inferred == \"string\":\n                         pass", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, name=name)\n                     elif inferred == \"string\":\n                         pass"}
{"id": "youtube-dl_16", "problem": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')", "fixed": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data)\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')"}
{"id": "pandas_76", "problem": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError):\n                 pass", "fixed": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError, OverflowError):\n                 pass"}
{"id": "fastapi_14", "problem": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[bool, Any]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None", "fixed": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[Dict[str, Any], bool]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None"}
{"id": "pandas_94", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self._data\n        if isinstance(values, type(self)):\n            values = values._data\n        attributes = self._get_attributes_dict()\n        if \"freq\" not in kwargs and self.freq is not None:\n            if isinstance(values, (DatetimeArray, TimedeltaArray)):\n                if values.freq is None:\n                    del attributes[\"freq\"]\n        attributes.update(kwargs)\n        return self._simple_new(values, **attributes)"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n     def __init__(\n             self,", "fixed": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n    Customize how values are represented as strings::\n        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))\n     def __init__(\n             self,"}
{"id": "black_6", "problem": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: set(),\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,", "fixed": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n    ASYNC_IS_VALID_IDENTIFIER = 6\n    ASYNC_IS_RESERVED_KEYWORD = 7\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY35: {\n        Feature.UNICODE_LITERALS,\n        Feature.TRAILING_COMMA_IN_CALL,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n    },\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):", "fixed": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):"}
{"name": "levenshtein.py", "problem": "def levenshtein(source, target):\n    if source == '' or target == '':\n        return len(source) or len(target)\n    elif source[0] == target[0]:\n        return 1 + levenshtein(source[1:], target[1:])\n    else:\n        return 1 + min(\n            levenshtein(source,     target[1:]),\n            levenshtein(source[1:], target[1:]),\n            levenshtein(source[1:], target)\n        )", "fixed": "def levenshtein(source, target):\n    if source == '' or target == '':\n        return len(source) or len(target)\n    elif source[0] == target[0]:\n        return levenshtein(source[1:], target[1:])\n    else:\n        return 1 + min(\n            levenshtein(source,     target[1:]),\n            levenshtein(source[1:], target[1:]),\n            levenshtein(source[1:], target)\n        )", "hint": "Levenshtein Distance\nCalculates the Levenshtein distance between two strings.  The Levenshtein distance is defined as the minimum amount of single-character edits (either removing a character, adding a character, or changing a character) necessary to transform a source string into a target string.\nInput:", "input": ["electron", "neutron"], "output": 3}
{"id": "pandas_125", "problem": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)\n    def replace(self, to_replace, value, inplace: bool = False):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                else:\n                    categories[index] = value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat"}
{"id": "ansible_15", "problem": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state') and not needs_update('vrf'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':", "fixed": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':"}
{"id": "sanic_5", "problem": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],", "fixed": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],"}
{"id": "youtube-dl_39", "problem": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output", "fixed": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output\ndef limit_length(s, length):\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s"}
{"id": "keras_19", "problem": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "luigi_6", "problem": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x))\n         except ValueError:\nreturn literal_eval(x)\n    def serialize(self, x):\n        return json.dumps(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n         except ValueError:\nreturn literal_eval(x)\n class NumericalParameter(Parameter):"}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n    BodySchema_kwargs: Dict[str, Any] = dict(default=None)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n     mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort)\n         for n in range(len(left_keys))\n     )\n     zipped = zip(*mapped)", "fixed": " def _get_join_indexers(\n     mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)\n         for n in range(len(left_keys))\n     )\n     zipped = zip(*mapped)"}
{"id": "pandas_54", "problem": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n         elif is_categorical(values):", "fixed": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n         elif is_categorical(values):"}
{"id": "ansible_9", "problem": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, 1\n        pool_ids[pool_id] = str(quantity)\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]", "fixed": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, None\n        pool_ids[pool_id] = quantity\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "youtube-dl_31", "problem": " def parse_duration(s):\n     m = re.match(", "fixed": " def parse_duration(s):\n     m = re.match(\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?"}
{"id": "keras_8", "problem": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:", "fixed": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    raise LookupError\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:"}
{"id": "pandas_34", "problem": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=\"infer\",\n             nonexistent=\"shift_forward\",\n         )", "fixed": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=True,\n             nonexistent=\"shift_forward\",\n         )"}
{"id": "ansible_11", "problem": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n    if text:\n        text = str(text).strip()\n     return {\n         'banner': module.params['banner'],\n         'text': text,", "fixed": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n     return {\n         'banner': module.params['banner'],\n         'text': text,"}
{"id": "pandas_83", "problem": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the", "fixed": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the"}
{"id": "tornado_16", "problem": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n         if self._finished:\n             self._return_result(self._finished.popleft())", "fixed": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n        self._running_future.add_done_callback(lambda f: self)\n         if self._finished:\n             self._return_result(self._finished.popleft())"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip()\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)", "fixed": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip('\\n')\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_105", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n    def transpose(self, *args, **kwargs):\n        axes, kwargs = self._construct_axes_from_arguments(\n            args, kwargs, require_all=True\n        )\n        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)\n        if len(axes) != len(set(axes)):\n            raise ValueError(f\"Must specify {self._AXIS_LEN} unique axes\")\n        new_axes = self._construct_axes_dict_from(\n            self, [self._get_axis(x) for x in axes_names]\n        )\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop(\"copy\", None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n        nv.validate_transpose(tuple(), kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)\n     def swapaxes(self, axis1, axis2, copy=True):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n     def swapaxes(self, axis1, axis2, copy=True):"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "youtube-dl_26", "problem": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "pandas_165", "problem": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n class TestAddSubNaTMasking:", "fixed": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n    def test_tda_add_sub_index(self):\n        tdi = TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\n        tda = tdi.array\n        dti = pd.date_range(\"1999-12-31\", periods=3, freq=\"D\")\n        result = tda + dti\n        expected = tdi + dti\n        tm.assert_index_equal(result, expected)\n        result = tda + tdi\n        expected = tdi + tdi\n        tm.assert_index_equal(result, expected)\n        result = tda - tdi\n        expected = tdi - tdi\n        tm.assert_index_equal(result, expected)\n class TestAddSubNaTMasking:"}
{"id": "thefuck_18", "problem": " patterns = ['permission denied',\n def match(command):\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():", "fixed": " patterns = ['permission denied',\n def match(command):\n    if command.script_parts and command.script_parts[0] == 'sudo':\n        return False\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():"}
{"id": "scrapy_31", "problem": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))\n     def header_items(self):\n         return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n             for k, v in self.request.headers.items()\n         ]", "fixed": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n     def header_items(self):\n         return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        epsilon: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced.", "fixed": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced."}
{"id": "pandas_138", "problem": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)", "fixed": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)"}
{"id": "pandas_167", "problem": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n     _accessors = {\"str\"}", "fixed": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n    _supports_partial_string_indexing = False\n     _accessors = {\"str\"}"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n         Returns\n         -------\n        axes: topmost axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy)]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:", "fixed": " class FigureCanvasBase:\n         Returns\n         -------\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy) and a.get_visible()]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:"}
{"id": "scrapy_23", "problem": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)", "fixed": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "ansible_10", "problem": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type", "fixed": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n        self.prev = None\n        self.next = None\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         this, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\n         new_data = {}\n        for col, series in this.items():\n            new_data[col] = func(series.values, other.values)\n         fill_value = self._get_op_result_fill_value(other, func)", "fixed": " class SparseDataFrame(DataFrame):\n         this, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\n         new_data = {}\n        for col in this.columns:\n            new_data[col] = func(this[col], other)\n         fill_value = self._get_op_result_fill_value(other, func)"}
{"id": "pandas_38", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n     comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n    if rlocs == []:\n         dummy_index = Index(obs_ids, name=\"__placeholder__\")\n     else:", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n     comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n    if not rlocs:\n         dummy_index = Index(obs_ids, name=\"__placeholder__\")\n     else:"}
{"id": "scrapy_6", "problem": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')", "fixed": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading"}
{"id": "matplotlib_22", "problem": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')", "fixed": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n        if not cbook.is_scalar_or_string(bins):\n            bins = self.convert_xunits(bins)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"id": "pandas_167", "problem": " def convert_to_index_sliceable(obj, key):\n        if idx.is_all_dates:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):", "fixed": " def convert_to_index_sliceable(obj, key):\n        if idx._supports_partial_string_indexing:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):"}
{"name": "wrap.py", "problem": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    return lines", "fixed": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    lines.append(text)\n    return lines", "hint": "Wrap Text\nGiven a long string and a column width, break the string on spaces into a list of lines such that each line is no longer than the column width.\nInput:", "input": [], "output": ""}
{"id": "pandas_128", "problem": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(", "fixed": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n    if encoding is None:\n        encoding = \"utf-8\"\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer("}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "matplotlib_17", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "tornado_1", "problem": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__(", "fixed": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__("}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "keras_16", "problem": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n         if layers:", "fixed": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n        self._build_input_shape = None\n         if layers:"}
{"id": "pandas_24", "problem": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:", "fixed": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:"}
{"id": "matplotlib_9", "problem": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin", "fixed": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n        self._unstale_viewLim()\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin"}
{"id": "matplotlib_3", "problem": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = True\n         self._marker_function()\n     def __bool__(self):", "fixed": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = self._fillstyle != 'none'\n         self._marker_function()\n     def __bool__(self):"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string in endwith:\n                         break\n                    elif string == ')':\n                         tokens.restore_last_token()\n                         break\n                    if string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, [','])\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':", "fixed": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            tokens.restore_last_token()\n                         break\n                    elif inside_merge and string in ['/', ',']:\n                         tokens.restore_last_token()\n                         break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,", "fixed": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n            dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,"}
{"id": "pandas_2", "problem": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n        key = list(self._convert_key(key, is_setter=True))\n         self.obj._set_value(*key, value=value, takeable=self._takeable)", "fixed": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key, is_setter=True))\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n         self.obj._set_value(*key, value=value, takeable=self._takeable)"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    path : str\n        File path where the pickled object will be loaded.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',\n        or '.zip' respectively, and no decompression otherwise.\n        Set to None for no decompression.\n     Returns\n     -------", "fixed": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be loaded from.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL is not limited to S3 and GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     Returns\n     -------"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_48", "problem": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except ValueError:\n                         result = result.reshape(1, -1)", "fixed": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except (ValueError, TypeError):\n                         result = result.reshape(1, -1)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )"}
{"id": "keras_23", "problem": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape", "fixed": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "pandas_39", "problem": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n             self._update_inplace(", "fixed": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n            self._reset_cacher()\n             self._update_inplace("}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "keras_1", "problem": " def get_variable_shape(x):\n     return int_shape(x)\n def print_tensor(x, message=''):", "fixed": " def get_variable_shape(x):\n     return int_shape(x)\n@symbolic\n def print_tensor(x, message=''):"}
{"id": "pandas_141", "problem": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):", "fixed": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            reverse = self._range[::-1]\n            start, stop, step = reverse.start, reverse.stop, reverse.step\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):"}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        return Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n     def _apply_to_column_groupbys(self, func):", "fixed": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        result = Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n        return self._reindex_output(result, fill_value=0)\n     def _apply_to_column_groupbys(self, func):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "fastapi_14", "problem": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None", "fixed": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[str, Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a same-typed block.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block.", "fixed": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block."}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)\n     ):\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes", "fixed": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\n     ):\n        assert is_categorical(lk) and is_categorical(rk)\n        lk = cast(Categorical, lk)\n        rk = cast(Categorical, rk)\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "black_22", "problem": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.leaves.pop()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.leaves.pop()\n                 return True", "fixed": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.remove_trailing_comma()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.remove_trailing_comma()\n                 return True"}
{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]", "fixed": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"C\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]"}
{"id": "tornado_15", "problem": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root)\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)", "fixed": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root) + os.path.sep\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)"}
{"id": "scrapy_7", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n     return urljoin(form.base_url, url)"}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 try:\n                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                         generator_output = next(self._generator)\n                        self.queue.put(generator_output)\n                     else:\n                         time.sleep(self.wait_time)\n                 except StopIteration:\n                     break\n                except Exception:\n                     self._stop_event.set()\n                    raise\n         try:\n             if self._use_multiprocessing:\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n                 self.queue = queue.Queue()", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 try:\n                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                         generator_output = next(self._generator)\n                        self.queue.put((True, generator_output))\n                     else:\n                         time.sleep(self.wait_time)\n                 except StopIteration:\n                     break\n                except Exception as e:\n                    if self._use_multiprocessing:\n                        traceback.print_exc()\n                        setattr(e, '__traceback__', None)\n                    elif not hasattr(e, '__traceback__'):\n                        setattr(e, '__traceback__', sys.exc_info()[2])\n                    self.queue.put((False, e))\n                     self._stop_event.set()\n                    break\n         try:\n             if self._use_multiprocessing:\n                self._manager = multiprocessing.Manager()\n                self.queue = self._manager.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n                 self.queue = queue.Queue()"}
{"id": "pandas_6", "problem": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):", "fixed": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError, ValueError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "ansible_2", "problem": " class _Numeric:\n         raise ValueError\n    def __gt__(self, other):\n        return not self.__lt__(other)\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n     def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n class SemanticVersion(Version):", "fixed": " class _Numeric:\n         raise ValueError\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n    def __gt__(self, other):\n        return not self.__le__(other)\n     def __ge__(self, other):\n        return not self.__lt__(other)\n class SemanticVersion(Version):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "tqdm_9", "problem": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable)\n     def __iter__(self):", "fixed": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable) if self.iterable else self.total\n     def __iter__(self):"}
{"id": "black_23", "problem": " def func_no_args():\n         print(i)\n         continue\n     return None\nasync def coroutine(arg):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n         print(i)\n         continue\n    exec(\"new-style exec\", {}, {})\n     return None\nasync def coroutine(arg, exec=False):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "youtube-dl_43", "problem": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?\n     if not m:\n         return u''\n     return m.group(1)", "fixed": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^?\n     if not m:\n         return u''\n     return m.group(1)"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "keras_36", "problem": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1, 1) + strides + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1, 1) + strides\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)", "fixed": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):", "fixed": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index + 1), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):"}
{"id": "youtube-dl_16", "problem": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:", "fixed": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with open(dfxp_file, 'rb') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "keras_11", "problem": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:", "fixed": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n     ) -> None:\n        route = self.route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,", "fixed": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n     ) -> None:\n        route_class = route_class_override or self.route_class\n        route = route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "ansible_4", "problem": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)\n     def _load_collections(self, attr, ds):", "fixed": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n     def _load_collections(self, attr, ds):"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "tqdm_9", "problem": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix", "fixed": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'][:-5])\n         thumbnail = None\n         duration = None", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'])\n         thumbnail = None\n         duration = None"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,"}
{"name": "shortest_path_length.py", "problem": "from heapq import *\ndef shortest_path_length(length_by_edge, startnode, goalnode):\nunvisited_nodes = []\n    heappush(unvisited_nodes, (0, startnode))\n    visited_nodes = set()\n    while len(unvisited_nodes) > 0:\n        distance, node = heappop(unvisited_nodes)\n        if node is goalnode:\n            return distance\n        visited_nodes.add(node)\n        for nextnode in node.successors:\n            if nextnode in visited_nodes:\n                continue\n            insert_or_update(unvisited_nodes,\n                (min(\n                    get(unvisited_nodes, nextnode) or float('inf'),\n                    get(unvisited_nodes, nextnode) + length_by_edge[node, nextnode]\n                ),\n                nextnode)\n            )\n    return float('inf')\ndef get(node_heap, wanted_node):\n    for dist, node in node_heap:\n        if node == wanted_node:\n            return dist\n    return 0\ndef insert_or_update(node_heap, dist_node):\n    dist, node = dist_node\n    for i, tpl in enumerate(node_heap):\n        a, b = tpl\n        if b == node:\nnode_heap[i] = dist_node\n            return None\n    heappush(node_heap, dist_node)\n    return None", "fixed": "from heapq import *\ndef shortest_path_length(length_by_edge, startnode, goalnode):\nunvisited_nodes = []\n    heappush(unvisited_nodes, (0, startnode))\n    visited_nodes = set()\n    while len(unvisited_nodes) > 0:\n        distance, node = heappop(unvisited_nodes)\n        if node is goalnode:\n            return distance\n        visited_nodes.add(node)\n        for nextnode in node.successors:\n            if nextnode in visited_nodes:\n                continue\n            insert_or_update(unvisited_nodes,\n                (min(\n                    get(unvisited_nodes, nextnode) or float('inf'),\n                    distance + length_by_edge[node, nextnode]\n                ),\n                nextnode)\n            )\n    return float('inf')\ndef get(node_heap, wanted_node):\n    for dist, node in node_heap:\n        if node == wanted_node:\n            return dist\n    return 0\ndef insert_or_update(node_heap, dist_node):\n    dist, node = dist_node\n    for i, tpl in enumerate(node_heap):\n        a, b = tpl\n        if b == node:\nnode_heap[i] = dist_node\n            return None\n    heappush(node_heap, dist_node)\n    return None\n", "hint": "Shortest Path\ndijkstra\nImplements Dijkstra's algorithm for finding a shortest path between two nodes in a directed graph.", "input": [], "output": ""}
{"id": "pandas_68", "problem": " class BaseMethodsTests(BaseExtensionTests):\n         expected = empty\n         self.assert_extension_array_equal(result, expected)\n     def test_shift_fill_value(self, data):\n         arr = data[:4]\n         fill_value = data[0]", "fixed": " class BaseMethodsTests(BaseExtensionTests):\n         expected = empty\n         self.assert_extension_array_equal(result, expected)\n    def test_shift_zero_copies(self, data):\n        result = data.shift(0)\n        assert result is not data\n        result = data[:0].shift(2)\n        assert result is not data\n     def test_shift_fill_value(self, data):\n         arr = data[:4]\n         fill_value = data[0]"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n        actual_value = dct.get(m.group('key'))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)", "fixed": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n                                   extra={'spider': spider, 'failure': f}))\n         self._scrape_next(spider, slot)\n         return dfd", "fixed": " class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n         self._scrape_next(spider, slot)\n         return dfd"}
{"id": "scrapy_12", "problem": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n         if _root is not None:", "fixed": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n         st = _st(response, type or self._default_type)\n         if _root is not None:"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n    def _ensure_listlike_indexer(self, key, axis=None):\n        column_axis = 1\n        if self.ndim != 2:\n            return\n        if isinstance(key, tuple):\n            key = key[column_axis]\n            axis = column_axis\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, ABCMultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            for k in key:\n                try:\n                    self.obj[k]\n                except KeyError:\n                    self.obj[k] = np.nan\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value))\nelse:\n                 self.reset_global()", "fixed": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value, task_name=task_name))\nelse:\n                 self.reset_global()"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "luigi_6", "problem": " class DictParameter(Parameter):\n         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)\n     def serialize(self, x):\n        return json.dumps(x, cls=DictParameter._DictParamEncoder)\n class ListParameter(Parameter):", "fixed": " class DictParameter(Parameter):\n         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)\n     def serialize(self, x):\n        return json.dumps(x, cls=_DictParamEncoder)\n class ListParameter(Parameter):"}
{"id": "luigi_14", "problem": " class Task(object):\n         self.failures.add_failure()\n     def has_excessive_failures(self):\n        if (self.failures.first_failure_time is not None and\n                self.disable_hard_timeout):\n             if (time.time() >= self.failures.first_failure_time +\n                     self.disable_hard_timeout):\n                 return True", "fixed": " class Task(object):\n         self.failures.add_failure()\n     def has_excessive_failures(self):\n        if self.failures.first_failure_time is not None:\n             if (time.time() >= self.failures.first_failure_time +\n                     self.disable_hard_timeout):\n                 return True"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "httpie_1", "problem": " def filename_from_url(url, content_type):\n     return fn\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        if not exists(filename + suffix):\n            return filename + suffix\n         attempt += 1", "fixed": " def filename_from_url(url, content_type):\n     return fn\ndef trim_filename(filename, max_len):\n    if len(filename) > max_len:\n        trim_by = len(filename) - max_len\n        name, ext = os.path.splitext(filename)\n        if trim_by >= len(name):\n            filename = filename[:-trim_by]\n        else:\n            filename = name[:-trim_by] + ext\n    return filename\ndef get_filename_max_length(directory):\n    try:\n        max_len = os.pathconf(directory, 'PC_NAME_MAX')\n    except OSError as e:\n        if e.errno == errno.EINVAL:\n            max_len = 255\n        else:\n            raise\n    return max_len\ndef trim_filename_if_needed(filename, directory='.', extra=0):\n    max_len = get_filename_max_length(directory) - extra\n    if len(filename) > max_len:\n        filename = trim_filename(filename, max_len)\n    return filename\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        try_filename = trim_filename_if_needed(filename, extra=len(suffix))\n        try_filename += suffix\n        if not exists(try_filename):\n            return try_filename\n         attempt += 1"}
{"id": "pandas_92", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n             if where < start:\n                 if not is_series:", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n             if where < start:\n                 if not is_series:"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "cookiecutter_4", "problem": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return", "fixed": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        try:\n            run_hook('pre_gen_project', project_dir, context)\n        except FailedHookException:\n            shutil.rmtree(project_dir, ignore_errors=True)\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return"}
{"id": "tornado_10", "problem": " class RequestHandler(object):\n         self._log()\n         self._finished = True\n         self.on_finish()\n         self.ui = None", "fixed": " class RequestHandler(object):\n         self._log()\n         self._finished = True\n         self.on_finish()\n        self._break_cycles()\n    def _break_cycles(self):\n         self.ui = None"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = compat_urllib_request.urlopen(request).read()\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'\"fb_dtsg\":\"(.*?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<input value=\"(.*?)\" name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = compat_urllib_request.urlopen(check_req).read()\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:", "fixed": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = self._download_webpage(request, None,\n                note='Logging in', errnote='unable to fetch login page')\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'name=\"fb_dtsg\" value=\"(.+?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<button[^>]+value=\"(.*?)\"[^>]+name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = self._download_webpage(check_req, None,\n                note='Confirming login')\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "cookiecutter_2", "problem": " def run_hook(hook_name, project_dir, context):\n     :param project_dir: The directory to execute the script from.\n     :param context: Cookiecutter project context.\n    script = find_hook(hook_name)\n    if script is None:\n         logger.debug('No %s hook found', hook_name)\n         return\n     logger.debug('Running hook %s', hook_name)\n    run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     :param project_dir: The directory to execute the script from.\n     :param context: Cookiecutter project context.\n    scripts = find_hook(hook_name)\n    if not scripts:\n         logger.debug('No %s hook found', hook_name)\n         return\n     logger.debug('Running hook %s', hook_name)\n    for script in scripts:\n        run_script_with_context(script, project_dir, context)"}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "luigi_16", "problem": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            if task.id not in necessary_tasks and self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)", "fixed": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            removed = self._state.prune(task, self._config)\n            if removed and task.id not in necessary_tasks:\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        return Series(res, index=ri, name=self._selection_name)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):", "fixed": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        result = Series(res, index=ri, name=self._selection_name)\n        return self._reindex_output(result, fill_value=0)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value):\n         return isinstance(value, self._holder)\n    def set(self, locs, values, check=False):\n         assert locs.tolist() == [0]\n        self.values = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,", "fixed": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value: ArrayLike) -> bool:\n         return isinstance(value, self._holder)\n    def set(self, locs, values):\n         assert locs.tolist() == [0]\n        self.values[:] = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,"}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n                 bar_format = _unicode(bar_format)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         elif bar_format:", "fixed": " class tqdm(Comparable):\n             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n                 bar_format = _unicode(bar_format)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         elif bar_format:"}
{"id": "black_15", "problem": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break", "fixed": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n        if parent.type == syms.file_input:\n            break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "luigi_1", "problem": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics = self._scheduler._state._metrics_collector.generate_latest()\n         if metrics:\n            metrics.configure_http_handler(self)\n             self.write(metrics)", "fixed": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n         if metrics:\n            metrics_collector.configure_http_handler(self)\n             self.write(metrics)"}
{"id": "cookiecutter_4", "problem": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.", "fixed": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.\n    Raised when a hook script fails"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "pandas_44", "problem": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n     def get_loc(self, key, method=None, tolerance=None):", "fixed": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_timedelta64_dtype(dtype)\n     def get_loc(self, key, method=None, tolerance=None):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                     elif string == '(':\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                        current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])\n                     elif string == '+':\n                         video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, [','])\n                        current_selector = None\n                        selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))\n                     else:\n                         raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                 elif type == tokenize.ENDMARKER:", "fixed": " class YoutubeDL(object):\n                     elif string == '(':\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                     elif string == '+':\n                         video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])\n                     else:\n                         raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                 elif type == tokenize.ENDMARKER:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_45", "problem": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)", "fixed": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n    elif isinstance(data, abc.Set):\n        raise TypeError(\"Set type is unordered\")\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj +\n               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj"}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "pandas_119", "problem": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n         result = result.append(margin_dummy)\n     except TypeError:", "fixed": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )\n         result = result.append(margin_dummy)\n     except TypeError:"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n             )\nelse:\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,", "fixed": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n                exclude_none=exclude_none,\n                exclude_defaults=exclude_defaults,\n             )\nelse:\n            if exclude_defaults:\n                raise ValueError(\"Cannot use exclude_defaults\")\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,"}
{"id": "pandas_159", "problem": " class DataFrame(NDFrame):\n             return ops.dispatch_to_series(this, other, _arith_op)\n         else:\n            result = _arith_op(this.values, other.values)\n             return self._constructor(\n                 result, index=new_index, columns=new_columns, copy=False\n             )", "fixed": " class DataFrame(NDFrame):\n             return ops.dispatch_to_series(this, other, _arith_op)\n         else:\n            with np.errstate(all=\"ignore\"):\n                result = _arith_op(this.values, other.values)\n            result = dispatch_fill_zeros(func, this.values, other.values, result)\n             return self._constructor(\n                 result, index=new_index, columns=new_columns, copy=False\n             )"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):", "fixed": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v.startswith('//') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.power(self._base,\n                       sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"name": "find_first_in_sorted.py", "problem": "def find_first_in_sorted(arr, x):\n    lo = 0\n    hi = len(arr)\n    while lo <= hi:\n        mid = (lo + hi) // 2\n        if x == arr[mid] and (mid == 0 or x != arr[mid - 1]):\n            return mid\n        elif x <= arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return -1", "fixed": "def find_first_in_sorted(arr, x):\n    lo = 0\n    hi = len(arr)\n    while lo < hi:\n        mid = (lo + hi) // 2\n        if x == arr[mid] and (mid == 0 or x != arr[mid - 1]):\n            return mid\n        elif x <= arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return -1\n", "hint": "Fancy Binary Search\nfancy-binsearch\nInput:", "input": [[3, 4, 5, 5, 5, 5, 6], 5], "output": 2}
{"id": "matplotlib_30", "problem": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    x = x * (N - 1)\n    xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n    ind = np.searchsorted(x, xind)[1:-1]\n    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n    lut = np.concatenate([\n        [y1[0]],\n        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n        [y0[-1]],\n    ])\n     return np.clip(lut, 0.0, 1.0)", "fixed": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    if N == 1:\n        lut = np.array(y0[-1])\n    else:\n        x = x * (N - 1)\n        xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n        ind = np.searchsorted(x, xind)[1:-1]\n        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n        lut = np.concatenate([\n            [y1[0]],\n            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n            [y0[-1]],\n        ])\n     return np.clip(lut, 0.0, 1.0)"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n     for leaf in line.leaves:\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n     for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                if responses is None:\n                    responses = {}\n                responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,", "fixed": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        if responses is None:\n            responses = {}\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,"}
{"id": "fastapi_1", "problem": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)", "fixed": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)"}
{"id": "keras_24", "problem": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:", "fixed": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "luigi_19", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable():\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"name": "depth_first_search.py", "problem": "def depth_first_search(startnode, goalnode):\n    nodesvisited = set()\n    def search_from(node):\n        if node in nodesvisited:\n            return False\n        elif node is goalnode:\n            return True\n        else:\n            return any(\n                search_from(nextnode) for nextnode in node.successors\n            )\n    return search_from(startnode)", "fixed": "def depth_first_search(startnode, goalnode):\n    nodesvisited = set()\n    def search_from(node):\n        if node in nodesvisited:\n            return False\n        elif node is goalnode:\n            return True\n        else:\n            nodesvisited.add(node)\n            return any(\n                search_from(nextnode) for nextnode in node.successors\n            )\n    return search_from(startnode)", "hint": "Depth-first Search\nInput:\n    startnode: A digraph node", "input": "", "output": ""}
{"id": "youtube-dl_4", "problem": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]+)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')])\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)", "fixed": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)"}
{"id": "luigi_17", "problem": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)", "fixed": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)"}
{"id": "pandas_67", "problem": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()", "fixed": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n    def iget(self, key):\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()"}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "scrapy_3", "problem": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['location'])\n         redirected_url = urljoin(request.url, location)", "fixed": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n         redirected_url = urljoin(request.url, location)"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "pandas_100", "problem": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        rs = rs.loc[~rs.index.duplicated()]\n        rs = rs.reindex_like(data)\n        if freq is None:\n            mask = isna(com.values_from_object(data))\n            np.putmask(rs.values, mask, np.nan)\n         return rs\n     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        if freq is not None:\n            rs = rs.loc[~rs.index.duplicated()]\n            rs = rs.reindex_like(data)\n         return rs\n     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '", "fixed": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "keras_33", "problem": " def text_to_word_sequence(text,\n     if lower:\n         text = text.lower()\n    if sys.version_info < (3,) and isinstance(text, unicode):\n        translate_map = dict((ord(c), unicode(split)) for c in filters)\n     else:\n        translate_map = maketrans(filters, split * len(filters))\n    text = text.translate(translate_map)\n     seq = text.split(split)\n     return [i for i in seq if i]", "fixed": " def text_to_word_sequence(text,\n     if lower:\n         text = text.lower()\n    if sys.version_info < (3,):\n        if isinstance(text, unicode):\n            translate_map = dict((ord(c), unicode(split)) for c in filters)\n            text = text.translate(translate_map)\n        elif len(split) == 1:\n            translate_map = maketrans(filters, split * len(filters))\n            text = text.translate(translate_map)\n        else:\n            for c in filters:\n                text = text.replace(c, split)\n     else:\n        translate_dict = dict((c, split) for c in filters)\n        translate_map = maketrans(translate_dict)\n        text = text.translate(translate_map)\n     seq = text.split(split)\n     return [i for i in seq if i]"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "keras_42", "problem": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,", "fixed": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "fastapi_12", "problem": " class HTTPBearer(HTTPBase):\n             else:\n                 return None\n         if scheme.lower() != \"bearer\":\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN,\n                detail=\"Invalid authentication credentials\",\n            )\n         return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)", "fixed": " class HTTPBearer(HTTPBase):\n             else:\n                 return None\n         if scheme.lower() != \"bearer\":\n            if self.auto_error:\n                raise HTTPException(\n                    status_code=HTTP_403_FORBIDDEN,\n                    detail=\"Invalid authentication credentials\",\n                )\n            else:\n                return None\n         return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq"}
{"id": "pandas_94", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        freq = self.freq if is_period_dtype(self) else None\n        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\n        return self._shallow_copy(result)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "pandas_33", "problem": " class IntegerArray(BaseMaskedArray):\n         ExtensionArray.argsort\n         data = self._data.copy()\n        data[self._mask] = data.min() - 1\n         return data\n     @classmethod", "fixed": " class IntegerArray(BaseMaskedArray):\n         ExtensionArray.argsort\n         data = self._data.copy()\n        if self._mask.any():\n            data[self._mask] = data.min() - 1\n         return data\n     @classmethod"}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "pandas_26", "problem": " class Categorical(ExtensionArray, PandasObject):\n         good = self._codes != -1\n         if not good.all():\n            if skipna:\n                 pointer = self._codes[good].max()\n             else:\n                 return np.nan", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         good = self._codes != -1\n         if not good.all():\n            if skipna and good.any():\n                 pointer = self._codes[good].max()\n             else:\n                 return np.nan"}
{"id": "tqdm_7", "problem": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')", "fixed": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')"}
{"id": "luigi_30", "problem": " class TaskProcess(AbstractTaskProcess):\n             self.task.trigger_event(Event.START, self.task)\n             t0 = time.time()\n             status = None\n            try:\n                new_deps = self._run_get_new_deps()\n                if new_deps is None:\n                    status = RUNNING\n                else:\n                    status = SUSPENDED\n                    logger.info(\n                        '[pid %s] Worker %s new requirements      %s',\n                        os.getpid(), self.worker_id, self.task.task_id)\n                    return\n            finally:\n                if status != SUSPENDED:\n                    self.task.trigger_event(\n                        Event.PROCESSING_TIME, self.task, time.time() - t0)\n                    error_message = json.dumps(self.task.on_success())\n                    logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                                self.worker_id, self.task.task_id)\n                    self.task.trigger_event(Event.SUCCESS, self.task)\n                    status = DONE\n         except KeyboardInterrupt:\n             raise", "fixed": " class TaskProcess(AbstractTaskProcess):\n             self.task.trigger_event(Event.START, self.task)\n             t0 = time.time()\n             status = None\n            new_deps = self._run_get_new_deps()\n            if new_deps is None:\n                status = DONE\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                error_message = json.dumps(self.task.on_success())\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task.task_id)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n            else:\n                status = SUSPENDED\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task.task_id)\n         except KeyboardInterrupt:\n             raise"}
{"id": "matplotlib_29", "problem": " class YAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposy\n     def set_default_intervals(self):\n         ymin, ymax = 0., 1.", "fixed": " class YAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposy\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         ymin, ymax = 0., 1."}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "pandas_69", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not ax.holds_integer():\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \"", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \""}
{"id": "pandas_41", "problem": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)", "fixed": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "scrapy_13", "problem": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 0\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'", "fixed": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 90\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or", "fixed": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('quotedstrval') is not None or\n            m.group('strval') is not None or"}
{"id": "tornado_8", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\",", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n        except ValueError:\n            self.handler.set_status(400)\n            log_msg = \"Missing/Invalid WebSocket headers\"\n            self.handler.finish(log_msg)\n            gen_log.debug(log_msg)\n            return\n        try:\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\","}
{"id": "youtube-dl_31", "problem": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = int_or_none(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?([0-9]+)\\s*s',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',", "fixed": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = parse_duration(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?class=\"bold\">(.*?)<',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',"}
{"id": "keras_42", "problem": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.", "fixed": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training."}
{"id": "youtube-dl_42", "problem": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_all_ampersand) \n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):", "fixed": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_ampersands)\n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "black_10", "problem": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char == ' ':\n                 current_column += 1\n            elif char == '\\t':\n                current_column += 4\n             elif char == '\\n':\n                 current_column = 0", "fixed": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char in ' \\t':\n                 current_column += 1\n             elif char == '\\n':\n                 current_column = 0"}
{"id": "keras_1", "problem": " def update(x, new_x):\n         The variable `x` updated.\n    return tf_state_ops.assign(x, new_x)\n @symbolic", "fixed": " def update(x, new_x):\n         The variable `x` updated.\n    op = tf_state_ops.assign(x, new_x)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "matplotlib_4", "problem": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,", "fixed": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,"}
{"id": "pandas_56", "problem": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._iget_item_cache(col)\n            return com.maybe_box_datetimelike(series._values[index])\n         series = self._get_item_cache(col)\n         engine = self.index._engine", "fixed": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n         series = self._get_item_cache(col)\n         engine = self.index._engine"}
{"id": "black_6", "problem": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n             result = drv.parse_string(src_txt, True)\n             break", "fixed": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n         elif (\n             is_datetime64_any_dtype(data)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n         elif (\n             is_datetime64_any_dtype(data)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n            if not isinstance(initial_state, list):\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, the state '\n                    'should be a list containing the states of the underlying RNNs. '\n                    'Found: ' + str(initial_state))\n             forward_state = initial_state[:len(initial_state) // 2]\n             backward_state = initial_state[len(initial_state) // 2:]\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)", "fixed": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n             forward_state = initial_state[:len(initial_state) // 2]\n             backward_state = initial_state[len(initial_state) // 2:]\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, endwith=[]):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:", "fixed": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:"}
{"id": "youtube-dl_32", "problem": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)\n def js_to_json(code):", "fixed": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(\n        r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?://[^\\n]*)*$', r'\\1', code)\n def js_to_json(code):"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n     return index", "fixed": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n    if copy:\n        index = index.copy()\n     return index"}
{"id": "pandas_20", "problem": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):", "fixed": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "youtube-dl_28", "problem": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        return compat_chr(int(numstr, base))\n     return ('&%s;' % entity)", "fixed": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass\n     return ('&%s;' % entity)"}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "pandas_105", "problem": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested", "fixed": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n    @skip_nested\n    def test_transpose(self, data):\n        super().test_transpose(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n         if auto is not None:\n             self._autoscaleYon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n        for ax in self._shared_y_axes.get_siblings(self):\n            ax._stale_viewlim_y = False\n         if auto is not None:\n             self._autoscaleYon = bool(auto)"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"name": "is_valid_parenthesization.py", "problem": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return True", "fixed": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return depth == 0\n", "hint": "Nested Parens\nInput:\n    parens: A string of parentheses", "input": ["((()()))()"], "output": "True"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):", "fixed": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        if six.PY2:\n            self.file_flags = 'b'\n            self._open_args = {}\n        else:\n            self.file_flags = ''\n            self._open_args = {'newline': '\\n'}\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "pandas_70", "problem": "                cls = dtype.construct_array_type()\n                result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)", "fixed": "                if len(result) and isinstance(result[0], dtype.type):\n                    cls = dtype.construct_array_type()\n                    result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "pandas_54", "problem": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, RangeIndex):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)", "fixed": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, (RangeIndex, CategoricalIndex)):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "scrapy_16", "problem": " def parse_url(url, encoding=None):", "fixed": " def parse_url(url, encoding=None):\n        Data are returned as a list of name, value pairs as bytes.\n        Arguments:\n        qs: percent-encoded query string to be parsed\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.  A\n            true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception."}
{"id": "keras_1", "problem": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        increment = np.random.randn(3, 4)\n        x += increment\n        K.eval(K.update_add(x_var, increment))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        decrement = np.random.randn(3, 4)\n        x -= decrement\n        K.eval(K.update_sub(x_var, decrement))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')", "fixed": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n    @pytest.mark.skipif(K.backend() == 'theano',\n                        reason='theano returns tuples for update ops')\n    def test_update(self):\n        x = np.ones((3, 4))\n        x_var = K.variable(x)\n        new_x = np.random.random((3, 4))\n        op = K.update(x_var, new_x)\n        K.eval(op)\n        assert_allclose(new_x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        increment = np.random.random((3, 4))\n        op = K.update_add(x_var, increment)\n        K.eval(op)\n        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        decrement = np.random.random((3, 4))\n        op = K.update_sub(x_var, decrement)\n        K.eval(op)\n        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')"}
{"id": "black_22", "problem": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36=py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")", "fixed": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")"}
{"name": "topological_ordering.py", "problem": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.outgoing_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "fixed": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.incoming_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "hint": "Topological Sort\nInput:\n    nodes: A list of directed graph nodes", "input": [], "output": ""}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"name": "detect_cycle.py", "problem": "def detect_cycle(node):\n    hare = tortoise = node\n    while True:\n        if hare.successor is None:\n            return False\n        tortoise = tortoise.successor\n        hare = hare.successor.successor\n        if hare is tortoise:\n            return True", "fixed": "def detect_cycle(node):\n    hare = tortoise = node\n    while True:\n        if hare is None or hare.successor is None:\n            return False\n        tortoise = tortoise.successor\n        hare = hare.successor.successor\n        if hare is tortoise:\n            return True\n", "hint": "Linked List Cycle Detection\ntortoise-hare\nImplements the tortoise-and-hare method of cycle detection.", "input": [], "output": ""}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
