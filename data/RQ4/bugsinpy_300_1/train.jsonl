{"id": "pandas_94", "problem": " from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\n from pandas.core import algorithms\n from pandas.core.accessor import PandasDelegate\nfrom pandas.core.arrays import ExtensionArray, ExtensionOpsMixin\n from pandas.core.arrays.datetimelike import (\n     DatetimeLikeArrayMixin,\n     _ensure_datetimelike_to_i8,", "fixed": " from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\n from pandas.core import algorithms\n from pandas.core.accessor import PandasDelegate\nfrom pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n    ExtensionOpsMixin,\n    TimedeltaArray,\n)\n from pandas.core.arrays.datetimelike import (\n     DatetimeLikeArrayMixin,\n     _ensure_datetimelike_to_i8,"}
{"id": "pandas_157", "problem": " from pandas.core.dtypes.common import (\n     is_bool,\n     is_bool_dtype,\n     is_categorical_dtype,\n    is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetimelike,\n     is_dtype_equal,", "fixed": " from pandas.core.dtypes.common import (\n     is_bool,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_datetime64tz_dtype,\n     is_datetimelike,\n     is_dtype_equal,"}
{"id": "keras_11", "problem": " from __future__ import print_function\n import warnings\n import numpy as np\n from .training_utils import iter_sequence_infinite\n from .. import backend as K\n from ..utils.data_utils import Sequence", "fixed": " from __future__ import print_function\n import warnings\n import numpy as np\nfrom .training_utils import is_sequence\n from .training_utils import iter_sequence_infinite\n from .. import backend as K\n from ..utils.data_utils import Sequence"}
{"id": "luigi_7", "problem": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n             if status == PENDING or status != task.status:", "fixed": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:\n             if status == PENDING or status != task.status:"}
{"id": "youtube-dl_31", "problem": " def parse_duration(s):\n     m = re.match(", "fixed": " def parse_duration(s):\n     m = re.match(\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)\n         yield current_line", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        yield current_line\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n    if current_line:\n         yield current_line"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "thefuck_21", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return (command.script.split()[1] == 'stash'\n            and 'usage:' in command.stderr)\n stash_commands = (", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    splited_script = command.script.split()\n    if len(splited_script) > 1:\n        return (splited_script[1] == 'stash'\n                and 'usage:' in command.stderr)\n    else:\n        return False\n stash_commands = ("}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         else:\n             return ((prefix + \": \") if prefix else '') + \\", "fixed": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         else:\n             return ((prefix + \": \") if prefix else '') + \\"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "thefuck_4", "problem": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip().split('\\n')\n    for alias in alias_out:\n        name, value = alias.replace('alias ', '', 1).split(' ', 1)\n         if name not in overridden:\n             aliases[name] = value\n     return aliases", "fixed": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip()\n    if not alias_out:\n        return aliases\n    for alias in alias_out.split('\\n'):\n        for separator in (' ', '='):\n            split_alias = alias.replace('alias ', '', 1).split(separator, 1)\n            if len(split_alias) == 2:\n                name, value = split_alias\n                break\n        else:\n            continue\n         if name not in overridden:\n             aliases[name] = value\n     return aliases"}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'", "fixed": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,\n                                                        use_marker=False)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops, use_marker=False)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops, use_marker=False)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta, use_marker=False)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'"}
{"id": "black_22", "problem": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n         if not (", "fixed": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n    @property\n    def contains_standalone_comments(self) -> bool:\n         if not ("}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):", "fixed": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):"}
{"id": "pandas_138", "problem": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)", "fixed": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)"}
{"id": "tornado_7", "problem": " class IOLoop(Configurable):\n                 from tornado.process import cpu_count\n                 self._executor = ThreadPoolExecutor(max_workers=(cpu_count() * 5))\n             executor = self._executor\n        return executor.submit(func, *args)\n     def set_default_executor(self, executor):", "fixed": " class IOLoop(Configurable):\n                 from tornado.process import cpu_count\n                 self._executor = ThreadPoolExecutor(max_workers=(cpu_count() * 5))\n             executor = self._executor\n        c_future = executor.submit(func, *args)\n        t_future = TracebackFuture()\n        self.add_future(c_future, lambda f: chain_future(f, t_future))\n        return t_future\n     def set_default_executor(self, executor):"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if is_sequence:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'", "fixed": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if use_sequence_api:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "ansible_14", "problem": " from ansible.module_utils.urls import open_url\n from ansible.utils.display import Display\n from ansible.utils.hashing import secure_hash_s\n display = Display()", "fixed": " from ansible.module_utils.urls import open_url\n from ansible.utils.display import Display\n from ansible.utils.hashing import secure_hash_s\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n display = Display()"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, PeriodIndex)\nnew_index = self.index.to_timestamp(freq=freq, how=how)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_timestamp\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_timestamp(freq=freq, how=how)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_timestamp\""}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds", "fixed": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "matplotlib_7", "problem": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if hasattr(intensity, 'mask'):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]", "fixed": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if np.ma.is_masked(intensity):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]"}
{"id": "tornado_5", "problem": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n         if self._next_timeout <= current_time:\n            callback_time_sec = self.callback_time / 1000.0\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec", "fixed": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n        callback_time_sec = self.callback_time / 1000.0\n         if self._next_timeout <= current_time:\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec\n        else:\n            self._next_timeout += callback_time_sec"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 16,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 16\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 8,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 8\n         },\n     },\n     {"}
{"id": "sanic_3", "problem": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:", "fixed": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = host or self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:"}
{"id": "tornado_16", "problem": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n         if self._finished:\n             self._return_result(self._finished.popleft())", "fixed": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n        self._running_future.add_done_callback(lambda f: self)\n         if self._finished:\n             self._return_result(self._finished.popleft())"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):", "fixed": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):"}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "pandas_104", "problem": " class GroupBy(_GroupBy):\n            order = np.roll(list(range(result.index.nlevels)), -1)\n            result = result.reorder_levels(order)\n            result = result.reindex(q, level=-1)\n            hi = len(q) * self.ngroups\n            arr = np.arange(0, hi, self.ngroups)\n            arrays = []\n            for i in range(self.ngroups):\n                arr2 = arr + i\n                arrays.append(arr2)\n            indices = np.concatenate(arrays)\n            assert len(indices) == len(result)\n             return result.take(indices)\n     @Substitution(name=\"groupby\")", "fixed": " class GroupBy(_GroupBy):\n            order = list(range(1, result.index.nlevels)) + [0]\n            index_names = np.array(result.index.names)\n            result.index.names = np.arange(len(index_names))\n            result = result.reorder_levels(order)\n            result.index.names = index_names[order]\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()\n             return result.take(indices)\n     @Substitution(name=\"groupby\")"}
{"id": "scrapy_3", "problem": " import logging\nfrom six.moves.urllib.parse import urljoin\n from w3lib.url import safe_url_string", "fixed": " import logging\nfrom six.moves.urllib.parse import urljoin, urlparse\n from w3lib.url import safe_url_string"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        return Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n     def _apply_to_column_groupbys(self, func):", "fixed": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        result = Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n        return self._reindex_output(result, fill_value=0)\n     def _apply_to_column_groupbys(self, func):"}
{"id": "pandas_79", "problem": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,", "fixed": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,"}
{"id": "pandas_84", "problem": " import pytest\n import pandas.util._test_decorators as td\n import pandas as pd\nfrom pandas import DataFrame, MultiIndex, Series\n import pandas._testing as tm", "fixed": " import pytest\n import pandas.util._test_decorators as td\n import pandas as pd\nfrom pandas import DataFrame, Series\n import pandas._testing as tm"}
{"id": "tqdm_9", "problem": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable)\n     def __iter__(self):", "fixed": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable) if self.iterable else self.total\n     def __iter__(self):"}
{"id": "pandas_108", "problem": " from .common import (\n     is_unsigned_integer_dtype,\n     pandas_dtype,\n )\nfrom .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype\n from .generic import (\n     ABCDataFrame,\n     ABCDatetimeArray,", "fixed": " from .common import (\n     is_unsigned_integer_dtype,\n     pandas_dtype,\n )\nfrom .dtypes import DatetimeTZDtype, ExtensionDtype, IntervalDtype, PeriodDtype\n from .generic import (\n     ABCDataFrame,\n     ABCDatetimeArray,"}
{"id": "pandas_68", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n    ABCExtensionArray,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,"}
{"id": "pandas_27", "problem": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\nimport pandas._libs.tslibs.frequencies as libfrequencies\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import ("}
{"id": "cookiecutter_4", "problem": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return EXIT_SUCCESS\n    return run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return\n    run_script_with_context(script, project_dir, context)"}
{"id": "thefuck_13", "problem": " from thefuck.utils import eager\n @git_support\n def match(command):\n    return ('branch' in command.script\n            and \"fatal: A branch named '\" in command.stderr\n             and \" already exists.\" in command.stderr)", "fixed": " from thefuck.utils import eager\n @git_support\n def match(command):\n    return (\"fatal: A branch named '\" in command.stderr\n             and \" already exists.\" in command.stderr)"}
{"id": "black_6", "problem": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the", "fixed": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_32", "problem": " class NPOIE(InfoExtractor):\n'http:\n             video_id,\ntransform_source=lambda j: re.sub(r'parseMetadata\\((.*?)\\);\\n\n         )\n         token_page = self._download_webpage(\n'http:", "fixed": " class NPOIE(InfoExtractor):\n'http:\n             video_id,\n            transform_source=strip_jsonp,\n         )\n         token_page = self._download_webpage(\n'http:"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is", "fixed": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is"}
{"id": "fastapi_1", "problem": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)", "fixed": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)"}
{"id": "black_15", "problem": " class Line:\n         return bool(self.leaves or self.comments)\nclass UnformattedLines(Line):\n        The `preformatted` argument is ignored.\n        Keeps track of indentation `depth`, which is useful when the user\n        says `\n        `depth` is not used for indentation in this case.\n        raise NotImplementedError(\"Unformatted lines don't store comments separately.\")\n    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class Line:\n         return bool(self.leaves or self.comments)\n @dataclass\n class EmptyLineTracker:"}
{"id": "keras_11", "problem": " def iter_sequence_infinite(seq):\n     while True:\n         for item in seq:\n             yield item", "fixed": " def iter_sequence_infinite(seq):\n     while True:\n         for item in seq:\n             yield item\ndef is_sequence(seq):\n    return (getattr(seq, 'use_sequence_api', False)\n            or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))"}
{"id": "thefuck_16", "problem": " from .generic import Generic\n class Zsh(Generic):\n     def app_alias(self, alias_name):\n        alias = \"alias {0}='TF_ALIAS={0}\" \\\n                 \" PYTHONIOENCODING=utf-8\" \\\n                ' TF_SHELL_ALIASES=$(alias)' \\\n                \" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&\" \\\n                 \" eval $TF_CMD\".format(alias_name)\n         if settings.alter_history:", "fixed": " from .generic import Generic\n class Zsh(Generic):\n     def app_alias(self, alias_name):\n        alias = \"alias {0}='TF_CMD=$(TF_ALIAS={0}\" \\\n                 \" PYTHONIOENCODING=utf-8\" \\\n                \" TF_SHELL_ALIASES=$(alias)\" \\\n                \" thefuck $(fc -ln -1 | tail -n 1)) &&\" \\\n                 \" eval $TF_CMD\".format(alias_name)\n         if settings.alter_history:"}
{"id": "thefuck_19", "problem": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force')\n enabled_by_default = False", "fixed": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force-with-lease')\n enabled_by_default = False"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    include_none: bool = True,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:", "fixed": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "fastapi_7", "problem": " from fastapi.exceptions import RequestValidationError\n from starlette.exceptions import HTTPException\n from starlette.requests import Request", "fixed": "from fastapi.encoders import jsonable_encoder\n from fastapi.exceptions import RequestValidationError\n from starlette.exceptions import HTTPException\n from starlette.requests import Request"}
{"id": "cookiecutter_4", "problem": " from binaryornot.check import is_binary\n from .exceptions import (\n     NonTemplatedInputDirException,\n     ContextDecodingException,\n     OutputDirExistsException\n )\n from .find import find_template\n from .utils import make_sure_path_exists, work_in\nfrom .hooks import run_hook, EXIT_SUCCESS\n def copy_without_render(path, context):", "fixed": " from binaryornot.check import is_binary\n from .exceptions import (\n     NonTemplatedInputDirException,\n     ContextDecodingException,\n    FailedHookException,\n     OutputDirExistsException\n )\n from .find import find_template\n from .utils import make_sure_path_exists, work_in\nfrom .hooks import run_hook\n def copy_without_render(path, context):"}
{"id": "pandas_6", "problem": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):", "fixed": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError, ValueError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size", "fixed": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "black_22", "problem": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for comment in self.comments.values():\n             res += str(comment)\n         return res + '\\n'", "fixed": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for _, comment in self.comments:\n             res += str(comment)\n         return res + '\\n'"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n     obj=\"Series\",\n ):", "fixed": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n    check_category_order=True,\n     obj=\"Series\",\n ):"}
{"id": "ansible_8", "problem": " import re\n import shlex\n import pkgutil\n import xml.etree.ElementTree as ET\n from ansible.errors import AnsibleError\n from ansible.module_utils._text import to_bytes, to_text", "fixed": " import re\n import shlex\n import pkgutil\n import xml.etree.ElementTree as ET\nimport ntpath\n from ansible.errors import AnsibleError\n from ansible.module_utils._text import to_bytes, to_text"}
{"id": "pandas_58", "problem": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_29", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        if needs_float_conversion:\n            left = left.astype(\"float\")\n        left.values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        if needs_float_conversion:\n            right = right.astype(\"float\")\n        right.values[key] = value_right\n         self._right = right\n     def __eq__(self, other):", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n        if needs_float_conversion:\n            raise ValueError(\"Cannot set float NaN to integer-backed IntervalArray\")\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        left._values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        right._values[key] = value_right\n         self._right = right\n     def __eq__(self, other):"}
{"id": "pandas_106", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "youtube-dl_31", "problem": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = int_or_none(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?([0-9]+)\\s*s',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',", "fixed": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = parse_duration(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?class=\"bold\">(.*?)<',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        column_margin = table.loc[:, margins_name].drop(margins_name)\n        index_margin = table.loc[margins_name, :].drop(margins_name)\n        table = table.drop(margins_name, axis=1).drop(margins_name)\n        table_index_names = table.index.names\n        table_columns_names = table.columns.names\n         table = _normalize(table, normalize=normalize, margins=False)", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        table_index = table.index\n        table_columns = table.columns\n        if (margins_name not in table.iloc[-1, :].name) | (\n            margins_name != table.iloc[:, -1].name\n        ):\n            raise ValueError(\"{} not in pivoted DataFrame\".format(margins_name))\n        column_margin = table.iloc[:-1, -1]\n        index_margin = table.iloc[-1, :-1]\n        table = table.iloc[:-1, :-1]\n         table = _normalize(table, normalize=normalize, margins=False)"}
{"id": "pandas_70", "problem": "                cls = dtype.construct_array_type()\n                result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)", "fixed": "                if len(result) and isinstance(result[0], dtype.type):\n                    cls = dtype.construct_array_type()\n                    result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)"}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "pandas_34", "problem": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=\"infer\",\n             nonexistent=\"shift_forward\",\n         )", "fixed": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=True,\n             nonexistent=\"shift_forward\",\n         )"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "scrapy_38", "problem": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:", "fixed": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:"}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "keras_20", "problem": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support():\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'", "fixed": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "scrapy_19", "problem": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n     def has_header(self, name):\n         return name in self.request.headers", "fixed": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n    @property\n    def full_url(self):\n        return self.get_full_url()\n    @property\n    def host(self):\n        return self.get_host()\n    @property\n    def type(self):\n        return self.get_type()\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n     def has_header(self, name):\n         return name in self.request.headers"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "pandas_112", "problem": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)", "fixed": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical(target_as_index):\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)"}
{"id": "tornado_1", "problem": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__(", "fixed": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__("}
{"id": "pandas_131", "problem": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n            result = take_1d(result, self.orig.cat.codes)\n             index = self.orig.index\n         else:\n             index = self._parent.index", "fixed": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n             index = self.orig.index\n         else:\n             index = self._parent.index"}
{"id": "pandas_70", "problem": " def test_resample_integerarray():\n     result = ts.resample(\"3T\").mean()\n     expected = Series(\n        [1, 4, 7], index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"), dtype=\"Int64\"\n     )\n     tm.assert_series_equal(result, expected)", "fixed": " def test_resample_integerarray():\n     result = ts.resample(\"3T\").mean()\n     expected = Series(\n        [1, 4, 7],\n        index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"),\n        dtype=\"float64\",\n     )\n     tm.assert_series_equal(result, expected)"}
{"id": "thefuck_14", "problem": " from .generic import Generic\n class Fish(Generic):\n     def _get_overridden_aliases(self):\n        overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()\n        if overridden_aliases:\n            return [alias.strip() for alias in overridden_aliases.split(',')]\n        else:\n            return ['cd', 'grep', 'ls', 'man', 'open']\n     def app_alias(self, fuck):", "fixed": " from .generic import Generic\n class Fish(Generic):\n     def _get_overridden_aliases(self):\n        default = {'cd', 'grep', 'ls', 'man', 'open'}\n        for alias in os.environ.get('TF_OVERRIDDEN_ALIASES', '').split(','):\n            default.add(alias.strip())\n        return default\n     def app_alias(self, fuck):"}
{"id": "youtube-dl_29", "problem": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "ansible_4", "problem": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)\n     def _load_collections(self, attr, ds):", "fixed": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n     def _load_collections(self, attr, ds):"}
{"id": "pandas_31", "problem": " from pandas.util._decorators import Appender, Substitution, cache_readonly, doc\n from pandas.core.dtypes.cast import maybe_cast_result\n from pandas.core.dtypes.common import (\n     ensure_float,\n     is_datetime64_dtype,\n     is_integer_dtype,\n     is_numeric_dtype,\n     is_object_dtype,", "fixed": " from pandas.util._decorators import Appender, Substitution, cache_readonly, doc\n from pandas.core.dtypes.cast import maybe_cast_result\n from pandas.core.dtypes.common import (\n     ensure_float,\n    is_bool_dtype,\n     is_datetime64_dtype,\n    is_extension_array_dtype,\n     is_integer_dtype,\n     is_numeric_dtype,\n     is_object_dtype,"}
{"id": "youtube-dl_28", "problem": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        return compat_chr(int(numstr, base))\n     return ('&%s;' % entity)", "fixed": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass\n     return ('&%s;' % entity)"}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "tqdm_1", "problem": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):", "fixed": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(description_original, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "scrapy_12", "problem": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n         if _root is not None:", "fixed": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n         st = _st(response, type or self._default_type)\n         if _root is not None:"}
{"id": "PySnooper_1", "problem": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')", "fixed": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\nif pycompat.PY2:\n    from io import open\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')"}
{"id": "pandas_136", "problem": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_int64_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_integer_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:"}
{"id": "black_15", "problem": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: str) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        list(v.visit(lib2to3_parse(code)))\n KEYWORDS = set(keyword.kwlist)", "fixed": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: Union[str, Leaf, Node]) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))\n KEYWORDS = set(keyword.kwlist)"}
{"id": "black_22", "problem": " import asyncio\n from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\nfrom functools import partial\n import keyword\n import os\n from pathlib import Path\n import tokenize\n import sys\n from typing import (\n    Dict, Generic, Iterable, Iterator, List, Optional, Set, Tuple, Type, TypeVar, Union\n )\n from attr import dataclass, Factory", "fixed": " import asyncio\n from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\nfrom functools import partial, wraps\n import keyword\n import os\n from pathlib import Path\n import tokenize\n import sys\n from typing import (\n    Callable,\n    Dict,\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n )\n from attr import dataclass, Factory"}
{"id": "pandas_67", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, algos as libalgos, lib, tslib, writers\n from pandas._libs.index import convert_scalar\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n from pandas._libs.index import convert_scalar\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "thefuck_2", "problem": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(':')\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]", "fixed": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(os.pathsep)\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]"}
{"id": "luigi_25", "problem": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path()\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):", "fixed": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))", "fixed": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))"}
{"id": "pandas_7", "problem": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where(", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        left_distances = np.abs(self[left_indexer] - target)\n        right_distances = np.abs(self[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where("}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        include_none=include_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )", "fixed": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n            if not isinstance(initial_state, list):\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, the state '\n                    'should be a list containing the states of the underlying RNNs. '\n                    'Found: ' + str(initial_state))\nforward_state = initial_state[:len(initial_state)\nbackward_state = initial_state[len(initial_state)\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)", "fixed": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\nforward_state = initial_state[:len(initial_state)\nbackward_state = initial_state[len(initial_state)\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)"}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "pandas_70", "problem": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})", "fixed": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})"}
{"id": "httpie_2", "problem": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)", "fixed": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n    requests_session.max_redirects = args.max_redirects\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)"}
{"id": "youtube-dl_40", "problem": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''", "fixed": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return struct_unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return struct_unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return struct_unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''"}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "youtube-dl_5", "problem": " def unified_timestamp(date_str, day_first=True):\n     for expression in date_formats(day_first):\n         try:\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta\n             return calendar.timegm(dt.timetuple())\n         except ValueError:\n             pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n        return calendar.timegm(timetuple.timetuple())\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_timestamp(date_str, day_first=True):\n     for expression in date_formats(day_first):\n         try:\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n         except ValueError:\n             pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "pandas_99", "problem": " def _convert_listlike_datetimes(\n     elif unit is not None:\n         if format is not None:\n             raise ValueError(\"cannot specify both format and unit\")\n        arg = getattr(arg, \"values\", arg)\n        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n         if errors == \"ignore\":\n             from pandas import Index", "fixed": " def _convert_listlike_datetimes(\n     elif unit is not None:\n         if format is not None:\n             raise ValueError(\"cannot specify both format and unit\")\n        arg = getattr(arg, \"_values\", arg)\n        if isinstance(arg, IntegerArray):\n            mask = arg.isna()\n            arg = arg._ndarray_values\n        else:\n            mask = None\n        result, tz_parsed = tslib.array_with_unit_to_datetime(\n            arg, mask, unit, errors=errors\n        )\n         if errors == \"ignore\":\n             from pandas import Index"}
{"id": "youtube-dl_18", "problem": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()", "fixed": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "ansible_10", "problem": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type", "fixed": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n        self.prev = None\n        self.next = None\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type"}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "luigi_24", "problem": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '\"{0}={1}\"'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):", "fixed": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return result\n     @classmethod\n     def _add_numeric_operations(cls):", "fixed": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return self._reindex_output(result, fill_value=0)\n     @classmethod\n     def _add_numeric_operations(cls):"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "black_23", "problem": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)", "fixed": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\npython_grammar_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_exec_statement.keywords[\"exec\"]\npython_grammar_no_print_statement_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"print\"]\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"exec\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "youtube-dl_16", "problem": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))\n     out = []\nparas = dfxp.findall(_x('.", "fixed": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data)\n     out = []\nparas = dfxp.findall(_x('."}
{"id": "pandas_136", "problem": " from pandas.core.dtypes.common import (\n     is_dtype_equal,\n     is_extension_array_dtype,\n     is_float_dtype,\n    is_int64_dtype,\n     is_integer,\n     is_integer_dtype,\n     is_list_like,", "fixed": " from pandas.core.dtypes.common import (\n     is_dtype_equal,\n     is_extension_array_dtype,\n     is_float_dtype,\n     is_integer,\n     is_integer_dtype,\n     is_list_like,"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "keras_19", "problem": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN.", "fixed": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state).\n            - a `output_size` attribute. This can be a single integer or a\n                TensorShape, which represent the shape of the output. For\n                backward compatible reason, if this attribute is not available\n                for the cell, the value will be inferred by the first element\n                of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN."}
{"id": "black_21", "problem": " def dump_to_file(*output: str) -> str:\n     import tempfile\n     with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False\n     ) as f:\n         for lines in output:\n             f.write(lines)", "fixed": " def dump_to_file(*output: str) -> str:\n     import tempfile\n     with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False, encoding=\"utf8\"\n     ) as f:\n         for lines in output:\n             f.write(lines)"}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "keras_1", "problem": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)", "fixed": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n            self.seed += 1\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)"}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if not self.get_visible():\n             return Bbox.unit()\n        if dpi is not None:\n            dpi_orig = self.figure.dpi\n            self.figure.dpi = dpi\n         if self.get_text() == '':\n            tx, ty = self._get_xy_display()\n            return Bbox.from_bounds(tx, ty, 0, 0)\n         if renderer is not None:\n             self._renderer = renderer", "fixed": " class Text(Artist):\n         if not self.get_visible():\n             return Bbox.unit()\n        if dpi is None:\n            dpi = self.figure.dpi\n         if self.get_text() == '':\n            with cbook._setattr_cm(self.figure, dpi=dpi):\n                tx, ty = self._get_xy_display()\n                return Bbox.from_bounds(tx, ty, 0, 0)\n         if renderer is not None:\n             self._renderer = renderer"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n         else:\n             left, right = other, self", "fixed": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n        elif sort is False:\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n         else:\n             left, right = other, self"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n     return index", "fixed": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n    if copy:\n        index = index.copy()\n     return index"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "scrapy_33", "problem": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n         return failure\n     dont_log = named.pop('dont_log', None)", "fixed": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n         return failure\n     dont_log = named.pop('dont_log', None)"}
{"id": "tornado_1", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "luigi_14", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_37", "problem": " import calendar\n import contextlib\n import ctypes\n import datetime", "fixed": " import calendar\nimport codecs\n import contextlib\n import ctypes\n import datetime"}
{"id": "black_22", "problem": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):", "fixed": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n     def __init__(self,", "fixed": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n        restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n     def __init__(self,"}
{"id": "pandas_2", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n         if is_setter:\n             return list(key)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)\n         if is_setter:\n             return list(key)"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:https?:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "pandas_153", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare"}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "thefuck_20", "problem": " import os\n import zipfile\n from thefuck.utils import for_app\n def _is_bad_zip(file):", "fixed": " import os\n import zipfile\n from thefuck.utils import for_app\nfrom thefuck.shells import quote\n def _is_bad_zip(file):"}
{"id": "thefuck_17", "problem": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)", "fixed": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n                self.wait += 1\n     def in_cooldown(self):\n         return self.cooldown_counter > 0", "fixed": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n     def in_cooldown(self):\n         return self.cooldown_counter > 0"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "thefuck_8", "problem": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)", "fixed": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)"}
{"id": "pandas_90", "problem": " import os\n from shutil import rmtree\n import string\n import tempfile\nfrom typing import List, Optional, Union, cast\n import warnings\n import zipfile", "fixed": " import os\n from shutil import rmtree\n import string\n import tempfile\nfrom typing import Any, List, Optional, Union, cast\n import warnings\n import zipfile"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:"}
{"id": "pandas_44", "problem": " from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\n from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\n from pandas.core.base import IndexOpsMixin\n import pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs\n from pandas.core.indexes.extension import (\n     ExtensionIndex,\n     inherit_names,", "fixed": " from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\n from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\n from pandas.core.base import IndexOpsMixin\n import pandas.core.indexes.base as ibase\nfrom pandas.core.indexes.base import Index, _index_shared_docs, ensure_index\n from pandas.core.indexes.extension import (\n     ExtensionIndex,\n     inherit_names,"}
{"id": "luigi_11", "problem": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items())):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)", "fixed": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":", "fixed": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role related operations.\",\n         )", "fixed": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n         )"}
{"id": "pandas_24", "problem": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:", "fixed": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:"}
{"id": "pandas_18", "problem": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            self.min_periods or 1,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,", "fixed": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(self.window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            window_indexer.window_size,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,"}
{"id": "pandas_41", "problem": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")", "fixed": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n    def should_store(self, value):\n        return is_dtype_equal(self.dtype, value.dtype)\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "keras_19", "problem": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n         self._threads = []\n         self._stop_event = None", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._manager:\n            self._manager.shutdown()\n         self._threads = []\n         self._stop_event = None"}
{"id": "thefuck_25", "problem": " def match(command, settings):\n @sudo_support\n def get_new_command(command, settings):\n    return re.sub('^mkdir (.*)', 'mkdir -p \\\\1', command.script)", "fixed": " def match(command, settings):\n @sudo_support\n def get_new_command(command, settings):\n    return re.sub('\\\\bmkdir (.*)', 'mkdir -p \\\\1', command.script)"}
{"id": "pandas_44", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\n from pandas.core.arrays.period import (\n     PeriodArray,", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\nfrom pandas.core.dtypes.dtypes import PeriodDtype\n from pandas.core.arrays.period import (\n     PeriodArray,"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n    _responseMatcher = re.compile('HTTP/1\\.. 200')\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n    _responseMatcher = re.compile(b'HTTP/1\\.. 200')\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "thefuck_18", "problem": " patterns = ['permission denied',\n def match(command):\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():", "fixed": " patterns = ['permission denied',\n def match(command):\n    if command.script_parts and command.script_parts[0] == 'sudo':\n        return False\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():"}
{"id": "keras_30", "problem": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if x is None or len(x) == 0:\n                    batch_size = 1\n                elif isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]"}
{"id": "scrapy_16", "problem": " def parse_url(url, encoding=None):", "fixed": " def parse_url(url, encoding=None):\n        Data are returned as a list of name, value pairs as bytes.\n        Arguments:\n        qs: percent-encoded query string to be parsed\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.  A\n            true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception."}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "spacy_7", "problem": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()", "fixed": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()"}
{"id": "cookiecutter_4", "problem": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return", "fixed": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        try:\n            run_hook('pre_gen_project', project_dir, context)\n        except FailedHookException:\n            shutil.rmtree(project_dir, ignore_errors=True)\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return"}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             ser.at[0]\n        with pytest.raises(TypeError, match=msg):\n             ser.loc[0]\n    def test_frame_raises_type_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]", "fixed": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.at[0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.loc[0]\n    def test_frame_raises_key_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]"}
{"id": "cookiecutter_2", "problem": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n    return None\n def run_script(script_path, cwd='.'):", "fixed": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n    scripts = []\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\n    if len(scripts) == 0:\n        return None\n    return scripts\n def run_script(script_path, cwd='.'):"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")", "fixed": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n    is_bool = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")"}
{"id": "pandas_9", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d"}
{"id": "black_22", "problem": " class UnformattedLines(Line):\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class UnformattedLines(Line):\n @dataclass\n class EmptyLineTracker:"}
{"id": "PySnooper_1", "problem": " import inspect\n import sys\n PY3 = (sys.version_info[0] == 3)\n if hasattr(abc, 'ABC'):\n     ABC = abc.ABC", "fixed": " import inspect\n import sys\n PY3 = (sys.version_info[0] == 3)\nPY2 = not PY3\n if hasattr(abc, 'ABC'):\n     ABC = abc.ABC"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )", "fixed": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters,\n                                                        filter_dilation=dilation_rate)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)"}
{"id": "youtube-dl_32", "problem": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)\n def js_to_json(code):", "fixed": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(\nr'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?:\n def js_to_json(code):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "tornado_14", "problem": " class IOLoop(Configurable):\n             if IOLoop.current(instance=False) is None:\n                 self.make_current()\n         elif make_current:\n            if IOLoop.current(instance=False) is None:\n                 raise RuntimeError(\"current IOLoop already exists\")\n             self.make_current()", "fixed": " class IOLoop(Configurable):\n             if IOLoop.current(instance=False) is None:\n                 self.make_current()\n         elif make_current:\n            if IOLoop.current(instance=False) is not None:\n                 raise RuntimeError(\"current IOLoop already exists\")\n             self.make_current()"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_21", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        if isinstance(key, list):\n            return self.loc[key]\n        return self.reindex(key)\n     def _get_values_tuple(self, key):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        return self.loc[key]\n     def _get_values_tuple(self, key):"}
{"id": "thefuck_23", "problem": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with shelve.open(cache_path) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:", "fixed": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with closing(shelve.open(cache_path)) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values.T, other.values).T\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":", "fixed": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n            other_vals = other.values.reshape(-1, 1)\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values, other_vals)\n            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "black_11", "problem": " def is_import(leaf: Leaf) -> bool:\n     )\n def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:", "fixed": " def is_import(leaf: Leaf) -> bool:\n     )\ndef is_special_comment(leaf: Leaf) -> bool:\n    t = leaf.type\n    v = leaf.value\n    return bool(\n        (t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith(\"\n    )\n def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)\n     ):\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes", "fixed": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\n     ):\n        assert is_categorical(lk) and is_categorical(rk)\n        lk = cast(Categorical, lk)\n        rk = cast(Categorical, rk)\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "pandas_113", "problem": " def any_int_dtype(request):\n     return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):", "fixed": " def any_int_dtype(request):\n     return request.param\n@pytest.fixture(params=ALL_EA_INT_DTYPES)\ndef any_nullable_int_dtype(request):\n    return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "keras_29", "problem": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:", "fixed": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:"}
{"id": "matplotlib_22", "problem": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')", "fixed": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n        if not cbook.is_scalar_or_string(bins):\n            bins = self.convert_xunits(bins)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')"}
{"id": "keras_1", "problem": " def get_variable_shape(x):\n     return int_shape(x)\n def print_tensor(x, message=''):", "fixed": " def get_variable_shape(x):\n     return int_shape(x)\n@symbolic\n def print_tensor(x, message=''):"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "pandas_137", "problem": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort", "fixed": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import array, extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n         Returns\n         -------\n        axes: topmost axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy)]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:", "fixed": " class FigureCanvasBase:\n         Returns\n         -------\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy) and a.get_visible()]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:"}
{"id": "pandas_72", "problem": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n         ):\n             values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:", "fixed": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n         ):\n             values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n        elif exact_match:\n            values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "pandas_161", "problem": " class Categorical(ExtensionArray, PandasObject):\n                     raise ValueError(\"fill value must be in categories\")\n                 values_codes = _get_codes_for_values(value, self.categories)\n                indexer = np.where(values_codes != -1)\n                codes[indexer] = values_codes[values_codes != -1]\n             elif is_hashable(value):", "fixed": " class Categorical(ExtensionArray, PandasObject):\n                     raise ValueError(\"fill value must be in categories\")\n                 values_codes = _get_codes_for_values(value, self.categories)\n                indexer = np.where(codes == -1)\n                codes[indexer] = values_codes[indexer]\n             elif is_hashable(value):"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "black_15", "problem": " from typing import (\n     Sequence,\n     Set,\n     Tuple,\n    Type,\n     TypeVar,\n     Union,\n     cast,", "fixed": " from typing import (\n     Sequence,\n     Set,\n     Tuple,\n     TypeVar,\n     Union,\n     cast,"}
{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]", "fixed": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"C\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]"}
{"id": "thefuck_5", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n     return ('push' in command.script_parts\n            and 'set-upstream' in command.output)\n def _get_upstream_option_index(command_parts):", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n     return ('push' in command.script_parts\n            and 'git push --set-upstream' in command.output)\n def _get_upstream_option_index(command_parts):"}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "luigi_16", "problem": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            if task.id not in necessary_tasks and self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)", "fixed": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            removed = self._state.prune(task, self._config)\n            if removed and task.id not in necessary_tasks:\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)"}
{"id": "pandas_44", "problem": " import numpy as np\n from pandas._libs import NaT, Period, Timestamp, index as libindex, lib, tslib as libts\n from pandas._libs.tslibs import fields, parsing, timezones\nfrom pandas._typing import Label\n from pandas.util._decorators import cache_readonly\nfrom pandas.core.dtypes.common import _NS_DTYPE, is_float, is_integer, is_scalar\n from pandas.core.dtypes.missing import is_valid_nat_for_dtype\n from pandas.core.arrays.datetimes import DatetimeArray, tz_to_dtype", "fixed": " import numpy as np\n from pandas._libs import NaT, Period, Timestamp, index as libindex, lib, tslib as libts\n from pandas._libs.tslibs import fields, parsing, timezones\nfrom pandas._typing import DtypeObj, Label\n from pandas.util._decorators import cache_readonly\nfrom pandas.core.dtypes.common import (\n    _NS_DTYPE,\n    is_datetime64_any_dtype,\n    is_datetime64_dtype,\n    is_datetime64tz_dtype,\n    is_float,\n    is_integer,\n    is_scalar,\n)\n from pandas.core.dtypes.missing import is_valid_nat_for_dtype\n from pandas.core.arrays.datetimes import DatetimeArray, tz_to_dtype"}
{"id": "matplotlib_1", "problem": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\n _log = logging.getLogger(__name__)", "fixed": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\nfrom matplotlib.cbook import _setattr_cm\n _log = logging.getLogger(__name__)"}
{"id": "pandas_114", "problem": " import pandas.core.algorithms as algos\n from pandas.core.arrays import ExtensionArray\n from pandas.core.base import IndexOpsMixin, PandasObject\n import pandas.core.common as com\n from pandas.core.indexers import maybe_convert_indices\n from pandas.core.indexes.frozen import FrozenList\n import pandas.core.missing as missing", "fixed": " import pandas.core.algorithms as algos\n from pandas.core.arrays import ExtensionArray\n from pandas.core.base import IndexOpsMixin, PandasObject\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array\n from pandas.core.indexers import maybe_convert_indices\n from pandas.core.indexes.frozen import FrozenList\n import pandas.core.missing as missing"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "thefuck_8", "problem": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read()\n     return _parse_operations(lines)", "fixed": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read().decode(\"utf-8\")\n     return _parse_operations(lines)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_41", "problem": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)", "fixed": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)"}
{"id": "black_8", "problem": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                if leaves[-1].type != token.COMMA:\n                    leaves.append(Leaf(token.COMMA, \",\"))\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)", "fixed": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                for i in range(len(leaves) - 1, -1, -1):\n                    if leaves[i].type == STANDALONE_COMMENT:\n                        continue\n                    elif leaves[i].type == token.COMMA:\n                        break\n                    else:\n                        leaves.insert(i + 1, Leaf(token.COMMA, \",\"))\n                        break\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)"}
{"id": "thefuck_24", "problem": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\nCorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))", "fixed": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))\nclass CorrectedCommand(object):\n    def __init__(self, script, side_effect, priority):\n        self.script = script\n        self.side_effect = side_effect\n        self.priority = priority\n    def __eq__(self, other):"}
{"id": "keras_2", "problem": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)", "fixed": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\ndef in_top_k(predictions, targets, k):\n    top_k = np.argsort(-predictions)[:, :k]\n    targets = targets.reshape(-1, 1)\n    return np.any(targets == top_k, axis=-1)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)"}
{"id": "tqdm_7", "problem": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')", "fixed": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\ninfo = self._download_xml('http:\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\ninfo = self._download_xml('http:\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"id": "thefuck_30", "problem": " def _search(stderr):\n def match(command, settings):\n    return 'EDITOR' in os.environ and _search(command.stderr)\n def get_new_command(command, settings):", "fixed": " def _search(stderr):\n def match(command, settings):\n    if 'EDITOR' not in os.environ:\n        return False\n    m = _search(command.stderr)\n    return m and os.path.isfile(m.group('file'))\n def get_new_command(command, settings):"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "black_18", "problem": " def format_str(\n     return dst_contents\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,", "fixed": " def format_str(\n     return dst_contents\ndef prepare_input(src: bytes) -> Tuple[str, str, str]:\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,"}
{"id": "pandas_44", "problem": " from pandas._libs.lib import no_default\n from pandas._libs.tslibs import frequencies as libfrequencies, resolution\n from pandas._libs.tslibs.parsing import parse_time_string\n from pandas._libs.tslibs.period import Period\nfrom pandas._typing import Label\n from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.lib import no_default\n from pandas._libs.tslibs import frequencies as libfrequencies, resolution\n from pandas._libs.tslibs.parsing import parse_time_string\n from pandas._libs.tslibs.period import Period\nfrom pandas._typing import DtypeObj, Label\n from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import ("}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "pandas_125", "problem": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result", "fixed": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        filter=None,\n        regex: bool = False,\n        convert: bool = True,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = self if inplace else self.copy()\n        if filter is None:\n            result.values.replace(to_replace, value, inplace=True)\n            if convert:\n                return result.convert(numeric=False, copy=not inplace)\n            else:\n                return result\n        else:\n            if not isna(value):\n                result.values.add_categories(value, inplace=True)\n            return super(CategoricalBlock, result).replace(\n                to_replace, value, inplace, filter, regex, convert\n            )"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n     mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort)\n         for n in range(len(left_keys))\n     )\n     zipped = zip(*mapped)", "fixed": " def _get_join_indexers(\n     mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)\n         for n in range(len(left_keys))\n     )\n     zipped = zip(*mapped)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "pandas_41", "problem": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values", "fixed": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()", "fixed": " def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n                if hidden_value.endswith(\"\\n\"):\n                    hidden_value = hidden_value[:-1]\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()"}
{"id": "youtube-dl_40", "problem": " import base64\n import io\n import itertools\n import os\nfrom struct import unpack, pack\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,", "fixed": " import base64\n import io\n import itertools\n import os\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n    struct_pack,\n    struct_unpack,\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,"}
{"id": "keras_25", "problem": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1.", "fixed": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n    x = x.astype(K.floatx())\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1."}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"id": "pandas_43", "problem": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n     if not isinstance(right, ABCDataFrame):\n         return False", "fixed": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n    if op is operator.pow or op is rpow:\n        return False\n     if not isinstance(right, ABCDataFrame):\n         return False"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\nif \"\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\nif \"\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "tornado_12", "problem": " import hmac\n import time\n import uuid\nfrom tornado.concurrent import TracebackFuture, return_future\n from tornado import gen\n from tornado import httpclient\n from tornado import escape", "fixed": " import hmac\n import time\n import uuid\nfrom tornado.concurrent import TracebackFuture, return_future, chain_future\n from tornado import gen\n from tornado import httpclient\n from tornado import escape"}
{"id": "pandas_80", "problem": " def make_data():\n @pytest.fixture\n def dtype():\n    return pd.BooleanDtype()\n @pytest.fixture", "fixed": " def make_data():\n @pytest.fixture\n def dtype():\n    return BooleanDtype()\n @pytest.fixture"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n            extra={'spider': spider, 'failure': _failure}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,", "fixed": " class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,"}
{"id": "ansible_7", "problem": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n    for key in to_remove:\n        commands.append(\"no {0}\".format(key))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands", "fixed": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n    for key in to_remove:\n        if key in to_set.keys():\n            continue\n        commands.append(\"no {0}\".format(key))\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "keras_8", "problem": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:", "fixed": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    raise LookupError\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:"}
{"id": "PySnooper_2", "problem": " import functools\n import inspect\n import opcode\n import sys\n import re\n import collections", "fixed": " import functools\n import inspect\n import opcode\nimport os\n import sys\n import re\n import collections"}
{"id": "scrapy_26", "problem": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n            compsett.update(self[name])\n             return compsett\n        else:\n            return self[name]\n     def getpriority(self, name):", "fixed": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[basename], priority='default')\n            for k in self[name]:\n                prio = self[name].getpriority(k)\n                if prio > get_settings_priority('default'):\n                    compsett.set(k, self[name][k], prio)\n             return compsett\n        return self[name]\n     def getpriority(self, name):"}
{"id": "pandas_65", "problem": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, BufferedIOBase):\n             handles.append(g)\n         f = g", "fixed": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, (BufferedIOBase, RawIOBase)):\n             handles.append(g)\n         f = g"}
{"id": "pandas_153", "problem": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            values = values.astype(str)\n         else:\n             values = np.array(values, dtype=\"object\")", "fixed": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            itemsize = writers.word_len(na_rep)\n            values = values.astype(\"<U{size}\".format(size=itemsize))\n         else:\n             values = np.array(values, dtype=\"object\")"}
{"id": "luigi_13", "problem": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.fs.mkdir(d)\n         os.rename(old_path, new_path)", "fixed": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.mkdir(d)\n         os.rename(old_path, new_path)"}
{"id": "pandas_111", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    return self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    return self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "keras_11", "problem": " import warnings\n from .. import backend as K\n from .. import losses\n from ..utils.generic_utils import to_list", "fixed": " import warnings\n from .. import backend as K\n from .. import losses\nfrom ..utils import Sequence\n from ..utils.generic_utils import to_list"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n        if not self.trainable:\n            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []"}
{"id": "black_22", "problem": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36=py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")", "fixed": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")"}
{"id": "pandas_59", "problem": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other)\n         def _get_corr(a, b):\n             a = a.rolling(", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other) if not self.is_freq_type else self.win_freq\n         def _get_corr(a, b):\n             a = a.rolling("}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        elif tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit"}
