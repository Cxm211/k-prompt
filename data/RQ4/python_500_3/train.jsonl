{"id": "pandas_66", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            new_values = self._data.fast_xs(loc)\n            if not is_list_like(new_values) or self.ndim == 1:\n                return com.maybe_box_datetimelike(new_values)\n             result = self._constructor_sliced(\n                 new_values,", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            if self.ndim == 1:\n                return self._values[loc]\n            new_values = self._data.fast_xs(loc)\n             result = self._constructor_sliced(\n                 new_values,"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):", "fixed": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, extra={'spider': info.spider, 'failure': f})\n         )\nreturn dfd.addBoth(lambda _: wad)", "fixed": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\nreturn dfd.addBoth(lambda _: wad)"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "scrapy_21", "problem": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        self._parsers.pop(netloc).callback(None)", "fixed": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)"}
{"id": "pandas_105", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n    def transpose(self, *args, **kwargs):\n        axes, kwargs = self._construct_axes_from_arguments(\n            args, kwargs, require_all=True\n        )\n        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)\n        if len(axes) != len(set(axes)):\n            raise ValueError(f\"Must specify {self._AXIS_LEN} unique axes\")\n        new_axes = self._construct_axes_dict_from(\n            self, [self._get_axis(x) for x in axes_names]\n        )\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop(\"copy\", None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n        nv.validate_transpose(tuple(), kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)\n     def swapaxes(self, axis1, axis2, copy=True):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n     def swapaxes(self, axis1, axis2, copy=True):"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "black_1", "problem": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        executor.shutdown()\n async def schedule_formatting(", "fixed": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        if executor is not None:\n            executor.shutdown()\n async def schedule_formatting("}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if is_sequence:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'", "fixed": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if use_sequence_api:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'"}
{"id": "scrapy_8", "problem": " class Field(dict):\n class ItemMeta(ABCMeta):\n     def __new__(mcs, class_name, bases, attrs):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)", "fixed": " class Field(dict):\n class ItemMeta(ABCMeta):\n     def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)"}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"name": "mergesort.py", "problem": "def mergesort(arr):\n    def merge(left, right):\n        result = []\n        i = 0\n        j = 0\n        while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[i:] or right[j:])\n        return result\n    if len(arr) == 0:\n        return arr\n    else:\n        middle = len(arr) // 2\n        left = mergesort(arr[:middle])\n        right = mergesort(arr[middle:])\n        return merge(left, right)", "fixed": "def mergesort(arr):\n    def merge(left, right):\n        result = []\n        i = 0\n        j = 0\n        while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[i:] or right[j:])\n        return result\n    if len(arr) <= 1:\n        return arr\n    else:\n        middle = len(arr) // 2\n        left = mergesort(arr[:middle])\n        right = mergesort(arr[middle:])\n        return merge(left, right)\n", "hint": "Merge Sort\nInput:\n    arr: A list of ints", "input": [[1, 2, 6, 72, 7, 33, 4]], "output": [1, 2, 4, 6, 7, 33, 72]}
{"id": "pandas_2", "problem": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n        key = list(self._convert_key(key, is_setter=True))\n         self.obj._set_value(*key, value=value, takeable=self._takeable)", "fixed": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key, is_setter=True))\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n         self.obj._set_value(*key, value=value, takeable=self._takeable)"}
{"id": "pandas_54", "problem": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n         elif is_categorical(values):", "fixed": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n         elif is_categorical(values):"}
{"id": "scrapy_15", "problem": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),", "fixed": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(netloc),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "black_15", "problem": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: str) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        list(v.visit(lib2to3_parse(code)))\n KEYWORDS = set(keyword.kwlist)", "fixed": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: Union[str, Leaf, Node]) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))\n KEYWORDS = set(keyword.kwlist)"}
{"id": "keras_42", "problem": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.", "fixed": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training."}
{"id": "black_23", "problem": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        raise AssertionError(f\"cannot parse source: {exc}\") from None\n     try:\n         dst_ast = ast.parse(dst)", "fixed": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        major, minor = sys.version_info[:2]\n        raise AssertionError(\n            f\"cannot use --safe with this file; failed to parse source file \"\n            f\"with Python {major}.{minor}'s builtin AST. Re-run with --fast \"\n            f\"or stop using deprecated Python 2 syntax. AST error message: {exc}\"\n        )\n     try:\n         dst_ast = ast.parse(dst)"}
{"id": "thefuck_32", "problem": " def match(command, settings):\n    return 'ls' in command.script and not ('ls -' in command.script)\n def get_new_command(command, settings):", "fixed": " def match(command, settings):\n    return (command.script == 'ls'\n            or command.script.startswith('ls ')\n            and not ('ls -' in command.script))\n def get_new_command(command, settings):"}
{"id": "pandas_9", "problem": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_scalar(key) and isna(key):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)", "fixed": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)"}
{"id": "keras_30", "problem": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if x is None or len(x) == 0:\n                        batch_size = 1\n                    elif isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')\n            quote = m.group('quote')\n            if quote is not None:\n                comparison_value = comparison_value.replace(r'\\%s' % quote, quote)\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )", "fixed": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "fastapi_1", "problem": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\"", "fixed": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\""}
{"id": "pandas_112", "problem": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)", "fixed": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical(target_as_index):\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "black_15", "problem": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break", "fixed": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n        if parent.type == syms.file_input:\n            break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break"}
{"name": "hanoi.py", "problem": "def hanoi(height, start=1, end=3):\n    steps = []\n    if height > 0:\n        helper = ({1, 2, 3} - {start} - {end}).pop()\n        steps.extend(hanoi(height - 1, start, helper))\n        steps.append((start, helper))\n        steps.extend(hanoi(height - 1, helper, end))\n    return steps", "fixed": "def hanoi(height, start=1, end=3):\n    steps = []\n    if height > 0:\n        helper = ({1, 2, 3} - {start} - {end}).pop()\n        steps.extend(hanoi(height - 1, start, helper))\n        steps.append((start, end))\n        steps.extend(hanoi(height - 1, helper, end))\n    return steps", "hint": "Towers of Hanoi\nhanoi\nAn algorithm for solving the Towers of Hanoi puzzle.  Three pegs exist, with a stack of differently-sized", "input": [1, 1, 3], "output": [[1, 3]]}
{"id": "thefuck_19", "problem": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force')\n enabled_by_default = False", "fixed": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force-with-lease')\n enabled_by_default = False"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n        attributes.pop(\"tz\", None)\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):"}
{"id": "pandas_18", "problem": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            self.min_periods or 1,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,", "fixed": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(self.window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            window_indexer.window_size,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,"}
{"id": "pandas_70", "problem": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})", "fixed": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "luigi_5", "problem": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        @task._task_wraps(task_that_inherits)\n        class Wrapped(task_that_inherits):\n            def clone_parent(_self, **args):\n                return _self.clone(cls=self.task_to_inherit, **args)\n        return Wrapped\n class requires(object):", "fixed": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        def clone_parent(_self, **args):\n            return _self.clone(cls=self.task_to_inherit, **args)\n        task_that_inherits.clone_parent = clone_parent\n        return task_that_inherits\n class requires(object):"}
{"id": "matplotlib_19", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "luigi_29", "problem": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n            if cls.run == NotImplemented:\n                continue\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\", "fixed": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)\n         login_page_req.add_header('Cookie', 'locale=en_US')\n        self.report_login()\n        login_page = self._download_webpage(login_page_req, None, note=False,\n             errnote='Unable to download login page')\n         lsd = self._search_regex(\n             r'<input type=\"hidden\" name=\"lsd\" value=\"([^\"]*)\"',", "fixed": " class FacebookIE(InfoExtractor):\n         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)\n         login_page_req.add_header('Cookie', 'locale=en_US')\n        login_page = self._download_webpage(login_page_req, None,\n            note='Downloading login page',\n             errnote='Unable to download login page')\n         lsd = self._search_regex(\n             r'<input type=\"hidden\" name=\"lsd\" value=\"([^\"]*)\"',"}
{"id": "scrapy_6", "problem": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')", "fixed": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n        Yields :class:`Line` objects.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            try:\n                for comment in generate_comments(node):\n                    if any_open_brackets:\n                        self.current_line.append(comment)\n                    elif comment.type == token.COMMENT:\n                        self.current_line.append(comment)\n                        yield from self.line()\n                    else:\n                        yield from self.line()\n                        self.current_line.append(comment)\n                        yield from self.line()\n            except FormatOff as f_off:\n                f_off.trim_prefix(node)\n                yield from self.line(type=UnformattedLines)\n                yield from self.visit(node)\n            except FormatOn as f_on:\n                f_on.trim_prefix(node)\n                yield from self.visit_default(node)\n            else:\n                normalize_prefix(node, inside_brackets=any_open_brackets)\n                if self.normalize_strings and node.type == token.STRING:\n                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                    normalize_string_quotes(node)\n                if node.type not in WHITESPACE:\n                    self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    self.current_line.append(comment)\n                    yield from self.line()\n                else:\n                    yield from self.line()\n                    self.current_line.append(comment)\n                    yield from self.line()\n            normalize_prefix(node, inside_brackets=any_open_brackets)\n            if self.normalize_strings and node.type == token.STRING:\n                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                normalize_string_quotes(node)\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):", "fixed": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):"}
{"id": "PySnooper_1", "problem": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = '\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch", "fixed": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = u'\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch"}
{"id": "youtube-dl_35", "problem": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',", "fixed": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n        '%d/%m/%Y %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',"}
{"id": "luigi_24", "problem": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '\"{0}={1}\"'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):", "fixed": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or", "fixed": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('quotedstrval') is not None or\n            m.group('strval') is not None or"}
{"id": "keras_6", "problem": " def weighted_masked_objective(fn):\n             score_array *= mask\n            score_array /= K.mean(mask)\n         if weights is not None:", "fixed": " def weighted_masked_objective(fn):\n             score_array *= mask\n            score_array /= K.mean(mask) + K.epsilon()\n         if weights is not None:"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")", "fixed": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n    is_bool = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "youtube-dl_26", "problem": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "youtube-dl_35", "problem": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }", "fixed": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n        upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(upload_date_str),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }"}
{"id": "pandas_78", "problem": " Wild         185.0\n                     result = coerce_to_dtypes(result, self.dtypes)\n         if constructor is not None:\n            result = Series(result, index=labels)\n         return result\n     def nunique(self, axis=0, dropna=True) -> Series:", "fixed": " Wild         185.0\n                     result = coerce_to_dtypes(result, self.dtypes)\n         if constructor is not None:\n            result = self._constructor_sliced(result, index=labels)\n         return result\n     def nunique(self, axis=0, dropna=True) -> Series:"}
{"id": "scrapy_16", "problem": " def parse_url(url, encoding=None):", "fixed": " def parse_url(url, encoding=None):\n        Data are returned as a list of name, value pairs as bytes.\n        Arguments:\n        qs: percent-encoded query string to be parsed\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.  A\n            true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception."}
{"id": "pandas_141", "problem": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):", "fixed": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            reverse = self._range[::-1]\n            start, stop, step = reverse.start, reverse.stop, reverse.step\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):"}
{"id": "pandas_123", "problem": " class RangeIndex(Int64Index):\n    @staticmethod\n    def _validate_dtype(dtype):", "fixed": " class RangeIndex(Int64Index):"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n                 f\"is not equal to {right._values}.\"\n             )\n             raise AssertionError(msg)\n    elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):\n         assert_interval_array_equal(left.array, right.array)\n     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):\n         _testing.assert_almost_equal(", "fixed": " def assert_series_equal(\n                 f\"is not equal to {right._values}.\"\n             )\n             raise AssertionError(msg)\n    elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):\n         assert_interval_array_equal(left.array, right.array)\n     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):\n         _testing.assert_almost_equal("}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "luigi_14", "problem": " class Task(object):\n         return False\n    def can_disable(self):\n        return (self.disable_failures is not None or\n                self.disable_hard_timeout is not None)\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())", "fixed": " class Task(object):\n         return False\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())"}
{"id": "keras_19", "problem": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n     obj=\"Series\",\n ):", "fixed": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n    check_category_order=True,\n     obj=\"Series\",\n ):"}
{"id": "black_20", "problem": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src.name}  (original)\"\n        dst_name = f\"{src.name}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()", "fixed": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src}  (original)\"\n        dst_name = f\"{src}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip()\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)", "fixed": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip('\\n')\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*."}
{"id": "pandas_94", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        freq = self.freq if is_period_dtype(self) else None\n        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\n        return self._shallow_copy(result)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "thefuck_13", "problem": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)", "fixed": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "pandas_115", "problem": " def interpolate_1d(\n                 inds = lib.maybe_convert_objects(inds)\n         else:\n             inds = xvalues\n        result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])\n         result[preserve_nans] = np.nan\n         return result", "fixed": " def interpolate_1d(\n                 inds = lib.maybe_convert_objects(inds)\n         else:\n             inds = xvalues\n        indexer = np.argsort(inds[valid])\n        result[invalid] = np.interp(\n            inds[invalid], inds[valid][indexer], yvalues[valid][indexer]\n        )\n         result[preserve_nans] = np.nan\n         return result"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\n        if \"//\" in self.app.config.SERVER_NAME:\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\n            if \"//\" in self.app.config.SERVER_NAME:\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "pandas_90", "problem": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    if path is None:\n        path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(path) as path:\n        pd.to_pickle(obj, path)\n        return pd.read_pickle(path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):", "fixed": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as path:\n        pd.to_pickle(obj, _path)\n        return pd.read_pickle(_path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):"}
{"id": "ansible_11", "problem": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n    if text:\n        text = str(text).strip()\n     return {\n         'banner': module.params['banner'],\n         'text': text,", "fixed": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n     return {\n         'banner': module.params['banner'],\n         'text': text,"}
{"id": "keras_19", "problem": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN.", "fixed": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state).\n            - a `output_size` attribute. This can be a single integer or a\n                TensorShape, which represent the shape of the output. For\n                backward compatible reason, if this attribute is not available\n                for the cell, the value will be inferred by the first element\n                of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN."}
{"id": "black_15", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n def normalize_fmt_off(node: Node) -> None:", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n def normalize_fmt_off(node: Node) -> None:\n    Returns True if a pair was converted."}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "pandas_41", "problem": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype", "fixed": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value: ArrayLike) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n            extra={'spider': spider, 'failure': _failure}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,", "fixed": " class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,"}
{"id": "keras_22", "problem": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '", "fixed": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n        self.supports_masking = True\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '"}
{"id": "pandas_87", "problem": " def crosstab(\n         **kwargs,\n     )\n     if normalize is not False:\n         table = _normalize(", "fixed": " def crosstab(\n         **kwargs,\n     )\n    if not table.empty:\n        cols_diff = df.columns.difference(original_df_cols)[0]\n        table = table[cols_diff]\n     if normalize is not False:\n         table = _normalize("}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "youtube-dl_16", "problem": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')", "fixed": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data)\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')"}
{"id": "pandas_119", "problem": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n         result = result.append(margin_dummy)\n     except TypeError:", "fixed": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )\n         result = result.append(margin_dummy)\n     except TypeError:"}
{"id": "black_10", "problem": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char == ' ':\n                 current_column += 1\n            elif char == '\\t':\n                current_column += 4\n             elif char == '\\n':\n                 current_column = 0", "fixed": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char in ' \\t':\n                 current_column += 1\n             elif char == '\\n':\n                 current_column = 0"}
{"id": "tqdm_6", "problem": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else self.total)\n     def __enter__(self):\n         return self", "fixed": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else getattr(self, \"total\", None))\n     def __enter__(self):\n         return self"}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "ansible_13", "problem": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)", "fixed": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    requirement = None\n                    if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \\\n                            urlparse(collection_input).scheme.lower() in ['http', 'https']:\n                        name = collection_input\n                    else:\n                        name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    path : str\n        File path where the pickled object will be loaded.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',\n        or '.zip' respectively, and no decompression otherwise.\n        Set to None for no decompression.\n     Returns\n     -------", "fixed": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be loaded from.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL is not limited to S3 and GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     Returns\n     -------"}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "pandas_81", "problem": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):", "fixed": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype\n        dtype = pandas_dtype(dtype)\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n        elif isinstance(dtype, BooleanDtype):\n            result = self._data.astype(\"bool\", copy=False)\n            return BooleanArray(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,"}
{"id": "luigi_6", "problem": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x))\n         except ValueError:\nreturn literal_eval(x)\n    def serialize(self, x):\n        return json.dumps(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n         except ValueError:\nreturn literal_eval(x)\n class NumericalParameter(Parameter):"}
{"id": "youtube-dl_43", "problem": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?\n     if not m:\n         return u''\n     return m.group(1)", "fixed": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^?\n     if not m:\n         return u''\n     return m.group(1)"}
{"id": "black_2", "problem": " def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n     while container is not None and container.type != token.ENDMARKER:\n        is_fmt_on = False\n        for comment in list_comments(container.prefix, is_endmarker=False):\n            if comment.value in FMT_ON:\n                is_fmt_on = True\n            elif comment.value in FMT_OFF:\n                is_fmt_on = False\n        if is_fmt_on:\n             return\n        yield container\n        container = container.next_sibling\n def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:", "fixed": " def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n     while container is not None and container.type != token.ENDMARKER:\n        if fmt_on(container):\n             return\n        if contains_fmt_on_at_column(container, leaf.column):\n            for child in container.children:\n                if contains_fmt_on_at_column(child, leaf.column):\n                    return\n                yield child\n        else:\n            yield container\n            container = container.next_sibling\ndef fmt_on(container: LN) -> bool:\n    is_fmt_on = False\n    for comment in list_comments(container.prefix, is_endmarker=False):\n        if comment.value in FMT_ON:\n            is_fmt_on = True\n        elif comment.value in FMT_OFF:\n            is_fmt_on = False\n    return is_fmt_on\ndef contains_fmt_on_at_column(container: LN, column: int) -> bool:\n    for child in container.children:\n        if (\n            isinstance(child, Node)\n            and first_leaf_column(child) == column\n            or isinstance(child, Leaf)\n            and child.column == column\n        ):\n            if fmt_on(child):\n                return True\n    return False\ndef first_leaf_column(node: Node) -> Optional[int]:\n    for child in node.children:\n        if isinstance(child, Leaf):\n            return child.column\n    return None\n def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route(", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                    route_class_override=type(route),\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route("}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "pandas_165", "problem": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):", "fixed": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n    def test_parr_add_sub_index(self):\n        pi = pd.period_range(\"2000-12-31\", periods=3)\n        parr = pi.array\n        result = parr - pi\n        expected = pi - pi\n        tm.assert_index_equal(result, expected)\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of decades to use for each half of the linear range. For\n            example, when *linscale* == 1.0 (the default), the space used for\n            the positive and negative halves of the linear range will be equal\n            to one decade in the logarithmic range.\n         Normalize.__init__(self, vmin, vmax, clip)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - np.e ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()", "fixed": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of powers of *base* (decades for base 10) to use for each\n            half of the linear range. For example, when *linscale* == 1.0\n            (the default), the space used for the positive and negative halves\n            of the linear range will be equal to a decade in the logarithmic\n            range if ``base=10``.\n        base : float, default: None\n            For v3.2 the default is the old value of ``np.e``, but that is\n            deprecated for v3.3 when base will default to 10.  During the\n            transition, specify the *base* kwarg to avoid a deprecation\n            warning.\n         Normalize.__init__(self, vmin, vmax, clip)\n        if base is None:\n            self._base = np.e\n            cbook.warn_deprecated(\"3.3\", message=\"default base will change \"\n                \"from np.e to 10.  To suppress this warning specify the base \"\n                \"kwarg.\")\n        else:\n            self._base = base\n        self._log_base = np.log(self._base)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - self._base ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n    BodySchema_kwargs: Dict[str, Any] = dict(default=None)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "luigi_33", "problem": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if p.significant]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))", "fixed": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if not p.is_global]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))"}
{"name": "is_valid_parenthesization.py", "problem": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return True", "fixed": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return depth == 0\n", "hint": "Nested Parens\nInput:\n    parens: A string of parentheses", "input": ["((()()))()"], "output": "True"}
{"id": "pandas_118", "problem": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(np.ravel(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\"", "fixed": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(com.flatten(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\""}
{"id": "black_22", "problem": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n         if not (", "fixed": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n    @property\n    def contains_standalone_comments(self) -> bool:\n         if not ("}
{"id": "pandas_14", "problem": " def id_func(x):\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])", "fixed": " def id_func(x):\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n    ]\n)\ndef names(request):\n    return request.param\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])"}
{"id": "pandas_43", "problem": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)", "fixed": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n     ) -> None:\n        route = self.route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,", "fixed": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n     ) -> None:\n        route_class = route_class_override or self.route_class\n        route = route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):", "fixed": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        if six.PY2:\n            self.file_flags = 'b'\n            self._open_args = {}\n        else:\n            self.file_flags = ''\n            self._open_args = {'newline': '\\n'}\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):"}
{"id": "pandas_136", "problem": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_int64_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_integer_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n        if len(weights) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())", "fixed": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n    class_sample_weight = None\n    if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if len(y.shape) == 2:\n            if y.shape[1] > 1:\n                y_classes = np.argmax(y, axis=1)\n            elif y.shape[1] == 1:\n                y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        class_sample_weight = np.asarray(\n            [class_weight[cls] for cls in y_classes if cls in class_weight])\n        if len(class_sample_weight) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())"}
{"id": "cookiecutter_4", "problem": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return EXIT_SUCCESS\n    return run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return\n    run_script_with_context(script, project_dir, context)"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        include_none=include_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )", "fixed": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )"}
{"id": "keras_29", "problem": " class Model(Container):\n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n            for i, m in enumerate(self.metrics):\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,"}
{"id": "pandas_114", "problem": " class Index(IndexOpsMixin, PandasObject):\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n        s = extract_array(series, extract_numpy=True)\n        if isinstance(s, ExtensionArray):\n            if is_scalar(key):\n                try:\n                    iloc = self.get_loc(key)\n                    return s[iloc]\n                except KeyError:\n                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                        raise\n                    elif is_integer(key):\n                        return s[key]\n            else:\n                raise InvalidIndexError(key)\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "matplotlib_3", "problem": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = True\n         self._marker_function()\n     def __bool__(self):", "fixed": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = self._fillstyle != 'none'\n         self._marker_function()\n     def __bool__(self):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "youtube-dl_32", "problem": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)\n def js_to_json(code):", "fixed": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(\n        r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?://[^\\n]*)*$', r'\\1', code)\n def js_to_json(code):"}
{"id": "black_6", "problem": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n             result = drv.parse_string(src_txt, True)\n             break", "fixed": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "pandas_46", "problem": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n     def test_empty(self):\n         X = [[], [0, 1], []]", "fixed": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n     def test_empty(self):\n         X = [[], [0, 1], []]"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "pandas_54", "problem": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, RangeIndex):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)", "fixed": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, (RangeIndex, CategoricalIndex)):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)"}
{"id": "pandas_57", "problem": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                 categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                 else:\n                    categories[index] = value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if is_list_like(to_replace):\n            replace_dict = {replace_value: value for replace_value in to_replace}\n        else:\n            replace_dict = {to_replace: value}\n        for replace_value, new_value in replace_dict.items():\n            if replace_value in cat.categories:\n                if isna(new_value):\n                    cat.remove_categories(replace_value, inplace=True)\n                    continue\n                 categories = cat.categories.tolist()\n                index = categories.index(replace_value)\n                if new_value in cat.categories:\n                    value_index = categories.index(new_value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(replace_value, inplace=True)\n                 else:\n                    categories[index] = new_value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat"}
{"id": "pandas_167", "problem": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n     _accessors = {\"str\"}", "fixed": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n    _supports_partial_string_indexing = False\n     _accessors = {\"str\"}"}
{"name": "find_first_in_sorted.py", "problem": "def find_first_in_sorted(arr, x):\n    lo = 0\n    hi = len(arr)\n    while lo <= hi:\n        mid = (lo + hi) // 2\n        if x == arr[mid] and (mid == 0 or x != arr[mid - 1]):\n            return mid\n        elif x <= arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return -1", "fixed": "def find_first_in_sorted(arr, x):\n    lo = 0\n    hi = len(arr)\n    while lo < hi:\n        mid = (lo + hi) // 2\n        if x == arr[mid] and (mid == 0 or x != arr[mid - 1]):\n            return mid\n        elif x <= arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return -1\n", "hint": "Fancy Binary Search\nfancy-binsearch\nInput:", "input": [[3, 4, 5, 5, 5, 5, 6], 5], "output": 2}
{"id": "pandas_7", "problem": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where(", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        left_distances = np.abs(self[left_indexer] - target)\n        right_distances = np.abs(self[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where("}
{"id": "pandas_39", "problem": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n             self._update_inplace(", "fixed": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n            self._reset_cacher()\n             self._update_inplace("}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,", "fixed": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n            dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,"}
{"id": "scrapy_30", "problem": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()\n        return comm[0].strip()\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\", "fixed": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_69", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not ax.holds_integer():\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \"", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \""}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "luigi_27", "problem": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_value:\n            description.append(\" [default: %s]\" % (self.value,))\n         if self.is_list:\n             action = \"append\"", "fixed": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_task_value(param_name=param_name, task_name=task_name):\n            value = self.task_value(param_name=param_name, task_name=task_name)\n            description.append(\" [default: %s]\" % (value,))\n         if self.is_list:\n             action = \"append\""}
{"id": "keras_16", "problem": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,", "fixed": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            self._build_input_shape = input_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "keras_34", "problem": " class Sequence(object):\n _SHARED_SEQUENCES = {}", "fixed": " class Sequence(object):\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item\n _SHARED_SEQUENCES = {}"}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         else:\n             return ((prefix + \": \") if prefix else '') + \\", "fixed": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         else:\n             return ((prefix + \": \") if prefix else '') + \\"}
{"id": "pandas_53", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not self.holds_integer():\n                     self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n                     self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if is_sequence:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'", "fixed": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if use_sequence_api:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_6", "problem": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):", "fixed": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError, ValueError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):"}
{"id": "scrapy_19", "problem": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n     def has_header(self, name):\n         return name in self.request.headers", "fixed": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n    @property\n    def full_url(self):\n        return self.get_full_url()\n    @property\n    def host(self):\n        return self.get_host()\n    @property\n    def type(self):\n        return self.get_type()\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n     def has_header(self, name):\n         return name in self.request.headers"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n             return out.sort_index() if self.sort else out", "fixed": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n            out = self._reindex_output(out)\n             return out.sort_index() if self.sort else out"}
{"id": "luigi_7", "problem": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n             if status == PENDING or status != task.status:", "fixed": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:\n             if status == PENDING or status != task.status:"}
{"id": "black_18", "problem": " def format_str(\n     return dst_contents\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,", "fixed": " def format_str(\n     return dst_contents\ndef prepare_input(src: bytes) -> Tuple[str, str, str]:\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)\n         yield current_line", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        yield current_line\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n    if current_line:\n         yield current_line"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n     @property\n     def trainable(self):", "fixed": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n        self.input_spec = layer.input_spec\n     @property\n     def trainable(self):"}
{"id": "pandas_20", "problem": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):", "fixed": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):"}
{"id": "pandas_10", "problem": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, np.ndarray) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)", "fixed": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "black_22", "problem": " class UnformattedLines(Line):\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class UnformattedLines(Line):\n @dataclass\n class EmptyLineTracker:"}
{"id": "httpie_1", "problem": " def filename_from_url(url, content_type):\n     return fn\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        if not exists(filename + suffix):\n            return filename + suffix\n         attempt += 1", "fixed": " def filename_from_url(url, content_type):\n     return fn\ndef trim_filename(filename, max_len):\n    if len(filename) > max_len:\n        trim_by = len(filename) - max_len\n        name, ext = os.path.splitext(filename)\n        if trim_by >= len(name):\n            filename = filename[:-trim_by]\n        else:\n            filename = name[:-trim_by] + ext\n    return filename\ndef get_filename_max_length(directory):\n    try:\n        max_len = os.pathconf(directory, 'PC_NAME_MAX')\n    except OSError as e:\n        if e.errno == errno.EINVAL:\n            max_len = 255\n        else:\n            raise\n    return max_len\ndef trim_filename_if_needed(filename, directory='.', extra=0):\n    max_len = get_filename_max_length(directory) - extra\n    if len(filename) > max_len:\n        filename = trim_filename(filename, max_len)\n    return filename\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        try_filename = trim_filename_if_needed(filename, extra=len(suffix))\n        try_filename += suffix\n        if not exists(try_filename):\n            return try_filename\n         attempt += 1"}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "tornado_4", "problem": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if (start is not None and start >= size) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n            if start is not None and start < 0:\n                start += size\n             if end is not None and end > size:", "fixed": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if start is not None and start < 0:\n                start += size\n                if start < 0:\n                    start = 0\n            if (\n                start is not None\n                and (start >= size or (end is not None and start >= end))\n            ) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n             if end is not None and end > size:"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "luigi_4", "problem": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)", "fixed": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if self.columns and len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output: Series or DataFrame\n             Object resulting from grouping and applying an operation.\n         Returns\n         -------", "fixed": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output : Series or DataFrame\n             Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n         Returns\n         -------"}
{"id": "tornado_11", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\") == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)"}
{"id": "youtube-dl_38", "problem": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]", "fixed": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]\ndef urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')"}
{"name": "rpn_eval.py", "problem": "def rpn_eval(tokens):\n    def op(symbol, a, b):\n        return {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b\n        }[symbol](a, b)\n    stack = []\n    for token in tokens:\n        if isinstance(token, float):\n            stack.append(token)\n        else:\n            a = stack.pop()\n            b = stack.pop()\n            stack.append(\n                op(token, a, b)\n            )\n    return stack.pop()", "fixed": "def rpn_eval(tokens):\n    def op(symbol, a, b):\n        return {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b\n        }[symbol](a, b)\n    stack = []\n    for token in tokens:\n        if isinstance(token, float):\n            stack.append(token)\n        else:\n            a = stack.pop()\n            b = stack.pop()\n            stack.append(\n                op(token, b, a)\n            )\n    return stack.pop()\n", "hint": "Reverse Polish Notation\nFour-function calculator with input given in Reverse Polish Notation (RPN).\nInput:", "input": [[3.0, 5.0, "+", 2.0, "/"]], "output": 4.0}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "luigi_19", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable():\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n         Returns\n         -------\n        axes: topmost axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy)]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:", "fixed": " class FigureCanvasBase:\n         Returns\n         -------\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy) and a.get_visible()]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a same-typed block.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block.", "fixed": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block."}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"id": "pandas_123", "problem": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n                     elif inferred == \"string\":\n                         pass", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, name=name)\n                     elif inferred == \"string\":\n                         pass"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    include_none: bool = True,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:", "fixed": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n             )\nelse:\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,", "fixed": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n                exclude_none=exclude_none,\n                exclude_defaults=exclude_defaults,\n             )\nelse:\n            if exclude_defaults:\n                raise ValueError(\"Cannot use exclude_defaults\")\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,"}
{"id": "scrapy_31", "problem": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n     getheaders = get_all", "fixed": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]\n     getheaders = get_all"}
{"name": "find_in_sorted.py", "problem": "def find_in_sorted(arr, x):\n    def binsearch(start, end):\n        if start == end:\n            return -1\n        mid = start + (end - start) // 2\n        if x < arr[mid]:\n            return binsearch(start, mid)\n        elif x > arr[mid]:\n            return binsearch(mid, end)\n        else:\n            return mid\n    return binsearch(0, len(arr))", "fixed": "def find_in_sorted(arr, x):\n    def binsearch(start, end):\n        if start == end:\n            return -1\n        mid = start + (end - start) // 2\n        if x < arr[mid]:\n            return binsearch(start, mid)\n        elif x > arr[mid]:\n            return binsearch(mid + 1, end)\n        else:\n            return mid\n    return binsearch(0, len(arr))", "hint": "Binary Search\nInput:\n    arr: A sorted list of ints", "input": [[3, 4, 5, 5, 5, 5, 6], 5], "output": 3}
{"id": "luigi_14", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "keras_42", "problem": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,", "fixed": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n             if not ax.is_unique:\n                 return False", "fixed": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                return False\n             if not ax.is_unique:\n                 return False"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values.T, other.values).T\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":", "fixed": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n            other_vals = other.values.reshape(-1, 1)\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values, other_vals)\n            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n         if fill_value is None:", "fixed": " class DataFrame(NDFrame):\n    def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n         if fill_value is None:"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n         if auto is not None:\n             self._autoscaleYon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n        for ax in self._shared_y_axes.get_siblings(self):\n            ax._stale_viewlim_y = False\n         if auto is not None:\n             self._autoscaleYon = bool(auto)"}
{"id": "scrapy_12", "problem": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n         if _root is not None:", "fixed": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n         st = _st(response, type or self._default_type)\n         if _root is not None:"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, endwith=[]):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:", "fixed": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:"}
{"id": "black_6", "problem": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n         return tiow.read(), encoding, newline\ndef get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return [\n            pygram.python_grammar_no_print_statement_no_exec_statement,\n            pygram.python_grammar_no_print_statement,\n            pygram.python_grammar,\n         ]\n     elif all(version.is_python2() for version in target_versions):\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n     else:\n        return [pygram.python_grammar_no_print_statement_no_exec_statement]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:", "fixed": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n         return tiow.read(), encoding, newline\n@dataclass(frozen=True)\nclass ParserConfig:\n    grammar: Grammar\n    tokenizer_config: TokenizerConfig = TokenizerConfig()\ndef get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:\n     if not target_versions:\n         return [\n            ParserConfig(\n                pygram.python_grammar_no_print_statement_no_exec_statement,\n                TokenizerConfig(async_is_reserved_keyword=True),\n            ),\n            ParserConfig(\n                pygram.python_grammar_no_print_statement_no_exec_statement,\n                TokenizerConfig(async_is_reserved_keyword=False),\n            ),\n            ParserConfig(pygram.python_grammar_no_print_statement),\n            ParserConfig(pygram.python_grammar),\n         ]\n     elif all(version.is_python2() for version in target_versions):\n        return [\n            ParserConfig(pygram.python_grammar_no_print_statement),\n            ParserConfig(pygram.python_grammar),\n        ]\n     else:\n        configs = []\n        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):\n            configs.append(\n                ParserConfig(\n                    pygram.python_grammar_no_print_statement_no_exec_statement,\n                    TokenizerConfig(async_is_reserved_keyword=True),\n                )\n            )\n        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):\n            configs.append(\n                ParserConfig(\n                    pygram.python_grammar_no_print_statement_no_exec_statement,\n                    TokenizerConfig(async_is_reserved_keyword=False),\n                )\n            )\n        return configs\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n     ):\n         self._write = get_write_function(output, overwrite)", "fixed": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n            custom_repr=(),\n     ):\n         self._write = get_write_function(output, overwrite)"}
{"id": "scrapy_38", "problem": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',\n             namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n         ]\n     if not clickables:", "fixed": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n             namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n         ]\n     if not clickables:"}
{"id": "scrapy_5", "problem": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,", "fixed": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,"}
{"id": "pandas_135", "problem": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n            return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)", "fixed": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n        except TypeError as err:\n            if \"ndarray\" in str(err):\n                pass\n            else:\n                raise\n        return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"name": "quicksort.py", "problem": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x > pivot])\n    return lesser + [pivot] + greater", "fixed": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x >= pivot])\n    return lesser + [pivot] + greater\n", "hint": "QuickSort\nInput:\n    arr: A list of ints", "input": [17, 0], "output": 17}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n     def inaxes(self, xy):\n        Check if a point is in an axes.\n         Parameters\n         ----------", "fixed": " class FigureCanvasBase:\n     def inaxes(self, xy):\n        Return the topmost visible `~.axes.Axes` containing the point *xy*.\n         Parameters\n         ----------"}
{"id": "fastapi_4", "problem": " def get_openapi_path(\n             operation_parameters = get_openapi_operation_parameters(all_route_params)\n             parameters.extend(operation_parameters)\n             if parameters:\n                operation[\"parameters\"] = parameters\n             if method in METHODS_WITH_BODY:\n                 request_body_oai = get_openapi_operation_request_body(\n                     body_field=route.body_field, model_name_map=model_name_map", "fixed": " def get_openapi_path(\n             operation_parameters = get_openapi_operation_parameters(all_route_params)\n             parameters.extend(operation_parameters)\n             if parameters:\n                operation[\"parameters\"] = list(\n                    {param[\"name\"]: param for param in parameters}.values()\n                )\n             if method in METHODS_WITH_BODY:\n                 request_body_oai = get_openapi_operation_request_body(\n                     body_field=route.body_field, model_name_map=model_name_map"}
{"id": "tqdm_1", "problem": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):", "fixed": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):"}
{"id": "black_9", "problem": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:", "fixed": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "youtube-dl_33", "problem": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()", "fixed": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'(\\.[0-9]+)?(?:Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         else:\n             raise ValueError(\"Not a valid normalize argument\")\n        table.index.names = table_index_names\n        table.columns.names = table_columns_names\n     else:\n         raise ValueError(\"Not a valid margins argument\")", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n            table.columns = table_columns\n         else:\n             raise ValueError(\"Not a valid normalize argument\")\n     else:\n         raise ValueError(\"Not a valid margins argument\")"}
{"name": "lcs_length.py", "problem": "def lcs_length(s, t):\n    from collections import Counter\n    dp = Counter()\n    for i in range(len(s)):\n        for j in range(len(t)):\n            if s[i] == t[j]:\n                dp[i, j] = dp[i - 1, j] + 1\n    return max(dp.values()) if dp else 0", "fixed": "def lcs_length(s, t):\n    from collections import Counter\n    dp = Counter()\n    for i in range(len(s)):\n        for j in range(len(t)):\n            if s[i] == t[j]:\n                dp[i, j] = dp[i - 1, j - 1] + 1\n    return max(dp.values()) if dp else 0", "hint": "Longest Common Substring\nlongest-common-substring\nInput:", "input": "", "output": ""}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "pandas_41", "problem": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)", "fixed": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq"}
{"id": "pandas_68", "problem": " class BaseMethodsTests(BaseExtensionTests):\n         expected = empty\n         self.assert_extension_array_equal(result, expected)\n     def test_shift_fill_value(self, data):\n         arr = data[:4]\n         fill_value = data[0]", "fixed": " class BaseMethodsTests(BaseExtensionTests):\n         expected = empty\n         self.assert_extension_array_equal(result, expected)\n    def test_shift_zero_copies(self, data):\n        result = data.shift(0)\n        assert result is not data\n        result = data[:0].shift(2)\n        assert result is not data\n     def test_shift_fill_value(self, data):\n         arr = data[:4]\n         fill_value = data[0]"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "youtube-dl_18", "problem": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()", "fixed": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()"}
{"id": "black_11", "problem": " def split_line(\n         return\n     line_str = str(line).strip(\"\\n\")\n    if not line.should_explode and is_line_short_enough(\n        line, line_length=line_length, line_str=line_str\n     ):\n         yield line\n         return", "fixed": " def split_line(\n         return\n     line_str = str(line).strip(\"\\n\")\n    has_special_comment = False\n    for leaf in line.leaves:\n        for comment in line.comments_after(leaf):\n            if leaf.type == token.COMMA and is_special_comment(comment):\n                has_special_comment = True\n    if (\n        not has_special_comment\n        and not line.should_explode\n        and is_line_short_enough(line, line_length=line_length, line_str=line_str)\n     ):\n         yield line\n         return"}
{"id": "pandas_65", "problem": " class CParserWrapper(ParserBase):\n            if isinstance(src, BufferedIOBase):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\"", "fixed": " class CParserWrapper(ParserBase):\n            if isinstance(src, (BufferedIOBase, RawIOBase)):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\""}
{"id": "luigi_25", "problem": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path()\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):", "fixed": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):"}
{"id": "pandas_92", "problem": " class TimeGrouper(Grouper):\n         rng += freq_mult\n         rng -= bin_shift\n        bins = memb.searchsorted(rng, side=\"left\")\n         if nat_count > 0:", "fixed": " class TimeGrouper(Grouper):\n         rng += freq_mult\n         rng -= bin_shift\n        prng = type(memb._data)(rng, dtype=memb.dtype)\n        bins = memb.searchsorted(prng, side=\"left\")\n         if nat_count > 0:"}
{"id": "pandas_92", "problem": " class TestPeriodIndex:\n         p2 = pd.Period(\"2014-01-04\", freq=freq)\n         assert pidx.searchsorted(p2) == 3\n        msg = \"Input has different freq=H from PeriodIndex\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"H\"))\n        msg = \"Input has different freq=5D from PeriodIndex\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"5D\"))\n class TestPeriodIndexConversion:\n     def test_tolist(self):", "fixed": " class TestPeriodIndex:\n         p2 = pd.Period(\"2014-01-04\", freq=freq)\n         assert pidx.searchsorted(p2) == 3\n        assert pidx.searchsorted(pd.NaT) == 0\n        msg = \"Input has different freq=H from PeriodArray\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"H\"))\n        msg = \"Input has different freq=5D from PeriodArray\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"5D\"))\n    def test_searchsorted_invalid(self):\n        pidx = pd.PeriodIndex(\n            [\"2014-01-01\", \"2014-01-02\", \"2014-01-03\", \"2014-01-04\", \"2014-01-05\"],\n            freq=\"D\",\n        )\n        other = np.array([0, 1], dtype=np.int64)\n        msg = \"requires either a Period or PeriodArray\"\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(other)\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(other.astype(\"timedelta64[ns]\"))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.timedelta64(4))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.timedelta64(\"NaT\", \"ms\"))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.datetime64(4, \"ns\"))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.datetime64(\"NaT\", \"ns\"))\n class TestPeriodIndexConversion:\n     def test_tolist(self):"}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'][:-5])\n         thumbnail = None\n         duration = None", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'])\n         thumbnail = None\n         duration = None"}
{"id": "pandas_154", "problem": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(obj.values)\n             if not cython_dtype:\n                cython_dtype = obj.values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = obj.values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:", "fixed": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n            values = obj._data._values\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(values)\n             if not cython_dtype:\n                cython_dtype = values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n         self._threads = []\n         self._stop_event = None", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._manager:\n            self._manager.shutdown()\n         self._threads = []\n         self._stop_event = None"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines."}
{"id": "pandas_91", "problem": " class TimedeltaIndex(\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n         if isinstance(value, (np.ndarray, Index)):\n            value = np.array(value, dtype=_TD_DTYPE, copy=False)\n        else:\n            value = Timedelta(value).asm8.view(_TD_DTYPE)\n        return self.values.searchsorted(value, side=side, sorter=sorter)\n     def is_type_compatible(self, typ) -> bool:\n         return typ == self.inferred_type or typ == \"timedelta\"", "fixed": " class TimedeltaIndex(\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n         if isinstance(value, (np.ndarray, Index)):\n            if not type(self._data)._is_recognized_dtype(value):\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar, \"\n                    f\"not {type(value).__name__}\"\n                )\n            value = type(self._data)(value)\n            self._data._check_compatible_with(value)\n        elif isinstance(value, self._data._recognized_scalars):\n            self._data._check_compatible_with(value)\n            value = self._data._scalar_type(value)\n        elif not isinstance(value, TimedeltaArray):\n            raise TypeError(\n                \"searchsorted requires compatible dtype or scalar, \"\n                f\"not {type(value).__name__}\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n     def is_type_compatible(self, typ) -> bool:\n         return typ == self.inferred_type or typ == \"timedelta\""}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                     elif string == '(':\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                        current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])\n                     elif string == '+':\n                         video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, [','])\n                        current_selector = None\n                        selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))\n                     else:\n                         raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                 elif type == tokenize.ENDMARKER:", "fixed": " class YoutubeDL(object):\n                     elif string == '(':\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                     elif string == '+':\n                         video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])\n                     else:\n                         raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                 elif type == tokenize.ENDMARKER:"}
{"id": "tornado_8", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\",", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n        except ValueError:\n            self.handler.set_status(400)\n            log_msg = \"Missing/Invalid WebSocket headers\"\n            self.handler.finish(log_msg)\n            gen_log.debug(log_msg)\n            return\n        try:\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\","}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "pandas_94", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self._data\n        if isinstance(values, type(self)):\n            values = values._data\n        attributes = self._get_attributes_dict()\n        if \"freq\" not in kwargs and self.freq is not None:\n            if isinstance(values, (DatetimeArray, TimedeltaArray)):\n                if values.freq is None:\n                    del attributes[\"freq\"]\n        attributes.update(kwargs)\n        return self._simple_new(values, **attributes)"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):", "fixed": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other, sort=sort)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):", "fixed": " class StackedRNNCells(Layer):\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):"}
{"id": "scrapy_33", "problem": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         extra={'spider': spider, 'failure': failure})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)", "fixed": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)"}
{"id": "pandas_128", "problem": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(", "fixed": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n    if encoding is None:\n        encoding = \"utf-8\"\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer("}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "pandas_76", "problem": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError):\n                 pass", "fixed": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError, OverflowError):\n                 pass"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "cookiecutter_2", "problem": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n    return None\n def run_script(script_path, cwd='.'):", "fixed": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n    scripts = []\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\n    if len(scripts) == 0:\n        return None\n    return scripts\n def run_script(script_path, cwd='.'):"}
{"id": "thefuck_8", "problem": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read()\n     return _parse_operations(lines)", "fixed": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read().decode(\"utf-8\")\n     return _parse_operations(lines)"}
{"id": "luigi_29", "problem": " class AmbiguousClass(luigi.Task):\n     pass\nclass NonAmbiguousClass(luigi.ExternalTask):\n    pass\nclass NonAmbiguousClass(luigi.Task):\n    def run(self):\n        NonAmbiguousClass.has_run = True\n class TaskWithSameName(luigi.Task):\n     def run(self):", "fixed": " class AmbiguousClass(luigi.Task):\n     pass\n class TaskWithSameName(luigi.Task):\n     def run(self):"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            self.csv_file = open(self.filename, 'a' + self.file_flags)\n         else:\n            self.csv_file = open(self.filename, 'w' + self.file_flags)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}", "fixed": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            mode = 'a'\n         else:\n            mode = 'w'\n        self.csv_file = io.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "pandas_26", "problem": " class Categorical(ExtensionArray, PandasObject):\n         good = self._codes != -1\n         if not good.all():\n            if skipna:\n                 pointer = self._codes[good].max()\n             else:\n                 return np.nan", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         good = self._codes != -1\n         if not good.all():\n            if skipna and good.any():\n                 pointer = self._codes[good].max()\n             else:\n                 return np.nan"}
{"id": "scrapy_26", "problem": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n            compsett.update(self[name])\n             return compsett\n        else:\n            return self[name]\n     def getpriority(self, name):", "fixed": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[basename], priority='default')\n            for k in self[name]:\n                prio = self[name].getpriority(k)\n                if prio > get_settings_priority('default'):\n                    compsett.set(k, self[name][k], prio)\n             return compsett\n        return self[name]\n     def getpriority(self, name):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "PySnooper_1", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False"}
{"id": "pandas_146", "problem": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if np.any(left_value != right_value):\n                    return False\n         return True", "fixed": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                try:\n                    if np.any(left_value != right_value):\n                        return False\n                except TypeError as err:\n                    if \"Cannot compare tz-naive\" in str(err):\n                        return False\n                    raise\n         return True"}
{"id": "keras_43", "problem": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1", "fixed": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n    if input_shape and input_shape[-1] == 1:\n        input_shape = tuple(input_shape[:-1])\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1"}
{"id": "luigi_9", "problem": " def _summary_format(set_tasks, worker):\n         str_output += 'Did not run any tasks'\n     smiley = \"\"\n     reason = \"\"\n    if set_tasks[\"failed\"]:\n        smiley = \":(\"\n        reason = \"there were failed tasks\"\n        if set_tasks[\"scheduling_error\"]:\n            reason += \" and tasks whose scheduling failed\"\n     elif set_tasks[\"scheduling_error\"]:\n         smiley = \":(\"\n         reason = \"there were tasks whose scheduling failed\"", "fixed": " def _summary_format(set_tasks, worker):\n         str_output += 'Did not run any tasks'\n     smiley = \"\"\n     reason = \"\"\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            smiley = \":)\"\n            reason = \"there were failed tasks but they all suceeded in a retry\"\n        else:\n            smiley = \":(\"\n            reason = \"there were failed tasks\"\n            if set_tasks[\"scheduling_error\"]:\n                reason += \" and tasks whose scheduling failed\"\n     elif set_tasks[\"scheduling_error\"]:\n         smiley = \":(\"\n         reason = \"there were tasks whose scheduling failed\""}
{"id": "pandas_83", "problem": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort\n         )\n     def _get_concat_axis(self) -> Index:", "fixed": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,\n         )\n     def _get_concat_axis(self) -> Index:"}
{"id": "thefuck_4", "problem": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip().split('\\n')\n    for alias in alias_out:\n        name, value = alias.replace('alias ', '', 1).split(' ', 1)\n         if name not in overridden:\n             aliases[name] = value\n     return aliases", "fixed": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip()\n    if not alias_out:\n        return aliases\n    for alias in alias_out.split('\\n'):\n        for separator in (' ', '='):\n            split_alias = alias.replace('alias ', '', 1).split(separator, 1)\n            if len(split_alias) == 2:\n                name, value = split_alias\n                break\n        else:\n            continue\n         if name not in overridden:\n             aliases[name] = value\n     return aliases"}
{"id": "matplotlib_4", "problem": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,", "fixed": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,"}
{"id": "black_15", "problem": " def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n        if pc.value in FMT_ON:\n            raise FormatOn(pc.consumed)\n        if pc.value in FMT_OFF:\n            if pc.type == STANDALONE_COMMENT:\n                raise FormatOff(pc.consumed)\n            prev = preceding_leaf(leaf)\n            if not prev or prev.type in WHITESPACE:\n                raise FormatOff(pc.consumed)\n @dataclass", "fixed": " def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n @dataclass"}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"name": "pascal.py", "problem": "def pascal(n):\n    rows = [[1]]\n    for r in range(1, n):\n        row = []\n        for c in range(0, r):\n            upleft = rows[r - 1][c - 1] if c > 0 else 0\n            upright = rows[r - 1][c] if c < r else 0\n            row.append(upleft + upright)\n        rows.append(row)\n    return rows", "fixed": "def pascal(n):\n    rows = [[1]]\n    for r in range(1, n):\n        row = []\n        for c in range(0, r + 1):\n            upleft = rows[r - 1][c - 1] if c > 0 else 0\n            upright = rows[r - 1][c] if c < r else 0\n            row.append(upleft + upright)\n        rows.append(row)\n    return rows", "hint": "Pascal's Triangle\npascal\nInput:", "input": [3], "output": [[1], [1, 1], [1, 2, 1]]}
{"id": "keras_19", "problem": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n     def get_config(self):\n        config = []\n         for layer in self.layers:\n            config.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n        return copy.deepcopy(config)\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n        model = cls()\n        for conf in config:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n         return model", "fixed": " class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n     def get_config(self):\n        layer_configs = []\n         for layer in self.layers:\n            layer_configs.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n        config = {\n            'name': self.name,\n            'layers': copy.deepcopy(layer_configs)\n        }\n        if self._build_input_shape:\n            config['build_input_shape'] = self._build_input_shape\n        return config\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n        if 'name' in config:\n            name = config['name']\n            build_input_shape = config.get('build_input_shape')\n            layer_configs = config['layers']\n        model = cls(name=name)\n        for conf in layer_configs:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n        if not model.inputs and build_input_shape:\n            model.build(build_input_shape)\n         return model"}
{"id": "scrapy_30", "problem": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).itervalues():\n             if inspect.isclass(obj) and \\\n               issubclass(obj, ScrapyCommand) and \\\n               obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):", "fixed": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).values():\n             if inspect.isclass(obj) and \\\n                    issubclass(obj, ScrapyCommand) and \\\n                    obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):"}
{"id": "fastapi_7", "problem": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={\"detail\": exc.errors()}\n     )", "fixed": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY,\n        content={\"detail\": jsonable_encoder(exc.errors())},\n     )"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         assert_allclose(y1, y2, atol=1e-05)\n     def test_random_normal(self):\n         for mean, std in [(0., 1.), (-10., 5.)]:\n            rand = K.eval(K.random_normal((300, 200),\n                                          mean=mean, stddev=std, seed=1337))\n            assert rand.shape == (300, 200)\n             assert np.abs(np.mean(rand) - mean) < std * 0.015\n             assert np.abs(np.std(rand) - std) < std * 0.015\n            r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n            samples = np.array([K.eval(r) for _ in range(200)])\n            assert np.abs(np.mean(samples) - mean) < std * 0.015\n            assert np.abs(np.std(samples) - std) < std * 0.015\n     def test_random_uniform(self):\n         min_val = -1.\n         max_val = 1.\n        rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n        assert rand.shape == (200, 100)\n         assert np.abs(np.mean(rand)) < 0.015\n         assert max_val - 0.015 < np.max(rand) <= max_val\n         assert min_val + 0.015 > np.min(rand) >= min_val\n        r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples)) < 0.015\n        assert max_val - 0.015 < np.max(samples) <= max_val\n        assert min_val + 0.015 > np.min(samples) >= min_val\n     def test_random_binomial(self):\n         p = 0.5\n        rand = K.eval(K.random_binomial((200, 100), p))\n        assert rand.shape == (200, 100)\n         assert np.abs(np.mean(rand) - p) < 0.015\n         assert np.max(rand) == 1\n         assert np.min(rand) == 0\n        r = K.random_binomial((10, 10), p)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - p) < 0.015\n        assert np.max(samples) == 1\n        assert np.min(samples) == 0\n     def test_truncated_normal(self):\n         mean = 0.\n         std = 1.\n         min_val = -2.\n         max_val = 2.\n        rand = K.eval(K.truncated_normal((300, 200),\n                                         mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n         assert np.abs(np.mean(rand) - mean) < 0.015\n         assert np.max(rand) <= max_val\n         assert np.min(rand) >= min_val", "fixed": " class TestBackend(object):\n         assert_allclose(y1, y2, atol=1e-05)\n     def test_random_normal(self):\n         for mean, std in [(0., 1.), (-10., 5.)]:\n            rand = K.eval(K.random_normal((200, 200),\n                                          mean=mean,\n                                          stddev=std))\n            assert rand.shape == (200, 200)\n             assert np.abs(np.mean(rand) - mean) < std * 0.015\n             assert np.abs(np.std(rand) - std) < std * 0.015\n     def test_random_uniform(self):\n         min_val = -1.\n         max_val = 1.\n        rand = K.eval(K.random_uniform((200, 200), min_val, max_val))\n        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand)) < 0.015\n         assert max_val - 0.015 < np.max(rand) <= max_val\n         assert min_val + 0.015 > np.min(rand) >= min_val\n     def test_random_binomial(self):\n         p = 0.5\n        rand = K.eval(K.random_binomial((200, 200), p))\n        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand) - p) < 0.015\n         assert np.max(rand) == 1\n         assert np.min(rand) == 0\n     def test_truncated_normal(self):\n         mean = 0.\n         std = 1.\n         min_val = -2.\n         max_val = 2.\n        rand = K.eval(K.truncated_normal((200, 200),\n                                         mean=mean,\n                                         stddev=std))\n        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand) - mean) < 0.015\n         assert np.max(rand) <= max_val\n         assert np.min(rand) >= min_val"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "keras_39", "problem": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    current < self.target):\n                 return\n             prev_total_width = self.total_width", "fixed": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    (self.target is not None and current < self.target)):\n                 return\n             prev_total_width = self.total_width"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n                self.wait += 1\n     def in_cooldown(self):\n         return self.cooldown_counter > 0", "fixed": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n     def in_cooldown(self):\n         return self.cooldown_counter > 0"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "keras_42", "problem": " class Model(Container):\n         if do_validation:\n             self._make_test_function()\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n        if val_gen and not validation_steps:\n            raise ValueError('When using a generator for validation data, '\n                             'you must specify a value for '\n                             '`validation_steps`.')\n         out_labels = self._get_deduped_metrics_names()", "fixed": " class Model(Container):\n         if do_validation:\n             self._make_test_function()\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if steps_per_epoch is None:\n            if is_sequence:\n                steps_per_epoch = len(generator)\n            else:\n                raise ValueError('`steps_per_epoch=None` is only valid for a'\n                                 ' generator based on the `keras.utils.Sequence`'\n                                 ' class. Please specify `steps_per_epoch` or use'\n                                 ' the `keras.utils.Sequence` class.')\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n        if (val_gen and not isinstance(validation_data, Sequence) and\n                not validation_steps):\n            raise ValueError('`validation_steps=None` is only valid for a'\n                             ' generator based on the `keras.utils.Sequence`'\n                             ' class. Please specify `validation_steps` or use'\n                             ' the `keras.utils.Sequence` class.')\n         out_labels = self._get_deduped_metrics_names()"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "pandas_131", "problem": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n            result = take_1d(result, self.orig.cat.codes)\n             index = self.orig.index\n         else:\n             index = self._parent.index", "fixed": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n             index = self.orig.index\n         else:\n             index = self._parent.index"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "ansible_4", "problem": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)\n     def _load_collections(self, attr, ds):", "fixed": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n     def _load_collections(self, attr, ds):"}
{"id": "pandas_86", "problem": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None", "fixed": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n    if columns is None:\n        raise TypeError(\"pivot() missing 1 required argument: 'columns'\")\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.non_trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n            if not self.trainable:\n                return self.cell.weights\n             return self.cell.non_trainable_weights\n         return []"}
{"id": "keras_1", "problem": " def update(x, new_x):\n         The variable `x` updated.\n    return tf_state_ops.assign(x, new_x)\n @symbolic", "fixed": " def update(x, new_x):\n         The variable `x` updated.\n    op = tf_state_ops.assign(x, new_x)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):", "fixed": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v.startswith('//') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n     return index", "fixed": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n    if copy:\n        index = index.copy()\n     return index"}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"name": "powerset.py", "problem": "def powerset(arr):\n    if arr:\nfirst, *rest = arr\n        rest_subsets = powerset(rest)\n        return [[first] + subset for subset in rest_subsets]\n    else:\n        return [[]]", "fixed": "def powerset(arr):\n    if arr:\n        first, *rest = arr\n        rest_subsets = powerset(rest)\n        return rest_subsets + [[first] + subset for subset in rest_subsets]\n    else:\n        return [[]]\n", "hint": "Power Set\nInput:\n    arr: A list", "input": [["a", "b", "c"]], "output": [[], ["c"], ["b"], ["b", "c"], ["a"], ["a", "c"], ["a", "b"], ["a", "b", "c"]]}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n                 bar_format = _unicode(bar_format)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         elif bar_format:", "fixed": " class tqdm(Comparable):\n             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n                 bar_format = _unicode(bar_format)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         elif bar_format:"}
{"id": "pandas_48", "problem": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except ValueError:\n                         result = result.reshape(1, -1)", "fixed": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except (ValueError, TypeError):\n                         result = result.reshape(1, -1)"}
{"id": "scrapy_7", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n     return urljoin(form.base_url, url)"}
{"id": "pandas_104", "problem": " class GroupBy(_GroupBy):\n            order = np.roll(list(range(result.index.nlevels)), -1)\n            result = result.reorder_levels(order)\n            result = result.reindex(q, level=-1)\n            hi = len(q) * self.ngroups\n            arr = np.arange(0, hi, self.ngroups)\n            arrays = []\n            for i in range(self.ngroups):\n                arr2 = arr + i\n                arrays.append(arr2)\n            indices = np.concatenate(arrays)\n            assert len(indices) == len(result)\n             return result.take(indices)\n     @Substitution(name=\"groupby\")", "fixed": " class GroupBy(_GroupBy):\n            order = list(range(1, result.index.nlevels)) + [0]\n            index_names = np.array(result.index.names)\n            result.index.names = np.arange(len(index_names))\n            result = result.reorder_levels(order)\n            result.index.names = index_names[order]\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()\n             return result.take(indices)\n     @Substitution(name=\"groupby\")"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensor,\n                                               computed_mask))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n     def __init__(self,", "fixed": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n        restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n     def __init__(self,"}
{"id": "pandas_160", "problem": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\"):\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif isinstance(o, np.ndarray):\n                     dtypes |= {o.dtype.name}", "fixed": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif hasattr(o, \"dtype\"):\n                     dtypes |= {o.dtype.name}"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "black_13", "problem": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token == 'def':\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            async_def = True\n                            async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],", "fixed": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token in ('def', 'for'):\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(left.values, right.values, obj=f\"{obj} category\")", "fixed": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(\n                left.values,\n                right.values,\n                obj=f\"{obj} category\",\n                check_category_order=check_category_order,\n            )"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = compat_urllib_request.urlopen(request).read()\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'\"fb_dtsg\":\"(.*?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<input value=\"(.*?)\" name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = compat_urllib_request.urlopen(check_req).read()\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:", "fixed": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = self._download_webpage(request, None,\n                note='Logging in', errnote='unable to fetch login page')\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'name=\"fb_dtsg\" value=\"(.+?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<button[^>]+value=\"(.*?)\"[^>]+name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = self._download_webpage(check_req, None,\n                note='Confirming login')\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "pandas_111", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    return self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    return self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "thefuck_2", "problem": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(':')\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]", "fixed": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(os.pathsep)\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]"}
{"id": "pandas_138", "problem": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n     if dtype is not None:", "fixed": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        x = x.astype(np.int64)\n     if dtype is not None:"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "PySnooper_3", "problem": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output_path, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)", "fixed": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n         else:\n             left, right = other, self", "fixed": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n        elif sort is False:\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n         else:\n             left, right = other, self"}
{"id": "pandas_134", "problem": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2030, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):", "fixed": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2200, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or include_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    include_none=include_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )", "fixed": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or not exclude_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "ansible_9", "problem": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, 1\n        pool_ids[pool_id] = str(quantity)\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]", "fixed": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, None\n        pool_ids[pool_id] = quantity\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n        if not self.trainable:\n            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_41", "problem": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")", "fixed": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n    def should_store(self, value):\n        return is_dtype_equal(self.dtype, value.dtype)\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")"}
{"id": "black_6", "problem": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the", "fixed": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))", "fixed": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))"}
{"id": "ansible_15", "problem": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state') and not needs_update('vrf'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':", "fixed": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':"}
{"id": "pandas_5", "problem": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)", "fixed": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)\ndef test_join_multi_return_indexers():\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)"}
{"id": "tornado_3", "problem": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            if self._instance_cache.get(self.io_loop) is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n            del self._instance_cache[self.io_loop]\n     def fetch(\n         self,", "fixed": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            if cached_val is not None and cached_val is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n     def fetch(\n         self,"}
{"name": "longest_common_subsequence.py", "problem": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b)\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "fixed": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b[1:])\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "hint": "Longest Common Subsequence\nCalculates the longest subsequence common to the two input strings. (A subsequence is any sequence of letters in the same order\nthey appear in the string, possibly skipping letters in between.)", "input": ["headache", "pentadactyl"], "output": "eadac"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:", "fixed": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                self.wait += 1\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:"}
{"id": "tornado_1", "problem": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__(", "fixed": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__("}
{"id": "pandas_167", "problem": " def convert_to_index_sliceable(obj, key):\n        if idx.is_all_dates:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):", "fixed": " def convert_to_index_sliceable(obj, key):\n        if idx._supports_partial_string_indexing:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):"}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "pandas_100", "problem": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        rs = rs.loc[~rs.index.duplicated()]\n        rs = rs.reindex_like(data)\n        if freq is None:\n            mask = isna(com.values_from_object(data))\n            np.putmask(rs.values, mask, np.nan)\n         return rs\n     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        if freq is not None:\n            rs = rs.loc[~rs.index.duplicated()]\n            rs = rs.reindex_like(data)\n         return rs\n     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return", "fixed": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'", "fixed": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,\n                                                        use_marker=False)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops, use_marker=False)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops, use_marker=False)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta, use_marker=False)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'"}
{"id": "black_23", "problem": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n   return None\nasync def coroutine(arg):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n  exec(\"new-style exec\", {}, {})\n   return None\nasync def coroutine(arg, exec=False):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "keras_36", "problem": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1, 1) + strides + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1, 1) + strides\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)", "fixed": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n            custom_generator(), good_batches + 1, 1,\n            workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n    workers = 4\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n            custom_generator(), good_batches * workers + 1, 1,\n            workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "pandas_44", "problem": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n     def get_loc(self, key, method=None, tolerance=None):", "fixed": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_timedelta64_dtype(dtype)\n     def get_loc(self, key, method=None, tolerance=None):"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.power(self._base,\n                       sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        return Series(res, index=ri, name=self._selection_name)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):", "fixed": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        result = Series(res, index=ri, name=self._selection_name)\n        return self._reindex_output(result, fill_value=0)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):"}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
{"id": "cookiecutter_4", "problem": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return", "fixed": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        try:\n            run_hook('pre_gen_project', project_dir, context)\n        except FailedHookException:\n            shutil.rmtree(project_dir, ignore_errors=True)\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return"}
{"id": "tqdm_4", "problem": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale", "fixed": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            if total:\n                total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale"}
{"id": "matplotlib_7", "problem": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if hasattr(intensity, 'mask'):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]", "fixed": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if np.ma.is_masked(intensity):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]"}
{"id": "keras_1", "problem": " def update_sub(x, decrement):\n         The variable `x` updated.\n    return tf_state_ops.assign_sub(x, decrement)\n @symbolic", "fixed": " def update_sub(x, decrement):\n         The variable `x` updated.\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(self, output):\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding", "fixed": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(\n        self, output: FrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> FrameOrSeries:\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"name": "topological_ordering.py", "problem": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.outgoing_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "fixed": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.incoming_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "hint": "Topological Sort\nInput:\n    nodes: A list of directed graph nodes", "input": [], "output": ""}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"name": "get_factors.py", "problem": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return []", "fixed": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return [n]\n", "hint": "Prime Factorization\nFactors an int using naive trial division.\nInput:", "input": [100], "output": [2, 2, 5, 5]}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +", "fixed": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):", "fixed": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):"}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n     }, {\n         'url': 'https://www.facebook.com/video.php?v=10204634152394104',\n         'only_matching': True,", "fixed": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n    }, {\n        'note': 'Video without discernible title',\n        'url': 'https://www.facebook.com/video.php?v=274175099429670',\n        'info_dict': {\n            'id': '274175099429670',\n            'ext': 'mp4',\n            'title': 'Facebook video\n        }\n     }, {\n         'url': 'https://www.facebook.com/video.php?v=10204634152394104',\n         'only_matching': True,"}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n            if not isinstance(initial_state, list):\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, the state '\n                    'should be a list containing the states of the underlying RNNs. '\n                    'Found: ' + str(initial_state))\n             forward_state = initial_state[:len(initial_state) // 2]\n             backward_state = initial_state[len(initial_state) // 2:]\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)", "fixed": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n             forward_state = initial_state[:len(initial_state) // 2]\n             backward_state = initial_state[len(initial_state) // 2:]\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)"}
{"id": "luigi_1", "problem": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics = self._scheduler._state._metrics_collector.generate_latest()\n         if metrics:\n            metrics.configure_http_handler(self)\n             self.write(metrics)", "fixed": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n         if metrics:\n            metrics_collector.configure_http_handler(self)\n             self.write(metrics)"}
{"id": "pandas_16", "problem": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other)\n         result._freq = new_freq\n         return result", "fixed": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other, result)\n         result._freq = new_freq\n         return result"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "ansible_9", "problem": " class Rhsm(RegistrationBase):\n         for pool_id, quantity in sorted(pool_ids.items()):\n             if pool_id in available_pool_ids:\n                args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]\n                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n             else:\n                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)", "fixed": " class Rhsm(RegistrationBase):\n         for pool_id, quantity in sorted(pool_ids.items()):\n             if pool_id in available_pool_ids:\n                args = [SUBMAN_CMD, 'attach', '--pool', pool_id]\n                if quantity is not None:\n                    args.extend(['--quantity', to_native(quantity)])\n                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n             else:\n                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "scrapy_16", "problem": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)\n def parse_url(url, encoding=None):", "fixed": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\ndef _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n    if not six.PY2:\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    if six.PY3:\n        return unquote_to_bytes(path)\n    else:\n        return unquote(path)\n def parse_url(url, encoding=None):"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n        body_param_media_types = [\n            getattr(f.schema, \"media_type\")\n            for f in flat_dependant.body_params\n            if isinstance(f.schema, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodySchema_kwargs[\"media_type\"] = body_param_media_types[0]\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "black_15", "problem": " class Line:\n         return bool(self.leaves or self.comments)\nclass UnformattedLines(Line):\n        The `preformatted` argument is ignored.\n        Keeps track of indentation `depth`, which is useful when the user\n        says `\n        `depth` is not used for indentation in this case.\n        raise NotImplementedError(\"Unformatted lines don't store comments separately.\")\n    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class Line:\n         return bool(self.leaves or self.comments)\n @dataclass\n class EmptyLineTracker:"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "ansible_16", "problem": " class LinuxHardware(Hardware):\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):\n             i = processor_occurence", "fixed": " class LinuxHardware(Hardware):\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n             i = processor_occurence"}
{"id": "keras_11", "problem": " def iter_sequence_infinite(seq):\n     while True:\n         for item in seq:\n             yield item", "fixed": " def iter_sequence_infinite(seq):\n     while True:\n         for item in seq:\n             yield item\ndef is_sequence(seq):\n    return (getattr(seq, 'use_sequence_api', False)\n            or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string in endwith:\n                         break\n                    elif string == ')':\n                         tokens.restore_last_token()\n                         break\n                    if string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, [','])\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':", "fixed": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            tokens.restore_last_token()\n                         break\n                    elif inside_merge and string in ['/', ',']:\n                         tokens.restore_last_token()\n                         break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':"}
{"id": "cookiecutter_4", "problem": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.", "fixed": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.\n    Raised when a hook script fails"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'", "fixed": " def fit_generator(model,\n    val_use_sequence_api = is_sequence(validation_data)\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"name": "next_permutation.py", "problem": "def next_permutation(perm):\n    for i in range(len(perm) - 2, -1, -1):\n        if perm[i] < perm[i + 1]:\n            for j in range(len(perm) - 1, i, -1):\n                if perm[j] < perm[i]:\n                    next_perm = list(perm)\n                    next_perm[i], next_perm[j] = perm[j], perm[i]\n                    next_perm[i + 1:] = reversed(next_perm[i + 1:])\n                    return next_perm", "fixed": "def next_permutation(perm):\n    for i in range(len(perm) - 2, -1, -1):\n        if perm[i] < perm[i + 1]:\n            for j in range(len(perm) - 1, i, -1):\n                if perm[i] < perm[j]:\n                    next_perm = list(perm)\n                    next_perm[i], next_perm[j] = perm[j], perm[i]\n                    next_perm[i + 1:] = reversed(next_perm[i + 1:])\n                    return next_perm\n", "hint": "Next Permutation\nnext-perm\nInput:", "input": [[3, 2, 4, 1]], "output": [3, 4, 1, 2]}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing"}
{"id": "keras_31", "problem": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length))\n    input_length = tf.to_int32(tf.squeeze(input_length))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())", "fixed": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())"}
{"id": "scrapy_18", "problem": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:", "fixed": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"name": "sqrt.py", "problem": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx", "fixed": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx ** 2) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx\n", "hint": "Square Root\nNewton-Raphson method implementation.\nInput:", "input": [2, 0.01], "output": 1.4166666666666665}
{"name": "lis.py", "problem": "def lis(arr):\n    ends = {}\n    longest = 0\n    for i, val in enumerate(arr):\n        prefix_lengths = [j for j in range(1, longest + 1) if arr[ends[j]] < val]\n        length = max(prefix_lengths) if prefix_lengths else 0\n        if length == longest or val < arr[ends[length + 1]]:\n            ends[length + 1] = i\n            longest = length + 1\n    return longest", "fixed": "def lis(arr):\n    ends = {}\n    longest = 0\n    for i, val in enumerate(arr):\n        prefix_lengths = [j for j in range(1, longest + 1) if arr[ends[j]] < val]\n        length = max(prefix_lengths) if prefix_lengths else 0\n        if length == longest or val < arr[ends[length + 1]]:\n            ends[length + 1] = i\n            longest = max(longest, length + 1)\n    return longest\n", "hint": "Longest Increasing Subsequence\nlongest-increasing-subsequence\nInput:", "input": [[4, 2, 1]], "output": 1}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading"}
{"id": "keras_20", "problem": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support():\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'", "fixed": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'"}
{"id": "sanic_3", "problem": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:", "fixed": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = host or self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:"}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if not self.get_visible():\n             return Bbox.unit()\n        if dpi is not None:\n            dpi_orig = self.figure.dpi\n            self.figure.dpi = dpi\n         if self.get_text() == '':\n            tx, ty = self._get_xy_display()\n            return Bbox.from_bounds(tx, ty, 0, 0)\n         if renderer is not None:\n             self._renderer = renderer", "fixed": " class Text(Artist):\n         if not self.get_visible():\n             return Bbox.unit()\n        if dpi is None:\n            dpi = self.figure.dpi\n         if self.get_text() == '':\n            with cbook._setattr_cm(self.figure, dpi=dpi):\n                tx, ty = self._get_xy_display()\n                return Bbox.from_bounds(tx, ty, 0, 0)\n         if renderer is not None:\n             self._renderer = renderer"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$", "fixed": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n            (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h)\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w)\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n        config.pop('dilation_rate')\n         config['output_padding'] = self.output_padding\n         return config", "fixed": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n         config['output_padding'] = self.output_padding\n         return config"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_3", "problem": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        if exclude_unset and isinstance(response_content, BaseModel):\n            if PYDANTIC_1:\n                response_content = response_content.dict(exclude_unset=exclude_unset)\n            else:\n                response_content = response_content.dict(\n                    skip_defaults=exclude_unset\n                )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:", "fixed": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n        )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj +\n               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n         else:\n             output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output", "fixed": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n         else:\n             output = None\n    else:\n        output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output"}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "pandas_41", "problem": " class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n         ).reshape(i8values.shape)\n         return np.atleast_2d(result)\n    def should_store(self, value) -> bool:\n        return is_datetime64_dtype(value.dtype)\n     def set(self, locs, values):\n         values = conversion.ensure_datetime64ns(values, copy=False)", "fixed": " class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n         ).reshape(i8values.shape)\n         return np.atleast_2d(result)\n     def set(self, locs, values):\n         values = conversion.ensure_datetime64ns(values, copy=False)"}
{"id": "pandas_157", "problem": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetimelike(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):"}
{"id": "pandas_138", "problem": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)", "fixed": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role related operations.\",\n         )", "fixed": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n         )"}
{"id": "luigi_3", "problem": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except ValueError:\n            return literal_eval(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))\n class NumericalParameter(Parameter):"}
{"id": "pandas_80", "problem": " def make_data():\n @pytest.fixture\n def dtype():\n    return pd.BooleanDtype()\n @pytest.fixture", "fixed": " def make_data():\n @pytest.fixture\n def dtype():\n    return BooleanDtype()\n @pytest.fixture"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value))\nelse:\n                 self.reset_global()", "fixed": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value, task_name=task_name))\nelse:\n                 self.reset_global()"}
{"id": "scrapy_23", "problem": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)", "fixed": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)"}
{"id": "spacy_7", "problem": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n     return result", "fixed": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n     return result"}
{"id": "pandas_127", "problem": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n         rs = rs.reindex_like(data)\n         if freq is None:\n             mask = isna(com.values_from_object(data))", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        rs = rs.loc[~rs.index.duplicated()]\n         rs = rs.reindex_like(data)\n         if freq is None:\n             mask = isna(com.values_from_object(data))"}
{"id": "keras_8", "problem": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:", "fixed": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    raise LookupError\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "scrapy_20", "problem": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)", "fixed": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)"}
{"id": "pandas_146", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False\n     def identical(self, other):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        return array_equivalent(\n            com.values_from_object(self), com.values_from_object(other)\n        )\n     def identical(self, other):"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,", "fixed": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,"}
{"id": "luigi_23", "problem": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler()\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)", "fixed": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)"}
{"id": "pandas_64", "problem": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df\n         self.columns = self.df.columns\n         self.float_format = float_format", "fixed": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df.reindex(columns=cols)\n         self.columns = self.df.columns\n         self.float_format = float_format"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:", "fixed": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:"}
{"name": "gcd.py", "problem": "def gcd(a, b):\n    if b == 0:\n        return a\n    else:\n        return gcd(a % b, b)", "fixed": "def gcd(a, b):\n    if b == 0:\n        return a\n    else:\n        return gcd(b, a % b)", "hint": "Input:\n    a: A nonnegative int\n    b: A nonnegative int", "input": [17, 0], "output": 17}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_34", "problem": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=\"infer\",\n             nonexistent=\"shift_forward\",\n         )", "fixed": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=True,\n             nonexistent=\"shift_forward\",\n         )"}
{"id": "pandas_131", "problem": " class CombinedDatetimelikeProperties(\n         orig = data if is_categorical_dtype(data) else None\n         if orig is not None:\n            data = Series(orig.values.categories, name=orig.name, copy=False)\n         if is_datetime64_dtype(data.dtype):\n             return DatetimeProperties(data, orig)", "fixed": " class CombinedDatetimelikeProperties(\n         orig = data if is_categorical_dtype(data) else None\n         if orig is not None:\n            data = Series(\n                orig.array,\n                name=orig.name,\n                copy=False,\n                dtype=orig.values.categories.dtype,\n            )\n         if is_datetime64_dtype(data.dtype):\n             return DatetimeProperties(data, orig)"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n             return [output_shape] + state_shape + copy.copy(state_shape)\n         return output_shape\n     def call(self, inputs, training=None, mask=None, initial_state=None):\n         kwargs = {}\n         if has_arg(self.layer.call, 'training'):", "fixed": " class Bidirectional(Wrapper):\n             return [output_shape] + state_shape + copy.copy(state_shape)\n         return output_shape\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        if isinstance(inputs, list):\n            if len(inputs) > 1:\n                initial_state = inputs[1:]\n            inputs = inputs[0]\n        if initial_state is None:\n            return super(Bidirectional, self).__call__(inputs, **kwargs)\n        if isinstance(initial_state, tuple):\n            initial_state = list(initial_state)\n        elif not isinstance(initial_state, list):\n            initial_state = [initial_state]\n        num_states = len(initial_state)\n        if num_states % 2 > 0:\n            raise ValueError(\n                'When passing `initial_state` to a Bidirectional RNN, the state '\n                'should be a list containing the states of the underlying RNNs. '\n                'Found: ' + str(initial_state))\n        kwargs['initial_state'] = initial_state\n        additional_inputs = initial_state\n        additional_specs = [InputSpec(shape=K.int_shape(state))\n                            for state in initial_state]\n        self.forward_layer.state_spec = additional_specs[:num_states // 2]\n        self.backward_layer.state_spec = additional_specs[num_states // 2:]\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n        for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                raise ValueError('The initial state of a Bidirectional'\n                                 ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n        if is_keras_tensor:\n            full_input = [inputs] + additional_inputs\n            full_input_spec = self.input_spec + additional_specs\n            original_input_spec = self.input_spec\n            self.input_spec = full_input_spec\n            output = super(Bidirectional, self).__call__(full_input, **kwargs)\n            self.input_spec = original_input_spec\n            return output\n        else:\n            return super(Bidirectional, self).__call__(inputs, **kwargs)\n     def call(self, inputs, training=None, mask=None, initial_state=None):\n         kwargs = {}\n         if has_arg(self.layer.call, 'training'):"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "thefuck_22", "problem": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        commands = self._remove_duplicates(self._commands)\n        self._cached = [self._cached[0]] + sorted(\n            commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)", "fixed": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        if self._cached:\n            commands = self._remove_duplicates(self._commands)\n            self._cached = [self._cached[0]] + sorted(\n                commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n        elif isinstance(other, int):\n            other = Period(other, freq=self.freq)\n            result = ordinal_op(other.ordinal)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)", "fixed": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)"}
{"id": "pandas_167", "problem": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n     _tz = None\n     _freq = None", "fixed": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n    _supports_partial_string_indexing = True\n     _tz = None\n     _freq = None"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "keras_26", "problem": " def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:", "fixed": " def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [\n                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n                             new_states[i], states[i]) for i in range(len(states))\n                ]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:"}
{"id": "pandas_166", "problem": " class DataFrame(NDFrame):\n             if can_concat:\n                 if how == \"left\":\n                    res = concat(frames, axis=1, join=\"outer\", verify_integrity=True)\n                     return res.reindex(self.index, copy=False)\n                 else:\n                    return concat(frames, axis=1, join=how, verify_integrity=True)\n             joined = frames[0]", "fixed": " class DataFrame(NDFrame):\n             if can_concat:\n                 if how == \"left\":\n                    res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                     return res.reindex(self.index, copy=False)\n                 else:\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )\n             joined = frames[0]"}
{"id": "pandas_41", "problem": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a a maybe different typed block.\n         Parameters\n         ----------", "fixed": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         Parameters\n         ----------"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "matplotlib_17", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "ansible_13", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)"}
{"id": "scrapy_28", "problem": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod", "fixed": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        return Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n     def _apply_to_column_groupbys(self, func):", "fixed": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        result = Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n        return self._reindex_output(result, fill_value=0)\n     def _apply_to_column_groupbys(self, func):"}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        bbox, info, descent = self._get_layout(self._renderer)\n        x, y = self.get_unitless_position()\n        x, y = self.get_transform().transform((x, y))\n        bbox = bbox.translated(x, y)\n        if dpi is not None:\n            self.figure.dpi = dpi_orig\n        return bbox\n     def set_backgroundcolor(self, color):", "fixed": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        with cbook._setattr_cm(self.figure, dpi=dpi):\n            bbox, info, descent = self._get_layout(self._renderer)\n            x, y = self.get_unitless_position()\n            x, y = self.get_transform().transform((x, y))\n            bbox = bbox.translated(x, y)\n            return bbox\n     def set_backgroundcolor(self, color):"}
{"id": "thefuck_30", "problem": " def _search(stderr):\n def match(command, settings):\n    return 'EDITOR' in os.environ and _search(command.stderr)\n def get_new_command(command, settings):", "fixed": " def _search(stderr):\n def match(command, settings):\n    if 'EDITOR' not in os.environ:\n        return False\n    m = _search(command.stderr)\n    return m and os.path.isfile(m.group('file'))\n def get_new_command(command, settings):"}
{"id": "pandas_79", "problem": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError):\n                 return False", "fixed": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError, InvalidIndexError):\n                 return False"}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"id": "black_22", "problem": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:", "fixed": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n    return split_wrapper\n@dont_increase_indentation\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"name": "shortest_paths.py", "problem": "def shortest_paths(source, weight_by_edge):\n    weight_by_node = {\n        v: float('inf') for u, v in weight_by_edge\n    }\n    weight_by_node[source] = 0\n    for i in range(len(weight_by_node) - 1):\n        for (u, v), weight in weight_by_edge.items():\n            weight_by_edge[u, v] = min(\n                weight_by_node[u] + weight,\n                weight_by_node[v]\n            )\n    return weight_by_node", "fixed": "def shortest_paths(source, weight_by_edge):\n    weight_by_node = {\n        v: float('inf') for u, v in weight_by_edge\n    }\n    weight_by_node[source] = 0\n    for i in range(len(weight_by_node) - 1):\n        for (u, v), weight in weight_by_edge.items():\n            weight_by_node[v] = min(\n                weight_by_node[u] + weight,\n                weight_by_node[v]\n            )\n    return weight_by_node", "hint": "Minimum-Weight Paths\nbellman-ford\nBellman-Ford algorithm implementation", "input": ["witch", "sandwich"], "output": "2"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"id": "thefuck_20", "problem": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])\n def side_effect(old_cmd, command):", "fixed": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))\n def side_effect(old_cmd, command):"}
{"id": "black_23", "problem": " def func_no_args():\n         print(i)\n         continue\n     return None\nasync def coroutine(arg):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n         print(i)\n         continue\n    exec(\"new-style exec\", {}, {})\n     return None\nasync def coroutine(arg, exec=False):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "youtube-dl_32", "problem": " class NPOIE(InfoExtractor):\n             'http://e.omroep.nl/metadata/aflevering/%s' % video_id,\n             video_id,\n            transform_source=lambda j: re.sub(r'parseMetadata\\((.*?)\\);\\n//.*$', r'\\1', j)\n         )\n         token_page = self._download_webpage(\n             'http://ida.omroep.nl/npoplayer/i.js',", "fixed": " class NPOIE(InfoExtractor):\n             'http://e.omroep.nl/metadata/aflevering/%s' % video_id,\n             video_id,\n            transform_source=strip_jsonp,\n         )\n         token_page = self._download_webpage(\n             'http://ida.omroep.nl/npoplayer/i.js',"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "black_22", "problem": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36=py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")", "fixed": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "tqdm_9", "problem": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix", "fixed": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix"}
{"id": "keras_35", "problem": " class DirectoryIterator(Iterator):\n             fname = self.filenames[j]\n             img = load_img(os.path.join(self.directory, fname),\n                            grayscale=grayscale,\n                           target_size=self.target_size,\n                            interpolation=self.interpolation)\n             x = img_to_array(img, data_format=self.data_format)\n             x = self.image_data_generator.random_transform(x)\n             x = self.image_data_generator.standardize(x)", "fixed": " class DirectoryIterator(Iterator):\n             fname = self.filenames[j]\n             img = load_img(os.path.join(self.directory, fname),\n                            grayscale=grayscale,\n                           target_size=None,\n                            interpolation=self.interpolation)\n            if self.image_data_generator.preprocessing_function:\n                img = self.image_data_generator.preprocessing_function(img)\n            if self.target_size is not None:\n                width_height_tuple = (self.target_size[1], self.target_size[0])\n                if img.size != width_height_tuple:\n                    if self.interpolation not in _PIL_INTERPOLATION_METHODS:\n                        raise ValueError(\n                            'Invalid interpolation method {} specified. Supported '\n                            'methods are {}'.format(\n                                self.interpolation,\n                                \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n                    resample = _PIL_INTERPOLATION_METHODS[self.interpolation]\n                    img = img.resize(width_height_tuple, resample)\n             x = img_to_array(img, data_format=self.data_format)\n             x = self.image_data_generator.random_transform(x)\n             x = self.image_data_generator.standardize(x)"}
{"id": "ansible_1", "problem": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:", "fixed": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                            if not os.path.isfile(os.path.join(to_text(b_search_path, errors='surrogate_or_strict'), 'MANIFEST.json')):\n                                raise AnsibleError(\n                                    message=\"Collection %s does not appear to have a MANIFEST.json. \" % collection_name +\n                                            \"A MANIFEST.json is expected if the collection has been built and installed via ansible-galaxy.\"\n                                )\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:"}
{"id": "pandas_21", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        if isinstance(key, list):\n            return self.loc[key]\n        return self.reindex(key)\n     def _get_values_tuple(self, key):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        return self.loc[key]\n     def _get_values_tuple(self, key):"}
{"id": "keras_20", "problem": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n         A tensor.", "fixed": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n         A tensor."}
