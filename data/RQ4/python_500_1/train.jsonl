{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'", "fixed": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,\n                                                        use_marker=False)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops, use_marker=False)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops, use_marker=False)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta, use_marker=False)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'"}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n         if fill_value is None:", "fixed": " class DataFrame(NDFrame):\n    def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n         if fill_value is None:"}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "pandas_105", "problem": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested", "fixed": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n    @skip_nested\n    def test_transpose(self, data):\n        super().test_transpose(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.resize(ymin, x.shape)\n        ymax = np.resize(ymax, x.shape)\n        verts = [((thisx, thisymin), (thisx, thisymax))\n                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.ravel(ymin)\n        ymax = np.ravel(ymax)\n        masked_verts = np.ma.empty((len(x), 2, 2))\n        masked_verts[:, 0, 0] = x\n        masked_verts[:, 0, 1] = ymin\n        masked_verts[:, 1, 0] = x\n        masked_verts[:, 1, 1] = ymax\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "pandas_118", "problem": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(np.ravel(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\"", "fixed": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(com.flatten(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\""}
{"id": "pandas_131", "problem": " class CombinedDatetimelikeProperties(\n         orig = data if is_categorical_dtype(data) else None\n         if orig is not None:\n            data = Series(orig.values.categories, name=orig.name, copy=False)\n         if is_datetime64_dtype(data.dtype):\n             return DatetimeProperties(data, orig)", "fixed": " class CombinedDatetimelikeProperties(\n         orig = data if is_categorical_dtype(data) else None\n         if orig is not None:\n            data = Series(\n                orig.array,\n                name=orig.name,\n                copy=False,\n                dtype=orig.values.categories.dtype,\n            )\n         if is_datetime64_dtype(data.dtype):\n             return DatetimeProperties(data, orig)"}
{"name": "depth_first_search.py", "problem": "def depth_first_search(startnode, goalnode):\n    nodesvisited = set()\n    def search_from(node):\n        if node in nodesvisited:\n            return False\n        elif node is goalnode:\n            return True\n        else:\n            return any(\n                search_from(nextnode) for nextnode in node.successors\n            )\n    return search_from(startnode)", "fixed": "def depth_first_search(startnode, goalnode):\n    nodesvisited = set()\n    def search_from(node):\n        if node in nodesvisited:\n            return False\n        elif node is goalnode:\n            return True\n        else:\n            nodesvisited.add(node)\n            return any(\n                search_from(nextnode) for nextnode in node.successors\n            )\n    return search_from(startnode)", "hint": "Depth-first Search\nInput:\n    startnode: A digraph node", "input": "", "output": ""}
{"id": "black_6", "problem": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n     stashed = None\n     async_def = False", "fixed": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n    async_is_reserved_keyword = config.async_is_reserved_keyword\n     stashed = None\n     async_def = False"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n             )\nelse:\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,", "fixed": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n                exclude_none=exclude_none,\n                exclude_defaults=exclude_defaults,\n             )\nelse:\n            if exclude_defaults:\n                raise ValueError(\"Cannot use exclude_defaults\")\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):", "fixed": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other, sort=sort)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):"}
{"id": "pandas_155", "problem": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            return self.obj.index\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "pandas_22", "problem": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type:\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")"}
{"id": "pandas_13", "problem": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=False)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n     else:\n         return False"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s was created successfully\" % obj_name)\n     def execute_info(self):", "fixed": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))\n     def execute_info(self):"}
{"id": "spacy_8", "problem": " class Errors(object):\n\"details: https://spacy.io/api/lemmatizer\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n @add_codes", "fixed": " class Errors(object):\n\"details: https://spacy.io/api/lemmatizer\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n    E175 = (\"Can't remove rule for unknown match pattern ID: {key}\")\n @add_codes"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n         else:\n             left, right = other, self", "fixed": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n        elif sort is False:\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n         else:\n             left, right = other, self"}
{"id": "scrapy_34", "problem": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = {}\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)", "fixed": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)"}
{"id": "fastapi_4", "problem": " def get_openapi_path(\n             operation_parameters = get_openapi_operation_parameters(all_route_params)\n             parameters.extend(operation_parameters)\n             if parameters:\n                operation[\"parameters\"] = parameters\n             if method in METHODS_WITH_BODY:\n                 request_body_oai = get_openapi_operation_request_body(\n                     body_field=route.body_field, model_name_map=model_name_map", "fixed": " def get_openapi_path(\n             operation_parameters = get_openapi_operation_parameters(all_route_params)\n             parameters.extend(operation_parameters)\n             if parameters:\n                operation[\"parameters\"] = list(\n                    {param[\"name\"]: param for param in parameters}.values()\n                )\n             if method in METHODS_WITH_BODY:\n                 request_body_oai = get_openapi_operation_request_body(\n                     body_field=route.body_field, model_name_map=model_name_map"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "tqdm_9", "problem": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable)\n     def __iter__(self):", "fixed": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable) if self.iterable else self.total\n     def __iter__(self):"}
{"id": "pandas_86", "problem": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None", "fixed": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n    if columns is None:\n        raise TypeError(\"pivot() missing 1 required argument: 'columns'\")\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None"}
{"name": "wrap.py", "problem": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    return lines", "fixed": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    lines.append(text)\n    return lines", "hint": "Wrap Text\nGiven a long string and a column width, break the string on spaces into a list of lines such that each line is no longer than the column width.\nInput:", "input": [], "output": ""}
{"id": "black_2", "problem": " def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n     while container is not None and container.type != token.ENDMARKER:\n        is_fmt_on = False\n        for comment in list_comments(container.prefix, is_endmarker=False):\n            if comment.value in FMT_ON:\n                is_fmt_on = True\n            elif comment.value in FMT_OFF:\n                is_fmt_on = False\n        if is_fmt_on:\n             return\n        yield container\n        container = container.next_sibling\n def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:", "fixed": " def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n     while container is not None and container.type != token.ENDMARKER:\n        if fmt_on(container):\n             return\n        if contains_fmt_on_at_column(container, leaf.column):\n            for child in container.children:\n                if contains_fmt_on_at_column(child, leaf.column):\n                    return\n                yield child\n        else:\n            yield container\n            container = container.next_sibling\ndef fmt_on(container: LN) -> bool:\n    is_fmt_on = False\n    for comment in list_comments(container.prefix, is_endmarker=False):\n        if comment.value in FMT_ON:\n            is_fmt_on = True\n        elif comment.value in FMT_OFF:\n            is_fmt_on = False\n    return is_fmt_on\ndef contains_fmt_on_at_column(container: LN, column: int) -> bool:\n    for child in container.children:\n        if (\n            isinstance(child, Node)\n            and first_leaf_column(child) == column\n            or isinstance(child, Leaf)\n            and child.column == column\n        ):\n            if fmt_on(child):\n                return True\n    return False\ndef first_leaf_column(node: Node) -> Optional[int]:\n    for child in node.children:\n        if isinstance(child, Leaf):\n            return child.column\n    return None\n def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:"}
{"id": "youtube-dl_16", "problem": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:", "fixed": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with open(dfxp_file, 'rb') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "thefuck_22", "problem": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        commands = self._remove_duplicates(self._commands)\n        self._cached = [self._cached[0]] + sorted(\n            commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)", "fixed": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        if self._cached:\n            commands = self._remove_duplicates(self._commands)\n            self._cached = [self._cached[0]] + sorted(\n                commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "cookiecutter_3", "problem": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default\n     )\n     return choice_map[user_choice]", "fixed": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default, show_choices=False\n     )\n     return choice_map[user_choice]"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "pandas_36", "problem": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj_old(values.ravel())\n            result[:] = vec.reshape(shape)\n    elif is_datetime64_dtype(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:", "fixed": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n    elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "pandas_141", "problem": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):", "fixed": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            reverse = self._range[::-1]\n            start, stop, step = reverse.start, reverse.stop, reverse.step\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):"}
{"id": "youtube-dl_26", "problem": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "black_23", "problem": " def func_no_args():\n         print(i)\n         continue\n     return None\nasync def coroutine(arg):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n         print(i)\n         continue\n    exec(\"new-style exec\", {}, {})\n     return None\nasync def coroutine(arg, exec=False):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "scrapy_13", "problem": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 0\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'", "fixed": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 90\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:", "fixed": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                self.wait += 1\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "scrapy_3", "problem": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['location'])\n         redirected_url = urljoin(request.url, location)", "fixed": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n         redirected_url = urljoin(request.url, location)"}
{"name": "pascal.py", "problem": "def pascal(n):\n    rows = [[1]]\n    for r in range(1, n):\n        row = []\n        for c in range(0, r):\n            upleft = rows[r - 1][c - 1] if c > 0 else 0\n            upright = rows[r - 1][c] if c < r else 0\n            row.append(upleft + upright)\n        rows.append(row)\n    return rows", "fixed": "def pascal(n):\n    rows = [[1]]\n    for r in range(1, n):\n        row = []\n        for c in range(0, r + 1):\n            upleft = rows[r - 1][c - 1] if c > 0 else 0\n            upright = rows[r - 1][c] if c < r else 0\n            row.append(upleft + upright)\n        rows.append(row)\n    return rows", "hint": "Pascal's Triangle\npascal\nInput:", "input": [3], "output": [[1], [1, 1], [1, 2, 1]]}
{"id": "luigi_19", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable():\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "black_23", "problem": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n   return None\nasync def coroutine(arg):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n  exec(\"new-style exec\", {}, {})\n   return None\nasync def coroutine(arg, exec=False):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"name": "breadth_first_search.py", "problem": "from collections import deque as Queue\ndef breadth_first_search(startnode, goalnode):\n    queue = Queue()\n    queue.append(startnode)\n    nodesseen = set()\n    nodesseen.add(startnode)\n    while True:\n        node = queue.popleft()\n        if node is goalnode:\n            return True\n        else:\n            queue.extend(node for node in node.successors if node not in nodesseen)\n            nodesseen.update(node.successors)\n    return False", "fixed": "from collections import deque as Queue\ndef breadth_first_search(startnode, goalnode):\n    queue = Queue()\n    queue.append(startnode)\n    nodesseen = set()\n    nodesseen.add(startnode)\n    while queue:\n        node = queue.popleft()\n        if node is goalnode:\n            return True\n        else:\n            queue.extend(node for node in node.successors if node not in nodesseen)\n            nodesseen.update(node.successors)\n    return False\n", "hint": "Breadth-First Search\nInput:\n    startnode: A digraph node", "input": [], "output": ""}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if is_sequence:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'", "fixed": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if use_sequence_api:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "keras_43", "problem": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1", "fixed": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n    if input_shape and input_shape[-1] == 1:\n        input_shape = tuple(input_shape[:-1])\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)\n         yield current_line", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        yield current_line\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n    if current_line:\n         yield current_line"}
{"id": "black_18", "problem": " def format_str(\n     return dst_contents\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,", "fixed": " def format_str(\n     return dst_contents\ndef prepare_input(src: bytes) -> Tuple[str, str, str]:\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n         return name, value\n     @memoize\n    @cache('.bashrc', '.bash_profile')\n     def get_aliases(self):\n        proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n        return dict(\n                self._parse_alias(alias)\n                for alias in proc.stdout.read().decode('utf-8').split('\\n')\n                if alias and '=' in alias)\n     def _get_history_file_name(self):\n         return os.environ.get(\"HISTFILE\",", "fixed": " class Bash(Generic):\n         return name, value\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n        return dict(self._parse_alias(alias)\n                    for alias in raw_aliases if alias and '=' in alias)\n     def _get_history_file_name(self):\n         return os.environ.get(\"HISTFILE\","}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_45", "problem": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)", "fixed": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n    elif isinstance(data, abc.Set):\n        raise TypeError(\"Set type is unordered\")\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)"}
{"id": "pandas_55", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            lower = self._take_with_is_copy(item, axis=self._info_axis_number)\n         return lower\n     def _box_item_values(self, key, values):", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item: int):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            return self._ixs(item, axis=1)\n         return lower\n     def _box_item_values(self, key, values):"}
{"id": "keras_37", "problem": " class RNN(Layer):\n             self._num_constants = len(constants)\n             additional_specs += self.constants_spec\n        is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n         for tensor in additional_inputs:\n            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n                 raise ValueError('The initial state or constants of an RNN'\n                                  ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors')\n         if is_keras_tensor:", "fixed": " class RNN(Layer):\n             self._num_constants = len(constants)\n             additional_specs += self.constants_spec\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n         for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                 raise ValueError('The initial state or constants of an RNN'\n                                  ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n         if is_keras_tensor:"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "black_1", "problem": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        executor.shutdown()\n async def schedule_formatting(", "fixed": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        if executor is not None:\n            executor.shutdown()\n async def schedule_formatting("}
{"id": "thefuck_23", "problem": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with shelve.open(cache_path) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:", "fixed": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with closing(shelve.open(cache_path)) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:"}
{"id": "keras_34", "problem": " class Sequence(object):\n _SHARED_SEQUENCES = {}", "fixed": " class Sequence(object):\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item\n _SHARED_SEQUENCES = {}"}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is", "fixed": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "pandas_110", "problem": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function).", "fixed": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        if kind == \"loc\":\n            return label\n        return super()._maybe_cast_slice_bound(label, side, kind)\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function)."}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "black_9", "problem": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:", "fixed": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:"}
{"id": "luigi_23", "problem": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler()\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)", "fixed": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)"}
{"id": "black_15", "problem": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break", "fixed": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n        if parent.type == syms.file_input:\n            break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "thefuck_32", "problem": " def match(command, settings):\n    return 'ls' in command.script and not ('ls -' in command.script)\n def get_new_command(command, settings):", "fixed": " def match(command, settings):\n    return (command.script == 'ls'\n            or command.script.startswith('ls ')\n            and not ('ls -' in command.script))\n def get_new_command(command, settings):"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "black_22", "problem": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:", "fixed": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n    return split_wrapper\n@dont_increase_indentation\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:"}
{"id": "keras_40", "problem": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            output_dim = self.cell.state_size[0]\n         else:\n            output_dim = self.cell.state_size\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            state_size = self.cell.state_size\n         else:\n            state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":", "fixed": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":"}
{"name": "kth.py", "problem": "def kth(arr, k):\n    pivot = arr[0]\n    below = [x for x in arr if x < pivot]\n    above = [x for x in arr if x > pivot]\n    num_less = len(below)\n    num_lessoreq = len(arr) - len(above)\n    if k < num_less:\n        return kth(below, k)\n    elif k >= num_lessoreq:\n        return kth(above, k)\n    else:\n        return pivot", "fixed": "def kth(arr, k):\n    pivot = arr[0]\n    below = [x for x in arr if x < pivot]\n    above = [x for x in arr if x > pivot]\n    num_less = len(below)\n    num_lessoreq = len(arr) - len(above)\n    if k < num_less:\n        return kth(below, k)\n    elif k >= num_lessoreq:\n        return kth(above, k - num_lessoreq)\n    else:\n        return pivot", "hint": "QuickSelect\nThis is an efficient equivalent to sorted(arr)[k].\nInput:", "input": [[1, 2, 3, 4, 5, 6, 7], 4], "output": 5}
{"id": "scrapy_33", "problem": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         extra={'spider': spider, 'failure': failure})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)", "fixed": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):", "fixed": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        if six.PY2:\n            self.file_flags = 'b'\n            self._open_args = {}\n        else:\n            self.file_flags = ''\n            self._open_args = {'newline': '\\n'}\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):", "fixed": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index + 1), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):"}
{"id": "scrapy_15", "problem": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),", "fixed": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(netloc),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),"}
{"id": "pandas_67", "problem": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()", "fixed": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n    def iget(self, key):\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()"}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n     def __init__(\n             self,", "fixed": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n    Customize how values are represented as strings::\n        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))\n     def __init__(\n             self,"}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             ser.at[0]\n        with pytest.raises(TypeError, match=msg):\n             ser.loc[0]\n    def test_frame_raises_type_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]", "fixed": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.at[0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.loc[0]\n    def test_frame_raises_key_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]"}
{"id": "pandas_119", "problem": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n         result = result.append(margin_dummy)\n     except TypeError:", "fixed": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )\n         result = result.append(margin_dummy)\n     except TypeError:"}
{"id": "httpie_3", "problem": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue", "fixed": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n            if value is None:\n                continue\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue"}
{"id": "black_22", "problem": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: Dict[LeafID, Leaf] = Factory(dict)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False", "fixed": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: List[Tuple[Index, Leaf]] = Factory(list)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):", "fixed": " class StackedRNNCells(Layer):\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):"}
{"id": "pandas_73", "problem": " def mask_zero_div_zero(x, y, result):\n         return result\n     if zmask.any():\n        shape = result.shape\n         zneg_mask = zmask & np.signbit(y)\n         zpos_mask = zmask & ~zneg_mask\n        nan_mask = (zmask & (x == 0)).ravel()\n         with np.errstate(invalid=\"ignore\"):\n            neginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()\n            posinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()\n         if nan_mask.any() or neginf_mask.any() or posinf_mask.any():\n            result = result.astype(\"float64\", copy=False).ravel()\n            np.putmask(result, nan_mask, np.nan)\n            np.putmask(result, posinf_mask, np.inf)\n            np.putmask(result, neginf_mask, -np.inf)\n            result = result.reshape(shape)\n     return result", "fixed": " def mask_zero_div_zero(x, y, result):\n         return result\n     if zmask.any():\n         zneg_mask = zmask & np.signbit(y)\n         zpos_mask = zmask & ~zneg_mask\n        nan_mask = zmask & (x == 0)\n         with np.errstate(invalid=\"ignore\"):\n            neginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))\n            posinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))\n         if nan_mask.any() or neginf_mask.any() or posinf_mask.any():\n            result = result.astype(\"float64\", copy=False)\n            result[nan_mask] = np.nan\n            result[posinf_mask] = np.inf\n            result[neginf_mask] = -np.inf\n     return result"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return", "fixed": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"name": "is_valid_parenthesization.py", "problem": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return True", "fixed": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return depth == 0\n", "hint": "Nested Parens\nInput:\n    parens: A string of parentheses", "input": ["((()()))()"], "output": "True"}
{"id": "ansible_10", "problem": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type", "fixed": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n        self.prev = None\n        self.next = None\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"name": "max_sublist_sum.py", "problem": "def max_sublist_sum(arr):\n    max_ending_here = 0\n    max_so_far = 0\n    for x in arr:\n        max_ending_here = max_ending_here + x\n        max_so_far = max(max_so_far, max_ending_here)\n    return max_so_far", "fixed": "def max_sublist_sum(arr):\n    max_ending_here = 0\n    max_so_far = 0\n    for x in arr:\n        max_ending_here = max(0, max_ending_here + x)\n        max_so_far = max(max_so_far, max_ending_here)\n    return max_so_far\n", "hint": "Max Sublist Sum\nmax-sublist-sum\nEfficient equivalent to max(sum(arr[i:j]) for 0 <= i <= j <= len(arr))", "input": [[4, -5, 2, 1, -1, 3]], "output": 5}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "sanic_5", "problem": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],", "fixed": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "fastapi_14", "problem": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[bool, SchemaBase]] = None\n class Example(BaseModel):", "fixed": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[SchemaBase, bool]] = None\n class Example(BaseModel):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):", "fixed": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n    def _ensure_listlike_indexer(self, key, axis=None):\n        column_axis = 1\n        if self.ndim != 2:\n            return\n        if isinstance(key, tuple):\n            key = key[column_axis]\n            axis = column_axis\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, ABCMultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            for k in key:\n                try:\n                    self.obj[k]\n                except KeyError:\n                    self.obj[k] = np.nan\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n     ):\n         self._write = get_write_function(output, overwrite)", "fixed": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n            custom_repr=(),\n     ):\n         self._write = get_write_function(output, overwrite)"}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "youtube-dl_35", "problem": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',", "fixed": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n        '%d/%m/%Y %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(left.values, right.values, obj=f\"{obj} category\")", "fixed": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(\n                left.values,\n                right.values,\n                obj=f\"{obj} category\",\n                check_category_order=check_category_order,\n            )"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n                                   extra={'spider': spider, 'failure': f}))\n         self._scrape_next(spider, slot)\n         return dfd", "fixed": " class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n         self._scrape_next(spider, slot)\n         return dfd"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n     for leaf in line.leaves:\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n     for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n     elif is_integer_dtype(dtype):\n         dtype = np.float64", "fixed": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n        is_bool = True\n     elif is_integer_dtype(dtype):\n         dtype = np.float64"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "pandas_16", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            return self.freq\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            if is_period_dtype(result.dtype):\n                return self.freq\n            return None\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n     ) -> None:\n        route = self.route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,", "fixed": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n     ) -> None:\n        route_class = route_class_override or self.route_class\n        route = route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,"}
{"id": "scrapy_8", "problem": " class Field(dict):\n class ItemMeta(ABCMeta):\n     def __new__(mcs, class_name, bases, attrs):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)", "fixed": " class Field(dict):\n class ItemMeta(ABCMeta):\n     def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)"}
{"id": "youtube-dl_16", "problem": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')", "fixed": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data)\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')"}
{"id": "matplotlib_9", "problem": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin", "fixed": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n        self._unstale_viewLim()\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin"}
{"id": "luigi_33", "problem": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if p.significant]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))", "fixed": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if not p.is_global]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))"}
{"id": "youtube-dl_43", "problem": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?\n     if not m:\n         return u''\n     return m.group(1)", "fixed": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^?\n     if not m:\n         return u''\n     return m.group(1)"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '", "fixed": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "thefuck_13", "problem": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)", "fixed": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)"}
{"id": "scrapy_6", "problem": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')", "fixed": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')"}
{"id": "youtube-dl_31", "problem": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = int_or_none(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?([0-9]+)\\s*s',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',", "fixed": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = parse_duration(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?class=\"bold\">(.*?)<',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',"}
{"id": "pandas_66", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            new_values = self._data.fast_xs(loc)\n            if not is_list_like(new_values) or self.ndim == 1:\n                return com.maybe_box_datetimelike(new_values)\n             result = self._constructor_sliced(\n                 new_values,", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            if self.ndim == 1:\n                return self._values[loc]\n            new_values = self._data.fast_xs(loc)\n             result = self._constructor_sliced(\n                 new_values,"}
{"id": "pandas_72", "problem": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n         ):\n             values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:", "fixed": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n         ):\n             values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n        elif exact_match:\n            values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "tqdm_9", "problem": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix", "fixed": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n         if isinstance(string, str):\n             msg = \"Could not construct DatetimeTZDtype from '{}'\"\n            try:\n                match = cls._match.match(string)\n                if match:\n                    d = match.groupdict()\n                     return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n            except Exception:\n                pass\n             raise TypeError(msg.format(string))\n         raise TypeError(\"Could not construct DatetimeTZDtype\")", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n         if isinstance(string, str):\n             msg = \"Could not construct DatetimeTZDtype from '{}'\"\n            match = cls._match.match(string)\n            if match:\n                d = match.groupdict()\n                try:\n                     return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n                except (KeyError, TypeError, ValueError) as err:\n                    raise TypeError(msg.format(string)) from err\n             raise TypeError(msg.format(string))\n         raise TypeError(\"Could not construct DatetimeTZDtype\")"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n    BodySchema_kwargs: Dict[str, Any] = dict(default=None)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):"}
{"name": "subsequences.py", "problem": "def subsequences(a, b, k):\n    if k == 0:\n        return []\n    ret = []\n    for i in range(a, b + 1 - k):\n        ret.extend(\n            [i] + rest for rest in subsequences(i + 1, b, k - 1)\n        )\n    return ret", "fixed": "def subsequences(a, b, k):\n    if k == 0:\n        return [[]]\n    ret = []\n    for i in range(a, b + 1 - k):\n        ret.extend(\n            [i] + rest for rest in subsequences(i + 1, b, k - 1)\n        )\n    return ret", "hint": "Subsequences\nInput:\n    a: An int", "input": [[1, 2, 6, 72, 7, 33, 4]], "output": [1, 2, 4, 6, 7, 33, 72]}
{"name": "mergesort.py", "problem": "def mergesort(arr):\n    def merge(left, right):\n        result = []\n        i = 0\n        j = 0\n        while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[i:] or right[j:])\n        return result\n    if len(arr) == 0:\n        return arr\n    else:\n        middle = len(arr) // 2\n        left = mergesort(arr[:middle])\n        right = mergesort(arr[middle:])\n        return merge(left, right)", "fixed": "def mergesort(arr):\n    def merge(left, right):\n        result = []\n        i = 0\n        j = 0\n        while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[i:] or right[j:])\n        return result\n    if len(arr) <= 1:\n        return arr\n    else:\n        middle = len(arr) // 2\n        left = mergesort(arr[:middle])\n        right = mergesort(arr[middle:])\n        return merge(left, right)\n", "hint": "Merge Sort\nInput:\n    arr: A list of ints", "input": [[1, 2, 6, 72, 7, 33, 4]], "output": [1, 2, 4, 6, 7, 33, 72]}
{"id": "scrapy_22", "problem": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        else:\n             self._xg_characters(serialized_value)\n         self.xg.endElement(name)", "fixed": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        elif isinstance(serialized_value, six.text_type):\n             self._xg_characters(serialized_value)\n        else:\n            self._xg_characters(str(serialized_value))\n         self.xg.endElement(name)"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "matplotlib_19", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "fastapi_6", "problem": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if field.shape in sequence_shapes and isinstance(\n                    received_body, FormData\n                ):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)", "fixed": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = compat_urllib_request.urlopen(request).read()\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'\"fb_dtsg\":\"(.*?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<input value=\"(.*?)\" name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = compat_urllib_request.urlopen(check_req).read()\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:", "fixed": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = self._download_webpage(request, None,\n                note='Logging in', errnote='unable to fetch login page')\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'name=\"fb_dtsg\" value=\"(.+?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<button[^>]+value=\"(.*?)\"[^>]+name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = self._download_webpage(check_req, None,\n                note='Confirming login')\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()", "fixed": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()"}
{"id": "youtube-dl_33", "problem": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()", "fixed": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'(\\.[0-9]+)?(?:Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()"}
{"id": "keras_11", "problem": " def fit_generator(model,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def fit_generator(model,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "spacy_10", "problem": " class Errors(object):\n     E168 = (\"Unknown field: {field}\")\n     E169 = (\"Can't find module: {module}\")\n     E170 = (\"Cannot apply transition {name}: invalid for the current state.\")\n @add_codes", "fixed": " class Errors(object):\n     E168 = (\"Unknown field: {field}\")\n     E169 = (\"Can't find module: {module}\")\n     E170 = (\"Cannot apply transition {name}: invalid for the current state.\")\n    E171 = (\"Matcher.add received invalid on_match callback argument: expected \"\n            \"callable or None, but got: {arg_type}\")\n @add_codes"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n }", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n }"}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n            if not isinstance(initial_state, list):\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, the state '\n                    'should be a list containing the states of the underlying RNNs. '\n                    'Found: ' + str(initial_state))\n             forward_state = initial_state[:len(initial_state) // 2]\n             backward_state = initial_state[len(initial_state) // 2:]\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)", "fixed": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n             forward_state = initial_state[:len(initial_state) // 2]\n             backward_state = initial_state[len(initial_state) // 2:]\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\ndef read_pickle(path, compression=\"infer\"):\n     Load pickled pandas object (or any object) from file.", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass\ndef read_pickle(\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\n):\n     Load pickled pandas object (or any object) from file."}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_83", "problem": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the", "fixed": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "luigi_23", "problem": " class CentralPlannerScheduler(Scheduler):\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})", "fixed": " class CentralPlannerScheduler(Scheduler):\n        if self._config.prune_on_get_work:\n            self.prune()\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})"}
{"name": "find_first_in_sorted.py", "problem": "def find_first_in_sorted(arr, x):\n    lo = 0\n    hi = len(arr)\n    while lo <= hi:\n        mid = (lo + hi) // 2\n        if x == arr[mid] and (mid == 0 or x != arr[mid - 1]):\n            return mid\n        elif x <= arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return -1", "fixed": "def find_first_in_sorted(arr, x):\n    lo = 0\n    hi = len(arr)\n    while lo < hi:\n        mid = (lo + hi) // 2\n        if x == arr[mid] and (mid == 0 or x != arr[mid - 1]):\n            return mid\n        elif x <= arr[mid]:\n            hi = mid\n        else:\n            lo = mid + 1\n    return -1\n", "hint": "Fancy Binary Search\nfancy-binsearch\nInput:", "input": [[3, 4, 5, 5, 5, 5, 6], 5], "output": 2}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        epsilon: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced.", "fixed": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced."}
{"id": "matplotlib_29", "problem": " class YAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposy\n     def set_default_intervals(self):\n         ymin, ymax = 0., 1.", "fixed": " class YAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposy\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         ymin, ymax = 0., 1."}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")", "fixed": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n    is_bool = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n     def get_config(self):\n        config = []\n         for layer in self.layers:\n            config.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n        return copy.deepcopy(config)\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n        model = cls()\n        for conf in config:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n         return model", "fixed": " class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n     def get_config(self):\n        layer_configs = []\n         for layer in self.layers:\n            layer_configs.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n        config = {\n            'name': self.name,\n            'layers': copy.deepcopy(layer_configs)\n        }\n        if self._build_input_shape:\n            config['build_input_shape'] = self._build_input_shape\n        return config\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n        if 'name' in config:\n            name = config['name']\n            build_input_shape = config.get('build_input_shape')\n            layer_configs = config['layers']\n        model = cls(name=name)\n        for conf in layer_configs:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n        if not model.inputs and build_input_shape:\n            model.build(build_input_shape)\n         return model"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()\n    def visit_unformatted(self, node: LN) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()"}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"name": "quicksort.py", "problem": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x > pivot])\n    return lesser + [pivot] + greater", "fixed": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x >= pivot])\n    return lesser + [pivot] + greater\n", "hint": "QuickSort\nInput:\n    arr: A list of ints", "input": [17, 0], "output": 17}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "pandas_165", "problem": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):", "fixed": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n    def test_parr_add_sub_index(self):\n        pi = pd.period_range(\"2000-12-31\", periods=3)\n        parr = pi.array\n        result = parr - pi\n        expected = pi - pi\n        tm.assert_index_equal(result, expected)\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):"}
{"id": "scrapy_19", "problem": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n     def has_header(self, name):\n         return name in self.request.headers", "fixed": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n    @property\n    def full_url(self):\n        return self.get_full_url()\n    @property\n    def host(self):\n        return self.get_host()\n    @property\n    def type(self):\n        return self.get_type()\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n     def has_header(self, name):\n         return name in self.request.headers"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"name": "bitcount.py", "problem": "def bitcount(n):\n    count = 0\n    while n:\n        n ^= n - 1\n        count += 1\n    return count", "fixed": "def bitcount(n):\n    count = 0\n    while n:\n        n &= n - 1\n        count += 1\n    return count", "hint": "Bitcount\nbitcount\nInput:", "input": [127], "output": 7}
{"id": "black_23", "problem": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)", "fixed": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\npython_grammar_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_exec_statement.keywords[\"exec\"]\npython_grammar_no_print_statement_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"print\"]\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"exec\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "pandas_85", "problem": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n         if len(level_index):\n             grouper = level_index.take(codes)", "fixed": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n        else:\n            level_index = level_index.copy()\n         if len(level_index):\n             grouper = level_index.take(codes)"}
{"id": "black_23", "problem": " def format_str(src_contents: str, line_length: int) -> FileContent:\n     return dst_contents\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    drv = driver.Driver(grammar, pytree.convert)\n     if src_txt[-1] != '\\n':\n         nl = '\\r\\n' if '\\r\\n' in src_txt[:1024] else '\\n'\n         src_txt += nl\n    try:\n        result = drv.parse_string(src_txt, True)\n    except ParseError as pe:\n        lineno, column = pe.context[1]\n        lines = src_txt.splitlines()\n         try:\n            faulty_line = lines[lineno - 1]\n        except IndexError:\n            faulty_line = \"<line number missing in source>\"\n        raise ValueError(f\"Cannot parse: {lineno}:{column}: {faulty_line}\") from None\n     if isinstance(result, Leaf):\n         result = Node(syms.file_input, [result])", "fixed": " def format_str(src_contents: str, line_length: int) -> FileContent:\n     return dst_contents\nGRAMMARS = [\n    pygram.python_grammar_no_print_statement_no_exec_statement,\n    pygram.python_grammar_no_print_statement,\n    pygram.python_grammar_no_exec_statement,\n    pygram.python_grammar,\n]\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n     if src_txt[-1] != '\\n':\n         nl = '\\r\\n' if '\\r\\n' in src_txt[:1024] else '\\n'\n         src_txt += nl\n    for grammar in GRAMMARS:\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n            result = drv.parse_string(src_txt, True)\n            break\n        except ParseError as pe:\n            lineno, column = pe.context[1]\n            lines = src_txt.splitlines()\n            try:\n                faulty_line = lines[lineno - 1]\n            except IndexError:\n                faulty_line = \"<line number missing in source>\"\n            exc = ValueError(f\"Cannot parse: {lineno}:{column}: {faulty_line}\")\n    else:\n        raise exc from None\n     if isinstance(result, Leaf):\n         result = Node(syms.file_input, [result])"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "keras_14", "problem": " def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),\n                   axis=-1)", "fixed": " def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),\n                   axis=-1)"}
{"id": "pandas_46", "problem": " def cartesian_product(X):\n         b = np.zeros_like(cumprodX)\n    return [\n        np.tile(\n            np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])\n        )\n        for i, x in enumerate(X)\n    ]", "fixed": " def cartesian_product(X):\n         b = np.zeros_like(cumprodX)\n    return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]\ndef _tile_compat(arr, num: int):\n    if isinstance(arr, np.ndarray):\n        return np.tile(arr, num)\n    taker = np.tile(np.arange(len(arr)), num)\n    return arr.take(taker)"}
{"id": "pandas_70", "problem": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})", "fixed": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_7", "problem": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where(", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        left_distances = np.abs(self[left_indexer] - target)\n        right_distances = np.abs(self[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where("}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n        ordinal_op = getattr(self.asi8, opname)\n         if isinstance(other, str):\n             try:", "fixed": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n         if isinstance(other, str):\n             try:"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,", "fixed": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            self._build_input_shape = input_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "tornado_3", "problem": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            if self._instance_cache.get(self.io_loop) is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n            del self._instance_cache[self.io_loop]\n     def fetch(\n         self,", "fixed": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            if cached_val is not None and cached_val is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n     def fetch(\n         self,"}
{"id": "tornado_15", "problem": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root)\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)", "fixed": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root) + os.path.sep\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)"}
{"id": "pandas_53", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not self.holds_integer():\n                     self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n                     self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "matplotlib_30", "problem": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    x = x * (N - 1)\n    xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n    ind = np.searchsorted(x, xind)[1:-1]\n    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n    lut = np.concatenate([\n        [y1[0]],\n        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n        [y0[-1]],\n    ])\n     return np.clip(lut, 0.0, 1.0)", "fixed": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    if N == 1:\n        lut = np.array(y0[-1])\n    else:\n        x = x * (N - 1)\n        xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n        ind = np.searchsorted(x, xind)[1:-1]\n        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n        lut = np.concatenate([\n            [y1[0]],\n            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n            [y0[-1]],\n        ])\n     return np.clip(lut, 0.0, 1.0)"}
{"id": "fastapi_16", "problem": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        if not obj.Config.json_encoders:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n            )\n        else:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n                custom_encoder=obj.Config.json_encoders,\n            )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):", "fixed": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        encoder = getattr(obj.Config, \"json_encoders\", custom_encoder)\n        return jsonable_encoder(\n            obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n            include_none=include_none,\n            custom_encoder=encoder,\n        )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"name": "sqrt.py", "problem": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx", "fixed": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx ** 2) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx\n", "hint": "Square Root\nNewton-Raphson method implementation.\nInput:", "input": [2, 0.01], "output": 1.4166666666666665}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"name": "get_factors.py", "problem": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return []", "fixed": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return [n]\n", "hint": "Prime Factorization\nFactors an int using naive trial division.\nInput:", "input": [100], "output": [2, 2, 5, 5]}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "pandas_135", "problem": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n            return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)", "fixed": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n        except TypeError as err:\n            if \"ndarray\" in str(err):\n                pass\n            else:\n                raise\n        return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)"}
{"id": "scrapy_20", "problem": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)", "fixed": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)"}
{"id": "thefuck_27", "problem": " def match(command, settings):\n def get_new_command(command, settings):\n    return 'open http://' + command.script[5:]", "fixed": " def match(command, settings):\n def get_new_command(command, settings):\n    return command.script.replace('open ', 'open http://')"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        return Series(res, index=ri, name=self._selection_name)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):", "fixed": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        result = Series(res, index=ri, name=self._selection_name)\n        return self._reindex_output(result, fill_value=0)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):"}
{"id": "tornado_6", "problem": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = weakref.WeakKeyDictionary()\n     @classmethod\n     def configure(cls, impl, **kwargs):", "fixed": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = dict()\n     @classmethod\n     def configure(cls, impl, **kwargs):"}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "pandas_160", "problem": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\"):\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif isinstance(o, np.ndarray):\n                     dtypes |= {o.dtype.name}", "fixed": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif hasattr(o, \"dtype\"):\n                     dtypes |= {o.dtype.name}"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n        Yields :class:`Line` objects.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            try:\n                for comment in generate_comments(node):\n                    if any_open_brackets:\n                        self.current_line.append(comment)\n                    elif comment.type == token.COMMENT:\n                        self.current_line.append(comment)\n                        yield from self.line()\n                    else:\n                        yield from self.line()\n                        self.current_line.append(comment)\n                        yield from self.line()\n            except FormatOff as f_off:\n                f_off.trim_prefix(node)\n                yield from self.line(type=UnformattedLines)\n                yield from self.visit(node)\n            except FormatOn as f_on:\n                f_on.trim_prefix(node)\n                yield from self.visit_default(node)\n            else:\n                normalize_prefix(node, inside_brackets=any_open_brackets)\n                if self.normalize_strings and node.type == token.STRING:\n                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                    normalize_string_quotes(node)\n                if node.type not in WHITESPACE:\n                    self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    self.current_line.append(comment)\n                    yield from self.line()\n                else:\n                    yield from self.line()\n                    self.current_line.append(comment)\n                    yield from self.line()\n            normalize_prefix(node, inside_brackets=any_open_brackets)\n            if self.normalize_strings and node.type == token.STRING:\n                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                normalize_string_quotes(node)\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"id": "fastapi_14", "problem": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None", "fixed": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[str, Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "scrapy_18", "problem": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:", "fixed": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:"}
{"id": "scrapy_31", "problem": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))\n     def header_items(self):\n         return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n             for k, v in self.request.headers.items()\n         ]", "fixed": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n     def header_items(self):\n         return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "fastapi_2", "problem": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:", "fixed": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:"}
{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]", "fixed": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"C\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]"}
{"id": "pandas_69", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not ax.holds_integer():\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \"", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \""}
{"id": "pandas_154", "problem": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(obj.values)\n             if not cython_dtype:\n                cython_dtype = obj.values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = obj.values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:", "fixed": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n            values = obj._data._values\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(values)\n             if not cython_dtype:\n                cython_dtype = values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:"}
{"id": "tornado_1", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n         self.seed = seed", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n        self._manager = None\n         self.queue = None\n         self.seed = seed"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n        body_param_media_types = [\n            getattr(f.schema, \"media_type\")\n            for f in flat_dependant.body_params\n            if isinstance(f.schema, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodySchema_kwargs[\"media_type\"] = body_param_media_types[0]\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "pandas_166", "problem": " class DataFrame(NDFrame):\n             if can_concat:\n                 if how == \"left\":\n                    res = concat(frames, axis=1, join=\"outer\", verify_integrity=True)\n                     return res.reindex(self.index, copy=False)\n                 else:\n                    return concat(frames, axis=1, join=how, verify_integrity=True)\n             joined = frames[0]", "fixed": " class DataFrame(NDFrame):\n             if can_concat:\n                 if how == \"left\":\n                    res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                     return res.reindex(self.index, copy=False)\n                 else:\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )\n             joined = frames[0]"}
{"id": "youtube-dl_25", "problem": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^0[xX][0-9a-fA-F]+', 16),\n            (r'^0+[0-7]+', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(0), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v", "fixed": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^(0[xX][0-9a-fA-F]+)\\s*:?$', 16),\n            (r'^(0+[0-7]+)\\s*:?$', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(1), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v"}
{"id": "keras_19", "problem": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_18", "problem": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            self.min_periods or 1,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,", "fixed": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(self.window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            window_indexer.window_size,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n    _responseMatcher = re.compile('HTTP/1\\.. 200')\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n    _responseMatcher = re.compile(b'HTTP/1\\.. 200')\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):"}
{"id": "luigi_14", "problem": " class Task(object):\n         return False\n    def can_disable(self):\n        return (self.disable_failures is not None or\n                self.disable_hard_timeout is not None)\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())", "fixed": " class Task(object):\n         return False\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())"}
{"id": "keras_29", "problem": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)", "fixed": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                            self.stateful_metric_functions.append(metric_fn)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size", "fixed": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size"}
{"id": "thefuck_4", "problem": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip().split('\\n')\n    for alias in alias_out:\n        name, value = alias.replace('alias ', '', 1).split(' ', 1)\n         if name not in overridden:\n             aliases[name] = value\n     return aliases", "fixed": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip()\n    if not alias_out:\n        return aliases\n    for alias in alias_out.split('\\n'):\n        for separator in (' ', '='):\n            split_alias = alias.replace('alias ', '', 1).split(separator, 1)\n            if len(split_alias) == 2:\n                name, value = split_alias\n                break\n        else:\n            continue\n         if name not in overridden:\n             aliases[name] = value\n     return aliases"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         this, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\n         new_data = {}\n        for col, series in this.items():\n            new_data[col] = func(series.values, other.values)\n         fill_value = self._get_op_result_fill_value(other, func)", "fixed": " class SparseDataFrame(DataFrame):\n         this, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\n         new_data = {}\n        for col in this.columns:\n            new_data[col] = func(this[col], other)\n         fill_value = self._get_op_result_fill_value(other, func)"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "pandas_5", "problem": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)", "fixed": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)\ndef test_join_multi_return_indexers():\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n        actual_value = dct.get(m.group('key'))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)", "fixed": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)"}
{"id": "scrapy_31", "problem": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n     getheaders = get_all", "fixed": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]\n     getheaders = get_all"}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "pandas_115", "problem": " def interpolate_1d(\n                 inds = lib.maybe_convert_objects(inds)\n         else:\n             inds = xvalues\n        result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])\n         result[preserve_nans] = np.nan\n         return result", "fixed": " def interpolate_1d(\n                 inds = lib.maybe_convert_objects(inds)\n         else:\n             inds = xvalues\n        indexer = np.argsort(inds[valid])\n        result[invalid] = np.interp(\n            inds[invalid], inds[valid][indexer], yvalues[valid][indexer]\n        )\n         result[preserve_nans] = np.nan\n         return result"}
{"id": "black_5", "problem": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type == syms.arglist:\n                     commas += 1\n                     break", "fixed": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type in {\n                    syms.arglist,\n                    syms.typedargslist,\n                }:\n                     commas += 1\n                     break"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"name": "topological_ordering.py", "problem": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.outgoing_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "fixed": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.incoming_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "hint": "Topological Sort\nInput:\n    nodes: A list of directed graph nodes", "input": [], "output": ""}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "pandas_57", "problem": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                 categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                 else:\n                    categories[index] = value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if is_list_like(to_replace):\n            replace_dict = {replace_value: value for replace_value in to_replace}\n        else:\n            replace_dict = {to_replace: value}\n        for replace_value, new_value in replace_dict.items():\n            if replace_value in cat.categories:\n                if isna(new_value):\n                    cat.remove_categories(replace_value, inplace=True)\n                    continue\n                 categories = cat.categories.tolist()\n                index = categories.index(replace_value)\n                if new_value in cat.categories:\n                    value_index = categories.index(new_value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(replace_value, inplace=True)\n                 else:\n                    categories[index] = new_value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat"}
{"id": "pandas_123", "problem": " class RangeIndex(Int64Index):\n    @staticmethod\n    def _validate_dtype(dtype):", "fixed": " class RangeIndex(Int64Index):"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "pandas_167", "problem": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n     _tz = None\n     _freq = None", "fixed": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n    _supports_partial_string_indexing = True\n     _tz = None\n     _freq = None"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n         if auto is not None:\n             self._autoscaleYon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n        for ax in self._shared_y_axes.get_siblings(self):\n            ax._stale_viewlim_y = False\n         if auto is not None:\n             self._autoscaleYon = bool(auto)"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "youtube-dl_42", "problem": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n        def fix_ampersand(s):\n            return s.replace(u'& ', '&amp; ')\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_ampersand)\n         return [self._get_video_info(item) for item in idoc.findall('.//item')]", "fixed": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_xml_ampersands)\n         return [self._get_video_info(item) for item in idoc.findall('.//item')]"}
{"id": "tornado_11", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\") == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)"}
{"id": "black_22", "problem": " class UnformattedLines(Line):\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class UnformattedLines(Line):\n @dataclass\n class EmptyLineTracker:"}
{"id": "matplotlib_23", "problem": " class _AxesBase(martist.Artist):\n             return\n         dL = self.dataLim\n        x0, x1 = map(x_trf.inverted().transform, dL.intervalx)\n        y0, y1 = map(y_trf.inverted().transform, dL.intervaly)\n         xr = 1.05 * (x1 - x0)\n         yr = 1.05 * (y1 - y0)", "fixed": " class _AxesBase(martist.Artist):\n             return\n         dL = self.dataLim\n        x0, x1 = map(x_trf.transform, dL.intervalx)\n        y0, y1 = map(y_trf.transform, dL.intervaly)\n         xr = 1.05 * (x1 - x0)\n         yr = 1.05 * (y1 - y0)"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or", "fixed": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('quotedstrval') is not None or\n            m.group('strval') is not None or"}
{"id": "keras_40", "problem": " class RNN(Layer):\n             output_shape = (input_shape[0], output_dim)\n         if self.return_state:\n            state_shape = [(input_shape[0], output_dim) for _ in self.states]\n             return [output_shape] + state_shape\n         else:\n             return output_shape", "fixed": " class RNN(Layer):\n             output_shape = (input_shape[0], output_dim)\n         if self.return_state:\n            state_shape = [(input_shape[0], dim) for dim in state_size]\n             return [output_shape] + state_shape\n         else:\n             return output_shape"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "luigi_3", "problem": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except ValueError:\n            return literal_eval(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))\n class NumericalParameter(Parameter):"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        column_margin = table.loc[:, margins_name].drop(margins_name)\n        index_margin = table.loc[margins_name, :].drop(margins_name)\n        table = table.drop(margins_name, axis=1).drop(margins_name)\n        table_index_names = table.index.names\n        table_columns_names = table.columns.names\n         table = _normalize(table, normalize=normalize, margins=False)", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        table_index = table.index\n        table_columns = table.columns\n        if (margins_name not in table.iloc[-1, :].name) | (\n            margins_name != table.iloc[:, -1].name\n        ):\n            raise ValueError(\"{} not in pivoted DataFrame\".format(margins_name))\n        column_margin = table.iloc[:-1, -1]\n        index_margin = table.iloc[-1, :-1]\n        table = table.iloc[:-1, :-1]\n         table = _normalize(table, normalize=normalize, margins=False)"}
{"id": "ansible_2", "problem": " class _Alpha:\n         raise ValueError\n    def __gt__(self, other):\n        return not self.__lt__(other)\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n     def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n class _Numeric:", "fixed": " class _Alpha:\n         raise ValueError\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n    def __gt__(self, other):\n        return not self.__le__(other)\n     def __ge__(self, other):\n        return not self.__lt__(other)\n class _Numeric:"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "cookiecutter_2", "problem": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n    return None\n def run_script(script_path, cwd='.'):", "fixed": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n    scripts = []\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\n    if len(scripts) == 0:\n        return None\n    return scripts\n def run_script(script_path, cwd='.'):"}
{"id": "youtube-dl_1", "problem": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: v is not None,\n        '!': lambda v: v is None,\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)", "fixed": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)"}
{"id": "pandas_44", "problem": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n     def get_loc(self, key, method=None, tolerance=None):", "fixed": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_timedelta64_dtype(dtype)\n     def get_loc(self, key, method=None, tolerance=None):"}
{"id": "pandas_80", "problem": " def make_data():\n @pytest.fixture\n def dtype():\n    return pd.BooleanDtype()\n @pytest.fixture", "fixed": " def make_data():\n @pytest.fixture\n def dtype():\n    return BooleanDtype()\n @pytest.fixture"}
{"id": "pandas_16", "problem": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other)\n         result._freq = new_freq\n         return result", "fixed": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other, result)\n         result._freq = new_freq\n         return result"}
{"id": "ansible_13", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)"}
{"id": "thefuck_19", "problem": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force')\n enabled_by_default = False", "fixed": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force-with-lease')\n enabled_by_default = False"}
{"id": "pandas_151", "problem": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        values = self._ndarray\n        t = np.result_type(value, values)\n        if t != self._ndarray.dtype:\n            values = values.astype(t, casting=\"safe\")\n            values[key] = value\n            self._dtype = PandasDtype(t)\n            self._ndarray = values\n        else:\n            self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)", "fixed": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        value = np.asarray(value, dtype=self._ndarray.dtype)\n        self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)"}
{"id": "pandas_41", "problem": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)", "fixed": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)"}
{"id": "pandas_104", "problem": " class GroupBy(_GroupBy):\n            order = np.roll(list(range(result.index.nlevels)), -1)\n            result = result.reorder_levels(order)\n            result = result.reindex(q, level=-1)\n            hi = len(q) * self.ngroups\n            arr = np.arange(0, hi, self.ngroups)\n            arrays = []\n            for i in range(self.ngroups):\n                arr2 = arr + i\n                arrays.append(arr2)\n            indices = np.concatenate(arrays)\n            assert len(indices) == len(result)\n             return result.take(indices)\n     @Substitution(name=\"groupby\")", "fixed": " class GroupBy(_GroupBy):\n            order = list(range(1, result.index.nlevels)) + [0]\n            index_names = np.array(result.index.names)\n            result.index.names = np.arange(len(index_names))\n            result = result.reorder_levels(order)\n            result.index.names = index_names[order]\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()\n             return result.take(indices)\n     @Substitution(name=\"groupby\")"}
{"id": "youtube-dl_42", "problem": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_all_ampersand) \n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):", "fixed": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_ampersands)\n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'", "fixed": " def fit_generator(model,\n    val_use_sequence_api = is_sequence(validation_data)\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period):\n            if value.freq != self.freq:\n                raise raise_on_incompatible(self, value)\n            value = value.ordinal\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq).ordinal\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period) or value is NaT:\n            self._data._check_compatible_with(value)\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq)\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        elif not isinstance(value, PeriodArray):\n            raise TypeError(\n                \"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    path : str\n        File path where the pickled object will be loaded.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',\n        or '.zip' respectively, and no decompression otherwise.\n        Set to None for no decompression.\n     Returns\n     -------", "fixed": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be loaded from.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL is not limited to S3 and GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     Returns\n     -------"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values.T, other.values).T\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":", "fixed": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n            other_vals = other.values.reshape(-1, 1)\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values, other_vals)\n            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":"}
{"id": "pandas_112", "problem": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)", "fixed": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical(target_as_index):\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value))\nelse:\n                 self.reset_global()", "fixed": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value, task_name=task_name))\nelse:\n                 self.reset_global()"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "pandas_28", "problem": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=others)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:", "fixed": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=idx)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "keras_20", "problem": " def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):\n         padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n         output_padding: Integer, amount of padding along the output dimension,\n             Can be set to `None` in which case the output length is inferred.\n         The output length (integer).", "fixed": " def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):\n         padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n         output_padding: Integer, amount of padding along the output dimension,\n             Can be set to `None` in which case the output length is inferred.\n        dilation: dilation rate, integer.\n         The output length (integer)."}
{"id": "keras_39", "problem": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    current < self.target):\n                 return\n             prev_total_width = self.total_width", "fixed": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    (self.target is not None and current < self.target)):\n                 return\n             prev_total_width = self.total_width"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"name": "find_in_sorted.py", "problem": "def find_in_sorted(arr, x):\n    def binsearch(start, end):\n        if start == end:\n            return -1\n        mid = start + (end - start) // 2\n        if x < arr[mid]:\n            return binsearch(start, mid)\n        elif x > arr[mid]:\n            return binsearch(mid, end)\n        else:\n            return mid\n    return binsearch(0, len(arr))", "fixed": "def find_in_sorted(arr, x):\n    def binsearch(start, end):\n        if start == end:\n            return -1\n        mid = start + (end - start) // 2\n        if x < arr[mid]:\n            return binsearch(start, mid)\n        elif x > arr[mid]:\n            return binsearch(mid + 1, end)\n        else:\n            return mid\n    return binsearch(0, len(arr))", "hint": "Binary Search\nInput:\n    arr: A sorted list of ints", "input": [[3, 4, 5, 5, 5, 5, 6], 5], "output": 3}
{"id": "thefuck_8", "problem": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read()\n     return _parse_operations(lines)", "fixed": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read().decode(\"utf-8\")\n     return _parse_operations(lines)"}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    include_none: bool = True,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:", "fixed": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:"}
{"id": "keras_42", "problem": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,", "fixed": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$", "fixed": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n            (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$"}
{"name": "longest_common_subsequence.py", "problem": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b)\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "fixed": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b[1:])\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "hint": "Longest Common Subsequence\nCalculates the longest subsequence common to the two input strings. (A subsequence is any sequence of letters in the same order\nthey appear in the string, possibly skipping letters in between.)", "input": ["headache", "pentadactyl"], "output": "eadac"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "black_15", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n def normalize_fmt_off(node: Node) -> None:", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n def normalize_fmt_off(node: Node) -> None:\n    Returns True if a pair was converted."}
{"id": "tqdm_4", "problem": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale", "fixed": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            if total:\n                total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale"}
{"id": "scrapy_23", "problem": " class RetryTest(unittest.TestCase):\n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req = self.mw.process_response(req, rsp, self.spider)", "fixed": " class RetryTest(unittest.TestCase):\n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req = self.mw.process_response(req, rsp, self.spider)"}
{"id": "youtube-dl_27", "problem": " def parse_dfxp_time_expr(time_expr):\n     if mobj:\n         return float(mobj.group('time_offset'))\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:\\.\\d+)?)$', time_expr)\n     if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))\n def srt_subtitles_timecode(seconds):", "fixed": " def parse_dfxp_time_expr(time_expr):\n     if mobj:\n         return float(mobj.group('time_offset'))\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n     if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n def srt_subtitles_timecode(seconds):"}
{"id": "pandas_48", "problem": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except ValueError:\n                         result = result.reshape(1, -1)", "fixed": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except (ValueError, TypeError):\n                         result = result.reshape(1, -1)"}
{"id": "pandas_105", "problem": " def box_df_fail(request):\n     return request.param\n@pytest.fixture(\n    params=[\n        (pd.Index, False),\n        (pd.Series, False),\n        (pd.DataFrame, False),\n        pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),\n        (tm.to_array, False),\n    ],\n    ids=id_func,\n)\ndef box_transpose_fail(request):\n    return request.param\n @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)\n def box_with_array(request):", "fixed": " def box_df_fail(request):\n     return request.param\n @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)\n def box_with_array(request):"}
{"id": "black_20", "problem": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src.name}  (original)\"\n        dst_name = f\"{src.name}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()", "fixed": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src}  (original)\"\n        dst_name = f\"{src}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip()\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)", "fixed": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip('\\n')\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)"}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):", "fixed": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v.startswith('//') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role related operations.\",\n         )", "fixed": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n         )"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(self, output):\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding", "fixed": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(\n        self, output: FrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> FrameOrSeries:\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding"}
{"name": "knapsack.py", "problem": "def knapsack(capacity, items):\n    from collections import defaultdict\n    memo = defaultdict(int)\n    for i in range(1, len(items) + 1):\n        weight, value = items[i - 1]\n        for j in range(1, capacity + 1):\n            memo[i, j] = memo[i - 1, j]\n            if weight < j:\n                memo[i, j] = max(\n                    memo[i, j],\n                    value + memo[i - 1, j - weight]\n                )\n    return memo[len(items), capacity]", "fixed": "def knapsack(capacity, items):\n    from collections import defaultdict\n    memo = defaultdict(int)\n    for i in range(1, len(items) + 1):\n        weight, value = items[i - 1]\n        for j in range(1, capacity + 1):\n            memo[i, j] = memo[i - 1, j]\n            if weight <= j:\n                memo[i, j] = max(\n                    memo[i, j],\n                    value + memo[i - 1, j - weight]\n                )\n    return memo[len(items), capacity]", "hint": "Knapsack\nknapsack\nYou have a knapsack that can hold a maximum weight. You are given a selection of items, each with a weight and a value. You may", "input": [100, [[60, 10], [50, 8], [20, 4], [20, 4], [8, 3], [3, 2]]], "output": 19}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "spacy_7", "problem": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n     return result", "fixed": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n     return result"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing"}
{"id": "pandas_159", "problem": " class DataFrame(NDFrame):\n             return ops.dispatch_to_series(this, other, _arith_op)\n         else:\n            result = _arith_op(this.values, other.values)\n             return self._constructor(\n                 result, index=new_index, columns=new_columns, copy=False\n             )", "fixed": " class DataFrame(NDFrame):\n             return ops.dispatch_to_series(this, other, _arith_op)\n         else:\n            with np.errstate(all=\"ignore\"):\n                result = _arith_op(this.values, other.values)\n            result = dispatch_fill_zeros(func, this.values, other.values, result)\n             return self._constructor(\n                 result, index=new_index, columns=new_columns, copy=False\n             )"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):", "fixed": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):"}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "scrapy_23", "problem": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)", "fixed": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:", "fixed": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:"}
{"id": "pandas_43", "problem": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)", "fixed": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        return Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n     def _apply_to_column_groupbys(self, func):", "fixed": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        result = Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n        return self._reindex_output(result, fill_value=0)\n     def _apply_to_column_groupbys(self, func):"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "thefuck_1", "problem": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \\\"([a-z]+)\\\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)", "fixed": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)"}
{"id": "pandas_157", "problem": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetimelike(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):"}
{"id": "pandas_79", "problem": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError):\n                 return False", "fixed": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError, InvalidIndexError):\n                 return False"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_39", "problem": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output", "fixed": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output\ndef limit_length(s, length):\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                if responses is None:\n                    responses = {}\n                responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,", "fixed": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        if responses is None:\n            responses = {}\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "scrapy_14", "problem": " def gunzip(data):\n                 raise\n     return output\n def is_gzipped(response):\n     ctype = response.headers.get('Content-Type', b'')\n    return ctype in (b'application/x-gzip', b'application/gzip')", "fixed": " def gunzip(data):\n                 raise\n     return output\n_is_gzipped_re = re.compile(br'^application/(x-)?gzip\\b', re.I)\n def is_gzipped(response):\n     ctype = response.headers.get('Content-Type', b'')\n    return _is_gzipped_re.search(ctype) is not None"}
{"id": "pandas_136", "problem": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_int64_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_integer_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                     elif string == '(':\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                        current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])\n                     elif string == '+':\n                         video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, [','])\n                        current_selector = None\n                        selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))\n                     else:\n                         raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                 elif type == tokenize.ENDMARKER:", "fixed": " class YoutubeDL(object):\n                     elif string == '(':\n                         if current_selector:\n                             raise syntax_error('Unexpected \"(\"', start)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                     elif string == '+':\n                         video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])\n                     else:\n                         raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                 elif type == tokenize.ENDMARKER:"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "scrapy_26", "problem": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n            compsett.update(self[name])\n             return compsett\n        else:\n            return self[name]\n     def getpriority(self, name):", "fixed": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[basename], priority='default')\n            for k in self[name]:\n                prio = self[name].getpriority(k)\n                if prio > get_settings_priority('default'):\n                    compsett.set(k, self[name][k], prio)\n             return compsett\n        return self[name]\n     def getpriority(self, name):"}
{"id": "pandas_34", "problem": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=\"infer\",\n             nonexistent=\"shift_forward\",\n         )", "fixed": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=True,\n             nonexistent=\"shift_forward\",\n         )"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_29", "problem": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "keras_26", "problem": " def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:", "fixed": " def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n                new_states = [\n                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n                             new_states[i], states[i]) for i in range(len(states))\n                ]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n     def __init__(self,", "fixed": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n        restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n     def __init__(self,"}
{"id": "luigi_26", "problem": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar() or not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")", "fixed": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")"}
{"id": "luigi_14", "problem": " class Task(object):\n         self.failures.add_failure()\n     def has_excessive_failures(self):\n        if (self.failures.first_failure_time is not None and\n                self.disable_hard_timeout):\n             if (time.time() >= self.failures.first_failure_time +\n                     self.disable_hard_timeout):\n                 return True", "fixed": " class Task(object):\n         self.failures.add_failure()\n     def has_excessive_failures(self):\n        if self.failures.first_failure_time is not None:\n             if (time.time() >= self.failures.first_failure_time +\n                     self.disable_hard_timeout):\n                 return True"}
{"id": "thefuck_8", "problem": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)", "fixed": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)"}
{"id": "keras_31", "problem": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length))\n    input_length = tf.to_int32(tf.squeeze(input_length))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())", "fixed": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())"}
{"id": "keras_6", "problem": " def weighted_masked_objective(fn):\n             score_array *= mask\n            score_array /= K.mean(mask)\n         if weights is not None:", "fixed": " def weighted_masked_objective(fn):\n             score_array *= mask\n            score_array /= K.mean(mask) + K.epsilon()\n         if weights is not None:"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\n        if \"//\" in self.app.config.SERVER_NAME:\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\n            if \"//\" in self.app.config.SERVER_NAME:\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "pandas_153", "problem": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            values = values.astype(str)\n         else:\n             values = np.array(values, dtype=\"object\")", "fixed": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            itemsize = writers.word_len(na_rep)\n            values = values.astype(\"<U{size}\".format(size=itemsize))\n         else:\n             values = np.array(values, dtype=\"object\")"}
{"id": "pandas_6", "problem": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):", "fixed": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError, ValueError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"id": "pandas_33", "problem": " class IntegerArray(BaseMaskedArray):\n         ExtensionArray.argsort\n         data = self._data.copy()\n        data[self._mask] = data.min() - 1\n         return data\n     @classmethod", "fixed": " class IntegerArray(BaseMaskedArray):\n         ExtensionArray.argsort\n         data = self._data.copy()\n        if self._mask.any():\n            data[self._mask] = data.min() - 1\n         return data\n     @classmethod"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n             return out.sort_index() if self.sort else out", "fixed": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n            out = self._reindex_output(out)\n             return out.sort_index() if self.sort else out"}
{"id": "scrapy_16", "problem": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)\n def parse_url(url, encoding=None):", "fixed": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\ndef _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n    if not six.PY2:\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    if six.PY3:\n        return unquote_to_bytes(path)\n    else:\n        return unquote(path)\n def parse_url(url, encoding=None):"}
{"id": "fastapi_3", "problem": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        if exclude_unset and isinstance(response_content, BaseModel):\n            if PYDANTIC_1:\n                response_content = response_content.dict(exclude_unset=exclude_unset)\n            else:\n                response_content = response_content.dict(\n                    skip_defaults=exclude_unset\n                )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:", "fixed": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n        )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:"}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\n    if re.match(r'^(?:https?:)?//', path):\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "tqdm_1", "problem": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):", "fixed": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        elif tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit"}
{"id": "scrapy_32", "problem": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(settings)\n        log_scrapy_info(settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)", "fixed": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings)\n        log_scrapy_info(self.settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)"}
{"id": "pandas_64", "problem": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df\n         self.columns = self.df.columns\n         self.float_format = float_format", "fixed": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df.reindex(columns=cols)\n         self.columns = self.df.columns\n         self.float_format = float_format"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "keras_1", "problem": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.random_normal(shape, self.mean, self.stddev,\n                            dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n        if len(weights) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())", "fixed": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n    class_sample_weight = None\n    if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if len(y.shape) == 2:\n            if y.shape[1] > 1:\n                y_classes = np.argmax(y, axis=1)\n            elif y.shape[1] == 1:\n                y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        class_sample_weight = np.asarray(\n            [class_weight[cls] for cls in y_classes if cls in class_weight])\n        if len(class_sample_weight) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'][:-5])\n         thumbnail = None\n         duration = None", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'])\n         thumbnail = None\n         duration = None"}
{"name": "lis.py", "problem": "def lis(arr):\n    ends = {}\n    longest = 0\n    for i, val in enumerate(arr):\n        prefix_lengths = [j for j in range(1, longest + 1) if arr[ends[j]] < val]\n        length = max(prefix_lengths) if prefix_lengths else 0\n        if length == longest or val < arr[ends[length + 1]]:\n            ends[length + 1] = i\n            longest = length + 1\n    return longest", "fixed": "def lis(arr):\n    ends = {}\n    longest = 0\n    for i, val in enumerate(arr):\n        prefix_lengths = [j for j in range(1, longest + 1) if arr[ends[j]] < val]\n        length = max(prefix_lengths) if prefix_lengths else 0\n        if length == longest or val < arr[ends[length + 1]]:\n            ends[length + 1] = i\n            longest = max(longest, length + 1)\n    return longest\n", "hint": "Longest Increasing Subsequence\nlongest-increasing-subsequence\nInput:", "input": [[4, 2, 1]], "output": 1}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n         self._threads = []\n         self._stop_event = None", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._manager:\n            self._manager.shutdown()\n         self._threads = []\n         self._stop_event = None"}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "tornado_16", "problem": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n         if self._finished:\n             self._return_result(self._finished.popleft())", "fixed": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n        self._running_future.add_done_callback(lambda f: self)\n         if self._finished:\n             self._return_result(self._finished.popleft())"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "matplotlib_3", "problem": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = True\n         self._marker_function()\n     def __bool__(self):", "fixed": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = self._fillstyle != 'none'\n         self._marker_function()\n     def __bool__(self):"}
{"id": "matplotlib_13", "problem": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n         return cls(vertices, codes)\n     def __repr__(self):", "fixed": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n        last_vert = None\n        if codes.size > 0 and codes[-1] == cls.STOP:\n            last_vert = vertices[-1]\n        vertices = vertices[codes != cls.STOP, :]\n        codes = codes[codes != cls.STOP]\n        if last_vert is not None:\n            vertices = np.append(vertices, [last_vert], axis=0)\n            codes = np.append(codes, cls.STOP)\n         return cls(vertices, codes)\n     def __repr__(self):"}
{"id": "youtube-dl_38", "problem": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]", "fixed": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]\ndef urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')"}
{"id": "keras_30", "problem": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if x is None or len(x) == 0:\n                        batch_size = 1\n                    elif isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of decades to use for each half of the linear range. For\n            example, when *linscale* == 1.0 (the default), the space used for\n            the positive and negative halves of the linear range will be equal\n            to one decade in the logarithmic range.\n         Normalize.__init__(self, vmin, vmax, clip)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - np.e ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()", "fixed": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of powers of *base* (decades for base 10) to use for each\n            half of the linear range. For example, when *linscale* == 1.0\n            (the default), the space used for the positive and negative halves\n            of the linear range will be equal to a decade in the logarithmic\n            range if ``base=10``.\n        base : float, default: None\n            For v3.2 the default is the old value of ``np.e``, but that is\n            deprecated for v3.3 when base will default to 10.  During the\n            transition, specify the *base* kwarg to avoid a deprecation\n            warning.\n         Normalize.__init__(self, vmin, vmax, clip)\n        if base is None:\n            self._base = np.e\n            cbook.warn_deprecated(\"3.3\", message=\"default base will change \"\n                \"from np.e to 10.  To suppress this warning specify the base \"\n                \"kwarg.\")\n        else:\n            self._base = base\n        self._log_base = np.log(self._base)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - self._base ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "thefuck_2", "problem": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(':')\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]", "fixed": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(os.pathsep)\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters,\n                                                        filter_dilation=dilation_rate)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "keras_24", "problem": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:", "fixed": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:"}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines."}
{"id": "youtube-dl_32", "problem": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)\n def js_to_json(code):", "fixed": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(\n        r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?://[^\\n]*)*$', r'\\1', code)\n def js_to_json(code):"}
{"id": "youtube-dl_18", "problem": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()", "fixed": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()"}
{"id": "pandas_41", "problem": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n     @property\n     def _holder(self):", "fixed": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n    should_store = DatetimeBlock.should_store\n     @property\n     def _holder(self):"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*."}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.power(self._base,\n                       sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj"}
{"id": "luigi_13", "problem": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.fs.mkdir(d)\n         os.rename(old_path, new_path)", "fixed": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.mkdir(d)\n         os.rename(old_path, new_path)"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "thefuck_20", "problem": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])\n def side_effect(old_cmd, command):", "fixed": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))\n def side_effect(old_cmd, command):"}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "pandas_14", "problem": " def id_func(x):\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])", "fixed": " def id_func(x):\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n    ]\n)\ndef names(request):\n    return request.param\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])"}
{"id": "keras_1", "problem": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        increment = np.random.randn(3, 4)\n        x += increment\n        K.eval(K.update_add(x_var, increment))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        decrement = np.random.randn(3, 4)\n        x -= decrement\n        K.eval(K.update_sub(x_var, decrement))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')", "fixed": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n    @pytest.mark.skipif(K.backend() == 'theano',\n                        reason='theano returns tuples for update ops')\n    def test_update(self):\n        x = np.ones((3, 4))\n        x_var = K.variable(x)\n        new_x = np.random.random((3, 4))\n        op = K.update(x_var, new_x)\n        K.eval(op)\n        assert_allclose(new_x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        increment = np.random.random((3, 4))\n        op = K.update_add(x_var, increment)\n        K.eval(op)\n        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        decrement = np.random.random((3, 4))\n        op = K.update_sub(x_var, decrement)\n        K.eval(op)\n        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_139", "problem": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    )\n                 )", "fixed": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    ),\n                    name=self.name,\n                 )"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "luigi_15", "problem": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks", "fixed": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks"}
{"id": "matplotlib_17", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value):\n         return isinstance(value, self._holder)\n    def set(self, locs, values, check=False):\n         assert locs.tolist() == [0]\n        self.values = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,", "fixed": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value: ArrayLike) -> bool:\n         return isinstance(value, self._holder)\n    def set(self, locs, values):\n         assert locs.tolist() == [0]\n        self.values[:] = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,"}
{"id": "ansible_11", "problem": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n    if text:\n        text = str(text).strip()\n     return {\n         'banner': module.params['banner'],\n         'text': text,", "fixed": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n     return {\n         'banner': module.params['banner'],\n         'text': text,"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "pandas_125", "problem": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)\n    def replace(self, to_replace, value, inplace: bool = False):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                else:\n                    categories[index] = value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     @staticmethod\n     def get_target_path(target):\n         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n             return target.path\n         elif isinstance(target, bigquery.BigQueryTarget):\n            \"{}:{}.{}\".format(target.project_id, target.dataset_id, target.table_id)\n         else:\n            raise ValueError(\"Target not supported\")", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     @staticmethod\n     def get_target_path(target):\n         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n             return target.path\n         elif isinstance(target, bigquery.BigQueryTarget):\n            return \"{}:{}.{}\".format(target.table.project_id, target.table.dataset_id, target.table.table_id)\n         else:\n            raise ValueError(\"Target %s not supported\" % target)"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "black_6", "problem": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n             result = drv.parse_string(src_txt, True)\n             break", "fixed": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"}
{"id": "youtube-dl_17", "problem": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]", "fixed": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n    if param is None:\n        return []\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]"}
{"id": "keras_29", "problem": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:", "fixed": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)"}
{"id": "pandas_76", "problem": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError):\n                 pass", "fixed": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError, OverflowError):\n                 pass"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n         elif (\n             is_datetime64_any_dtype(data)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n         elif (\n             is_datetime64_any_dtype(data)"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "pandas_81", "problem": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):", "fixed": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype\n        dtype = pandas_dtype(dtype)\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n        elif isinstance(dtype, BooleanDtype):\n            result = self._data.astype(\"bool\", copy=False)\n            return BooleanArray(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):"}
{"id": "ansible_9", "problem": " class Rhsm(RegistrationBase):\n         for pool_id, quantity in sorted(pool_ids.items()):\n             if pool_id in available_pool_ids:\n                args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]\n                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n             else:\n                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)", "fixed": " class Rhsm(RegistrationBase):\n         for pool_id, quantity in sorted(pool_ids.items()):\n             if pool_id in available_pool_ids:\n                args = [SUBMAN_CMD, 'attach', '--pool', pool_id]\n                if quantity is not None:\n                    args.extend(['--quantity', to_native(quantity)])\n                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n             else:\n                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)"}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "scrapy_40", "problem": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        if self.binary:\n            return to_bytes(value, encoding=self.encoding)\n        else:\n            return to_unicode(value, encoding=self.encoding)\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):", "fixed": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):"}
{"id": "keras_1", "problem": " def get_variable_shape(x):\n     return int_shape(x)\n def print_tensor(x, message=''):", "fixed": " def get_variable_shape(x):\n     return int_shape(x)\n@symbolic\n def print_tensor(x, message=''):"}
{"id": "luigi_17", "problem": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)", "fixed": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "spacy_7", "problem": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()", "fixed": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()"}
{"id": "fastapi_1", "problem": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)", "fixed": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)"}
{"id": "pandas_56", "problem": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._iget_item_cache(col)\n            return com.maybe_box_datetimelike(series._values[index])\n         series = self._get_item_cache(col)\n         engine = self.index._engine", "fixed": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n         series = self._get_item_cache(col)\n         engine = self.index._engine"}
{"id": "fastapi_1", "problem": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\"", "fixed": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\""}
{"id": "pandas_167", "problem": " def convert_to_index_sliceable(obj, key):\n        if idx.is_all_dates:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):", "fixed": " def convert_to_index_sliceable(obj, key):\n        if idx._supports_partial_string_indexing:\n             try:\n                 return idx._get_string_slice(key)\n             except (KeyError, ValueError, NotImplementedError):"}
{"id": "keras_1", "problem": " def update_sub(x, decrement):\n         The variable `x` updated.\n    return tf_state_ops.assign_sub(x, decrement)\n @symbolic", "fixed": " def update_sub(x, decrement):\n         The variable `x` updated.\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h)\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w)\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n        config.pop('dilation_rate')\n         config['output_padding'] = self.output_padding\n         return config", "fixed": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n         config['output_padding'] = self.output_padding\n         return config"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "luigi_11", "problem": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items())):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)", "fixed": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)"}
{"id": "thefuck_18", "problem": " patterns = ['permission denied',\n def match(command):\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():", "fixed": " patterns = ['permission denied',\n def match(command):\n    if command.script_parts and command.script_parts[0] == 'sudo':\n        return False\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"name": "next_permutation.py", "problem": "def next_permutation(perm):\n    for i in range(len(perm) - 2, -1, -1):\n        if perm[i] < perm[i + 1]:\n            for j in range(len(perm) - 1, i, -1):\n                if perm[j] < perm[i]:\n                    next_perm = list(perm)\n                    next_perm[i], next_perm[j] = perm[j], perm[i]\n                    next_perm[i + 1:] = reversed(next_perm[i + 1:])\n                    return next_perm", "fixed": "def next_permutation(perm):\n    for i in range(len(perm) - 2, -1, -1):\n        if perm[i] < perm[i + 1]:\n            for j in range(len(perm) - 1, i, -1):\n                if perm[i] < perm[j]:\n                    next_perm = list(perm)\n                    next_perm[i], next_perm[j] = perm[j], perm[i]\n                    next_perm[i + 1:] = reversed(next_perm[i + 1:])\n                    return next_perm\n", "hint": "Next Permutation\nnext-perm\nInput:", "input": [[3, 2, 4, 1]], "output": [3, 4, 1, 2]}
{"id": "pandas_31", "problem": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals):\n                 inference = np.int64\n            elif is_datetime64_dtype(vals):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)", "fixed": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                 inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)"}
{"id": "pandas_146", "problem": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if np.any(left_value != right_value):\n                    return False\n         return True", "fixed": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                try:\n                    if np.any(left_value != right_value):\n                        return False\n                except TypeError as err:\n                    if \"Cannot compare tz-naive\" in str(err):\n                        return False\n                    raise\n         return True"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "scrapy_33", "problem": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n         return failure\n     dont_log = named.pop('dont_log', None)", "fixed": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n         return failure\n     dont_log = named.pop('dont_log', None)"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = df.loc[\"a\", \"A\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             df.at[\"a\", 0]\n        with pytest.raises(TypeError, match=msg):\n             df.loc[\"a\", 0]\n     def test_series_at_raises_key_error(self):", "fixed": " class TestScalar2:\n         result = df.loc[\"a\", \"A\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             df.at[\"a\", 0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             df.loc[\"a\", 0]\n     def test_series_at_raises_key_error(self):"}
{"id": "pandas_62", "problem": "                     missing_value = StataMissingValue(um)\n                     loc = missing_loc[umissing_loc == j]\n                     replacement.iloc[loc] = missing_value\nelse:\n                 dtype = series.dtype", "fixed": "                     missing_value = StataMissingValue(um)\n                     loc = missing_loc[umissing_loc == j]\n                    if loc.ndim == 2 and loc.shape[1] == 1:\n                        loc = loc[:, 0]\n                     replacement.iloc[loc] = missing_value\nelse:\n                 dtype = series.dtype"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n                self.wait += 1\n     def in_cooldown(self):\n         return self.cooldown_counter > 0", "fixed": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n     def in_cooldown(self):\n         return self.cooldown_counter > 0"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:", "fixed": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:"}
{"id": "httpie_2", "problem": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)", "fixed": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n    requests_session.max_redirects = args.max_redirects\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensor,\n                                               computed_mask))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 try:\n                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                         generator_output = next(self._generator)\n                        self.queue.put(generator_output)\n                     else:\n                         time.sleep(self.wait_time)\n                 except StopIteration:\n                     break\n                except Exception:\n                     self._stop_event.set()\n                    raise\n         try:\n             if self._use_multiprocessing:\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n                 self.queue = queue.Queue()", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 try:\n                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                         generator_output = next(self._generator)\n                        self.queue.put((True, generator_output))\n                     else:\n                         time.sleep(self.wait_time)\n                 except StopIteration:\n                     break\n                except Exception as e:\n                    if self._use_multiprocessing:\n                        traceback.print_exc()\n                        setattr(e, '__traceback__', None)\n                    elif not hasattr(e, '__traceback__'):\n                        setattr(e, '__traceback__', sys.exc_info()[2])\n                    self.queue.put((False, e))\n                     self._stop_event.set()\n                    break\n         try:\n             if self._use_multiprocessing:\n                self._manager = multiprocessing.Manager()\n                self.queue = self._manager.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n                 self.queue = queue.Queue()"}
{"id": "youtube-dl_30", "problem": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f", "fixed": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                    formats = list(formats)\n                    if not formats:\n                        return\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f"}
