{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing"}
{"id": "pandas_80", "problem": " class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):\n     def __len__(self) -> int:\n         return len(self._data)\n     def to_numpy(\n         self, dtype=None, copy=False, na_value: \"Scalar\" = lib.no_default,\n     ):", "fixed": " class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):\n     def __len__(self) -> int:\n         return len(self._data)\n    def __invert__(self):\n        return type(self)(~self._data, self._mask)\n     def to_numpy(\n         self, dtype=None, copy=False, na_value: \"Scalar\" = lib.no_default,\n     ):"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,", "fixed": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,"}
{"id": "keras_20", "problem": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n         A tensor.", "fixed": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n         A tensor."}
{"id": "luigi_3", "problem": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except ValueError:\n            return literal_eval(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))\n class NumericalParameter(Parameter):"}
{"id": "matplotlib_10", "problem": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])", "fixed": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n            if 'label1On' in kwtrans or 'label2On' in kwtrans:\n                self.offsetText.set_visible(\n                    self._major_tick_kw.get('label1On', False)\n                    or self._major_tick_kw.get('label2On', False))\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])"}
{"id": "matplotlib_7", "problem": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if hasattr(intensity, 'mask'):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]", "fixed": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if np.ma.is_masked(intensity):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]"}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'][:-5])\n         thumbnail = None\n         duration = None", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'])\n         thumbnail = None\n         duration = None"}
{"id": "pandas_142", "problem": " class TestTimeSeries(TestData):\n         )\n         tm.assert_index_equal(expected.index, result.index)\n    def test_diff(self):\n        self.ts.diff()\n        a = 10000000000000000\n        b = a + 1\n        s = Series([a, b])\n        rs = s.diff()\n        assert rs[1] == 1\n        rs = self.ts.diff(-1)\n        xp = self.ts - self.ts.shift(-1)\n        assert_series_equal(rs, xp)\n        rs = self.ts.diff(0)\n        xp = self.ts - self.ts\n        assert_series_equal(rs, xp)\n        s = Series(date_range(\"20130102\", periods=5))\n        rs = s - s.shift(1)\n        xp = s.diff()\n        assert_series_equal(rs, xp)\n        nrs = rs - rs.shift(1)\n        nxp = xp.diff()\n        assert_series_equal(nrs, nxp)\n        s = Series(\n            date_range(\"2000-01-01 09:00:00\", periods=5, tz=\"US/Eastern\"), name=\"foo\"\n        )\n        result = s.diff()\n        assert_series_equal(\n            result, Series(TimedeltaIndex([\"NaT\"] + [\"1 days\"] * 4), name=\"foo\")\n        )\n     def test_pct_change(self):\n         rs = self.ts.pct_change(fill_method=None)\n         assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)", "fixed": " class TestTimeSeries(TestData):\n         )\n         tm.assert_index_equal(expected.index, result.index)\n     def test_pct_change(self):\n         rs = self.ts.pct_change(fill_method=None)\n         assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)"}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"id": "pandas_105", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n    def transpose(self, *args, **kwargs):\n        axes, kwargs = self._construct_axes_from_arguments(\n            args, kwargs, require_all=True\n        )\n        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)\n        if len(axes) != len(set(axes)):\n            raise ValueError(f\"Must specify {self._AXIS_LEN} unique axes\")\n        new_axes = self._construct_axes_dict_from(\n            self, [self._get_axis(x) for x in axes_names]\n        )\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop(\"copy\", None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n        nv.validate_transpose(tuple(), kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)\n     def swapaxes(self, axis1, axis2, copy=True):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n     def swapaxes(self, axis1, axis2, copy=True):"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "scrapy_16", "problem": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)\n def parse_url(url, encoding=None):", "fixed": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\ndef _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n    if not six.PY2:\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    if six.PY3:\n        return unquote_to_bytes(path)\n    else:\n        return unquote(path)\n def parse_url(url, encoding=None):"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n                             extra={'spider': spider, 'failure': download_failure})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:", "fixed": " class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n                             exc_info=failure_to_exc_info(download_failure),\n                             extra={'spider': spider})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:"}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "pandas_28", "problem": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=others)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:", "fixed": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=idx)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(description_original, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):"}
{"id": "pandas_26", "problem": " class Categorical(ExtensionArray, PandasObject):\n         good = self._codes != -1\n         if not good.all():\n            if skipna:\n                 pointer = self._codes[good].max()\n             else:\n                 return np.nan", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         good = self._codes != -1\n         if not good.all():\n            if skipna and good.any():\n                 pointer = self._codes[good].max()\n             else:\n                 return np.nan"}
{"id": "pandas_29", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        if needs_float_conversion:\n            left = left.astype(\"float\")\n        left.values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        if needs_float_conversion:\n            right = right.astype(\"float\")\n        right.values[key] = value_right\n         self._right = right\n     def __eq__(self, other):", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n        if needs_float_conversion:\n            raise ValueError(\"Cannot set float NaN to integer-backed IntervalArray\")\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        left._values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        right._values[key] = value_right\n         self._right = right\n     def __eq__(self, other):"}
{"id": "youtube-dl_28", "problem": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        return compat_chr(int(numstr, base))\n     return ('&%s;' % entity)", "fixed": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass\n     return ('&%s;' % entity)"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,"}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"name": "sieve.py", "problem": "def sieve(max):\n    primes = []\n    for n in range(2, max + 1):\n        if any(n % p > 0 for p in primes):\n            primes.append(n)\n    return primes", "fixed": "def sieve(max):\n    primes = []\n    for n in range(2, max + 1):\n        if all(n % p > 0 for p in primes):\n            primes.append(n)\n    return primes\n", "hint": "Sieve of Eratosthenes\nprime-sieve\nInput:", "input": [4], "output": [2, 3]}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "pandas_41", "problem": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")", "fixed": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n    def should_store(self, value):\n        return is_dtype_equal(self.dtype, value.dtype)\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")"}
{"id": "keras_1", "problem": " def update_sub(x, decrement):\n         The variable `x` updated.\n    return tf_state_ops.assign_sub(x, decrement)\n @symbolic", "fixed": " def update_sub(x, decrement):\n         The variable `x` updated.\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n         Returns\n         -------\n        axes: topmost axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy)]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:", "fixed": " class FigureCanvasBase:\n         Returns\n         -------\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy) and a.get_visible()]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "scrapy_31", "problem": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))\n     def header_items(self):\n         return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n             for k, v in self.request.headers.items()\n         ]", "fixed": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n     def header_items(self):\n         return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]"}
{"id": "keras_36", "problem": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1, 1) + strides + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1, 1) + strides\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)", "fixed": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)"}
{"id": "luigi_29", "problem": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n            if cls.run == NotImplemented:\n                continue\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\", "fixed": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):", "fixed": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):"}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "thefuck_17", "problem": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)", "fixed": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)"}
{"name": "sqrt.py", "problem": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx", "fixed": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx ** 2) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx\n", "hint": "Square Root\nNewton-Raphson method implementation.\nInput:", "input": [2, 0.01], "output": 1.4166666666666665}
{"id": "PySnooper_1", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False"}
{"id": "keras_42", "problem": " class Model(Container):\n         if do_validation:\n             self._make_test_function()\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n        if val_gen and not validation_steps:\n            raise ValueError('When using a generator for validation data, '\n                             'you must specify a value for '\n                             '`validation_steps`.')\n         out_labels = self._get_deduped_metrics_names()", "fixed": " class Model(Container):\n         if do_validation:\n             self._make_test_function()\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if steps_per_epoch is None:\n            if is_sequence:\n                steps_per_epoch = len(generator)\n            else:\n                raise ValueError('`steps_per_epoch=None` is only valid for a'\n                                 ' generator based on the `keras.utils.Sequence`'\n                                 ' class. Please specify `steps_per_epoch` or use'\n                                 ' the `keras.utils.Sequence` class.')\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n        if (val_gen and not isinstance(validation_data, Sequence) and\n                not validation_steps):\n            raise ValueError('`validation_steps=None` is only valid for a'\n                             ' generator based on the `keras.utils.Sequence`'\n                             ' class. Please specify `validation_steps` or use'\n                             ' the `keras.utils.Sequence` class.')\n         out_labels = self._get_deduped_metrics_names()"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "matplotlib_22", "problem": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')", "fixed": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n        if not cbook.is_scalar_or_string(bins):\n            bins = self.convert_xunits(bins)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')"}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n     ) -> None:\n        route = self.route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,", "fixed": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n     ) -> None:\n        route_class = route_class_override or self.route_class\n        route = route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "pandas_41", "problem": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values", "fixed": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:", "fixed": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                self.wait += 1\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:"}
{"id": "pandas_57", "problem": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                 categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                 else:\n                    categories[index] = value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if is_list_like(to_replace):\n            replace_dict = {replace_value: value for replace_value in to_replace}\n        else:\n            replace_dict = {to_replace: value}\n        for replace_value, new_value in replace_dict.items():\n            if replace_value in cat.categories:\n                if isna(new_value):\n                    cat.remove_categories(replace_value, inplace=True)\n                    continue\n                 categories = cat.categories.tolist()\n                index = categories.index(replace_value)\n                if new_value in cat.categories:\n                    value_index = categories.index(new_value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(replace_value, inplace=True)\n                 else:\n                    categories[index] = new_value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat"}
{"id": "fastapi_3", "problem": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        if exclude_unset and isinstance(response_content, BaseModel):\n            if PYDANTIC_1:\n                response_content = response_content.dict(exclude_unset=exclude_unset)\n            else:\n                response_content = response_content.dict(\n                    skip_defaults=exclude_unset\n                )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:", "fixed": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n        )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n             )\nelse:\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,", "fixed": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n                exclude_none=exclude_none,\n                exclude_defaults=exclude_defaults,\n             )\nelse:\n            if exclude_defaults:\n                raise ValueError(\"Cannot use exclude_defaults\")\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,"}
{"id": "keras_31", "problem": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length))\n    input_length = tf.to_int32(tf.squeeze(input_length))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())", "fixed": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         else:\n             return ((prefix + \": \") if prefix else '') + \\", "fixed": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         else:\n             return ((prefix + \": \") if prefix else '') + \\"}
{"id": "pandas_136", "problem": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_int64_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_integer_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "pandas_139", "problem": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    )\n                 )", "fixed": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    ),\n                    name=self.name,\n                 )"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "cookiecutter_3", "problem": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default\n     )\n     return choice_map[user_choice]", "fixed": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default, show_choices=False\n     )\n     return choice_map[user_choice]"}
{"id": "youtube-dl_1", "problem": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: v is not None,\n        '!': lambda v: v is None,\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)", "fixed": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)"}
{"id": "fastapi_14", "problem": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[bool, SchemaBase]] = None\n class Example(BaseModel):", "fixed": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[SchemaBase, bool]] = None\n class Example(BaseModel):"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "youtube-dl_41", "problem": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',',' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9:]*$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',", "fixed": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',', ' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',"}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
{"id": "pandas_138", "problem": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)", "fixed": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)"}
{"id": "luigi_16", "problem": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            if task.id not in necessary_tasks and self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)", "fixed": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            removed = self._state.prune(task, self._config)\n            if removed and task.id not in necessary_tasks:\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)"}
{"id": "youtube-dl_16", "problem": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')", "fixed": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data)\n     out = []\n     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')"}
{"id": "keras_34", "problem": " class Sequence(object):\n _SHARED_SEQUENCES = {}", "fixed": " class Sequence(object):\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item\n _SHARED_SEQUENCES = {}"}
{"name": "longest_common_subsequence.py", "problem": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b)\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "fixed": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b[1:])\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "hint": "Longest Common Subsequence\nCalculates the longest subsequence common to the two input strings. (A subsequence is any sequence of letters in the same order\nthey appear in the string, possibly skipping letters in between.)", "input": ["headache", "pentadactyl"], "output": "eadac"}
{"id": "pandas_155", "problem": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            return self.obj.index\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()\n    def visit_unformatted(self, node: LN) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq"}
{"id": "fastapi_1", "problem": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)", "fixed": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)"}
{"id": "youtube-dl_40", "problem": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''", "fixed": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return struct_unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return struct_unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return struct_unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''"}
{"id": "luigi_14", "problem": " class Task(object):\n         return False\n    def can_disable(self):\n        return (self.disable_failures is not None or\n                self.disable_hard_timeout is not None)\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())", "fixed": " class Task(object):\n         return False\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "pandas_114", "problem": " class Index(IndexOpsMixin, PandasObject):\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n        s = extract_array(series, extract_numpy=True)\n        if isinstance(s, ExtensionArray):\n            if is_scalar(key):\n                try:\n                    iloc = self.get_loc(key)\n                    return s[iloc]\n                except KeyError:\n                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                        raise\n                    elif is_integer(key):\n                        return s[key]\n            else:\n                raise InvalidIndexError(key)\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\n        if \"//\" in self.app.config.SERVER_NAME:\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\n            if \"//\" in self.app.config.SERVER_NAME:\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "tornado_8", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\",", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n        except ValueError:\n            self.handler.set_status(400)\n            log_msg = \"Missing/Invalid WebSocket headers\"\n            self.handler.finish(log_msg)\n            gen_log.debug(log_msg)\n            return\n        try:\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\","}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         else:\n             raise ValueError(\"Not a valid normalize argument\")\n        table.index.names = table_index_names\n        table.columns.names = table_columns_names\n     else:\n         raise ValueError(\"Not a valid margins argument\")", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n            table.columns = table_columns\n         else:\n             raise ValueError(\"Not a valid normalize argument\")\n     else:\n         raise ValueError(\"Not a valid margins argument\")"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "pandas_21", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        if isinstance(key, list):\n            return self.loc[key]\n        return self.reindex(key)\n     def _get_values_tuple(self, key):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        return self.loc[key]\n     def _get_values_tuple(self, key):"}
{"id": "luigi_23", "problem": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler()\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)", "fixed": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,", "fixed": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            self._build_input_shape = input_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,"}
{"id": "keras_19", "problem": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n     ):\n         self._write = get_write_function(output, overwrite)", "fixed": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n            custom_repr=(),\n     ):\n         self._write = get_write_function(output, overwrite)"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "pandas_83", "problem": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort\n         )\n     def _get_concat_axis(self) -> Index:", "fixed": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,\n         )\n     def _get_concat_axis(self) -> Index:"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "pandas_131", "problem": " class CombinedDatetimelikeProperties(\n         orig = data if is_categorical_dtype(data) else None\n         if orig is not None:\n            data = Series(orig.values.categories, name=orig.name, copy=False)\n         if is_datetime64_dtype(data.dtype):\n             return DatetimeProperties(data, orig)", "fixed": " class CombinedDatetimelikeProperties(\n         orig = data if is_categorical_dtype(data) else None\n         if orig is not None:\n            data = Series(\n                orig.array,\n                name=orig.name,\n                copy=False,\n                dtype=orig.values.categories.dtype,\n            )\n         if is_datetime64_dtype(data.dtype):\n             return DatetimeProperties(data, orig)"}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:", "fixed": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:"}
{"id": "scrapy_21", "problem": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        self._parsers.pop(netloc).callback(None)", "fixed": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "fastapi_7", "problem": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={\"detail\": exc.errors()}\n     )", "fixed": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY,\n        content={\"detail\": jsonable_encoder(exc.errors())},\n     )"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"name": "hanoi.py", "problem": "def hanoi(height, start=1, end=3):\n    steps = []\n    if height > 0:\n        helper = ({1, 2, 3} - {start} - {end}).pop()\n        steps.extend(hanoi(height - 1, start, helper))\n        steps.append((start, helper))\n        steps.extend(hanoi(height - 1, helper, end))\n    return steps", "fixed": "def hanoi(height, start=1, end=3):\n    steps = []\n    if height > 0:\n        helper = ({1, 2, 3} - {start} - {end}).pop()\n        steps.extend(hanoi(height - 1, start, helper))\n        steps.append((start, end))\n        steps.extend(hanoi(height - 1, helper, end))\n    return steps", "hint": "Towers of Hanoi\nhanoi\nAn algorithm for solving the Towers of Hanoi puzzle.  Three pegs exist, with a stack of differently-sized", "input": [1, 1, 3], "output": [[1, 3]]}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "scrapy_23", "problem": " class RetryTest(unittest.TestCase):\n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req = self.mw.process_response(req, rsp, self.spider)", "fixed": " class RetryTest(unittest.TestCase):\n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req = self.mw.process_response(req, rsp, self.spider)"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "pandas_60", "problem": " class _Rolling_and_Expanding(_Rolling):\n             raise ValueError(\"engine must be either 'numba' or 'cython'\")\n         return self._apply(\n             apply_func,\n             center=False,\n             floor=0,\n             name=func,\n             use_numba_cache=engine == \"numba\",\n         )\n     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             raise ValueError(\"engine must be either 'numba' or 'cython'\")\n         return self._apply(\n             apply_func,\n             center=False,\n             floor=0,\n             name=func,\n             use_numba_cache=engine == \"numba\",\n            raw=raw,\n         )\n     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading"}
{"id": "ansible_3", "problem": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path == '/etc/lsb-release' and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:", "fixed": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:"}
{"id": "youtube-dl_31", "problem": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = int_or_none(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?([0-9]+)\\s*s',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',", "fixed": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = parse_duration(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?class=\"bold\">(.*?)<',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route(", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                    route_class_override=type(route),\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route("}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(left.values, right.values, obj=f\"{obj} category\")", "fixed": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(\n                left.values,\n                right.values,\n                obj=f\"{obj} category\",\n                check_category_order=check_category_order,\n            )"}
{"id": "youtube-dl_6", "problem": " def dfxp2srt(dfxp_data):\n         raise ValueError('Invalid dfxp/TTML subtitle')\n     for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib['begin'])\n         end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n         if not end_time:\n            end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])\n         out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n             index,\n             srt_subtitles_timecode(begin_time),", "fixed": " def dfxp2srt(dfxp_data):\n         raise ValueError('Invalid dfxp/TTML subtitle')\n     for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n         end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n         if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n         out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n             index,\n             srt_subtitles_timecode(begin_time),"}
{"id": "youtube-dl_26", "problem": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines."}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "pandas_58", "problem": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")"}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'", "fixed": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,\n                                                        use_marker=False)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops, use_marker=False)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops, use_marker=False)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta, use_marker=False)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'"}
{"id": "luigi_24", "problem": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '\"{0}={1}\"'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):", "fixed": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):"}
{"id": "fastapi_14", "problem": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None", "fixed": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[str, Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None"}
{"id": "pandas_138", "problem": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n     if dtype is not None:", "fixed": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        x = x.astype(np.int64)\n     if dtype is not None:"}
{"id": "thefuck_32", "problem": " def match(command, settings):\n    return 'ls' in command.script and not ('ls -' in command.script)\n def get_new_command(command, settings):", "fixed": " def match(command, settings):\n    return (command.script == 'ls'\n            or command.script.startswith('ls ')\n            and not ('ls -' in command.script))\n def get_new_command(command, settings):"}
{"id": "pandas_167", "problem": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n     _tz = None\n     _freq = None", "fixed": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n    _supports_partial_string_indexing = True\n     _tz = None\n     _freq = None"}
{"id": "youtube-dl_42", "problem": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_all_ampersand) \n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):", "fixed": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_ampersands)\n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):"}
{"id": "luigi_26", "problem": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar() or not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")", "fixed": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")"}
{"name": "minimum_spanning_tree.py", "problem": "def minimum_spanning_tree(weight_by_edge):\n    group_by_node = {}\n    mst_edges = set()\n    for edge in sorted(weight_by_edge, key=weight_by_edge.__getitem__):\n        u, v = edge\n        if group_by_node.setdefault(u, {u}) != group_by_node.setdefault(v, {v}):\n            mst_edges.add(edge)\n            group_by_node[u].update(group_by_node[v])\n            for node in group_by_node[v]:\n                group_by_node[node].update(group_by_node[u])\n    return mst_edges", "fixed": "def minimum_spanning_tree(weight_by_edge):\n    group_by_node = {}\n    mst_edges = set()\n    for edge in sorted(weight_by_edge, key=weight_by_edge.__getitem__):\n        u, v = edge\n        if group_by_node.setdefault(u, {u}) != group_by_node.setdefault(v, {v}):\n            mst_edges.add(edge)\n            group_by_node[u].update(group_by_node[v])\n            for node in group_by_node[v]:\n                group_by_node[node] = group_by_node[u]\n    return mst_edges", "hint": "Minimum Spanning Tree\nKruskal's algorithm implementation.\nInput:", "input": "", "output": ""}
{"id": "scrapy_34", "problem": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = {}\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)", "fixed": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n     obj=\"Series\",\n ):", "fixed": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n    check_category_order=True,\n     obj=\"Series\",\n ):"}
{"name": "shortest_paths.py", "problem": "def shortest_paths(source, weight_by_edge):\n    weight_by_node = {\n        v: float('inf') for u, v in weight_by_edge\n    }\n    weight_by_node[source] = 0\n    for i in range(len(weight_by_node) - 1):\n        for (u, v), weight in weight_by_edge.items():\n            weight_by_edge[u, v] = min(\n                weight_by_node[u] + weight,\n                weight_by_node[v]\n            )\n    return weight_by_node", "fixed": "def shortest_paths(source, weight_by_edge):\n    weight_by_node = {\n        v: float('inf') for u, v in weight_by_edge\n    }\n    weight_by_node[source] = 0\n    for i in range(len(weight_by_node) - 1):\n        for (u, v), weight in weight_by_edge.items():\n            weight_by_node[v] = min(\n                weight_by_node[u] + weight,\n                weight_by_node[v]\n            )\n    return weight_by_node", "hint": "Minimum-Weight Paths\nbellman-ford\nBellman-Ford algorithm implementation", "input": ["witch", "sandwich"], "output": "2"}
{"id": "black_23", "problem": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)", "fixed": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\npython_grammar_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_exec_statement.keywords[\"exec\"]\npython_grammar_no_print_statement_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"print\"]\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"exec\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)"}
{"id": "black_1", "problem": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        executor.shutdown()\n async def schedule_formatting(", "fixed": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        if executor is not None:\n            executor.shutdown()\n async def schedule_formatting("}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n }", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n }"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.non_trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n            if not self.trainable:\n                return self.cell.weights\n             return self.cell.non_trainable_weights\n         return []"}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "fastapi_6", "problem": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if field.shape in sequence_shapes and isinstance(\n                    received_body, FormData\n                ):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)", "fixed": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)"}
{"id": "matplotlib_23", "problem": " class _AxesBase(martist.Artist):\n             return\n         dL = self.dataLim\n        x0, x1 = map(x_trf.inverted().transform, dL.intervalx)\n        y0, y1 = map(y_trf.inverted().transform, dL.intervaly)\n         xr = 1.05 * (x1 - x0)\n         yr = 1.05 * (y1 - y0)", "fixed": " class _AxesBase(martist.Artist):\n             return\n         dL = self.dataLim\n        x0, x1 = map(x_trf.transform, dL.intervalx)\n        y0, y1 = map(y_trf.transform, dL.intervaly)\n         xr = 1.05 * (x1 - x0)\n         yr = 1.05 * (y1 - y0)"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "pandas_22", "problem": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type:\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"id": "pandas_53", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not self.holds_integer():\n                     self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n                     self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"name": "max_sublist_sum.py", "problem": "def max_sublist_sum(arr):\n    max_ending_here = 0\n    max_so_far = 0\n    for x in arr:\n        max_ending_here = max_ending_here + x\n        max_so_far = max(max_so_far, max_ending_here)\n    return max_so_far", "fixed": "def max_sublist_sum(arr):\n    max_ending_here = 0\n    max_so_far = 0\n    for x in arr:\n        max_ending_here = max(0, max_ending_here + x)\n        max_so_far = max(max_so_far, max_ending_here)\n    return max_so_far\n", "hint": "Max Sublist Sum\nmax-sublist-sum\nEfficient equivalent to max(sum(arr[i:j]) for 0 <= i <= j <= len(arr))", "input": [[4, -5, 2, 1, -1, 3]], "output": 5}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            self.csv_file = open(self.filename, 'a' + self.file_flags)\n         else:\n            self.csv_file = open(self.filename, 'w' + self.file_flags)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}", "fixed": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            mode = 'a'\n         else:\n            mode = 'w'\n        self.csv_file = io.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue", "fixed": " class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue"}
{"id": "pandas_5", "problem": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)", "fixed": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)\ndef test_join_multi_return_indexers():\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"name": "pascal.py", "problem": "def pascal(n):\n    rows = [[1]]\n    for r in range(1, n):\n        row = []\n        for c in range(0, r):\n            upleft = rows[r - 1][c - 1] if c > 0 else 0\n            upright = rows[r - 1][c] if c < r else 0\n            row.append(upleft + upright)\n        rows.append(row)\n    return rows", "fixed": "def pascal(n):\n    rows = [[1]]\n    for r in range(1, n):\n        row = []\n        for c in range(0, r + 1):\n            upleft = rows[r - 1][c - 1] if c > 0 else 0\n            upright = rows[r - 1][c] if c < r else 0\n            row.append(upleft + upright)\n        rows.append(row)\n    return rows", "hint": "Pascal's Triangle\npascal\nInput:", "input": [3], "output": [[1], [1, 1], [1, 2, 1]]}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "keras_30", "problem": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if x is None or len(x) == 0:\n                    batch_size = 1\n                elif isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":", "fixed": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":"}
{"id": "luigi_29", "problem": " class AmbiguousClass(luigi.Task):\n     pass\nclass NonAmbiguousClass(luigi.ExternalTask):\n    pass\nclass NonAmbiguousClass(luigi.Task):\n    def run(self):\n        NonAmbiguousClass.has_run = True\n class TaskWithSameName(luigi.Task):\n     def run(self):", "fixed": " class AmbiguousClass(luigi.Task):\n     pass\n class TaskWithSameName(luigi.Task):\n     def run(self):"}
{"id": "youtube-dl_18", "problem": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()", "fixed": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()"}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n         Raises\n         ------\n         TypeError", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n        .. versionchanged:: 1.0.0\n           Returns an NA value on empty arrays\n         Raises\n         ------\n         TypeError"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n     def __init__(self,", "fixed": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n        restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n     def __init__(self,"}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):", "fixed": " class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n        if six.PY2:\n            self.file_flags = 'b'\n            self._open_args = {}\n        else:\n            self.file_flags = ''\n            self._open_args = {'newline': '\\n'}\n         super(CSVLogger, self).__init__()\n     def on_train_begin(self, logs=None):"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"name": "mergesort.py", "problem": "def mergesort(arr):\n    def merge(left, right):\n        result = []\n        i = 0\n        j = 0\n        while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[i:] or right[j:])\n        return result\n    if len(arr) == 0:\n        return arr\n    else:\n        middle = len(arr) // 2\n        left = mergesort(arr[:middle])\n        right = mergesort(arr[middle:])\n        return merge(left, right)", "fixed": "def mergesort(arr):\n    def merge(left, right):\n        result = []\n        i = 0\n        j = 0\n        while i < len(left) and j < len(right):\n            if left[i] <= right[j]:\n                result.append(left[i])\n                i += 1\n            else:\n                result.append(right[j])\n                j += 1\n        result.extend(left[i:] or right[j:])\n        return result\n    if len(arr) <= 1:\n        return arr\n    else:\n        middle = len(arr) // 2\n        left = mergesort(arr[:middle])\n        right = mergesort(arr[middle:])\n        return merge(left, right)\n", "hint": "Merge Sort\nInput:\n    arr: A list of ints", "input": [[1, 2, 6, 72, 7, 33, 4]], "output": [1, 2, 4, 6, 7, 33, 72]}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\n    if re.match(r'^(?:https?:)?//', path):\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n             False,\n             padding,\n             padding],\n        output_shape=output_shape)\n     return _postprocess_conv2d_output(x, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n             False,\n             padding,\n             padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n     return _postprocess_conv2d_output(x, data_format)"}
{"id": "pandas_80", "problem": " def make_data():\n @pytest.fixture\n def dtype():\n    return pd.BooleanDtype()\n @pytest.fixture", "fixed": " def make_data():\n @pytest.fixture\n def dtype():\n    return BooleanDtype()\n @pytest.fixture"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):", "fixed": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other, sort=sort)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):"}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if is_sequence:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'", "fixed": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if use_sequence_api:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'"}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "thefuck_31", "problem": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return '{} --staged'.format(command.script)", "fixed": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return command.script.replace(' diff', ' diff --staged')"}
{"id": "pandas_125", "problem": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)\n    def replace(self, to_replace, value, inplace: bool = False):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                else:\n                    categories[index] = value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat"}
{"id": "pandas_16", "problem": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other)\n         result._freq = new_freq\n         return result", "fixed": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other, result)\n         result._freq = new_freq\n         return result"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "luigi_11", "problem": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items())):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)", "fixed": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)"}
{"id": "fastapi_14", "problem": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[bool, Any]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None", "fixed": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[Dict[str, Any], bool]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)"}
{"id": "matplotlib_5", "problem": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            linewidths = rcParams['lines.linewidth']\n         offsets = np.ma.column_stack([x, y])", "fixed": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            if linewidths is None:\n                linewidths = rcParams['lines.linewidth']\n            elif np.iterable(linewidths):\n                linewidths = [\n                    lw if lw is not None else rcParams['lines.linewidth']\n                    for lw in linewidths]\n         offsets = np.ma.column_stack([x, y])"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "pandas_94", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self._data\n        if isinstance(values, type(self)):\n            values = values._data\n        attributes = self._get_attributes_dict()\n        if \"freq\" not in kwargs and self.freq is not None:\n            if isinstance(values, (DatetimeArray, TimedeltaArray)):\n                if values.freq is None:\n                    del attributes[\"freq\"]\n        attributes.update(kwargs)\n        return self._simple_new(values, **attributes)"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "matplotlib_4", "problem": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,", "fixed": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,"}
{"id": "pandas_48", "problem": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except ValueError:\n                         result = result.reshape(1, -1)", "fixed": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except (ValueError, TypeError):\n                         result = result.reshape(1, -1)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "tornado_4", "problem": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if (start is not None and start >= size) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n            if start is not None and start < 0:\n                start += size\n             if end is not None and end > size:", "fixed": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if start is not None and start < 0:\n                start += size\n                if start < 0:\n                    start = 0\n            if (\n                start is not None\n                and (start >= size or (end is not None and start >= end))\n            ) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n             if end is not None and end > size:"}
{"name": "wrap.py", "problem": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    return lines", "fixed": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    lines.append(text)\n    return lines", "hint": "Wrap Text\nGiven a long string and a column width, break the string on spaces into a list of lines such that each line is no longer than the column width.\nInput:", "input": [], "output": ""}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period):\n            if value.freq != self.freq:\n                raise raise_on_incompatible(self, value)\n            value = value.ordinal\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq).ordinal\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period) or value is NaT:\n            self._data._check_compatible_with(value)\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq)\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        elif not isinstance(value, PeriodArray):\n            raise TypeError(\n                \"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:"}
{"id": "pandas_24", "problem": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq='D')\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):", "fixed": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n         else:\n             left, right = other, self", "fixed": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n        elif sort is False:\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n         else:\n             left, right = other, self"}
{"id": "keras_1", "problem": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        increment = np.random.randn(3, 4)\n        x += increment\n        K.eval(K.update_add(x_var, increment))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        decrement = np.random.randn(3, 4)\n        x -= decrement\n        K.eval(K.update_sub(x_var, decrement))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')", "fixed": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n    @pytest.mark.skipif(K.backend() == 'theano',\n                        reason='theano returns tuples for update ops')\n    def test_update(self):\n        x = np.ones((3, 4))\n        x_var = K.variable(x)\n        new_x = np.random.random((3, 4))\n        op = K.update(x_var, new_x)\n        K.eval(op)\n        assert_allclose(new_x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        increment = np.random.random((3, 4))\n        op = K.update_add(x_var, increment)\n        K.eval(op)\n        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        decrement = np.random.random((3, 4))\n        op = K.update_sub(x_var, decrement)\n        K.eval(op)\n        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')"}
{"id": "pandas_105", "problem": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested", "fixed": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n    @skip_nested\n    def test_transpose(self, data):\n        super().test_transpose(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "youtube-dl_39", "problem": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output", "fixed": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output\ndef limit_length(s, length):\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s"}
{"name": "shortest_path_length.py", "problem": "from heapq import *\ndef shortest_path_length(length_by_edge, startnode, goalnode):\nunvisited_nodes = []\n    heappush(unvisited_nodes, (0, startnode))\n    visited_nodes = set()\n    while len(unvisited_nodes) > 0:\n        distance, node = heappop(unvisited_nodes)\n        if node is goalnode:\n            return distance\n        visited_nodes.add(node)\n        for nextnode in node.successors:\n            if nextnode in visited_nodes:\n                continue\n            insert_or_update(unvisited_nodes,\n                (min(\n                    get(unvisited_nodes, nextnode) or float('inf'),\n                    get(unvisited_nodes, nextnode) + length_by_edge[node, nextnode]\n                ),\n                nextnode)\n            )\n    return float('inf')\ndef get(node_heap, wanted_node):\n    for dist, node in node_heap:\n        if node == wanted_node:\n            return dist\n    return 0\ndef insert_or_update(node_heap, dist_node):\n    dist, node = dist_node\n    for i, tpl in enumerate(node_heap):\n        a, b = tpl\n        if b == node:\nnode_heap[i] = dist_node\n            return None\n    heappush(node_heap, dist_node)\n    return None", "fixed": "from heapq import *\ndef shortest_path_length(length_by_edge, startnode, goalnode):\nunvisited_nodes = []\n    heappush(unvisited_nodes, (0, startnode))\n    visited_nodes = set()\n    while len(unvisited_nodes) > 0:\n        distance, node = heappop(unvisited_nodes)\n        if node is goalnode:\n            return distance\n        visited_nodes.add(node)\n        for nextnode in node.successors:\n            if nextnode in visited_nodes:\n                continue\n            insert_or_update(unvisited_nodes,\n                (min(\n                    get(unvisited_nodes, nextnode) or float('inf'),\n                    distance + length_by_edge[node, nextnode]\n                ),\n                nextnode)\n            )\n    return float('inf')\ndef get(node_heap, wanted_node):\n    for dist, node in node_heap:\n        if node == wanted_node:\n            return dist\n    return 0\ndef insert_or_update(node_heap, dist_node):\n    dist, node = dist_node\n    for i, tpl in enumerate(node_heap):\n        a, b = tpl\n        if b == node:\nnode_heap[i] = dist_node\n            return None\n    heappush(node_heap, dist_node)\n    return None\n", "hint": "Shortest Path\ndijkstra\nImplements Dijkstra's algorithm for finding a shortest path between two nodes in a directed graph.", "input": [], "output": ""}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "luigi_15", "problem": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks", "fixed": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is", "fixed": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is"}
{"id": "tornado_5", "problem": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n         if self._next_timeout <= current_time:\n            callback_time_sec = self.callback_time / 1000.0\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec", "fixed": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n        callback_time_sec = self.callback_time / 1000.0\n         if self._next_timeout <= current_time:\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec\n        else:\n            self._next_timeout += callback_time_sec"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n        body_param_media_types = [\n            getattr(f.schema, \"media_type\")\n            for f in flat_dependant.body_params\n            if isinstance(f.schema, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodySchema_kwargs[\"media_type\"] = body_param_media_types[0]\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"name": "bitcount.py", "problem": "def bitcount(n):\n    count = 0\n    while n:\n        n ^= n - 1\n        count += 1\n    return count", "fixed": "def bitcount(n):\n    count = 0\n    while n:\n        n &= n - 1\n        count += 1\n    return count", "hint": "Bitcount\nbitcount\nInput:", "input": [127], "output": 7}
{"id": "tqdm_4", "problem": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale", "fixed": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            if total:\n                total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale"}
{"id": "matplotlib_3", "problem": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = True\n         self._marker_function()\n     def __bool__(self):", "fixed": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = self._fillstyle != 'none'\n         self._marker_function()\n     def __bool__(self):"}
{"id": "scrapy_32", "problem": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(settings)\n        log_scrapy_info(settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)", "fixed": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings)\n        log_scrapy_info(self.settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)"}
{"id": "black_15", "problem": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: str) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        list(v.visit(lib2to3_parse(code)))\n KEYWORDS = set(keyword.kwlist)", "fixed": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: Union[str, Leaf, Node]) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))\n KEYWORDS = set(keyword.kwlist)"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "keras_1", "problem": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)", "fixed": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n            self.seed += 1\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)"}
{"id": "pandas_45", "problem": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)", "fixed": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n    elif isinstance(data, abc.Set):\n        raise TypeError(\"Set type is unordered\")\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n     stashed = None\n     async_def = False", "fixed": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n    async_is_reserved_keyword = config.async_is_reserved_keyword\n     stashed = None\n     async_def = False"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n    BodySchema_kwargs: Dict[str, Any] = dict(default=None)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        if m.group('strval') is not None:\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or\n            actual_value is not None and m.group('intval') is not None and\n                isinstance(actual_value, compat_str)):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "pandas_70", "problem": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})", "fixed": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n        if len(weights) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())", "fixed": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n    class_sample_weight = None\n    if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if len(y.shape) == 2:\n            if y.shape[1] > 1:\n                y_classes = np.argmax(y, axis=1)\n            elif y.shape[1] == 1:\n                y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        class_sample_weight = np.asarray(\n            [class_weight[cls] for cls in y_classes if cls in class_weight])\n        if len(class_sample_weight) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '", "fixed": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '"}
{"id": "youtube-dl_11", "problem": " def str_or_none(v, default=None):\n def str_to_int(int_str):\n    if int_str is None:\n        return None\n     int_str = re.sub(r'[,\\.\\+]', '', int_str)\n     return int(int_str)", "fixed": " def str_or_none(v, default=None):\n def str_to_int(int_str):\n    if not isinstance(int_str, compat_str):\n        return int_str\n     int_str = re.sub(r'[,\\.\\+]', '', int_str)\n     return int(int_str)"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n        Yields :class:`Line` objects.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            try:\n                for comment in generate_comments(node):\n                    if any_open_brackets:\n                        self.current_line.append(comment)\n                    elif comment.type == token.COMMENT:\n                        self.current_line.append(comment)\n                        yield from self.line()\n                    else:\n                        yield from self.line()\n                        self.current_line.append(comment)\n                        yield from self.line()\n            except FormatOff as f_off:\n                f_off.trim_prefix(node)\n                yield from self.line(type=UnformattedLines)\n                yield from self.visit(node)\n            except FormatOn as f_on:\n                f_on.trim_prefix(node)\n                yield from self.visit_default(node)\n            else:\n                normalize_prefix(node, inside_brackets=any_open_brackets)\n                if self.normalize_strings and node.type == token.STRING:\n                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                    normalize_string_quotes(node)\n                if node.type not in WHITESPACE:\n                    self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    self.current_line.append(comment)\n                    yield from self.line()\n                else:\n                    yield from self.line()\n                    self.current_line.append(comment)\n                    yield from self.line()\n            normalize_prefix(node, inside_brackets=any_open_brackets)\n            if self.normalize_strings and node.type == token.STRING:\n                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                normalize_string_quotes(node)\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:"}
{"id": "tornado_3", "problem": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            if self._instance_cache.get(self.io_loop) is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n            del self._instance_cache[self.io_loop]\n     def fetch(\n         self,", "fixed": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            if cached_val is not None and cached_val is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n     def fetch(\n         self,"}
{"id": "pandas_161", "problem": " class Categorical(ExtensionArray, PandasObject):\n                     raise ValueError(\"fill value must be in categories\")\n                 values_codes = _get_codes_for_values(value, self.categories)\n                indexer = np.where(values_codes != -1)\n                codes[indexer] = values_codes[values_codes != -1]\n             elif is_hashable(value):", "fixed": " class Categorical(ExtensionArray, PandasObject):\n                     raise ValueError(\"fill value must be in categories\")\n                 values_codes = _get_codes_for_values(value, self.categories)\n                indexer = np.where(codes == -1)\n                codes[indexer] = values_codes[indexer]\n             elif is_hashable(value):"}
{"id": "pandas_151", "problem": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        values = self._ndarray\n        t = np.result_type(value, values)\n        if t != self._ndarray.dtype:\n            values = values.astype(t, casting=\"safe\")\n            values[key] = value\n            self._dtype = PandasDtype(t)\n            self._ndarray = values\n        else:\n            self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)", "fixed": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        value = np.asarray(value, dtype=self._ndarray.dtype)\n        self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.resize(ymin, x.shape)\n        ymax = np.resize(ymax, x.shape)\n        verts = [((thisx, thisymin), (thisx, thisymax))\n                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.ravel(ymin)\n        ymax = np.ravel(ymax)\n        masked_verts = np.ma.empty((len(x), 2, 2))\n        masked_verts[:, 0, 0] = x\n        masked_verts[:, 0, 1] = ymin\n        masked_verts[:, 1, 0] = x\n        masked_verts[:, 1, 1] = ymax\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "fastapi_15", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n     def get(\n         self,", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n     def get(\n         self,"}
{"id": "pandas_123", "problem": " class RangeIndex(Int64Index):\n    @staticmethod\n    def _validate_dtype(dtype):", "fixed": " class RangeIndex(Int64Index):"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "keras_14", "problem": " def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),\n                   axis=-1)", "fixed": " def top_k_categorical_accuracy(y_true, y_pred, k=5):\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),\n                   axis=-1)"}
{"id": "pandas_36", "problem": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj_old(values.ravel())\n            result[:] = vec.reshape(shape)\n    elif is_datetime64_dtype(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:", "fixed": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n    elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, extra={'spider': info.spider, 'failure': f})\n         )\nreturn dfd.addBoth(lambda _: wad)", "fixed": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\nreturn dfd.addBoth(lambda _: wad)"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n        attributes.pop(\"tz\", None)\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):"}
{"id": "ansible_10", "problem": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type", "fixed": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n        self.prev = None\n        self.next = None\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type"}
{"id": "thefuck_20", "problem": " def _zip_file(command):\n    for c in command.script.split()[1:]:\n         if not c.startswith('-'):\n             if c.endswith('.zip'):\n                 return c", "fixed": " def _zip_file(command):\n    for c in command.split_script[1:]:\n         if not c.startswith('-'):\n             if c.endswith('.zip'):\n                 return c"}
{"id": "ansible_9", "problem": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, 1\n        pool_ids[pool_id] = str(quantity)\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]", "fixed": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, None\n        pool_ids[pool_id] = quantity\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]"}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "scrapy_3", "problem": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['location'])\n         redirected_url = urljoin(request.url, location)", "fixed": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['Location'])\n        if response.headers['Location'].startswith(b'//'):\n            request_scheme = urlparse(request.url).scheme\n            location = request_scheme + '://' + location.lstrip('/')\n         redirected_url = urljoin(request.url, location)"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "thefuck_23", "problem": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with shelve.open(cache_path) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:", "fixed": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with closing(shelve.open(cache_path)) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"name": "quicksort.py", "problem": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x > pivot])\n    return lesser + [pivot] + greater", "fixed": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x >= pivot])\n    return lesser + [pivot] + greater\n", "hint": "QuickSort\nInput:\n    arr: A list of ints", "input": [17, 0], "output": 17}
{"name": "kth.py", "problem": "def kth(arr, k):\n    pivot = arr[0]\n    below = [x for x in arr if x < pivot]\n    above = [x for x in arr if x > pivot]\n    num_less = len(below)\n    num_lessoreq = len(arr) - len(above)\n    if k < num_less:\n        return kth(below, k)\n    elif k >= num_lessoreq:\n        return kth(above, k)\n    else:\n        return pivot", "fixed": "def kth(arr, k):\n    pivot = arr[0]\n    below = [x for x in arr if x < pivot]\n    above = [x for x in arr if x > pivot]\n    num_less = len(below)\n    num_lessoreq = len(arr) - len(above)\n    if k < num_less:\n        return kth(below, k)\n    elif k >= num_lessoreq:\n        return kth(above, k - num_lessoreq)\n    else:\n        return pivot", "hint": "QuickSelect\nThis is an efficient equivalent to sorted(arr)[k].\nInput:", "input": [[1, 2, 3, 4, 5, 6, 7], 4], "output": 5}
{"id": "pandas_20", "problem": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):", "fixed": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):"}
{"id": "scrapy_20", "problem": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)", "fixed": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)"}
