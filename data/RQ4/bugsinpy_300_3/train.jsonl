{"id": "pandas_31", "problem": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals):\n                 inference = np.int64\n            elif is_datetime64_dtype(vals):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)", "fixed": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                 inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)"}
{"id": "black_22", "problem": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36=py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")", "fixed": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")"}
{"id": "black_20", "problem": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src.name}  (original)\"\n        dst_name = f\"{src.name}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()", "fixed": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src}  (original)\"\n        dst_name = f\"{src}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()"}
{"id": "keras_1", "problem": " def update_sub(x, decrement):\n         The variable `x` updated.\n    return tf_state_ops.assign_sub(x, decrement)\n @symbolic", "fixed": " def update_sub(x, decrement):\n         The variable `x` updated.\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "keras_1", "problem": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.random_normal(shape, self.mean, self.stddev,\n                            dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "pandas_112", "problem": " import re\n import numpy as np\n import pytest\nfrom pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm", "fixed": " import re\n import numpy as np\n import pytest\nfrom pandas import (\n    CategoricalIndex,\n    Interval,\n    IntervalIndex,\n    Timedelta,\n    date_range,\n    timedelta_range,\n)\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm"}
{"id": "pandas_55", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            lower = self._take_with_is_copy(item, axis=self._info_axis_number)\n         return lower\n     def _box_item_values(self, key, values):", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item: int):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            return self._ixs(item, axis=1)\n         return lower\n     def _box_item_values(self, key, values):"}
{"id": "thefuck_17", "problem": "from subprocess import Popen, PIPE\n import os\n from ..conf import settings\nfrom ..utils import DEVNULL, memoize, cache\n from .generic import Generic", "fixed": " import os\n from ..conf import settings\nfrom ..utils import memoize\n from .generic import Generic"}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"id": "PySnooper_1", "problem": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')", "fixed": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\nif pycompat.PY2:\n    from io import open\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2", "fixed": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n     }, {\n'url': 'https:\n         'only_matching': True,", "fixed": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n    }, {\n        'note': 'Video without discernible title',\n'url': 'https:\n        'info_dict': {\n            'id': '274175099429670',\n            'ext': 'mp4',\n            'title': 'Facebook video\n        }\n     }, {\n'url': 'https:\n         'only_matching': True,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):", "fixed": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\nelif v.startswith('/*') or v.startswith('\n             return \"\"\n         if v[0] in (\"'\", '\"'):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_41", "problem": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")", "fixed": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n    def should_store(self, value):\n        return is_dtype_equal(self.dtype, value.dtype)\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")"}
{"id": "pandas_80", "problem": "from .interface import BaseInterfaceTests\nfrom .io import BaseParsingTests\nfrom .methods import BaseMethodsTests\nfrom .missing import BaseMissingTests\nfrom .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil\nfrom .printing import BasePrintingTests\nfrom .reduce import (\n     BaseBooleanReduceTests,", "fixed": "from .interface import BaseInterfaceTests\nfrom .io import BaseParsingTests\nfrom .methods import BaseMethodsTests\nfrom .missing import BaseMissingTests\nfrom .ops import (\n    BaseArithmeticOpsTests,\n    BaseComparisonOpsTests,\n    BaseOpsUtil,\n    BaseUnaryOpsTests,\n)\nfrom .printing import BasePrintingTests\nfrom .reduce import (\n     BaseBooleanReduceTests,"}
{"id": "spacy_8", "problem": " class Errors(object):\n\"details: https:\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n @add_codes", "fixed": " class Errors(object):\n\"details: https:\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n    E175 = (\"Can't remove rule for unknown match pattern ID: {key}\")\n @add_codes"}
{"id": "pandas_112", "problem": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)", "fixed": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical(target_as_index):\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)"}
{"id": "thefuck_21", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return (command.script.split()[1] == 'stash'\n            and 'usage:' in command.stderr)\n stash_commands = (", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    splited_script = command.script.split()\n    if len(splited_script) > 1:\n        return (splited_script[1] == 'stash'\n                and 'usage:' in command.stderr)\n    else:\n        return False\n stash_commands = ("}
{"id": "pandas_105", "problem": " def box_df_fail(request):\n     return request.param\n@pytest.fixture(\n    params=[\n        (pd.Index, False),\n        (pd.Series, False),\n        (pd.DataFrame, False),\n        pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),\n        (tm.to_array, False),\n    ],\n    ids=id_func,\n)\ndef box_transpose_fail(request):\n    return request.param\n @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)\n def box_with_array(request):", "fixed": " def box_df_fail(request):\n     return request.param\n @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)\n def box_with_array(request):"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "keras_23", "problem": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape", "fixed": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.power(self._base,\n                       sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj"}
{"id": "pandas_41", "problem": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )", "fixed": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )"}
{"id": "pandas_125", "problem": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result", "fixed": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        filter=None,\n        regex: bool = False,\n        convert: bool = True,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = self if inplace else self.copy()\n        if filter is None:\n            result.values.replace(to_replace, value, inplace=True)\n            if convert:\n                return result.convert(numeric=False, copy=not inplace)\n            else:\n                return result\n        else:\n            if not isna(value):\n                result.values.add_categories(value, inplace=True)\n            return super(CategoricalBlock, result).replace(\n                to_replace, value, inplace, filter, regex, convert\n            )"}
{"id": "pandas_46", "problem": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n     def test_empty(self):\n         X = [[], [0, 1], []]", "fixed": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n     def test_empty(self):\n         X = [[], [0, 1], []]"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:"}
{"id": "ansible_14", "problem": " from ansible.module_utils.urls import open_url\n from ansible.utils.display import Display\n from ansible.utils.hashing import secure_hash_s\n display = Display()", "fixed": " from ansible.module_utils.urls import open_url\n from ansible.utils.display import Display\n from ansible.utils.hashing import secure_hash_s\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n display = Display()"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "scrapy_12", "problem": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n         if _root is not None:", "fixed": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n         st = _st(response, type or self._default_type)\n         if _root is not None:"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n        ordinal_op = getattr(self.asi8, opname)\n         if isinstance(other, str):\n             try:", "fixed": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n         if isinstance(other, str):\n             try:"}
{"id": "pandas_108", "problem": " from .common import (\n     is_unsigned_integer_dtype,\n     pandas_dtype,\n )\nfrom .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype\n from .generic import (\n     ABCDataFrame,\n     ABCDatetimeArray,", "fixed": " from .common import (\n     is_unsigned_integer_dtype,\n     pandas_dtype,\n )\nfrom .dtypes import DatetimeTZDtype, ExtensionDtype, IntervalDtype, PeriodDtype\n from .generic import (\n     ABCDataFrame,\n     ABCDatetimeArray,"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "pandas_34", "problem": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=\"infer\",\n             nonexistent=\"shift_forward\",\n         )", "fixed": " class TimeGrouper(Grouper):\n         binner = labels = date_range(\n             freq=self.freq,\n             start=first,\n             end=last,\n             tz=ax.tz,\n             name=ax.name,\n            ambiguous=True,\n             nonexistent=\"shift_forward\",\n         )"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"id": "black_6", "problem": " from blib2to3 import pygram, pytree\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\n __version__ = \"19.3b0\"", "fixed": " from blib2to3 import pygram, pytree\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenizerConfig\n __version__ = \"19.3b0\""}
{"id": "scrapy_33", "problem": " from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n logger = logging.getLogger(__name__)", "fixed": " from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "pandas_67", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, algos as libalgos, lib, tslib, writers\n from pandas._libs.index import convert_scalar\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n from pandas._libs.index import convert_scalar\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion"}
{"id": "tornado_15", "problem": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root)\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)", "fixed": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root) + os.path.sep\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)"}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "pandas_153", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "pandas_5", "problem": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)", "fixed": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)\ndef test_join_multi_return_indexers():\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "PySnooper_2", "problem": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')\ndef get_local_reprs(frame, watch=()):\n     code = frame.f_code\n     vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())\n    result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]\n     result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))\n     result = collections.OrderedDict(result_items)", "fixed": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\nif pycompat.PY2:\n    from io import open\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')\ndef get_local_reprs(frame, watch=(), custom_repr=()):\n     code = frame.f_code\n     vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())\n    result_items = [(key, utils.get_shortish_repr(value, custom_repr=custom_repr)) for key, value in frame.f_locals.items()]\n     result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))\n     result = collections.OrderedDict(result_items)"}
{"id": "pandas_16", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            return self.freq\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            if is_period_dtype(result.dtype):\n                return self.freq\n            return None\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):"}
{"id": "black_23", "problem": " def func_no_args():\n         print(i)\n         continue\n     return None\nasync def coroutine(arg):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n         print(i)\n         continue\n    exec(\"new-style exec\", {}, {})\n     return None\nasync def coroutine(arg, exec=False):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "pandas_155", "problem": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            return self.obj.index\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "black_8", "problem": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                if leaves[-1].type != token.COMMA:\n                    leaves.append(Leaf(token.COMMA, \",\"))\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)", "fixed": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                for i in range(len(leaves) - 1, -1, -1):\n                    if leaves[i].type == STANDALONE_COMMENT:\n                        continue\n                    elif leaves[i].type == token.COMMA:\n                        break\n                    else:\n                        leaves.insert(i + 1, Leaf(token.COMMA, \",\"))\n                        break\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)"}
{"id": "black_21", "problem": " def dump_to_file(*output: str) -> str:\n     import tempfile\n     with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False\n     ) as f:\n         for lines in output:\n             f.write(lines)", "fixed": " def dump_to_file(*output: str) -> str:\n     import tempfile\n     with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False, encoding=\"utf8\"\n     ) as f:\n         for lines in output:\n             f.write(lines)"}
{"id": "pandas_68", "problem": " class BaseMethodsTests(BaseExtensionTests):\n         expected = empty\n         self.assert_extension_array_equal(result, expected)\n     def test_shift_fill_value(self, data):\n         arr = data[:4]\n         fill_value = data[0]", "fixed": " class BaseMethodsTests(BaseExtensionTests):\n         expected = empty\n         self.assert_extension_array_equal(result, expected)\n    def test_shift_zero_copies(self, data):\n        result = data.shift(0)\n        assert result is not data\n        result = data[:0].shift(2)\n        assert result is not data\n     def test_shift_fill_value(self, data):\n         arr = data[:4]\n         fill_value = data[0]"}
{"id": "scrapy_30", "problem": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()\n        return comm[0].strip()\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\", "fixed": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "pandas_151", "problem": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        values = self._ndarray\n        t = np.result_type(value, values)\n        if t != self._ndarray.dtype:\n            values = values.astype(t, casting=\"safe\")\n            values[key] = value\n            self._dtype = PandasDtype(t)\n            self._ndarray = values\n        else:\n            self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)", "fixed": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        value = np.asarray(value, dtype=self._ndarray.dtype)\n        self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)"}
{"id": "pandas_9", "problem": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_scalar(key) and isna(key):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)", "fixed": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)"}
{"id": "pandas_137", "problem": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort", "fixed": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import array, extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort"}
{"id": "pandas_53", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not self.holds_integer():\n                     self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n                     self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "pandas_41", "problem": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype", "fixed": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value: ArrayLike) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype"}
{"id": "scrapy_19", "problem": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n     def has_header(self, name):\n         return name in self.request.headers", "fixed": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n    @property\n    def full_url(self):\n        return self.get_full_url()\n    @property\n    def host(self):\n        return self.get_host()\n    @property\n    def type(self):\n        return self.get_type()\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n     def has_header(self, name):\n         return name in self.request.headers"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n     obj=\"Series\",\n ):", "fixed": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n    check_category_order=True,\n     obj=\"Series\",\n ):"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):", "fixed": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):"}
{"id": "scrapy_26", "problem": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n            compsett.update(self[name])\n             return compsett\n        else:\n            return self[name]\n     def getpriority(self, name):", "fixed": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[basename], priority='default')\n            for k in self[name]:\n                prio = self[name].getpriority(k)\n                if prio > get_settings_priority('default'):\n                    compsett.set(k, self[name][k], prio)\n             return compsett\n        return self[name]\n     def getpriority(self, name):"}
{"id": "scrapy_33", "problem": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n         return failure\n     dont_log = named.pop('dont_log', None)", "fixed": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n         return failure\n     dont_log = named.pop('dont_log', None)"}
{"id": "thefuck_2", "problem": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(':')\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]", "fixed": " def get_all_executables():\n     tf_entry_points = ['thefuck', 'fuck']\n     bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(os.pathsep)\n             for exe in _safe(lambda: list(Path(path).iterdir()), [])\n             if not _safe(exe.is_dir, True)\n             and exe.name not in tf_entry_points]"}
{"id": "luigi_7", "problem": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n             if status == PENDING or status != task.status:", "fixed": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:\n             if status == PENDING or status != task.status:"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n                 f\"is not equal to {right._values}.\"\n             )\n             raise AssertionError(msg)\n    elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):\n         assert_interval_array_equal(left.array, right.array)\n     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):\n         _testing.assert_almost_equal(", "fixed": " def assert_series_equal(\n                 f\"is not equal to {right._values}.\"\n             )\n             raise AssertionError(msg)\n    elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):\n         assert_interval_array_equal(left.array, right.array)\n     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):\n         _testing.assert_almost_equal("}
{"id": "cookiecutter_4", "problem": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.", "fixed": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.\n    Raised when a hook script fails"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "ansible_4", "problem": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)\n     def _load_collections(self, attr, ds):", "fixed": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n     def _load_collections(self, attr, ds):"}
{"id": "thefuck_12", "problem": " from difflib import get_close_matches\n from thefuck.utils import get_all_executables, \\\n    get_valid_history_without_current, get_closest\n from thefuck.specific.sudo import sudo_support\n @sudo_support\n def match(command):\n    return (command.script_parts\n             and 'not found' in command.stderr\n             and bool(get_close_matches(command.script_parts[0],\n                                        get_all_executables())))", "fixed": " from difflib import get_close_matches\n from thefuck.utils import get_all_executables, \\\n    get_valid_history_without_current, get_closest, which\n from thefuck.specific.sudo import sudo_support\n @sudo_support\n def match(command):\n    return (not which(command.script_parts[0])\n             and 'not found' in command.stderr\n             and bool(get_close_matches(command.script_parts[0],\n                                        get_all_executables())))"}
{"id": "cookiecutter_4", "problem": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return EXIT_SUCCESS\n    return run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return\n    run_script_with_context(script, project_dir, context)"}
{"id": "spacy_7", "problem": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n     return result", "fixed": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n     return result"}
{"id": "fastapi_14", "problem": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[bool, Any]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None", "fixed": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[Dict[str, Any], bool]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n     ) -> None:\n        route = self.route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,", "fixed": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n     ) -> None:\n        route_class = route_class_override or self.route_class\n        route = route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "pandas_65", "problem": " def get_handle(\n     try:\n         from s3fs import S3File\n        need_text_wrapping = (BufferedIOBase, S3File)\n     except ImportError:\n        need_text_wrapping = BufferedIOBase\n     handles: List[IO] = list()\n     f = path_or_buf", "fixed": " def get_handle(\n     try:\n         from s3fs import S3File\n        need_text_wrapping = (BufferedIOBase, RawIOBase, S3File)\n     except ImportError:\n        need_text_wrapping = (BufferedIOBase, RawIOBase)\n     handles: List[IO] = list()\n     f = path_or_buf"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:", "fixed": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:"}
{"id": "luigi_26", "problem": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar() or not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")", "fixed": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")"}
{"id": "scrapy_38", "problem": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:", "fixed": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "scrapy_16", "problem": " to the w3lib.url module. Always import those from there instead.\n import posixpath\n import re\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                    unquote)\n from w3lib.url import *\n from w3lib.url import _safe_chars\nfrom scrapy.utils.python import to_native_str\n def url_is_from_any_domain(url, domains):", "fixed": " to the w3lib.url module. Always import those from there instead.\n import posixpath\n import re\nimport six\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                    quote, unquote)\nif six.PY3:\n    from urllib.parse import unquote_to_bytes\n from w3lib.url import *\n from w3lib.url import _safe_chars\nfrom scrapy.utils.python import to_bytes, to_native_str, to_unicode\n def url_is_from_any_domain(url, domains):"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(description_original, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is", "fixed": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is"}
{"id": "ansible_11", "problem": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.connection import exec_command\nfrom ansible.module_utils.network.ios.ios import load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nimport re\n def map_obj_to_commands(updates, module):", "fixed": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.network.ios.ios import get_config, load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nfrom re import search, M\n def map_obj_to_commands(updates, module):"}
{"id": "thefuck_25", "problem": " def match(command, settings):\n @sudo_support\n def get_new_command(command, settings):\n    return re.sub('^mkdir (.*)', 'mkdir -p \\\\1', command.script)", "fixed": " def match(command, settings):\n @sudo_support\n def get_new_command(command, settings):\n    return re.sub('\\\\bmkdir (.*)', 'mkdir -p \\\\1', command.script)"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]", "fixed": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"C\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]"}
{"id": "pandas_87", "problem": " def crosstab(\n         **kwargs,\n     )\n     if normalize is not False:\n         table = _normalize(", "fixed": " def crosstab(\n         **kwargs,\n     )\n    if not table.empty:\n        cols_diff = df.columns.difference(original_df_cols)[0]\n        table = table[cols_diff]\n     if normalize is not False:\n         table = _normalize("}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "keras_1", "problem": " def update(x, new_x):\n         The variable `x` updated.\n    return tf_state_ops.assign(x, new_x)\n @symbolic", "fixed": " def update(x, new_x):\n         The variable `x` updated.\n    op = tf_state_ops.assign(x, new_x)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "pandas_37", "problem": " class StringArray(PandasArray):\n             if copy:\n                 return self.copy()\n             return self\n         return super().astype(dtype, copy)\n     def _reduce(self, name, skipna=True, **kwargs):", "fixed": " class StringArray(PandasArray):\n             if copy:\n                 return self.copy()\n             return self\n        elif isinstance(dtype, _IntegerDtype):\n            arr = self._ndarray.copy()\n            mask = self.isna()\n            arr[mask] = 0\n            values = arr.astype(dtype.numpy_dtype)\n            return IntegerArray(values, mask, copy=False)\n         return super().astype(dtype, copy)\n     def _reduce(self, name, skipna=True, **kwargs):"}
{"id": "matplotlib_25", "problem": " class EventCollection(LineCollection):\n         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py\n         segment = (lineoffset + linelength / 2.,\n                    lineoffset - linelength / 2.)\n        if positions is None or len(positions) == 0:\n             segments = []\n        elif hasattr(positions, 'ndim') and positions.ndim > 1:\n             raise ValueError('positions cannot be an array with more than '\n                              'one dimension.')\n         elif (orientation is None or orientation.lower() == 'none' or", "fixed": " class EventCollection(LineCollection):\n         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py\n        if positions is None:\n            raise ValueError('positions must be an array-like object')\n        positions = np.array(positions, copy=True)\n         segment = (lineoffset + linelength / 2.,\n                    lineoffset - linelength / 2.)\n        if positions.size == 0:\n             segments = []\n        elif positions.ndim > 1:\n             raise ValueError('positions cannot be an array with more than '\n                              'one dimension.')\n         elif (orientation is None or orientation.lower() == 'none' or"}
{"id": "luigi_6", "problem": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x))\n         except ValueError:\nreturn literal_eval(x)\n    def serialize(self, x):\n        return json.dumps(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n         except ValueError:\nreturn literal_eval(x)\n class NumericalParameter(Parameter):"}
{"id": "pandas_46", "problem": " import numpy as np\n from pandas.core.dtypes.common import is_list_like\nimport pandas.core.common as com\n def cartesian_product(X):", "fixed": " import numpy as np\n from pandas.core.dtypes.common import is_list_like\n def cartesian_product(X):"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "black_23", "problem": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        raise AssertionError(f\"cannot parse source: {exc}\") from None\n     try:\n         dst_ast = ast.parse(dst)", "fixed": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        major, minor = sys.version_info[:2]\n        raise AssertionError(\n            f\"cannot use --safe with this file; failed to parse source file \"\n            f\"with Python {major}.{minor}'s builtin AST. Re-run with --fast \"\n            f\"or stop using deprecated Python 2 syntax. AST error message: {exc}\"\n        )\n     try:\n         dst_ast = ast.parse(dst)"}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "black_6", "problem": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: set(),\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,", "fixed": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n    ASYNC_IS_VALID_IDENTIFIER = 6\n    ASYNC_IS_RESERVED_KEYWORD = 7\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY35: {\n        Feature.UNICODE_LITERALS,\n        Feature.TRAILING_COMMA_IN_CALL,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n    },\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,"}
{"id": "luigi_11", "problem": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items())):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)", "fixed": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)"}
{"id": "black_22", "problem": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.leaves.pop()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.leaves.pop()\n                 return True", "fixed": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.remove_trailing_comma()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.remove_trailing_comma()\n                 return True"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "ansible_2", "problem": " class _Alpha:\n         raise ValueError\n    def __gt__(self, other):\n        return not self.__lt__(other)\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n     def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n class _Numeric:", "fixed": " class _Alpha:\n         raise ValueError\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n    def __gt__(self, other):\n        return not self.__le__(other)\n     def __ge__(self, other):\n        return not self.__lt__(other)\n class _Numeric:"}
{"id": "tornado_6", "problem": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = weakref.WeakKeyDictionary()\n     @classmethod\n     def configure(cls, impl, **kwargs):", "fixed": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = dict()\n     @classmethod\n     def configure(cls, impl, **kwargs):"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "fastapi_3", "problem": "except ImportError:\nfrom pydantic.fields import Field as ModelField\n async def serialize_response(\n     *,\n     field: ModelField = None,", "fixed": "except ImportError:\nfrom pydantic.fields import Field as ModelField\ndef _prepare_response_content(\n    res: Any, *, by_alias: bool = True, exclude_unset: bool\n) -> Any:\n    if isinstance(res, BaseModel):\n        if PYDANTIC_1:\n            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)\n        else:\n            return res.dict(\n                by_alias=by_alias, skip_defaults=exclude_unset\n            )\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(v, exclude_unset=exclude_unset)\n            for k, v in res.items()\n        }\n    return res\n async def serialize_response(\n     *,\n     field: ModelField = None,"}
{"id": "keras_33", "problem": " def text_to_word_sequence(text,\n     if lower:\n         text = text.lower()\n    if sys.version_info < (3,) and isinstance(text, unicode):\n        translate_map = dict((ord(c), unicode(split)) for c in filters)\n     else:\n        translate_map = maketrans(filters, split * len(filters))\n    text = text.translate(translate_map)\n     seq = text.split(split)\n     return [i for i in seq if i]", "fixed": " def text_to_word_sequence(text,\n     if lower:\n         text = text.lower()\n    if sys.version_info < (3,):\n        if isinstance(text, unicode):\n            translate_map = dict((ord(c), unicode(split)) for c in filters)\n            text = text.translate(translate_map)\n        elif len(split) == 1:\n            translate_map = maketrans(filters, split * len(filters))\n            text = text.translate(translate_map)\n        else:\n            for c in filters:\n                text = text.replace(c, split)\n     else:\n        translate_dict = dict((c, split) for c in filters)\n        translate_map = maketrans(translate_dict)\n        text = text.translate(translate_map)\n     seq = text.split(split)\n     return [i for i in seq if i]"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)\n     ):\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes", "fixed": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\n     ):\n        assert is_categorical(lk) and is_categorical(rk)\n        lk = cast(Categorical, lk)\n        rk = cast(Categorical, rk)\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "keras_16", "problem": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n         if layers:", "fixed": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n        self._build_input_shape = None\n         if layers:"}
{"id": "youtube-dl_40", "problem": " import base64\n import io\n import itertools\n import os\nfrom struct import unpack, pack\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,", "fixed": " import base64\n import io\n import itertools\n import os\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n    struct_pack,\n    struct_unpack,\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,"}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "youtube-dl_4", "problem": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]+)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')])\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)", "fixed": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     @staticmethod\n     def get_target_path(target):\n         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n             return target.path\n         elif isinstance(target, bigquery.BigQueryTarget):\n            \"{}:{}.{}\".format(target.project_id, target.dataset_id, target.table_id)\n         else:\n            raise ValueError(\"Target not supported\")", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     @staticmethod\n     def get_target_path(target):\n         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n             return target.path\n         elif isinstance(target, bigquery.BigQueryTarget):\n            return \"{}:{}.{}\".format(target.table.project_id, target.table.dataset_id, target.table.table_id)\n         else:\n            raise ValueError(\"Target %s not supported\" % target)"}
{"id": "black_15", "problem": " from typing import (\n     Sequence,\n     Set,\n     Tuple,\n    Type,\n     TypeVar,\n     Union,\n     cast,", "fixed": " from typing import (\n     Sequence,\n     Set,\n     Tuple,\n     TypeVar,\n     Union,\n     cast,"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"id": "pandas_14", "problem": " def id_func(x):\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])", "fixed": " def id_func(x):\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n    ]\n)\ndef names(request):\n    return request.param\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])"}
{"id": "keras_29", "problem": " class Model(Container):\n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n            for i, m in enumerate(self.metrics):\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_118", "problem": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(np.ravel(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\"", "fixed": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(com.flatten(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\""}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "pandas_90", "problem": "from pandas._config.localization import (\n )\n import pandas._libs.testing as _testing\nfrom pandas._typing import FrameOrSeries\n from pandas.compat import _get_lzma_file, _import_lzma\n from pandas.core.dtypes.common import (", "fixed": "from pandas._config.localization import (\n )\n import pandas._libs.testing as _testing\nfrom pandas._typing import FilePathOrBuffer, FrameOrSeries\n from pandas.compat import _get_lzma_file, _import_lzma\n from pandas.core.dtypes.common import ("}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n         else:\n             output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output", "fixed": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n         else:\n             output = None\n    else:\n        output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output"}
{"id": "pandas_112", "problem": " from pandas.core.dtypes.cast import (\n )\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_dtype_equal,", "fixed": " from pandas.core.dtypes.cast import (\n )\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n    is_categorical,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_dtype_equal,"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"id": "black_14", "problem": " Ur\"hello\"\nfrom __future__ import unicode_literals\n \"hello\"\n \"hello\"", "fixed": " Ur\"hello\"\nfrom __future__ import unicode_literals as _unicode_literals\nfrom __future__ import absolute_import\nfrom __future__ import print_function as lol, with_function\n \"hello\"\n \"hello\""}
{"id": "tornado_3", "problem": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            if self._instance_cache.get(self.io_loop) is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n            del self._instance_cache[self.io_loop]\n     def fetch(\n         self,", "fixed": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            if cached_val is not None and cached_val is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n     def fetch(\n         self,"}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "pandas_68", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n    ABCExtensionArray,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,"}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "pandas_99", "problem": " def _convert_listlike_datetimes(\n     elif unit is not None:\n         if format is not None:\n             raise ValueError(\"cannot specify both format and unit\")\n        arg = getattr(arg, \"values\", arg)\n        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n         if errors == \"ignore\":\n             from pandas import Index", "fixed": " def _convert_listlike_datetimes(\n     elif unit is not None:\n         if format is not None:\n             raise ValueError(\"cannot specify both format and unit\")\n        arg = getattr(arg, \"_values\", arg)\n        if isinstance(arg, IntegerArray):\n            mask = arg.isna()\n            arg = arg._ndarray_values\n        else:\n            mask = None\n        result, tz_parsed = tslib.array_with_unit_to_datetime(\n            arg, mask, unit, errors=errors\n        )\n         if errors == \"ignore\":\n             from pandas import Index"}
{"id": "pandas_54", "problem": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n         elif is_categorical(values):", "fixed": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n         elif is_categorical(values):"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role related operations.\",\n         )", "fixed": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n         )"}
{"id": "pandas_9", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n        if not self.trainable:\n            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []"}
{"id": "thefuck_15", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return ('did not match any file(s) known to git.' in command.stderr\n            and \"Did you forget to 'git add'?\" in command.stderr)\n @git_support\n def get_new_command(command):\n     missing_file = re.findall(\n            r\"error: pathspec '([^']*)' \"\n            r\"did not match any file\\(s\\) known to git.\", command.stderr)[0]\n     formatme = shell.and_('git add -- {}', '{}')\n     return formatme.format(missing_file, command.script)", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return 'did not match any file(s) known to git.' in command.stderr\n @git_support\n def get_new_command(command):\n     missing_file = re.findall(\n        r\"error: pathspec '([^']*)' \"\n        r'did not match any file\\(s\\) known to git.', command.stderr)[0]\n     formatme = shell.and_('git add -- {}', '{}')\n     return formatme.format(missing_file, command.script)"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,", "fixed": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.non_trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n            if not self.trainable:\n                return self.cell.weights\n             return self.cell.non_trainable_weights\n         return []"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n     for leaf in line.leaves:\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n     for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,"}
{"id": "pandas_160", "problem": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\"):\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif isinstance(o, np.ndarray):\n                     dtypes |= {o.dtype.name}", "fixed": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif hasattr(o, \"dtype\"):\n                     dtypes |= {o.dtype.name}"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":", "fixed": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_44", "problem": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,", "fixed": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import DtypeObj, Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n    ensure_platform_int,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "pandas_165", "problem": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n class TestAddSubNaTMasking:", "fixed": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n    def test_tda_add_sub_index(self):\n        tdi = TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\n        tda = tdi.array\n        dti = pd.date_range(\"1999-12-31\", periods=3, freq=\"D\")\n        result = tda + dti\n        expected = tdi + dti\n        tm.assert_index_equal(result, expected)\n        result = tda + tdi\n        expected = tdi + tdi\n        tm.assert_index_equal(result, expected)\n        result = tda - tdi\n        expected = tdi - tdi\n        tm.assert_index_equal(result, expected)\n class TestAddSubNaTMasking:"}
{"id": "pandas_41", "problem": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values", "fixed": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"id": "luigi_14", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "youtube-dl_16", "problem": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:", "fixed": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with open(dfxp_file, 'rb') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:"}
{"id": "pandas_69", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not ax.holds_integer():\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \"", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n                         \"can only have integer indexers\"\n                     )\n             else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                     raise ValueError(\n                         \"At based indexing on an non-integer \"\n                         \"index can only have non-integer \""}
{"id": "fastapi_16", "problem": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        if not obj.Config.json_encoders:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n            )\n        else:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n                custom_encoder=obj.Config.json_encoders,\n            )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):", "fixed": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        encoder = getattr(obj.Config, \"json_encoders\", custom_encoder)\n        return jsonable_encoder(\n            obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n            include_none=include_none,\n            custom_encoder=encoder,\n        )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):"}
{"id": "youtube-dl_39", "problem": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output", "fixed": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output\ndef limit_length(s, length):\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "pandas_70", "problem": " def test_resample_integerarray():\n     result = ts.resample(\"3T\").mean()\n     expected = Series(\n        [1, 4, 7], index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"), dtype=\"Int64\"\n     )\n     tm.assert_series_equal(result, expected)", "fixed": " def test_resample_integerarray():\n     result = ts.resample(\"3T\").mean()\n     expected = Series(\n        [1, 4, 7],\n        index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"),\n        dtype=\"float64\",\n     )\n     tm.assert_series_equal(result, expected)"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):"}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"id": "keras_11", "problem": " from keras.utils import Sequence\n from keras import backend as K\n pytestmark = pytest.mark.skipif(\n    K.backend() == 'tensorflow',\n     reason='Temporarily disabled until the use_multiprocessing problem is solved')\n STEPS_PER_EPOCH = 100", "fixed": " from keras.utils import Sequence\n from keras import backend as K\n pytestmark = pytest.mark.skipif(\n    K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,\n     reason='Temporarily disabled until the use_multiprocessing problem is solved')\n STEPS_PER_EPOCH = 100"}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "youtube-dl_32", "problem": " class NPOIE(InfoExtractor):\n'http:\n             video_id,\ntransform_source=lambda j: re.sub(r'parseMetadata\\((.*?)\\);\\n\n         )\n         token_page = self._download_webpage(\n'http:", "fixed": " class NPOIE(InfoExtractor):\n'http:\n             video_id,\n            transform_source=strip_jsonp,\n         )\n         token_page = self._download_webpage(\n'http:"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n    def _extract_chapters(self, webpage, description, video_id, duration):\n        return (self._extract_chapters_from_json(webpage, video_id, duration)\n                or self._extract_chapters_from_description(description, duration))\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})"}
{"id": "pandas_111", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    return self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    return self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "pandas_79", "problem": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError):\n                 return False", "fixed": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError, InvalidIndexError):\n                 return False"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "pandas_71", "problem": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_integer,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,", "fixed": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n    is_extension_array_dtype,\n     is_integer,\n    is_integer_dtype,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,"}
{"id": "pandas_27", "problem": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\nimport pandas._libs.tslibs.frequencies as libfrequencies\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import ("}
{"id": "pandas_148", "problem": " class FrameApply:\n         from pandas import Series\n         if not should_reduce:\n            EMPTY_SERIES = Series([])\n             try:\n                r = self.f(EMPTY_SERIES, *self.args, **self.kwds)\n             except Exception:\n                 pass\n             else:\n                 should_reduce = not isinstance(r, Series)\n         if should_reduce:\n            return self.obj._constructor_sliced(np.nan, index=self.agg_axis)\n         else:\n             return self.obj.copy()", "fixed": " class FrameApply:\n         from pandas import Series\n         if not should_reduce:\n             try:\n                r = self.f(Series([]))\n             except Exception:\n                 pass\n             else:\n                 should_reduce = not isinstance(r, Series)\n         if should_reduce:\n            if len(self.agg_axis):\n                r = self.f(Series([]))\n            else:\n                r = np.nan\n            return self.obj._constructor_sliced(r, index=self.agg_axis)\n         else:\n             return self.obj.copy()"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "matplotlib_10", "problem": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])", "fixed": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n            if 'label1On' in kwtrans or 'label2On' in kwtrans:\n                self.offsetText.set_visible(\n                    self._major_tick_kw.get('label1On', False)\n                    or self._major_tick_kw.get('label2On', False))\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])"}
{"id": "luigi_17", "problem": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)", "fixed": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)"}
{"id": "pandas_134", "problem": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2030, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):", "fixed": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2200, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s was created successfully\" % obj_name)\n     def execute_info(self):", "fixed": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))\n     def execute_info(self):"}
{"id": "thefuck_13", "problem": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)", "fixed": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)"}
{"id": "pandas_123", "problem": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n                     elif inferred == \"string\":\n                         pass", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, name=name)\n                     elif inferred == \"string\":\n                         pass"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "youtube-dl_25", "problem": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^0[xX][0-9a-fA-F]+', 16),\n            (r'^0+[0-7]+', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(0), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v", "fixed": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^(0[xX][0-9a-fA-F]+)\\s*:?$', 16),\n            (r'^(0+[0-7]+)\\s*:?$', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(1), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a same-typed block.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block.", "fixed": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block."}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n         Raises\n         ------\n         TypeError", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n        .. versionchanged:: 1.0.0\n           Returns an NA value on empty arrays\n         Raises\n         ------\n         TypeError"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\nbase_url = \"%s:\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "ansible_3", "problem": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path == '/etc/lsb-release' and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:", "fixed": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:"}
{"id": "pandas_41", "problem": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)", "fixed": " class ComplexBlock(FloatOrComplexBlock):\n             element, (float, int, complex, np.float_, np.int_)\n         ) and not isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.complexfloating)"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    path : str\n        File path where the pickled object will be loaded.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',\n        or '.zip' respectively, and no decompression otherwise.\n        Set to None for no decompression.\n     Returns\n     -------", "fixed": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be loaded from.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL is not limited to S3 and GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     Returns\n     -------"}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "matplotlib_22", "problem": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')", "fixed": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n        if not cbook.is_scalar_or_string(bins):\n            bins = self.convert_xunits(bins)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n             if not ax.is_unique:\n                 return False", "fixed": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                return False\n             if not ax.is_unique:\n                 return False"}
{"id": "black_22", "problem": " class UnformattedLines(Line):\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class UnformattedLines(Line):\n @dataclass\n class EmptyLineTracker:"}
{"id": "keras_20", "problem": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n         A tensor.", "fixed": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n         A tensor."}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         else:\n             return ((prefix + \": \") if prefix else '') + \\", "fixed": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         else:\n             return ((prefix + \": \") if prefix else '') + \\"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n         self.seed = seed", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n        self._manager = None\n         self.queue = None\n         self.seed = seed"}
{"id": "youtube-dl_35", "problem": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }", "fixed": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n        upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(upload_date_str),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }"}
{"id": "youtube-dl_38", "problem": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]", "fixed": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]\ndef urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')"}
{"id": "pandas_44", "problem": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n     def get_loc(self, key, method=None, tolerance=None):", "fixed": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_timedelta64_dtype(dtype)\n     def get_loc(self, key, method=None, tolerance=None):"}
{"id": "sanic_5", "problem": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],", "fixed": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h)\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w)\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n        config.pop('dilation_rate')\n         config['output_padding'] = self.output_padding\n         return config", "fixed": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n         config['output_padding'] = self.output_padding\n         return config"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "luigi_29", "problem": " class AmbiguousClass(luigi.Task):\n     pass\nclass NonAmbiguousClass(luigi.ExternalTask):\n    pass\nclass NonAmbiguousClass(luigi.Task):\n    def run(self):\n        NonAmbiguousClass.has_run = True\n class TaskWithSameName(luigi.Task):\n     def run(self):", "fixed": " class AmbiguousClass(luigi.Task):\n     pass\n class TaskWithSameName(luigi.Task):\n     def run(self):"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "cookiecutter_2", "problem": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            return os.path.abspath(os.path.join(hooks_dir, hook_file))\n    return None\n def run_script(script_path, cwd='.'):", "fixed": " def find_hook(hook_name, hooks_dir='hooks'):\n         logger.debug('No hooks/dir in template_dir')\n         return None\n    scripts = []\n     for hook_file in os.listdir(hooks_dir):\n         if valid_hook(hook_file, hook_name):\n            scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))\n    if len(scripts) == 0:\n        return None\n    return scripts\n def run_script(script_path, cwd='.'):"}
{"id": "pandas_40", "problem": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution", "fixed": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union, cast\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import ArrayLike, FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution"}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "thefuck_20", "problem": " import os\n import zipfile\n from thefuck.utils import for_app\n def _is_bad_zip(file):", "fixed": " import os\n import zipfile\n from thefuck.utils import for_app\nfrom thefuck.shells import quote\n def _is_bad_zip(file):"}
{"id": "matplotlib_13", "problem": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n         return cls(vertices, codes)\n     def __repr__(self):", "fixed": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n        last_vert = None\n        if codes.size > 0 and codes[-1] == cls.STOP:\n            last_vert = vertices[-1]\n        vertices = vertices[codes != cls.STOP, :]\n        codes = codes[codes != cls.STOP]\n        if last_vert is not None:\n            vertices = np.append(vertices, [last_vert], axis=0)\n            codes = np.append(codes, cls.STOP)\n         return cls(vertices, codes)\n     def __repr__(self):"}
{"id": "pandas_104", "problem": " class GroupBy(_GroupBy):\n            order = np.roll(list(range(result.index.nlevels)), -1)\n            result = result.reorder_levels(order)\n            result = result.reindex(q, level=-1)\n            hi = len(q) * self.ngroups\n            arr = np.arange(0, hi, self.ngroups)\n            arrays = []\n            for i in range(self.ngroups):\n                arr2 = arr + i\n                arrays.append(arr2)\n            indices = np.concatenate(arrays)\n            assert len(indices) == len(result)\n             return result.take(indices)\n     @Substitution(name=\"groupby\")", "fixed": " class GroupBy(_GroupBy):\n            order = list(range(1, result.index.nlevels)) + [0]\n            index_names = np.array(result.index.names)\n            result.index.names = np.arange(len(index_names))\n            result = result.reorder_levels(order)\n            result.index.names = index_names[order]\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()\n             return result.take(indices)\n     @Substitution(name=\"groupby\")"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_36", "problem": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj_old(values.ravel())\n            result[:] = vec.reshape(shape)\n    elif is_datetime64_dtype(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:", "fixed": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n    elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "matplotlib_17", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "scrapy_33", "problem": " from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\nfrom scrapy.utils.log import logformatter_adapter\n logger = logging.getLogger(__name__)", "fixed": " from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\nfrom scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "pandas_80", "problem": "import operator\n import numpy as np\n import pytest\nimport pandas.util._test_decorators as td\n import pandas as pd\n import pandas._testing as tm\nfrom pandas.arrays import BooleanArray\nfrom pandas.core.arrays.boolean import coerce_to_array\nfrom pandas.tests.extension.base import BaseOpsUtil\n def make_data():", "fixed": " import numpy as np\n import pytest\nfrom pandas.compat.numpy import _np_version_under1p14\n import pandas as pd\n import pandas._testing as tm\nfrom pandas.core.arrays.boolean import BooleanDtype\nfrom pandas.tests.extension import base\n def make_data():"}
{"id": "luigi_1", "problem": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics = self._scheduler._state._metrics_collector.generate_latest()\n         if metrics:\n            metrics.configure_http_handler(self)\n             self.write(metrics)", "fixed": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n         if metrics:\n            metrics_collector.configure_http_handler(self)\n             self.write(metrics)"}
{"id": "thefuck_19", "problem": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force')\n enabled_by_default = False", "fixed": " def match(command):\n @git_support\n def get_new_command(command):\n    return replace_argument(command.script, 'push', 'push --force-with-lease')\n enabled_by_default = False"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"id": "httpie_3", "problem": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue", "fixed": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n            if value is None:\n                continue\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue"}
{"id": "scrapy_31", "problem": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n     getheaders = get_all", "fixed": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]\n     getheaders = get_all"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "scrapy_33", "problem": " from scrapy.pipelines.media import MediaPipeline\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n logger = logging.getLogger(__name__)", "fixed": " from scrapy.pipelines.media import MediaPipeline\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "keras_41", "problem": " import sys\n import tarfile\n import threading\n import time\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool", "fixed": " import sys\n import tarfile\n import threading\n import time\nimport traceback\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool"}
{"id": "luigi_15", "problem": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks", "fixed": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks"}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:https?:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "PySnooper_1", "problem": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = '\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch", "fixed": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = u'\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch"}
{"id": "pandas_79", "problem": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,", "fixed": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,"}
{"id": "scrapy_33", "problem": " from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\nfrom scrapy.utils.log import logformatter_adapter\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response", "fixed": " from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\nfrom scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response"}
{"id": "pandas_67", "problem": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()", "fixed": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n    def iget(self, key):\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()"}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "pandas_28", "problem": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=others)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:", "fixed": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=idx)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:"}
{"id": "youtube-dl_17", "problem": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]", "fixed": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n    if param is None:\n        return []\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]"}
{"id": "thefuck_5", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n     return ('push' in command.script_parts\n            and 'set-upstream' in command.output)\n def _get_upstream_option_index(command_parts):", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n     return ('push' in command.script_parts\n            and 'git push --set-upstream' in command.output)\n def _get_upstream_option_index(command_parts):"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n     ):\n         self._write = get_write_function(output, overwrite)", "fixed": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n            custom_repr=(),\n     ):\n         self._write = get_write_function(output, overwrite)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "scrapy_3", "problem": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['location'])\n         redirected_url = urljoin(request.url, location)", "fixed": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['Location'])\nif response.headers['Location'].startswith(b'\n            request_scheme = urlparse(request.url).scheme\nlocation = request_scheme + ':\n         redirected_url = urljoin(request.url, location)"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n        elif isinstance(other, int):\n            other = Period(other, freq=self.freq)\n            result = ordinal_op(other.ordinal)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)", "fixed": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):", "fixed": " class Driver(object):\n     def parse_stream_raw(self, stream, debug=False):\n        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)\n         return self.parse_tokens(tokens, debug)\n     def parse_stream(self, stream, debug=False):"}
{"id": "keras_1", "problem": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)", "fixed": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n            self.seed += 1\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:", "fixed": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                self.wait += 1\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:"}
{"id": "pandas_86", "problem": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None", "fixed": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n    if columns is None:\n        raise TypeError(\"pivot() missing 1 required argument: 'columns'\")\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n             np.putmask(rlab, rmask, count)\n         count += 1\n     return llab, rlab, count", "fixed": " def _factorize_keys(lk, rk, sort=True):\n             np.putmask(rlab, rmask, count)\n         count += 1\n    if how == \"right\":\n        return rlab, llab, count\n     return llab, rlab, count"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing"}
{"id": "pandas_51", "problem": " import pandas.core.indexes.base as ibase\n from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name\n from pandas.core.indexes.extension import ExtensionIndex, inherit_names\n import pandas.core.missing as missing\n _index_doc_kwargs = dict(ibase._index_doc_kwargs)\n _index_doc_kwargs.update(dict(target_klass=\"CategoricalIndex\"))", "fixed": " import pandas.core.indexes.base as ibase\n from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name\n from pandas.core.indexes.extension import ExtensionIndex, inherit_names\n import pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name\n _index_doc_kwargs = dict(ibase._index_doc_kwargs)\n _index_doc_kwargs.update(dict(target_klass=\"CategoricalIndex\"))"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "tornado_4", "problem": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if (start is not None and start >= size) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n            if start is not None and start < 0:\n                start += size\n             if end is not None and end > size:", "fixed": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if start is not None and start < 0:\n                start += size\n                if start < 0:\n                    start = 0\n            if (\n                start is not None\n                and (start >= size or (end is not None and start >= end))\n            ) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n             if end is not None and end > size:"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "tqdm_1", "problem": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):", "fixed": " def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,\n         if isinstance(iterable, np.ndarray):\n             return tqdm_class(np.ndenumerate(iterable),\n                               total=total or len(iterable), **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)\n def _tzip(iter1, *iter2plus, **tqdm_kwargs):"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n     @property\n     def trainable(self):", "fixed": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n        self.input_spec = layer.input_spec\n     @property\n     def trainable(self):"}
{"id": "luigi_13", "problem": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.fs.mkdir(d)\n         os.rename(old_path, new_path)", "fixed": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.mkdir(d)\n         os.rename(old_path, new_path)"}
{"id": "pandas_70", "problem": "                cls = dtype.construct_array_type()\n                result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)", "fixed": "                if len(result) and isinstance(result[0], dtype.type):\n                    cls = dtype.construct_array_type()\n                    result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)"}
{"id": "black_14", "problem": " from typing import (\n     Callable,\n     Collection,\n     Dict,\n     Generic,\n     Iterable,\n     Iterator,", "fixed": " from typing import (\n     Callable,\n     Collection,\n     Dict,\n    Generator,\n     Generic,\n     Iterable,\n     Iterator,"}
{"id": "black_13", "problem": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token == 'def':\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            async_def = True\n                            async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],", "fixed": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token in ('def', 'for'):\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],"}
