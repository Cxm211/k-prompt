{"id": "ansible_6", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\n     if existing and not collection_info.force:\n        existing[0].add_requirement(to_text(collection_info), requirement)\n         collection_info = existing[0]\n     dep_map[to_text(collection_info)] = collection_info", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\n     if existing and not collection_info.force:\n        existing[0].add_requirement(parent, requirement)\n         collection_info = existing[0]\n     dep_map[to_text(collection_info)] = collection_info"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             callback_model.stop_training = False", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             callback_model.stop_training = False"}
{"id": "tornado_13", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n             return connection_header != \"close\"\n         elif (\"Content-Length\" in headers\n               or headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\"\n              or start_line.method in (\"HEAD\", \"GET\")):\n             return connection_header == \"keep-alive\"\n         return False", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n             return connection_header != \"close\"\n         elif (\"Content-Length\" in headers\n               or headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\"\n              or getattr(start_line, 'method', None) in (\"HEAD\", \"GET\")):\n             return connection_header == \"keep-alive\"\n         return False"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n                     result._set_freq(\"infer\")\n             return result\n    def _fast_union(self, other):\n         if len(other) == 0:\n             return self.view(type(self))", "fixed": " class TimedeltaIndex(\n                     result._set_freq(\"infer\")\n             return result\n    def _fast_union(self, other, sort=None):\n         if len(other) == 0:\n             return self.view(type(self))"}
{"id": "black_23", "problem": "def whitespace(leaf: Leaf) -> str:\n         ):\n             return NO\n     elif prev.type in OPENING_BRACKETS:\n         return NO", "fixed": "def whitespace(leaf: Leaf) -> str:\n         ):\n             return NO\n        elif (\n            prevp.type == token.RIGHTSHIFT\n            and prevp.parent\n            and prevp.parent.type == syms.shift_expr\n            and prevp.prev_sibling\n            and prevp.prev_sibling.type == token.NAME\n            and prevp.prev_sibling.value == 'print'\n        ):\n            return NO\n     elif prev.type in OPENING_BRACKETS:\n         return NO"}
{"id": "ansible_5", "problem": " def check_required_arguments(argument_spec, module_parameters):\n             missing.append(k)\n     if missing:\n        msg = \"missing required arguments: %s\" % \", \".join(missing)\n         raise TypeError(to_native(msg))\n     return missing", "fixed": " def check_required_arguments(argument_spec, module_parameters):\n             missing.append(k)\n     if missing:\n        msg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\n         raise TypeError(to_native(msg))\n     return missing"}
{"id": "black_6", "problem": " single_quoted = (\n tabsize = 8\n class TokenError(Exception): pass\n class StopTokenizing(Exception): pass", "fixed": " single_quoted = (\n tabsize = 8\n@dataclass(frozen=True)\nclass TokenizerConfig:\n    async_is_reserved_keyword: bool = False\n class TokenError(Exception): pass\n class StopTokenizing(Exception): pass"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n         Compare datetime-like which is comparable ignoring dtype.\n     check_categorical : bool, default True\n         Whether to compare internal Categorical exactly.\n     obj : str, default 'Series'\n         Specify object name being compared, internally used to show appropriate\n         assertion message.", "fixed": " def assert_series_equal(\n         Compare datetime-like which is comparable ignoring dtype.\n     check_categorical : bool, default True\n         Whether to compare internal Categorical exactly.\n    check_category_order : bool, default True\n        Whether to compare category order of internal Categoricals\n        .. versionadded:: 1.0.2\n     obj : str, default 'Series'\n         Specify object name being compared, internally used to show appropriate\n         assertion message."}
{"id": "keras_45", "problem": " class LSTMCell(Layer):\n                 inputs_f = inputs\n                 inputs_c = inputs\n                 inputs_o = inputs\n            x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i\n            x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f\n            x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c\n            x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o\n             if 0 < self.recurrent_dropout < 1.:\n                 h_tm1_i = h_tm1 * rec_dp_mask[0]", "fixed": " class LSTMCell(Layer):\n                 inputs_f = inputs\n                 inputs_c = inputs\n                 inputs_o = inputs\n            x_i = K.dot(inputs_i, self.kernel_i)\n            x_f = K.dot(inputs_f, self.kernel_f)\n            x_c = K.dot(inputs_c, self.kernel_c)\n            x_o = K.dot(inputs_o, self.kernel_o)\n            if self.use_bias:\n                x_i = K.bias_add(x_i, self.bias_i)\n                x_f = K.bias_add(x_f, self.bias_f)\n                x_c = K.bias_add(x_c, self.bias_c)\n                x_o = K.bias_add(x_o, self.bias_o)\n             if 0 < self.recurrent_dropout < 1.:\n                 h_tm1_i = h_tm1 * rec_dp_mask[0]"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n         out_height = conv_utils.deconv_length(height,\n                                               stride_h, kernel_h,\n                                               self.padding,\n                                              out_pad_h)\n         out_width = conv_utils.deconv_length(width,\n                                              stride_w, kernel_w,\n                                              self.padding,\n                                             out_pad_w)\n         if self.data_format == 'channels_first':\n             output_shape = (batch_size, self.filters, out_height, out_width)\n         else:", "fixed": " class Conv2DTranspose(Conv2D):\n         out_height = conv_utils.deconv_length(height,\n                                               stride_h, kernel_h,\n                                               self.padding,\n                                              out_pad_h,\n                                              self.dilation_rate[0])\n         out_width = conv_utils.deconv_length(width,\n                                              stride_w, kernel_w,\n                                              self.padding,\n                                             out_pad_w,\n                                             self.dilation_rate[1])\n         if self.data_format == 'channels_first':\n             output_shape = (batch_size, self.filters, out_height, out_width)\n         else:"}
{"id": "pandas_84", "problem": " class TestSeriesAnalytics:\n         assert s.is_monotonic is False\n         assert s.is_monotonic_decreasing is True\n    def test_unstack(self):\n        index = MultiIndex(\n            levels=[[\"bar\", \"foo\"], [\"one\", \"three\", \"two\"]],\n            codes=[[1, 1, 0, 0], [0, 1, 0, 2]],\n        )\n        s = Series(np.arange(4.0), index=index)\n        unstacked = s.unstack()\n        expected = DataFrame(\n            [[2.0, np.nan, 3.0], [0.0, 1.0, np.nan]],\n            index=[\"bar\", \"foo\"],\n            columns=[\"one\", \"three\", \"two\"],\n        )\n        tm.assert_frame_equal(unstacked, expected)\n        unstacked = s.unstack(level=0)\n        tm.assert_frame_equal(unstacked, expected.T)\n        index = MultiIndex(\n            levels=[[\"bar\"], [\"one\", \"two\", \"three\"], [0, 1]],\n            codes=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],\n        )\n        s = Series(np.random.randn(6), index=index)\n        exp_index = MultiIndex(\n            levels=[[\"one\", \"two\", \"three\"], [0, 1]],\n            codes=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],\n        )\n        expected = DataFrame({\"bar\": s.values}, index=exp_index).sort_index(level=0)\n        unstacked = s.unstack(0).sort_index()\n        tm.assert_frame_equal(unstacked, expected)\n        idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])\n        ts = pd.Series([1, 2], index=idx)\n        left = ts.unstack()\n        right = DataFrame(\n            [[np.nan, 1], [2, np.nan]], index=[101, 102], columns=[np.nan, 3.5]\n        )\n        tm.assert_frame_equal(left, right)\n        idx = pd.MultiIndex.from_arrays(\n            [\n                [\"cat\", \"cat\", \"cat\", \"dog\", \"dog\"],\n                [\"a\", \"a\", \"b\", \"a\", \"b\"],\n                [1, 2, 1, 1, np.nan],\n            ]\n        )\n        ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)\n        right = DataFrame(\n            [[1.0, 1.3], [1.1, np.nan], [np.nan, 1.4], [1.2, np.nan]],\n            columns=[\"cat\", \"dog\"],\n        )\n        tpls = [(\"a\", 1), (\"a\", 2), (\"b\", np.nan), (\"b\", 1)]\n        right.index = pd.MultiIndex.from_tuples(tpls)\n        tm.assert_frame_equal(ts.unstack(level=0), right)\n     @pytest.mark.parametrize(\"func\", [np.any, np.all])\n     @pytest.mark.parametrize(\"kwargs\", [dict(keepdims=True), dict(out=object())])\n     @td.skip_if_np_lt(\"1.15\")", "fixed": " class TestSeriesAnalytics:\n         assert s.is_monotonic is False\n         assert s.is_monotonic_decreasing is True\n     @pytest.mark.parametrize(\"func\", [np.any, np.all])\n     @pytest.mark.parametrize(\"kwargs\", [dict(keepdims=True), dict(out=object())])\n     @td.skip_if_np_lt(\"1.15\")"}
{"id": "luigi_20", "problem": " class Task(object):\n         params_str = {}\n         params = dict(self.get_params())\n         for param_name, param_value in six.iteritems(self.param_kwargs):\n            if params[param_name].significant:\n                params_str[param_name] = params[param_name].serialize(param_value)\n         return params_str", "fixed": " class Task(object):\n         params_str = {}\n         params = dict(self.get_params())\n         for param_name, param_value in six.iteritems(self.param_kwargs):\n            params_str[param_name] = params[param_name].serialize(param_value)\n         return params_str"}
{"id": "youtube-dl_3", "problem": " def unescapeHTML(s):\n     assert type(s) == compat_str\n     return re.sub(\n        r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n def get_subprocess_encoding():", "fixed": " def unescapeHTML(s):\n     assert type(s) == compat_str\n     return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n def get_subprocess_encoding():"}
{"id": "keras_20", "problem": " def in_top_k(predictions, targets, k):\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):\n     data_format = normalize_data_format(data_format)\n     x = _preprocess_conv2d_input(x, data_format)", "fixed": " def in_top_k(predictions, targets, k):\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n     data_format = normalize_data_format(data_format)\n     x = _preprocess_conv2d_input(x, data_format)"}
{"id": "keras_18", "problem": " class Function(object):\n                                 feed_symbols,\n                                 symbol_vals,\n                                 session)\n        fetched = self._callable_fn(*array_vals)\n         return fetched[:len(self.outputs)]\n     def _legacy_call(self, inputs):", "fixed": " class Function(object):\n                                 feed_symbols,\n                                 symbol_vals,\n                                 session)\n        if self.run_metadata:\n            fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\n        else:\n            fetched = self._callable_fn(*array_vals)\n         return fetched[:len(self.outputs)]\n     def _legacy_call(self, inputs):"}
{"id": "thefuck_3", "problem": " class Fish(Generic):\n     def info(self):\n        proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],\n                      stdout=PIPE, stderr=DEVNULL)\n        version = proc.stdout.read().decode('utf-8').strip()\n         return u'Fish Shell {}'.format(version)\n     def put_to_history(self, command):", "fixed": " class Fish(Generic):\n     def info(self):\n        proc = Popen(['fish', '--version'],\n                      stdout=PIPE, stderr=DEVNULL)\n        version = proc.stdout.read().decode('utf-8').split()[-1]\n         return u'Fish Shell {}'.format(version)\n     def put_to_history(self, command):"}
{"id": "pandas_70", "problem": " class BaseGrouper:\n             if mask.any():\n                 result = result.astype(\"float64\")\n                 result[mask] = np.nan\n         if kind == \"aggregate\" and self._filter_empty_groups and not counts.all():\n             assert result.ndim != 2", "fixed": " class BaseGrouper:\n             if mask.any():\n                 result = result.astype(\"float64\")\n                 result[mask] = np.nan\n        elif (\n            how == \"add\"\n            and is_integer_dtype(orig_values.dtype)\n            and is_extension_array_dtype(orig_values.dtype)\n        ):\n            result = result.astype(\"int64\")\n         if kind == \"aggregate\" and self._filter_empty_groups and not counts.all():\n             assert result.ndim != 2"}
{"id": "pandas_61", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 indexer = self.index.get_indexer_for(key)\n                 return self.iloc[indexer]\n             else:\n                return self._get_values(key)\n         if isinstance(key, (list, tuple)):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 indexer = self.index.get_indexer_for(key)\n                 return self.iloc[indexer]\n             else:\n                return self.iloc[key]\n         if isinstance(key, (list, tuple)):"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n                     spider=spider, exception=output.value)\n             else:\n                 logger.error('Error processing %(item)s', {'item': item},\n                             extra={'spider': spider, 'failure': output})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})", "fixed": " class Scraper(object):\n                     spider=spider, exception=output.value)\n             else:\n                 logger.error('Error processing %(item)s', {'item': item},\n                             exc_info=failure_to_exc_info(output),\n                             extra={'spider': spider})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\n         inject_data = dict(\n            description='your description',\n             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\n         )\n         if galaxy_type == 'role':", "fixed": " class GalaxyCLI(CLI):\n         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\n         inject_data = dict(\n            description='your {0} description'.format(galaxy_type),\n             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\n         )\n         if galaxy_type == 'role':"}
{"id": "youtube-dl_19", "problem": " class YoutubeDL(object):\n                         FORMAT_RE.format(numeric_field),\n                         r'%({0})s'.format(numeric_field), outtmpl)\n            filename = expand_path(outtmpl % template_dict)", "fixed": " class YoutubeDL(object):\n                         FORMAT_RE.format(numeric_field),\n                         r'%({0})s'.format(numeric_field), outtmpl)\n            sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])\n            outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))\n            filename = expand_path(outtmpl).replace(sep, '') % template_dict"}
{"id": "scrapy_9", "problem": " class MailSender(object):\n             msg = MIMEMultipart()\n         else:\n             msg = MIMENonMultipart(*mimetype.split('/', 1))\n         msg['From'] = self.mailfrom\n         msg['To'] = COMMASPACE.join(to)\n         msg['Date'] = formatdate(localtime=True)", "fixed": " class MailSender(object):\n             msg = MIMEMultipart()\n         else:\n             msg = MIMENonMultipart(*mimetype.split('/', 1))\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n         msg['From'] = self.mailfrom\n         msg['To'] = COMMASPACE.join(to)\n         msg['Date'] = formatdate(localtime=True)"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def predict_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "youtube-dl_34", "problem": " def js_to_json(code):\n             ([{,]\\s*)\n             (\"[^\"]*\"|\\'[^\\']*\\'|[a-z0-9A-Z]+)\n             (:\\s*)\n            ([0-9.]+|true|false|\"[^\"]*\"|\\'[^\\']*\\'|\\[|\\{)\n     res = re.sub(r',(\\s*\\])', lambda m: m.group(1), res)\n     return res", "fixed": " def js_to_json(code):\n             ([{,]\\s*)\n             (\"[^\"]*\"|\\'[^\\']*\\'|[a-z0-9A-Z]+)\n             (:\\s*)\n            ([0-9.]+|true|false|\"[^\"]*\"|\\'[^\\']*\\'|\n                (?=\\[|\\{)\n            )\n     res = re.sub(r',(\\s*\\])', lambda m: m.group(1), res)\n     return res"}
{"id": "pandas_121", "problem": " class BlockManager(PandasObject):\n                         convert=convert,\n                         regex=regex,\n                     )\n                    if m.any():\n                         new_rb = _extend_blocks(result, new_rb)\n                     else:\n                         new_rb.append(b)", "fixed": " class BlockManager(PandasObject):\n                         convert=convert,\n                         regex=regex,\n                     )\n                    if m.any() or convert:\n                         new_rb = _extend_blocks(result, new_rb)\n                     else:\n                         new_rb.append(b)"}
{"id": "youtube-dl_40", "problem": " def uppercase_escape(s):\n     return re.sub(\n         r'\\\\U([0-9a-fA-F]{8})',\n         lambda m: compat_chr(int(m.group(1), base=16)), s)", "fixed": " def uppercase_escape(s):\n     return re.sub(\n         r'\\\\U([0-9a-fA-F]{8})',\n         lambda m: compat_chr(int(m.group(1), base=16)), s)\ntry:\n    struct.pack(u'!I', 0)\nexcept TypeError:\n    def struct_pack(spec, *args):\n        if isinstance(spec, compat_str):\n            spec = spec.encode('ascii')\n        return struct.pack(spec, *args)\n    def struct_unpack(spec, *args):\n        if isinstance(spec, compat_str):\n            spec = spec.encode('ascii')\n        return struct.unpack(spec, *args)\nelse:\n    struct_pack = struct.pack\n    struct_unpack = struct.unpack"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n         self.response_model_exclude = response_model_exclude\n         self.response_model_by_alias = response_model_by_alias\n         self.response_model_exclude_unset = response_model_exclude_unset\n         self.include_in_schema = include_in_schema\n         self.response_class = response_class", "fixed": " class APIRoute(routing.Route):\n         self.response_model_exclude = response_model_exclude\n         self.response_model_by_alias = response_model_by_alias\n         self.response_model_exclude_unset = response_model_exclude_unset\n        self.response_model_exclude_defaults = response_model_exclude_defaults\n        self.response_model_exclude_none = response_model_exclude_none\n         self.include_in_schema = include_in_schema\n         self.response_class = response_class"}
{"id": "spacy_5", "problem": " class Language(object):\n             kwargs = component_cfg.get(name, {})\n             kwargs.setdefault(\"batch_size\", batch_size)\n             if not hasattr(pipe, \"pipe\"):\n                docs = _pipe(pipe, docs, kwargs)\n             else:\n                 docs = pipe.pipe(docs, **kwargs)\n         for doc, gold in zip(docs, golds):", "fixed": " class Language(object):\n             kwargs = component_cfg.get(name, {})\n             kwargs.setdefault(\"batch_size\", batch_size)\n             if not hasattr(pipe, \"pipe\"):\n                docs = _pipe(docs, pipe, kwargs)\n             else:\n                 docs = pipe.pipe(docs, **kwargs)\n         for doc, gold in zip(docs, golds):"}
{"id": "pandas_145", "problem": " def dispatch_to_series(left, right, func, str_rep=None, axis=None):\n         assert right.index.equals(left.columns)\n        def column_op(a, b):\n            return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}\n     elif isinstance(right, ABCSeries):\nassert right.index.equals(left.index)", "fixed": " def dispatch_to_series(left, right, func, str_rep=None, axis=None):\n         assert right.index.equals(left.columns)\n        if right.dtype == \"timedelta64[ns]\":\n            right = np.asarray(right)\n            def column_op(a, b):\n                return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}\n        else:\n            def column_op(a, b):\n                return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}\n     elif isinstance(right, ABCSeries):\nassert right.index.equals(left.index)"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n         if not self.writer:\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n             self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)\n             if self.append_header:\n                 self.writer.writeheader()", "fixed": " class CSVLogger(Callback):\n         if not self.writer:\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n            fieldnames = ['epoch'] + self.keys\n            if six.PY2:\n                fieldnames = [unicode(x) for x in fieldnames]\n             self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=fieldnames,\n                                         dialect=CustomDialect)\n             if self.append_header:\n                 self.writer.writeheader()"}
{"id": "luigi_6", "problem": " class DictParameter(Parameter):\n     tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related\n     values (like a database connection config).\n        JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n         Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.", "fixed": " class DictParameter(Parameter):\n     tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related\n     values (like a database connection config).\n         Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed."}
{"id": "ansible_6", "problem": " class CollectionRequirement:\n                 requirement = req\n                 op = operator.eq\n                if parent and version == '*' and requirement != '*':\n                    break\n                elif requirement == '*' or version == '*':\n                    continue\n             if not op(LooseVersion(version), LooseVersion(requirement)):\n                 break", "fixed": " class CollectionRequirement:\n                 requirement = req\n                 op = operator.eq\n            if parent and version == '*' and requirement != '*':\n                display.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\n                                \"install does not have a version set, the collection may not work.\"\n                                % (to_text(self), req, parent))\n                continue\n            elif requirement == '*' or version == '*':\n                continue\n             if not op(LooseVersion(version), LooseVersion(requirement)):\n                 break"}
{"id": "pandas_79", "problem": " class MultiIndex(Index):\n                     indexer = self._get_level_indexer(key, level=level)\n                     new_index = maybe_mi_droplevels(indexer, [0], drop_level)\n                     return indexer, new_index\n            except TypeError:\n                 pass\n             if not any(isinstance(k, slice) for k in key):", "fixed": " class MultiIndex(Index):\n                     indexer = self._get_level_indexer(key, level=level)\n                     new_index = maybe_mi_droplevels(indexer, [0], drop_level)\n                     return indexer, new_index\n            except (TypeError, InvalidIndexError):\n                 pass\n             if not any(isinstance(k, slice) for k in key):"}
{"id": "youtube-dl_16", "problem": " def srt_subtitles_timecode(seconds):\n def dfxp2srt(dfxp_data):\n     LEGACY_NAMESPACES = (\n        ('http://www.w3.org/ns/ttml', [\n            'http://www.w3.org/2004/11/ttaf1',\n            'http://www.w3.org/2006/04/ttaf1',\n            'http://www.w3.org/2006/10/ttaf1',\n         ]),\n        ('http://www.w3.org/ns/ttml\n            'http://www.w3.org/ns/ttml\n         ]),\n     )", "fixed": " def srt_subtitles_timecode(seconds):\n def dfxp2srt(dfxp_data):\n     LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n         ]),\n        (b'http://www.w3.org/ns/ttml\n            b'http://www.w3.org/ns/ttml\n         ]),\n     )"}
{"id": "pandas_36", "problem": " def _isna_old(obj):\n     elif hasattr(obj, \"__array__\"):\n         return _isna_ndarraylike_old(np.asarray(obj))\n     else:\n        return obj is None\n _isna = _isna_new", "fixed": " def _isna_old(obj):\n     elif hasattr(obj, \"__array__\"):\n         return _isna_ndarraylike_old(np.asarray(obj))\n     else:\n        return False\n _isna = _isna_new"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         self._write(s)\n     def __enter__(self):\n         calling_frame = inspect.currentframe().f_back\n         if not self._is_internal_frame(calling_frame):\n             calling_frame.f_trace = self.trace\n             self.target_frames.add(calling_frame)\n        stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])\n         stack.append(sys.gettrace())\n         sys.settrace(self.trace)\n     def __exit__(self, exc_type, exc_value, exc_traceback):\n         stack = self.thread_local.original_trace_functions\n         sys.settrace(stack.pop())\n         calling_frame = inspect.currentframe().f_back", "fixed": " class Tracer:\n         self._write(s)\n     def __enter__(self):\n        if DISABLED:\n            return\n         calling_frame = inspect.currentframe().f_back\n         if not self._is_internal_frame(calling_frame):\n             calling_frame.f_trace = self.trace\n             self.target_frames.add(calling_frame)\n        stack = self.thread_local.__dict__.setdefault(\n            'original_trace_functions', []\n        )\n         stack.append(sys.gettrace())\n         sys.settrace(self.trace)\n     def __exit__(self, exc_type, exc_value, exc_traceback):\n        if DISABLED:\n            return\n         stack = self.thread_local.original_trace_functions\n         sys.settrace(stack.pop())\n         calling_frame = inspect.currentframe().f_back"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n     Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array.\n         y: Numpy array of model targets to be weighted.", "fixed": " def standardize_weights(y,\n     Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array. If both `sample_weights` and `class_weights` are provided,\n    the weights are multiplied together.\n         y: Numpy array of model targets to be weighted."}
{"id": "keras_19", "problem": " class SimpleRNNCell(Layer):\n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class SimpleRNNCell(Layer):\n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "youtube-dl_40", "problem": " def write_flv_header(stream, metadata):\n     stream.write(b'\\x12')\n    stream.write(pack('!L', len(metadata))[1:])\n     stream.write(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n     stream.write(metadata)", "fixed": " def write_flv_header(stream, metadata):\n     stream.write(b'\\x12')\n    stream.write(struct_pack('!L', len(metadata))[1:])\n     stream.write(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n     stream.write(metadata)"}
{"id": "tqdm_8", "problem": " class tqdm(object):\n                     l_bar_user, r_bar_user = bar_format.split('{bar}')\n                    l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)\n                 else:\n                     return bar_format.format(**bar_args)", "fixed": " class tqdm(object):\n                     l_bar_user, r_bar_user = bar_format.split('{bar}')\n                    l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)\n                 else:\n                     return bar_format.format(**bar_args)"}
{"id": "pandas_35", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     @cache_readonly\n     def _engine(self):\n        period = weakref.ref(self)\n         return self._engine_type(period, len(self))\n     @doc(Index.__contains__)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     @cache_readonly\n     def _engine(self):\n        period = weakref.ref(self._values)\n         return self._engine_type(period, len(self))\n     @doc(Index.__contains__)"}
{"id": "black_18", "problem": " def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n     if src_txt[-1] != \"\\n\":\n        nl = \"\\r\\n\" if \"\\r\\n\" in src_txt[:1024] else \"\\n\"\n        src_txt += nl\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)\n         try:", "fixed": " def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n     if src_txt[-1] != \"\\n\":\n        src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)\n         try:"}
{"id": "tornado_10", "problem": " class WebSocketHandler(tornado.web.RequestHandler):\n         if not self._on_close_called:\n             self._on_close_called = True\n             self.on_close()\n     def send_error(self, *args, **kwargs):\n         if self.stream is None:", "fixed": " class WebSocketHandler(tornado.web.RequestHandler):\n         if not self._on_close_called:\n             self._on_close_called = True\n             self.on_close()\n            self._break_cycles()\n    def _break_cycles(self):\n        if self.get_status() != 101 or self._on_close_called:\n            super(WebSocketHandler, self)._break_cycles()\n     def send_error(self, *args, **kwargs):\n         if self.stream is None:"}
{"id": "pandas_96", "problem": " class BusinessHourMixin(BusinessMixin):\n             if bd != 0:\n                skip_bd = BusinessDay(n=bd)\n                 if not self.next_bday.is_on_offset(other):\n                     prev_open = self._prev_opening_time(other)", "fixed": " class BusinessHourMixin(BusinessMixin):\n             if bd != 0:\n                if isinstance(self, _CustomMixin):\n                    skip_bd = CustomBusinessDay(\n                        n=bd,\n                        weekmask=self.weekmask,\n                        holidays=self.holidays,\n                        calendar=self.calendar,\n                    )\n                else:\n                    skip_bd = BusinessDay(n=bd)\n                 if not self.next_bday.is_on_offset(other):\n                     prev_open = self._prev_opening_time(other)"}
{"id": "pandas_150", "problem": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if left_value != right_value:\n                     return False\n         return True", "fixed": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if np.any(left_value != right_value):\n                     return False\n         return True"}
{"id": "black_15", "problem": " class CannotSplit(Exception):\n    It holds the number of bytes of the prefix consumed before the format\n    control comment appeared.\n        unformatted_prefix = leaf.prefix[: self.consumed]\n        return Leaf(token.NEWLINE, unformatted_prefix)\nclass FormatOn(FormatError):\n class WriteBack(Enum):\n     NO = 0\n     YES = 1", "fixed": " class CannotSplit(Exception):\n class WriteBack(Enum):\n     NO = 0\n     YES = 1"}
{"id": "thefuck_28", "problem": " patterns = (\n         '^lua: {file}:{line}:',\n        '^{file} \\(line {line}\\):',\n         '^{file}: line {line}: ',\n        '^{file}:{line}:',\n         '^{file}:{line}:{col}',\n         'at {file} line {line}',\n     )", "fixed": " patterns = (\n         '^lua: {file}:{line}:',\n        '^{file} \\\\(line {line}\\\\):',\n         '^{file}: line {line}: ',\n         '^{file}:{line}:{col}',\n        '^{file}:{line}:',\n         'at {file} line {line}',\n     )"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n             output_shape,\n             self.strides,\n             padding=self.padding,\n            data_format=self.data_format)\n         if self.use_bias:\n             outputs = K.bias_add(", "fixed": " class Conv2DTranspose(Conv2D):\n             output_shape,\n             self.strides,\n             padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate)\n         if self.use_bias:\n             outputs = K.bias_add("}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to\n     :func:`format_file_contents`.\n    src = sys.stdin.read()\n     dst = src\n     try:\n         dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)", "fixed": " def format_stdin_to_stdout(\n     `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to\n     :func:`format_file_contents`.\n    newline, encoding, src = prepare_input(sys.stdin.buffer.read())\n     dst = src\n     try:\n         dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)"}
{"id": "fastapi_1", "problem": " def get_request_handler(\n                 exclude=response_model_exclude,\n                 by_alias=response_model_by_alias,\n                 exclude_unset=response_model_exclude_unset,\n                 is_coroutine=is_coroutine,\n             )\n             response = response_class(", "fixed": " def get_request_handler(\n                 exclude=response_model_exclude,\n                 by_alias=response_model_by_alias,\n                 exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                 is_coroutine=is_coroutine,\n             )\n             response = response_class("}
{"id": "youtube-dl_20", "problem": " def get_elements_by_attribute(attribute, value, html, escape_value=True):\n     retlist = []\n         <([a-zA-Z0-9:._-]+)\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'))*?\n          \\s+%s=['\"]?%s['\"]?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'))*?\n         \\s*>\n         (?P<content>.*?)\n         </\\1>", "fixed": " def get_elements_by_attribute(attribute, value, html, escape_value=True):\n     retlist = []\n         <([a-zA-Z0-9:._-]+)\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n          \\s+%s=['\"]?%s['\"]?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n         \\s*>\n         (?P<content>.*?)\n         </\\1>"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                                  '`state_size` attribute. '\n                                  'received cells:', cells)\n         self.cells = cells\n         super(StackedRNNCells, self).__init__(**kwargs)\n     @property\n     def state_size(self):\n         state_size = []\n        for cell in self.cells[::-1]:\n             if hasattr(cell.state_size, '__len__'):\n                 state_size += list(cell.state_size)\n             else:\n                 state_size.append(cell.state_size)\n         return tuple(state_size)\n     def call(self, inputs, states, constants=None, **kwargs):\n         nested_states = []\n        for cell in self.cells[::-1]:\n             if hasattr(cell.state_size, '__len__'):\n                 nested_states.append(states[:len(cell.state_size)])\n                 states = states[len(cell.state_size):]\n             else:\n                 nested_states.append([states[0]])\n                 states = states[1:]\n        nested_states = nested_states[::-1]\n         new_nested_states = []", "fixed": " class StackedRNNCells(Layer):\n                                  '`state_size` attribute. '\n                                  'received cells:', cells)\n         self.cells = cells\n        self.reverse_state_order = kwargs.pop('reverse_state_order', False)\n        if self.reverse_state_order:\n            warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '\n                          'will soon be deprecated. Please update the code to '\n                          'work with the natural order of states if you '\n                          'reply on the RNN states, '\n                          'eg `RNN(return_state=True)`.')\n         super(StackedRNNCells, self).__init__(**kwargs)\n     @property\n     def state_size(self):\n         state_size = []\n        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n             if hasattr(cell.state_size, '__len__'):\n                 state_size += list(cell.state_size)\n             else:\n                 state_size.append(cell.state_size)\n         return tuple(state_size)\n    @property\n    def output_size(self):\n        if getattr(self.cells[-1], 'output_size', None) is not None:\n            return self.cells[-1].output_size\n        if hasattr(self.cells[-1].state_size, '__len__'):\n            return self.cells[-1].state_size[0]\n        else:\n            return self.cells[-1].state_size\n     def call(self, inputs, states, constants=None, **kwargs):\n         nested_states = []\n        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:\n             if hasattr(cell.state_size, '__len__'):\n                 nested_states.append(states[:len(cell.state_size)])\n                 states = states[len(cell.state_size):]\n             else:\n                 nested_states.append([states[0]])\n                 states = states[1:]\n        if self.reverse_state_order:\n            nested_states = nested_states[::-1]\n         new_nested_states = []"}
{"id": "keras_42", "problem": " class Model(Container):\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Integer. Maximum size for the generator queue.", "fixed": " class Model(Container):\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Integer. Maximum size for the generator queue."}
{"id": "luigi_9", "problem": " class RetcodesTest(LuigiTestCase):\n         with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):\n             self.run_and_expect('RequiringTask', 0)\n             self.run_and_expect('RequiringTask --retcode-not-run 5', 5)", "fixed": " class RetcodesTest(LuigiTestCase):\n         with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):\n             self.run_and_expect('RequiringTask', 0)\n             self.run_and_expect('RequiringTask --retcode-not-run 5', 5)\n    def test_retry_sucess_task(self):\n        class Foo(luigi.Task):\n            run_count = 0\n            def run(self):\n                self.run_count += 1\n                if self.run_count == 1:\n                    raise ValueError()\n            def complete(self):\n                return self.run_count > 0\n        self.run_and_expect('Foo --scheduler-retry-delay=0', 0)\n        self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)\n        self.run_with_config(dict(task_failed='3'), 'Foo', 0)"}
{"id": "keras_9", "problem": " def count_leading_spaces(s):\n def process_list_block(docstring, starting_point, section_end,\n                        leading_spaces, marker):\n     ending_point = docstring.find('\\n\\n', starting_point)\n    block = docstring[starting_point:(None if ending_point == -1 else\n                                      ending_point - 1)]\n     docstring_slice = docstring[starting_point:section_end].replace(block, marker)\n     docstring = (docstring[:starting_point]", "fixed": " def count_leading_spaces(s):\n def process_list_block(docstring, starting_point, section_end,\n                        leading_spaces, marker):\n     ending_point = docstring.find('\\n\\n', starting_point)\n    block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else\n                                      section_end)]\n     docstring_slice = docstring[starting_point:section_end].replace(block, marker)\n     docstring = (docstring[:starting_point]"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         collection = mcoll.PathCollection(\n                 (path,), scales,\n                facecolors=colors,\n                edgecolors=edgecolors,\n                 linewidths=linewidths,\n                 offsets=offsets,\n                 transOffset=kwargs.pop('transform', self.transData),", "fixed": " default: :rc:`scatter.edgecolors`\n         collection = mcoll.PathCollection(\n                 (path,), scales,\n                facecolors=colors if marker_obj.is_filled() else 'none',\n                edgecolors=edgecolors if marker_obj.is_filled() else colors,\n                 linewidths=linewidths,\n                 offsets=offsets,\n                 transOffset=kwargs.pop('transform', self.transData),"}
{"id": "black_17", "problem": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n     srcbuf = io.BytesIO(src)\n     encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n     newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n     srcbuf.seek(0)\n     with io.TextIOWrapper(srcbuf, encoding) as tiow:", "fixed": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n     srcbuf = io.BytesIO(src)\n     encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    if not lines:\n        return \"\", encoding, \"\\n\"\n     newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n     srcbuf.seek(0)\n     with io.TextIOWrapper(srcbuf, encoding) as tiow:"}
{"id": "scrapy_36", "problem": " def create_instance(objcls, settings, crawler, *args, **kwargs):\n     ``*args`` and ``**kwargs`` are forwarded to the constructors.\n     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n     if settings is None:\n         if crawler is None:\n             raise ValueError(\"Specify at least one of settings and crawler.\")\n         settings = crawler.settings\n     if crawler and hasattr(objcls, 'from_crawler'):\n        return objcls.from_crawler(crawler, *args, **kwargs)\n     elif hasattr(objcls, 'from_settings'):\n        return objcls.from_settings(settings, *args, **kwargs)\n     else:\n        return objcls(*args, **kwargs)\n @contextmanager", "fixed": " def create_instance(objcls, settings, crawler, *args, **kwargs):\n     ``*args`` and ``**kwargs`` are forwarded to the constructors.\n     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n    Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n    extension has not been implemented correctly).\n     if settings is None:\n         if crawler is None:\n             raise ValueError(\"Specify at least one of settings and crawler.\")\n         settings = crawler.settings\n     if crawler and hasattr(objcls, 'from_crawler'):\n        instance = objcls.from_crawler(crawler, *args, **kwargs)\n        method_name = 'from_crawler'\n     elif hasattr(objcls, 'from_settings'):\n        instance = objcls.from_settings(settings, *args, **kwargs)\n        method_name = 'from_settings'\n     else:\n        instance = objcls(*args, **kwargs)\n        method_name = '__new__'\n    if instance is None:\n        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n    return instance\n @contextmanager"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n         pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n         maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n         for fields in mtab_entries:\n             device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]", "fixed": " class LinuxHardware(Hardware):\n         pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n         maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n         for fields in mtab_entries:\n            fields = [self._replace_octal_escapes(field) for field in fields]\n             device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]"}
{"id": "luigi_8", "problem": " class S3CopyToTable(rdbms.CopyToTable):\n         if '.' in self.table:\n             query = (\"select 1 as table_exists \"\n                      \"from information_schema.tables \"\n                     \"where table_schema = %s and table_name = %s limit 1\")\n         else:\n             query = (\"select 1 as table_exists \"\n                      \"from pg_table_def \"\n                     \"where tablename = %s limit 1\")\n         cursor = connection.cursor()\n         try:\n             cursor.execute(query, tuple(self.table.split('.')))", "fixed": " class S3CopyToTable(rdbms.CopyToTable):\n         if '.' in self.table:\n             query = (\"select 1 as table_exists \"\n                      \"from information_schema.tables \"\n                     \"where table_schema = lower(%s) and table_name = lower(%s) limit 1\")\n         else:\n             query = (\"select 1 as table_exists \"\n                      \"from pg_table_def \"\n                     \"where tablename = lower(%s) limit 1\")\n         cursor = connection.cursor()\n         try:\n             cursor.execute(query, tuple(self.table.split('.')))"}
{"id": "tornado_13", "problem": " TEST_MODULES = [\n     'tornado.test.curl_httpclient_test',\n     'tornado.test.escape_test',\n     'tornado.test.gen_test',\n     'tornado.test.httpclient_test',\n     'tornado.test.httpserver_test',\n     'tornado.test.httputil_test',", "fixed": " TEST_MODULES = [\n     'tornado.test.curl_httpclient_test',\n     'tornado.test.escape_test',\n     'tornado.test.gen_test',\n    'tornado.test.http1connection_test',\n     'tornado.test.httpclient_test',\n     'tornado.test.httpserver_test',\n     'tornado.test.httputil_test',"}
{"id": "pandas_111", "problem": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     @Appender(_index_shared_docs[\"_convert_scalar_indexer\"])\n     def _convert_scalar_indexer(self, key, kind=None):\n        if self.categories._defer_to_indexing:\n            return self.categories._convert_scalar_indexer(key, kind=kind)\n         return super()._convert_scalar_indexer(key, kind=kind)\n     @Appender(_index_shared_docs[\"_convert_list_indexer\"])", "fixed": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     @Appender(_index_shared_docs[\"_convert_scalar_indexer\"])\n     def _convert_scalar_indexer(self, key, kind=None):\n        if kind == \"loc\":\n            try:\n                return self.categories._convert_scalar_indexer(key, kind=kind)\n            except TypeError:\n                self._invalid_indexer(\"label\", key)\n         return super()._convert_scalar_indexer(key, kind=kind)\n     @Appender(_index_shared_docs[\"_convert_list_indexer\"])"}
{"id": "pandas_36", "problem": " def na_value_for_dtype(dtype, compat: bool = True):\n     if is_extension_array_dtype(dtype):\n         return dtype.na_value\n    if (\n        is_datetime64_dtype(dtype)\n        or is_datetime64tz_dtype(dtype)\n        or is_timedelta64_dtype(dtype)\n        or is_period_dtype(dtype)\n    ):\n         return NaT\n     elif is_float_dtype(dtype):\n         return np.nan", "fixed": " def na_value_for_dtype(dtype, compat: bool = True):\n     if is_extension_array_dtype(dtype):\n         return dtype.na_value\n    if needs_i8_conversion(dtype):\n         return NaT\n     elif is_float_dtype(dtype):\n         return np.nan"}
{"id": "black_15", "problem": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines) or line.is_comment:\n         yield line\n         return", "fixed": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if line.is_comment:\n         yield line\n         return"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         left, right = sorted([left, right], reverse=bool(reverse))\n         self._viewLim.intervalx = (left, right)\n         if auto is not None:\n             self._autoscaleXon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         left, right = sorted([left, right], reverse=bool(reverse))\n         self._viewLim.intervalx = (left, right)\n        for ax in self._shared_x_axes.get_siblings(self):\n            ax._stale_viewlim_x = False\n         if auto is not None:\n             self._autoscaleXon = bool(auto)"}
{"id": "thefuck_11", "problem": " def match(command):\n @git_support\n def get_new_command(command):\n     push_upstream = command.stderr.split('\\n')[-3].strip().partition('git ')[2]\n    return replace_argument(command.script, 'push', push_upstream)", "fixed": " def match(command):\n @git_support\n def get_new_command(command):\n    upstream_option_index = -1\n    try:\n        upstream_option_index = command.script_parts.index('--set-upstream')\n    except ValueError:\n        pass\n    try:\n        upstream_option_index = command.script_parts.index('-u')\n    except ValueError:\n        pass\n    if upstream_option_index is not -1:\n        command.script_parts.pop(upstream_option_index)\n        command.script_parts.pop(upstream_option_index)\n     push_upstream = command.stderr.split('\\n')[-3].strip().partition('git ')[2]\n    return replace_argument(\" \".join(command.script_parts), 'push', push_upstream)"}
{"id": "keras_11", "problem": " def fit_generator(model,\n             if val_gen and workers > 0:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer = OrderedEnqueuer(\n                         val_data,\n                         use_multiprocessing=use_multiprocessing)", "fixed": " def fit_generator(model,\n             if val_gen and workers > 0:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer = OrderedEnqueuer(\n                         val_data,\n                         use_multiprocessing=use_multiprocessing)"}
{"id": "scrapy_33", "problem": " class FeedExporter(object):\n         d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                             extra={'spider': spider}))\n         d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            extra={'spider': spider, 'failure': f}))\n         return d\n     def item_scraped(self, item, spider):", "fixed": " class FeedExporter(object):\n         d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                             extra={'spider': spider}))\n         d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n         return d\n     def item_scraped(self, item, spider):"}
{"id": "tornado_15", "problem": " setup(\n             \"options_test.cfg\",\n             \"static/robots.txt\",\n             \"static/dir/index.html\",\n             \"templates/utf8.html\",\n             \"test.crt\",\n             \"test.key\",", "fixed": " setup(\n             \"options_test.cfg\",\n             \"static/robots.txt\",\n             \"static/dir/index.html\",\n            \"static_foo.txt\",\n             \"templates/utf8.html\",\n             \"test.crt\",\n             \"test.key\","}
{"id": "keras_1", "problem": " def update_add(x, increment):\n         The variable `x` updated.\n    return tf_state_ops.assign_add(x, increment)\n @symbolic", "fixed": " def update_add(x, increment):\n         The variable `x` updated.\n    op = tf_state_ops.assign_add(x, increment)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         else:\n             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n    def test_print_tensor(self):\n         check_single_tensor_operation('print_tensor', (), WITH_NP)\n         check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n        check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n        check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), WITH_NP)", "fixed": " class TestBackend(object):\n         else:\n             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n    def test_print_tensor(self, capsys):\n        for k in [KTH, KTF]:\n            x = k.placeholder((1, 1))\n            y = k.print_tensor(x, 'msg')\n            fn = k.function([x], [y])\n            _ = fn([np.ones((1, 1))])\n            out, err = capsys.readouterr()\n            assert out.replace('__str__ = ', '') == 'msg [[1.]]\\n'\n         check_single_tensor_operation('print_tensor', (), WITH_NP)\n         check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), WITH_NP)"}
{"id": "pandas_84", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n     index = data.index\n     clocs = [index._get_level_number(i) for i in clocs]\n     rlocs = [i for i in range(index.nlevels) if i not in clocs]", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n     index = data.index\n    if clocs in index.names:\n        clocs = [clocs]\n     clocs = [index._get_level_number(i) for i in clocs]\n     rlocs = [i for i in range(index.nlevels) if i not in clocs]"}
{"id": "pandas_1", "problem": " def is_string_dtype(arr_or_dtype) -> bool:\n        is_excluded_checks = (is_period_dtype, is_interval_dtype)\n         return any(is_excluded(dtype) for is_excluded in is_excluded_checks)\n     return _is_dtype(arr_or_dtype, condition)", "fixed": " def is_string_dtype(arr_or_dtype) -> bool:\n        is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)\n         return any(is_excluded(dtype) for is_excluded in is_excluded_checks)\n     return _is_dtype(arr_or_dtype, condition)"}
{"id": "keras_20", "problem": " def conv_input_length(output_length, filter_size, padding, stride):\n     return (output_length - 1) * stride - 2 * pad + filter_size\ndef deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):", "fixed": " def conv_input_length(output_length, filter_size, padding, stride):\n     return (output_length - 1) * stride - 2 * pad + filter_size\ndef deconv_length(dim_size, stride_size, kernel_size, padding,\n                  output_padding, dilation=1):"}
{"id": "pandas_70", "problem": " def test_resample_categorical_data_with_timedeltaindex():\n         index=pd.to_timedelta([0, 10], unit=\"s\"),\n     )\n     expected = expected.reindex([\"Group_obj\", \"Group\"], axis=1)\n    expected[\"Group\"] = expected[\"Group_obj\"].astype(\"category\")\n     tm.assert_frame_equal(result, expected)", "fixed": " def test_resample_categorical_data_with_timedeltaindex():\n         index=pd.to_timedelta([0, 10], unit=\"s\"),\n     )\n     expected = expected.reindex([\"Group_obj\", \"Group\"], axis=1)\n    expected[\"Group\"] = expected[\"Group_obj\"]\n     tm.assert_frame_equal(result, expected)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n             )\n         return jsonable_encoder(\n             obj_dict,\n            include_none=include_none,\n             custom_encoder=encoder,\n             sqlalchemy_safe=sqlalchemy_safe,\n         )", "fixed": " def jsonable_encoder(\n             )\n         return jsonable_encoder(\n             obj_dict,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n             custom_encoder=encoder,\n             sqlalchemy_safe=sqlalchemy_safe,\n         )"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_13", "problem": " def _isna_old(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike_old(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_old(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=True)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=True)\n     else:\n         return False"}
{"id": "ansible_5", "problem": " def test_check_mutually_exclusive_none():\n def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):\n     with pytest.raises(TypeError) as te:\n         check_mutually_exclusive(mutually_exclusive_terms, None)\n        assert \"TypeError: 'NoneType' object is not iterable\" in to_native(te.error)", "fixed": " def test_check_mutually_exclusive_none():\n def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):\n     with pytest.raises(TypeError) as te:\n         check_mutually_exclusive(mutually_exclusive_terms, None)\n    assert \"'NoneType' object is not iterable\" in to_native(te.value)"}
{"id": "matplotlib_27", "problem": " class ColorbarBase(_ColorbarMappableDummy):\n     def set_label(self, label, **kw):\n        self._label = str(label)\n         self._labelkw = kw\n         self._set_label()", "fixed": " class ColorbarBase(_ColorbarMappableDummy):\n     def set_label(self, label, **kw):\n        self._label = label\n         self._labelkw = kw\n         self._set_label()"}
{"id": "pandas_80", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             return self\n        arr = operator.inv(com.values_from_object(self))\n        return self.__array_wrap__(arr)\n     def __nonzero__(self):\n         raise ValueError(", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             return self\n        new_data = self._data.apply(operator.invert)\n        result = self._constructor(new_data).__finalize__(self)\n        return result\n     def __nonzero__(self):\n         raise ValueError("}
{"id": "scrapy_1", "problem": " class OffsiteMiddleware(object):\n         if not allowed_domains:\nreturn re.compile('')\n         url_pattern = re.compile(\"^https?://.*$\")\n         for domain in allowed_domains:\n            if url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                 warnings.warn(message, URLWarning)\n        domains = [re.escape(d) for d in allowed_domains if d is not None]\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)", "fixed": " class OffsiteMiddleware(object):\n         if not allowed_domains:\nreturn re.compile('')\n         url_pattern = re.compile(\"^https?://.*$\")\n        domains = []\n         for domain in allowed_domains:\n            if domain is None:\n                continue\n            elif url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                 warnings.warn(message, URLWarning)\n            else:\n                domains.append(re.escape(domain))\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)"}
{"id": "luigi_23", "problem": " class scheduler(Config):\n     visualization_graph = parameter.Parameter(default=\"svg\", config_path=dict(section='scheduler', name='visualization-graph'))\n def fix_time(x):", "fixed": " class scheduler(Config):\n     visualization_graph = parameter.Parameter(default=\"svg\", config_path=dict(section='scheduler', name='visualization-graph'))\n    prune_on_get_work = parameter.BoolParameter(default=False)\n def fix_time(x):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_137", "problem": " class Categorical(ExtensionArray, PandasObject):\n             if dtype == self.dtype:\n                 return self\n             return self._set_dtype(dtype)\n         if is_integer_dtype(dtype) and self.isna().any():\n             msg = \"Cannot convert float NaN to integer\"\n             raise ValueError(msg)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             if dtype == self.dtype:\n                 return self\n             return self._set_dtype(dtype)\n        if is_extension_array_dtype(dtype):\n            return array(self, dtype=dtype, copy=copy)\n         if is_integer_dtype(dtype) and self.isna().any():\n             msg = \"Cannot convert float NaN to integer\"\n             raise ValueError(msg)"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n             - 'none': No patch boundary will be drawn.\n             - A color or sequence of colors.\n            For non-filled markers, the *edgecolors* kwarg is ignored and\n            forced to 'face' internally.\n         plotnonfinite : bool, default: False\n             Set to plot points with nonfinite *c*, in conjunction with", "fixed": " default: :rc:`scatter.edgecolors`\n             - 'none': No patch boundary will be drawn.\n             - A color or sequence of colors.\n            For non-filled markers, *edgecolors* is ignored. Instead, the color\n            is determined like with 'face', i.e. from *c*, *colors*, or\n            *facecolors*.\n         plotnonfinite : bool, default: False\n             Set to plot points with nonfinite *c*, in conjunction with"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         self.target_codes = set()\n         self.target_frames = set()\n         self.thread_local = threading.local()\n     def __call__(self, function):\n         self.target_codes.add(function.__code__)\n         @functools.wraps(function)", "fixed": " class Tracer:\n         self.target_codes = set()\n         self.target_frames = set()\n         self.thread_local = threading.local()\n        if len(custom_repr) == 2 and not all(isinstance(x,\n                      pycompat.collections_abc.Iterable) for x in custom_repr):\n            custom_repr = (custom_repr,)\n        self.custom_repr = custom_repr\n     def __call__(self, function):\n        if DISABLED:\n            return function\n         self.target_codes.add(function.__code__)\n         @functools.wraps(function)"}
{"id": "matplotlib_15", "problem": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')", "fixed": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n         d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                           extra={'spider': spider, 'failure': f}))\n         d.addBoth(lambda _: slot.remove_request(request))\n         d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                           extra={'spider': spider, 'failure': f}))\n         d.addBoth(lambda _: slot.nextcall.schedule())\n         d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                           extra={'spider': spider, 'failure': f}))\n         return d\n     def _handle_downloader_output(self, response, request, spider):", "fixed": " class ExecutionEngine(object):\n         d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n         d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n         d.addBoth(lambda _: slot.remove_request(request))\n         d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n         d.addBoth(lambda _: slot.nextcall.schedule())\n         d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n         return d\n     def _handle_downloader_output(self, response, request, spider):"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                              use_multiprocessing=use_multiprocessing)\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10, workers=1,\n                           use_multiprocessing=False, verbose=0):", "fixed": " class Sequential(Model):\n                                              use_multiprocessing=use_multiprocessing)\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10, workers=1,\n                           use_multiprocessing=False, verbose=0):"}
{"id": "matplotlib_15", "problem": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')", "fixed": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')"}
{"id": "matplotlib_28", "problem": " class _AxesBase(martist.Artist):\n             if right is None:\n                 right = old_right\n        if self.get_xscale() == 'log':\n             if left <= 0:\n                 cbook._warn_external(\n                     'Attempted to set non-positive left xlim on a '", "fixed": " class _AxesBase(martist.Artist):\n             if right is None:\n                 right = old_right\n        if self.get_xscale() == 'log' and (left <= 0 or right <= 0):\n            old_left, old_right = self.get_xlim()\n             if left <= 0:\n                 cbook._warn_external(\n                     'Attempted to set non-positive left xlim on a '"}
{"id": "pandas_89", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n             result = data\n             for i in range(len(clocs)):\n                 val = clocs[i]\n                result = result.unstack(val)\n                 clocs = [v if i > v else v - 1 for v in clocs]\n             return result", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n             result = data\n             for i in range(len(clocs)):\n                 val = clocs[i]\n                result = result.unstack(val, fill_value=fill_value)\n                 clocs = [v if i > v else v - 1 for v in clocs]\n             return result"}
{"id": "youtube-dl_37", "problem": " class PagedList(object):\n def uppercase_escape(s):\n     return re.sub(\n         r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: m.group(0).decode('unicode-escape'), s)\n try:\n     struct.pack(u'!I', 0)", "fixed": " class PagedList(object):\n def uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n     return re.sub(\n         r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n try:\n     struct.pack(u'!I', 0)"}
{"id": "youtube-dl_6", "problem": " def match_filter_func(filter_str):\n def parse_dfxp_time_expr(time_expr):\n     if not time_expr:\n        return 0.0\n     mobj = re.match(r'^(?P<time_offset>\\d+(?:\\.\\d+)?)s?$', time_expr)\n     if mobj:", "fixed": " def match_filter_func(filter_str):\n def parse_dfxp_time_expr(time_expr):\n     if not time_expr:\n        return\n     mobj = re.match(r'^(?P<time_offset>\\d+(?:\\.\\d+)?)s?$', time_expr)\n     if mobj:"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n         )\n        msg = \"cannot insert TimedeltaIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, pd.Timestamp(\"2012-01-01\"))\n        msg = \"cannot insert TimedeltaIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n         )\n        msg = \"cannot insert TimedeltaArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, pd.Timestamp(\"2012-01-01\"))\n        msg = \"cannot insert TimedeltaArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if method == \"pearson\":\n            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)\n         elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)\n         elif method == \"kendall\" or callable(method):\n             if min_periods is None:\n                 min_periods = 1\n            mat = ensure_float64(mat).T\n             corrf = nanops.get_corr_func(method)\n             K = len(cols)\n             correl = np.empty((K, K), dtype=float)", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if method == \"pearson\":\n            correl = libalgos.nancorr(mat, minp=min_periods)\n         elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n         elif method == \"kendall\" or callable(method):\n             if min_periods is None:\n                 min_periods = 1\n            mat = mat.T\n             corrf = nanops.get_corr_func(method)\n             K = len(cols)\n             correl = np.empty((K, K), dtype=float)"}
{"name": "flatten.py", "problem": "def flatten(arr):\n    for x in arr:\n        if isinstance(x, list):\n            for y in flatten(x):\n                yield y\n        else:\n            yield flatten(x)", "fixed": "def flatten(arr):\n    for x in arr:\n        if isinstance(x, list):\n            for y in flatten(x):\n                yield y\n        else:\n            yield x", "hint": "Flatten\nFlattens a nested list data structure into a single list.\nInput:", "input": [["moe", "curly", "larry"]], "output": ["moe", "curly", "larry"]}
{"name": "reverse_linked_list.py", "problem": "def reverse_linked_list(node):\n    prevnode = None\n    while node:\n        nextnode = node.successor\n        node.successor = prevnode\n        node = nextnode\n    return prevnode", "fixed": "def reverse_linked_list(node):\n    prevnode = None\n    while node:\n        nextnode = node.successor\n        node.successor = prevnode\n        prevnode = node\n        node = nextnode\n    return prevnode\n", "hint": "Reverse Linked List\nReverses a linked list and returns the new head.\nInput:", "input": "", "output": ""}
{"name": "shortest_path_lengths.py", "problem": "from collections import defaultdict\ndef shortest_path_lengths(n, length_by_edge):\n    length_by_path = defaultdict(lambda: float('inf'))\n    length_by_path.update({(i, i): 0 for i in range(n)})\n    length_by_path.update(length_by_edge)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                length_by_path[i, j] = min(\n                    length_by_path[i, j],\n                    length_by_path[i, k] + length_by_path[j, k]\n                )\n    return length_by_path", "fixed": "from collections import defaultdict\ndef shortest_path_lengths(n, length_by_edge):\n    length_by_path = defaultdict(lambda: float('inf'))\n    length_by_path.update({(i, i): 0 for i in range(n)})\n    length_by_path.update(length_by_edge)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                length_by_path[i, j] = min(\n                    length_by_path[i, j],\n                    length_by_path[i, k] + length_by_path[k, j]\n                )\n    return length_by_path", "hint": "All Shortest Paths\nfloyd-warshall\nFloyd-Warshall algorithm implementation.", "input": "", "output": ""}
{"name": "possible_change.py", "problem": "def possible_change(coins, total):\n    if total == 0:\n        return 1\n    if total < 0:\n        return 0\n    first, *rest = coins\n    return possible_change(coins, total - first) + possible_change(rest, total)", "fixed": "def possible_change(coins, total):\n    if total == 0:\n        return 1\n    if total < 0 or not coins:\n        return 0\n    first, *rest = coins\n    return possible_change(coins, total - first) + possible_change(rest, total)\n", "hint": "Making Change\nchange\nInput:", "input": [[1, 4, 2], -7], "output":0}
