{"name": "levenshtein.py", "problem": "def levenshtein(source, target):\n    if source == '' or target == '':\n        return len(source) or len(target)\n    elif source[0] == target[0]:\n        return 1 + levenshtein(source[1:], target[1:])\n    else:\n        return 1 + min(\n            levenshtein(source,     target[1:]),\n            levenshtein(source[1:], target[1:]),\n            levenshtein(source[1:], target)\n        )", "fixed": "def levenshtein(source, target):\n    if source == '' or target == '':\n        return len(source) or len(target)\n    elif source[0] == target[0]:\n        return levenshtein(source[1:], target[1:])\n    else:\n        return 1 + min(\n            levenshtein(source,     target[1:]),\n            levenshtein(source[1:], target[1:]),\n            levenshtein(source[1:], target)\n        )", "hint": "Levenshtein Distance\nCalculates the Levenshtein distance between two strings.  The Levenshtein distance is defined as the minimum amount of single-character edits (either removing a character, adding a character, or changing a character) necessary to transform a source string into a target string.\nInput:", "input": ["electron", "neutron"], "output": 3}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "scrapy_34", "problem": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = {}\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)", "fixed": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n            extra={'spider': spider, 'failure': _failure}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,", "fixed": " class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj +\n               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj"}
{"id": "pandas_160", "problem": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\"):\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif isinstance(o, np.ndarray):\n                     dtypes |= {o.dtype.name}", "fixed": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif hasattr(o, \"dtype\"):\n                     dtypes |= {o.dtype.name}"}
{"id": "black_10", "problem": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char == ' ':\n                 current_column += 1\n            elif char == '\\t':\n                current_column += 4\n             elif char == '\\n':\n                 current_column = 0", "fixed": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char in ' \\t':\n                 current_column += 1\n             elif char == '\\n':\n                 current_column = 0"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "ansible_11", "problem": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n    if text:\n        text = str(text).strip()\n     return {\n         'banner': module.params['banner'],\n         'text': text,", "fixed": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n     return {\n         'banner': module.params['banner'],\n         'text': text,"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "pandas_166", "problem": " class DataFrame(NDFrame):\n             if can_concat:\n                 if how == \"left\":\n                    res = concat(frames, axis=1, join=\"outer\", verify_integrity=True)\n                     return res.reindex(self.index, copy=False)\n                 else:\n                    return concat(frames, axis=1, join=how, verify_integrity=True)\n             joined = frames[0]", "fixed": " class DataFrame(NDFrame):\n             if can_concat:\n                 if how == \"left\":\n                    res = concat(\n                        frames, axis=1, join=\"outer\", verify_integrity=True, sort=sort\n                    )\n                     return res.reindex(self.index, copy=False)\n                 else:\n                    return concat(\n                        frames, axis=1, join=how, verify_integrity=True, sort=sort\n                    )\n             joined = frames[0]"}
{"id": "luigi_29", "problem": " class AmbiguousClass(luigi.Task):\n     pass\nclass NonAmbiguousClass(luigi.ExternalTask):\n    pass\nclass NonAmbiguousClass(luigi.Task):\n    def run(self):\n        NonAmbiguousClass.has_run = True\n class TaskWithSameName(luigi.Task):\n     def run(self):", "fixed": " class AmbiguousClass(luigi.Task):\n     pass\n class TaskWithSameName(luigi.Task):\n     def run(self):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "thefuck_30", "problem": " def _search(stderr):\n def match(command, settings):\n    return 'EDITOR' in os.environ and _search(command.stderr)\n def get_new_command(command, settings):", "fixed": " def _search(stderr):\n def match(command, settings):\n    if 'EDITOR' not in os.environ:\n        return False\n    m = _search(command.stderr)\n    return m and os.path.isfile(m.group('file'))\n def get_new_command(command, settings):"}
{"id": "scrapy_40", "problem": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        if self.binary:\n            return to_bytes(value, encoding=self.encoding)\n        else:\n            return to_unicode(value, encoding=self.encoding)\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):", "fixed": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "cookiecutter_3", "problem": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default\n     )\n     return choice_map[user_choice]", "fixed": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default, show_choices=False\n     )\n     return choice_map[user_choice]"}
{"name": "longest_common_subsequence.py", "problem": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b)\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "fixed": "def longest_common_subsequence(a, b):\n    if not a or not b:\n        return ''\n    elif a[0] == b[0]:\n        return a[0] + longest_common_subsequence(a[1:], b[1:])\n    else:\n        return max(\n            longest_common_subsequence(a, b[1:]),\n            longest_common_subsequence(a[1:], b),\n            key=len\n        )", "hint": "Longest Common Subsequence\nCalculates the longest subsequence common to the two input strings. (A subsequence is any sequence of letters in the same order\nthey appear in the string, possibly skipping letters in between.)", "input": ["headache", "pentadactyl"], "output": "eadac"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "youtube-dl_38", "problem": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]", "fixed": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]\ndef urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')"}
{"id": "thefuck_1", "problem": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \\\"([a-z]+)\\\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)", "fixed": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)"}
{"id": "keras_42", "problem": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.", "fixed": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training."}
{"id": "pandas_92", "problem": " class TestPeriodIndex:\n         p2 = pd.Period(\"2014-01-04\", freq=freq)\n         assert pidx.searchsorted(p2) == 3\n        msg = \"Input has different freq=H from PeriodIndex\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"H\"))\n        msg = \"Input has different freq=5D from PeriodIndex\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"5D\"))\n class TestPeriodIndexConversion:\n     def test_tolist(self):", "fixed": " class TestPeriodIndex:\n         p2 = pd.Period(\"2014-01-04\", freq=freq)\n         assert pidx.searchsorted(p2) == 3\n        assert pidx.searchsorted(pd.NaT) == 0\n        msg = \"Input has different freq=H from PeriodArray\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"H\"))\n        msg = \"Input has different freq=5D from PeriodArray\"\n         with pytest.raises(IncompatibleFrequency, match=msg):\n             pidx.searchsorted(pd.Period(\"2014-01-01\", freq=\"5D\"))\n    def test_searchsorted_invalid(self):\n        pidx = pd.PeriodIndex(\n            [\"2014-01-01\", \"2014-01-02\", \"2014-01-03\", \"2014-01-04\", \"2014-01-05\"],\n            freq=\"D\",\n        )\n        other = np.array([0, 1], dtype=np.int64)\n        msg = \"requires either a Period or PeriodArray\"\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(other)\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(other.astype(\"timedelta64[ns]\"))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.timedelta64(4))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.timedelta64(\"NaT\", \"ms\"))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.datetime64(4, \"ns\"))\n        with pytest.raises(TypeError, match=msg):\n            pidx.searchsorted(np.datetime64(\"NaT\", \"ns\"))\n class TestPeriodIndexConversion:\n     def test_tolist(self):"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n         return name, value\n     @memoize\n    @cache('.bashrc', '.bash_profile')\n     def get_aliases(self):\n        proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n        return dict(\n                self._parse_alias(alias)\n                for alias in proc.stdout.read().decode('utf-8').split('\\n')\n                if alias and '=' in alias)\n     def _get_history_file_name(self):\n         return os.environ.get(\"HISTFILE\",", "fixed": " class Bash(Generic):\n         return name, value\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n        return dict(self._parse_alias(alias)\n                    for alias in raw_aliases if alias and '=' in alias)\n     def _get_history_file_name(self):\n         return os.environ.get(\"HISTFILE\","}
{"id": "thefuck_20", "problem": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])\n def side_effect(old_cmd, command):", "fixed": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))\n def side_effect(old_cmd, command):"}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
{"id": "pandas_2", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n         if is_setter:\n             return list(key)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)\n         if is_setter:\n             return list(key)"}
{"id": "keras_35", "problem": " class DirectoryIterator(Iterator):\n             fname = self.filenames[j]\n             img = load_img(os.path.join(self.directory, fname),\n                            grayscale=grayscale,\n                           target_size=self.target_size,\n                            interpolation=self.interpolation)\n             x = img_to_array(img, data_format=self.data_format)\n             x = self.image_data_generator.random_transform(x)\n             x = self.image_data_generator.standardize(x)", "fixed": " class DirectoryIterator(Iterator):\n             fname = self.filenames[j]\n             img = load_img(os.path.join(self.directory, fname),\n                            grayscale=grayscale,\n                           target_size=None,\n                            interpolation=self.interpolation)\n            if self.image_data_generator.preprocessing_function:\n                img = self.image_data_generator.preprocessing_function(img)\n            if self.target_size is not None:\n                width_height_tuple = (self.target_size[1], self.target_size[0])\n                if img.size != width_height_tuple:\n                    if self.interpolation not in _PIL_INTERPOLATION_METHODS:\n                        raise ValueError(\n                            'Invalid interpolation method {} specified. Supported '\n                            'methods are {}'.format(\n                                self.interpolation,\n                                \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n                    resample = _PIL_INTERPOLATION_METHODS[self.interpolation]\n                    img = img.resize(width_height_tuple, resample)\n             x = img_to_array(img, data_format=self.data_format)\n             x = self.image_data_generator.random_transform(x)\n             x = self.image_data_generator.standardize(x)"}
{"id": "thefuck_25", "problem": " def match(command, settings):\n @sudo_support\n def get_new_command(command, settings):\n    return re.sub('^mkdir (.*)', 'mkdir -p \\\\1', command.script)", "fixed": " def match(command, settings):\n @sudo_support\n def get_new_command(command, settings):\n    return re.sub('\\\\bmkdir (.*)', 'mkdir -p \\\\1', command.script)"}
{"id": "pandas_110", "problem": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function).", "fixed": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        if kind == \"loc\":\n            return label\n        return super()._maybe_cast_slice_bound(label, side, kind)\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function)."}
{"id": "pandas_39", "problem": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n             self._update_inplace(", "fixed": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n            self._reset_cacher()\n             self._update_inplace("}
{"id": "matplotlib_3", "problem": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = True\n         self._marker_function()\n     def __bool__(self):", "fixed": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = self._fillstyle != 'none'\n         self._marker_function()\n     def __bool__(self):"}
{"id": "luigi_26", "problem": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar() or not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")", "fixed": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")"}
{"id": "keras_43", "problem": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1", "fixed": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n    if input_shape and input_shape[-1] == 1:\n        input_shape = tuple(input_shape[:-1])\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "pandas_85", "problem": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n         if len(level_index):\n             grouper = level_index.take(codes)", "fixed": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n        else:\n            level_index = level_index.copy()\n         if len(level_index):\n             grouper = level_index.take(codes)"}
{"id": "black_22", "problem": " def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):", "fixed": " def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):"}
{"id": "black_6", "problem": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n         return tiow.read(), encoding, newline\ndef get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return [\n            pygram.python_grammar_no_print_statement_no_exec_statement,\n            pygram.python_grammar_no_print_statement,\n            pygram.python_grammar,\n         ]\n     elif all(version.is_python2() for version in target_versions):\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n     else:\n        return [pygram.python_grammar_no_print_statement_no_exec_statement]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:", "fixed": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n         return tiow.read(), encoding, newline\n@dataclass(frozen=True)\nclass ParserConfig:\n    grammar: Grammar\n    tokenizer_config: TokenizerConfig = TokenizerConfig()\ndef get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:\n     if not target_versions:\n         return [\n            ParserConfig(\n                pygram.python_grammar_no_print_statement_no_exec_statement,\n                TokenizerConfig(async_is_reserved_keyword=True),\n            ),\n            ParserConfig(\n                pygram.python_grammar_no_print_statement_no_exec_statement,\n                TokenizerConfig(async_is_reserved_keyword=False),\n            ),\n            ParserConfig(pygram.python_grammar_no_print_statement),\n            ParserConfig(pygram.python_grammar),\n         ]\n     elif all(version.is_python2() for version in target_versions):\n        return [\n            ParserConfig(pygram.python_grammar_no_print_statement),\n            ParserConfig(pygram.python_grammar),\n        ]\n     else:\n        configs = []\n        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):\n            configs.append(\n                ParserConfig(\n                    pygram.python_grammar_no_print_statement_no_exec_statement,\n                    TokenizerConfig(async_is_reserved_keyword=True),\n                )\n            )\n        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):\n            configs.append(\n                ParserConfig(\n                    pygram.python_grammar_no_print_statement_no_exec_statement,\n                    TokenizerConfig(async_is_reserved_keyword=False),\n                )\n            )\n        return configs\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:"}
{"id": "pandas_45", "problem": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)", "fixed": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n    elif isinstance(data, abc.Set):\n        raise TypeError(\"Set type is unordered\")\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "keras_40", "problem": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            output_dim = self.cell.state_size[0]\n         else:\n            output_dim = self.cell.state_size\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            state_size = self.cell.state_size\n         else:\n            state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_5", "problem": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)", "fixed": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)\ndef test_join_multi_return_indexers():\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 16,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 16\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 8,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 8\n         },\n     },\n     {"}
{"id": "pandas_105", "problem": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested", "fixed": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n    @skip_nested\n    def test_transpose(self, data):\n        super().test_transpose(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested"}
{"id": "luigi_23", "problem": " class CentralPlannerScheduler(Scheduler):\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})", "fixed": " class CentralPlannerScheduler(Scheduler):\n        if self._config.prune_on_get_work:\n            self.prune()\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})"}
{"id": "pandas_131", "problem": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n            result = take_1d(result, self.orig.cat.codes)\n             index = self.orig.index\n         else:\n             index = self._parent.index", "fixed": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n             index = self.orig.index\n         else:\n             index = self._parent.index"}
{"id": "pandas_64", "problem": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df\n         self.columns = self.df.columns\n         self.float_format = float_format", "fixed": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df.reindex(columns=cols)\n         self.columns = self.df.columns\n         self.float_format = float_format"}
{"id": "luigi_6", "problem": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x))\n         except ValueError:\nreturn literal_eval(x)\n    def serialize(self, x):\n        return json.dumps(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n         except ValueError:\nreturn literal_eval(x)\n class NumericalParameter(Parameter):"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "httpie_1", "problem": " def filename_from_url(url, content_type):\n     return fn\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        if not exists(filename + suffix):\n            return filename + suffix\n         attempt += 1", "fixed": " def filename_from_url(url, content_type):\n     return fn\ndef trim_filename(filename, max_len):\n    if len(filename) > max_len:\n        trim_by = len(filename) - max_len\n        name, ext = os.path.splitext(filename)\n        if trim_by >= len(name):\n            filename = filename[:-trim_by]\n        else:\n            filename = name[:-trim_by] + ext\n    return filename\ndef get_filename_max_length(directory):\n    try:\n        max_len = os.pathconf(directory, 'PC_NAME_MAX')\n    except OSError as e:\n        if e.errno == errno.EINVAL:\n            max_len = 255\n        else:\n            raise\n    return max_len\ndef trim_filename_if_needed(filename, directory='.', extra=0):\n    max_len = get_filename_max_length(directory) - extra\n    if len(filename) > max_len:\n        filename = trim_filename(filename, max_len)\n    return filename\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        try_filename = trim_filename_if_needed(filename, extra=len(suffix))\n        try_filename += suffix\n        if not exists(try_filename):\n            return try_filename\n         attempt += 1"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n    def _ensure_listlike_indexer(self, key, axis=None):\n        column_axis = 1\n        if self.ndim != 2:\n            return\n        if isinstance(key, tuple):\n            key = key[column_axis]\n            axis = column_axis\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, ABCMultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            for k in key:\n                try:\n                    self.obj[k]\n                except KeyError:\n                    self.obj[k] = np.nan\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "keras_23", "problem": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape", "fixed": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape"}
{"id": "scrapy_16", "problem": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)\n def parse_url(url, encoding=None):", "fixed": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\ndef _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n    if not six.PY2:\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    if six.PY3:\n        return unquote_to_bytes(path)\n    else:\n        return unquote(path)\n def parse_url(url, encoding=None):"}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n     }\n     def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n         programcard = self._download_json(\n             'http://www.dr.dk/mu/programcard/expanded/%s' % video_id, video_id, 'Downloading video JSON')", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n     }\n     def _real_extract(self, url):\n        video_id = self._match_id(url)\n         programcard = self._download_json(\n             'http://www.dr.dk/mu/programcard/expanded/%s' % video_id, video_id, 'Downloading video JSON')"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "scrapy_21", "problem": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        self._parsers.pop(netloc).callback(None)", "fixed": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)"}
{"id": "luigi_27", "problem": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_value:\n            description.append(\" [default: %s]\" % (self.value,))\n         if self.is_list:\n             action = \"append\"", "fixed": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_task_value(param_name=param_name, task_name=task_name):\n            value = self.task_value(param_name=param_name, task_name=task_name)\n            description.append(\" [default: %s]\" % (value,))\n         if self.is_list:\n             action = \"append\""}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s was created successfully\" % obj_name)\n     def execute_info(self):", "fixed": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))\n     def execute_info(self):"}
{"id": "thefuck_13", "problem": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)", "fixed": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)"}
{"id": "keras_24", "problem": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:", "fixed": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:"}
{"id": "fastapi_14", "problem": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[bool, SchemaBase]] = None\n class Example(BaseModel):", "fixed": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[SchemaBase, bool]] = None\n class Example(BaseModel):"}
{"id": "youtube-dl_35", "problem": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }", "fixed": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n        upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(upload_date_str),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "tqdm_9", "problem": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable)\n     def __iter__(self):", "fixed": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable) if self.iterable else self.total\n     def __iter__(self):"}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "ansible_13", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)"}
{"id": "pandas_86", "problem": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None", "fixed": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n    if columns is None:\n        raise TypeError(\"pivot() missing 1 required argument: 'columns'\")\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None"}
{"id": "pandas_46", "problem": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n     def test_empty(self):\n         X = [[], [0, 1], []]", "fixed": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n     def test_empty(self):\n         X = [[], [0, 1], []]"}
{"id": "scrapy_18", "problem": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:", "fixed": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))", "fixed": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"name": "lis.py", "problem": "def lis(arr):\n    ends = {}\n    longest = 0\n    for i, val in enumerate(arr):\n        prefix_lengths = [j for j in range(1, longest + 1) if arr[ends[j]] < val]\n        length = max(prefix_lengths) if prefix_lengths else 0\n        if length == longest or val < arr[ends[length + 1]]:\n            ends[length + 1] = i\n            longest = length + 1\n    return longest", "fixed": "def lis(arr):\n    ends = {}\n    longest = 0\n    for i, val in enumerate(arr):\n        prefix_lengths = [j for j in range(1, longest + 1) if arr[ends[j]] < val]\n        length = max(prefix_lengths) if prefix_lengths else 0\n        if length == longest or val < arr[ends[length + 1]]:\n            ends[length + 1] = i\n            longest = max(longest, length + 1)\n    return longest\n", "hint": "Longest Increasing Subsequence\nlongest-increasing-subsequence\nInput:", "input": [[4, 2, 1]], "output": 1}
{"id": "ansible_5", "problem": " def test_check_mutually_exclusive_found(mutually_exclusive_terms):\n         'fox': 'red',\n         'socks': 'blue',\n     }\n    expected = \"TypeError('parameters are mutually exclusive: string1|string2, box|fox|socks',)\"\n     with pytest.raises(TypeError) as e:\n         check_mutually_exclusive(mutually_exclusive_terms, params)\n        assert e.value == expected\n def test_check_mutually_exclusive_none():", "fixed": " def test_check_mutually_exclusive_found(mutually_exclusive_terms):\n         'fox': 'red',\n         'socks': 'blue',\n     }\n    expected = \"parameters are mutually exclusive: string1|string2, box|fox|socks\"\n     with pytest.raises(TypeError) as e:\n         check_mutually_exclusive(mutually_exclusive_terms, params)\n    assert to_native(e.value) == expected\n def test_check_mutually_exclusive_none():"}
{"id": "luigi_14", "problem": " class Task(object):\n         return False\n    def can_disable(self):\n        return (self.disable_failures is not None or\n                self.disable_hard_timeout is not None)\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())", "fixed": " class Task(object):\n         return False\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "tornado_1", "problem": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__(", "fixed": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__("}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "pandas_125", "problem": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)\n    def replace(self, to_replace, value, inplace: bool = False):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                else:\n                    categories[index] = value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat"}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"id": "pandas_154", "problem": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(obj.values)\n             if not cython_dtype:\n                cython_dtype = obj.values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = obj.values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:", "fixed": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n            values = obj._data._values\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(values)\n             if not cython_dtype:\n                cython_dtype = values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:"}
{"id": "PySnooper_1", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):", "fixed": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index + 1), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):"}
{"id": "black_22", "problem": " class UnformattedLines(Line):\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class UnformattedLines(Line):\n @dataclass\n class EmptyLineTracker:"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +", "fixed": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n         elif (\n             is_datetime64_any_dtype(data)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n         elif (\n             is_datetime64_any_dtype(data)"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "pandas_155", "problem": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            return self.obj.index\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "pandas_44", "problem": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n     def get_loc(self, key, method=None, tolerance=None):", "fixed": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_timedelta64_dtype(dtype)\n     def get_loc(self, key, method=None, tolerance=None):"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values.T, other.values).T\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":", "fixed": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n            other_vals = other.values.reshape(-1, 1)\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values, other_vals)\n            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n             return out.sort_index() if self.sort else out", "fixed": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n            out = self._reindex_output(out)\n             return out.sort_index() if self.sort else out"}
{"id": "scrapy_8", "problem": " class Field(dict):\n class ItemMeta(ABCMeta):\n     def __new__(mcs, class_name, bases, attrs):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)", "fixed": " class Field(dict):\n class ItemMeta(ABCMeta):\n     def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)"}
{"name": "gcd.py", "problem": "def gcd(a, b):\n    if b == 0:\n        return a\n    else:\n        return gcd(a % b, b)", "fixed": "def gcd(a, b):\n    if b == 0:\n        return a\n    else:\n        return gcd(b, a % b)", "hint": "Input:\n    a: A nonnegative int\n    b: A nonnegative int", "input": [17, 0], "output": 17}
{"id": "pandas_9", "problem": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_scalar(key) and isna(key):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)", "fixed": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)"}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'][:-5])\n         thumbnail = None\n         duration = None", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n         title = data['Title']\n         description = data['Description']\n        timestamp = parse_iso8601(data['CreatedTime'])\n         thumbnail = None\n         duration = None"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         else:\n             raise ValueError(\"Not a valid normalize argument\")\n        table.index.names = table_index_names\n        table.columns.names = table_columns_names\n     else:\n         raise ValueError(\"Not a valid margins argument\")", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n            table.columns = table_columns\n         else:\n             raise ValueError(\"Not a valid normalize argument\")\n     else:\n         raise ValueError(\"Not a valid margins argument\")"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )", "fixed": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n         self._threads = []\n         self._stop_event = None", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 else:\n                     thread.join(timeout)\n        if self._manager:\n            self._manager.shutdown()\n         self._threads = []\n         self._stop_event = None"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         dest = self.parser_dest(param_name, task_name, glob=False)\n         if dest is not None:\n             value = getattr(args, dest, None)\n            params[param_name] = self.parse_from_input(param_name, value)\n     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):", "fixed": " class Parameter(object):\n         dest = self.parser_dest(param_name, task_name, glob=False)\n         if dest is not None:\n             value = getattr(args, dest, None)\n            params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)\n     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):"}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):", "fixed": " class StackedRNNCells(Layer):\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):"}
{"id": "black_2", "problem": " def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n     while container is not None and container.type != token.ENDMARKER:\n        is_fmt_on = False\n        for comment in list_comments(container.prefix, is_endmarker=False):\n            if comment.value in FMT_ON:\n                is_fmt_on = True\n            elif comment.value in FMT_OFF:\n                is_fmt_on = False\n        if is_fmt_on:\n             return\n        yield container\n        container = container.next_sibling\n def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:", "fixed": " def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n     while container is not None and container.type != token.ENDMARKER:\n        if fmt_on(container):\n             return\n        if contains_fmt_on_at_column(container, leaf.column):\n            for child in container.children:\n                if contains_fmt_on_at_column(child, leaf.column):\n                    return\n                yield child\n        else:\n            yield container\n            container = container.next_sibling\ndef fmt_on(container: LN) -> bool:\n    is_fmt_on = False\n    for comment in list_comments(container.prefix, is_endmarker=False):\n        if comment.value in FMT_ON:\n            is_fmt_on = True\n        elif comment.value in FMT_OFF:\n            is_fmt_on = False\n    return is_fmt_on\ndef contains_fmt_on_at_column(container: LN, column: int) -> bool:\n    for child in container.children:\n        if (\n            isinstance(child, Node)\n            and first_leaf_column(child) == column\n            or isinstance(child, Leaf)\n            and child.column == column\n        ):\n            if fmt_on(child):\n                return True\n    return False\ndef first_leaf_column(node: Node) -> Optional[int]:\n    for child in node.children:\n        if isinstance(child, Leaf):\n            return child.column\n    return None\n def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:"}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n                 bar_format = _unicode(bar_format)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         elif bar_format:", "fixed": " class tqdm(Comparable):\n             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):\n                 bar_format = _unicode(bar_format)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         elif bar_format:"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "black_22", "problem": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: Dict[LeafID, Leaf] = Factory(dict)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False", "fixed": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: List[Tuple[Index, Leaf]] = Factory(list)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False"}
{"id": "spacy_8", "problem": " class Errors(object):\n\"details: https://spacy.io/api/lemmatizer\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n @add_codes", "fixed": " class Errors(object):\n\"details: https://spacy.io/api/lemmatizer\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n    E175 = (\"Can't remove rule for unknown match pattern ID: {key}\")\n @add_codes"}
{"name": "depth_first_search.py", "problem": "def depth_first_search(startnode, goalnode):\n    nodesvisited = set()\n    def search_from(node):\n        if node in nodesvisited:\n            return False\n        elif node is goalnode:\n            return True\n        else:\n            return any(\n                search_from(nextnode) for nextnode in node.successors\n            )\n    return search_from(startnode)", "fixed": "def depth_first_search(startnode, goalnode):\n    nodesvisited = set()\n    def search_from(node):\n        if node in nodesvisited:\n            return False\n        elif node is goalnode:\n            return True\n        else:\n            nodesvisited.add(node)\n            return any(\n                search_from(nextnode) for nextnode in node.successors\n            )\n    return search_from(startnode)", "hint": "Depth-first Search\nInput:\n    startnode: A digraph node", "input": "", "output": ""}
{"id": "pandas_41", "problem": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")", "fixed": " class DatetimeLikeBlockMixin:\n     def _holder(self):\n         return DatetimeArray\n    def should_store(self, value):\n        return is_dtype_equal(self.dtype, value.dtype)\n     @property\n     def fill_value(self):\n         return np.datetime64(\"NaT\", \"ns\")"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n         if auto is not None:\n             self._autoscaleYon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n        for ax in self._shared_y_axes.get_siblings(self):\n            ax._stale_viewlim_y = False\n         if auto is not None:\n             self._autoscaleYon = bool(auto)"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n         Returns\n         -------\n        axes: topmost axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy)]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:", "fixed": " class FigureCanvasBase:\n         Returns\n         -------\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy) and a.get_visible()]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:"}
{"id": "pandas_43", "problem": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n     if not isinstance(right, ABCDataFrame):\n         return False", "fixed": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n    if op is operator.pow or op is rpow:\n        return False\n     if not isinstance(right, ABCDataFrame):\n         return False"}
{"id": "pandas_24", "problem": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:", "fixed": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:"}
{"id": "tornado_15", "problem": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root)\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)", "fixed": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root) + os.path.sep\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)"}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "pandas_146", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False\n     def identical(self, other):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        return array_equivalent(\n            com.values_from_object(self), com.values_from_object(other)\n        )\n     def identical(self, other):"}
{"id": "pandas_18", "problem": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            self.min_periods or 1,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,", "fixed": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(self.window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            window_indexer.window_size,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,"}
{"id": "matplotlib_13", "problem": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n         return cls(vertices, codes)\n     def __repr__(self):", "fixed": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n        last_vert = None\n        if codes.size > 0 and codes[-1] == cls.STOP:\n            last_vert = vertices[-1]\n        vertices = vertices[codes != cls.STOP, :]\n        codes = codes[codes != cls.STOP]\n        if last_vert is not None:\n            vertices = np.append(vertices, [last_vert], axis=0)\n            codes = np.append(codes, cls.STOP)\n         return cls(vertices, codes)\n     def __repr__(self):"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'", "fixed": " class Axes(_AxesBase):\n                     cbook.normalize_kwargs(\n                         boxprops, mpatches.PathPatch._alias_map))\n         else:\n            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,\n                                                        use_marker=False)\n         final_whiskerprops = line_props_with_rcdefaults(\n            'whiskerprops', whiskerprops, use_marker=False)\n         final_capprops = line_props_with_rcdefaults(\n            'capprops', capprops, use_marker=False)\n         final_flierprops = line_props_with_rcdefaults(\n             'flierprops', flierprops)\n         final_medianprops = line_props_with_rcdefaults(\n            'medianprops', medianprops, zdelta, use_marker=False)\n         final_meanprops = line_props_with_rcdefaults(\n             'meanprops', meanprops, zdelta)\n         removed_prop = 'marker' if meanline else 'linestyle'"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "youtube-dl_18", "problem": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()", "fixed": " class YoutubeDL(object):\n             force_properties = dict(\n                 (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):\n                 if f in force_properties:\n                     del force_properties[f]\n             new_result = info.copy()"}
{"id": "pandas_16", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            return self.freq\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            if is_period_dtype(result.dtype):\n                return self.freq\n            return None\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):"}
{"id": "ansible_9", "problem": " class Rhsm(RegistrationBase):\n         for pool_id, quantity in sorted(pool_ids.items()):\n             if pool_id in available_pool_ids:\n                args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]\n                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n             else:\n                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)", "fixed": " class Rhsm(RegistrationBase):\n         for pool_id, quantity in sorted(pool_ids.items()):\n             if pool_id in available_pool_ids:\n                args = [SUBMAN_CMD, 'attach', '--pool', pool_id]\n                if quantity is not None:\n                    args.extend(['--quantity', to_native(quantity)])\n                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n             else:\n                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)"}
{"id": "black_13", "problem": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token == 'def':\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            async_def = True\n                            async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],", "fixed": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token in ('def', 'for'):\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],"}
{"id": "pandas_134", "problem": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2030, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):", "fixed": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2200, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "ansible_9", "problem": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, 1\n        pool_ids[pool_id] = str(quantity)\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]", "fixed": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, None\n        pool_ids[pool_id] = quantity\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]"}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n         if fill_value is None:", "fixed": " class DataFrame(NDFrame):\n    def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n         if fill_value is None:"}
{"id": "youtube-dl_40", "problem": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''", "fixed": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return struct_unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return struct_unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return struct_unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''"}
{"id": "scrapy_28", "problem": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod", "fixed": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h)\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w)\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n        config.pop('dilation_rate')\n         config['output_padding'] = self.output_padding\n         return config", "fixed": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n         config['output_padding'] = self.output_padding\n         return config"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "tornado_3", "problem": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            if self._instance_cache.get(self.io_loop) is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n            del self._instance_cache[self.io_loop]\n     def fetch(\n         self,", "fixed": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            if cached_val is not None and cached_val is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n     def fetch(\n         self,"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n         else:\n             output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output", "fixed": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n         else:\n             output = None\n    else:\n        output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output"}
{"name": "topological_ordering.py", "problem": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.outgoing_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "fixed": "def topological_ordering(nodes):\n    ordered_nodes = [node for node in nodes if not node.incoming_nodes]\n    for node in ordered_nodes:\n        for nextnode in node.outgoing_nodes:\n            if set(ordered_nodes).issuperset(nextnode.incoming_nodes) and nextnode not in ordered_nodes:\n                ordered_nodes.append(nextnode)\n    return ordered_nodes", "hint": "Topological Sort\nInput:\n    nodes: A list of directed graph nodes", "input": [], "output": ""}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":", "fixed": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "black_6", "problem": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n             result = drv.parse_string(src_txt, True)\n             break", "fixed": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"}
{"id": "luigi_19", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable():\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.non_trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n            if not self.trainable:\n                return self.cell.weights\n             return self.cell.non_trainable_weights\n         return []"}
{"id": "pandas_151", "problem": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        values = self._ndarray\n        t = np.result_type(value, values)\n        if t != self._ndarray.dtype:\n            values = values.astype(t, casting=\"safe\")\n            values[key] = value\n            self._dtype = PandasDtype(t)\n            self._ndarray = values\n        else:\n            self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)", "fixed": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        value = np.asarray(value, dtype=self._ndarray.dtype)\n        self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)"}
{"id": "youtube-dl_39", "problem": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output", "fixed": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output\ndef limit_length(s, length):\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n     ):\n         self._write = get_write_function(output, overwrite)", "fixed": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n            custom_repr=(),\n     ):\n         self._write = get_write_function(output, overwrite)"}
{"id": "luigi_13", "problem": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.fs.mkdir(d)\n         os.rename(old_path, new_path)", "fixed": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.mkdir(d)\n         os.rename(old_path, new_path)"}
{"id": "pandas_33", "problem": " class IntegerArray(BaseMaskedArray):\n         ExtensionArray.argsort\n         data = self._data.copy()\n        data[self._mask] = data.min() - 1\n         return data\n     @classmethod", "fixed": " class IntegerArray(BaseMaskedArray):\n         ExtensionArray.argsort\n         data = self._data.copy()\n        if self._mask.any():\n            data[self._mask] = data.min() - 1\n         return data\n     @classmethod"}
{"id": "pandas_55", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            lower = self._take_with_is_copy(item, axis=self._info_axis_number)\n         return lower\n     def _box_item_values(self, key, values):", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item: int):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            return self._ixs(item, axis=1)\n         return lower\n     def _box_item_values(self, key, values):"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "ansible_15", "problem": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state') and not needs_update('vrf'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':", "fixed": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n             if not ax.is_unique:\n                 return False", "fixed": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                return False\n             if not ax.is_unique:\n                 return False"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "youtube-dl_28", "problem": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        return compat_chr(int(numstr, base))\n     return ('&%s;' % entity)", "fixed": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass\n     return ('&%s;' % entity)"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):", "fixed": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):"}
{"id": "pandas_41", "problem": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a a maybe different typed block.\n         Parameters\n         ----------", "fixed": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         Parameters\n         ----------"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")", "fixed": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n    is_bool = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")"}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "pandas_94", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        freq = self.freq if is_period_dtype(self) else None\n        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\n        return self._shallow_copy(result)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "pandas_142", "problem": " class TestTimeSeries(TestData):\n         )\n         tm.assert_index_equal(expected.index, result.index)\n    def test_diff(self):\n        self.ts.diff()\n        a = 10000000000000000\n        b = a + 1\n        s = Series([a, b])\n        rs = s.diff()\n        assert rs[1] == 1\n        rs = self.ts.diff(-1)\n        xp = self.ts - self.ts.shift(-1)\n        assert_series_equal(rs, xp)\n        rs = self.ts.diff(0)\n        xp = self.ts - self.ts\n        assert_series_equal(rs, xp)\n        s = Series(date_range(\"20130102\", periods=5))\n        rs = s - s.shift(1)\n        xp = s.diff()\n        assert_series_equal(rs, xp)\n        nrs = rs - rs.shift(1)\n        nxp = xp.diff()\n        assert_series_equal(nrs, nxp)\n        s = Series(\n            date_range(\"2000-01-01 09:00:00\", periods=5, tz=\"US/Eastern\"), name=\"foo\"\n        )\n        result = s.diff()\n        assert_series_equal(\n            result, Series(TimedeltaIndex([\"NaT\"] + [\"1 days\"] * 4), name=\"foo\")\n        )\n     def test_pct_change(self):\n         rs = self.ts.pct_change(fill_method=None)\n         assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)", "fixed": " class TestTimeSeries(TestData):\n         )\n         tm.assert_index_equal(expected.index, result.index)\n     def test_pct_change(self):\n         rs = self.ts.pct_change(fill_method=None)\n         assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n     stashed = None\n     async_def = False", "fixed": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n    async_is_reserved_keyword = config.async_is_reserved_keyword\n     stashed = None\n     async_def = False"}
{"id": "pandas_56", "problem": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._iget_item_cache(col)\n            return com.maybe_box_datetimelike(series._values[index])\n         series = self._get_item_cache(col)\n         engine = self.index._engine", "fixed": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n         series = self._get_item_cache(col)\n         engine = self.index._engine"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "keras_19", "problem": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "spacy_10", "problem": " class Errors(object):\n     E168 = (\"Unknown field: {field}\")\n     E169 = (\"Can't find module: {module}\")\n     E170 = (\"Cannot apply transition {name}: invalid for the current state.\")\n @add_codes", "fixed": " class Errors(object):\n     E168 = (\"Unknown field: {field}\")\n     E169 = (\"Can't find module: {module}\")\n     E170 = (\"Cannot apply transition {name}: invalid for the current state.\")\n    E171 = (\"Matcher.add received invalid on_match callback argument: expected \"\n            \"callable or None, but got: {arg_type}\")\n @add_codes"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with numpy.\n         Returns\n         -------", "fixed": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n         Returns\n         -------"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq"}
{"id": "scrapy_31", "problem": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n     getheaders = get_all", "fixed": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]\n     getheaders = get_all"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds", "fixed": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "pandas_92", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n             if where < start:\n                 if not is_series:", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n             if where < start:\n                 if not is_series:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "pandas_46", "problem": " def cartesian_product(X):\n         b = np.zeros_like(cumprodX)\n    return [\n        np.tile(\n            np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])\n        )\n        for i, x in enumerate(X)\n    ]", "fixed": " def cartesian_product(X):\n         b = np.zeros_like(cumprodX)\n    return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]\ndef _tile_compat(arr, num: int):\n    if isinstance(arr, np.ndarray):\n        return np.tile(arr, num)\n    taker = np.tile(np.arange(len(arr)), num)\n    return arr.take(taker)"}
{"id": "fastapi_2", "problem": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:", "fixed": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:"}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "pandas_113", "problem": " def any_int_dtype(request):\n     return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):", "fixed": " def any_int_dtype(request):\n     return request.param\n@pytest.fixture(params=ALL_EA_INT_DTYPES)\ndef any_nullable_int_dtype(request):\n    return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):"}
{"id": "pandas_83", "problem": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort\n         )\n     def _get_concat_axis(self) -> Index:", "fixed": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,\n         )\n     def _get_concat_axis(self) -> Index:"}
{"name": "subsequences.py", "problem": "def subsequences(a, b, k):\n    if k == 0:\n        return []\n    ret = []\n    for i in range(a, b + 1 - k):\n        ret.extend(\n            [i] + rest for rest in subsequences(i + 1, b, k - 1)\n        )\n    return ret", "fixed": "def subsequences(a, b, k):\n    if k == 0:\n        return [[]]\n    ret = []\n    for i in range(a, b + 1 - k):\n        ret.extend(\n            [i] + rest for rest in subsequences(i + 1, b, k - 1)\n        )\n    return ret", "hint": "Subsequences\nInput:\n    a: An int", "input": [[1, 2, 6, 72, 7, 33, 4]], "output": [1, 2, 4, 6, 7, 33, 72]}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip()\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)", "fixed": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip('\\n')\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or", "fixed": " def _match_one(filter_part, dct):\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n         actual_value = dct.get(m.group('key'))\n        if (m.group('quotedstrval') is not None or\n            m.group('strval') is not None or"}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of decades to use for each half of the linear range. For\n            example, when *linscale* == 1.0 (the default), the space used for\n            the positive and negative halves of the linear range will be equal\n            to one decade in the logarithmic range.\n         Normalize.__init__(self, vmin, vmax, clip)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - np.e ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()", "fixed": " class SymLogNorm(Normalize):\n         linscale : float, default: 1\n             This allows the linear range (-*linthresh* to *linthresh*) to be\n             stretched relative to the logarithmic range. Its value is the\n            number of powers of *base* (decades for base 10) to use for each\n            half of the linear range. For example, when *linscale* == 1.0\n            (the default), the space used for the positive and negative halves\n            of the linear range will be equal to a decade in the logarithmic\n            range if ``base=10``.\n        base : float, default: None\n            For v3.2 the default is the old value of ``np.e``, but that is\n            deprecated for v3.3 when base will default to 10.  During the\n            transition, specify the *base* kwarg to avoid a deprecation\n            warning.\n         Normalize.__init__(self, vmin, vmax, clip)\n        if base is None:\n            self._base = np.e\n            cbook.warn_deprecated(\"3.3\", message=\"default base will change \"\n                \"from np.e to 10.  To suppress this warning specify the base \"\n                \"kwarg.\")\n        else:\n            self._base = base\n        self._log_base = np.log(self._base)\n         self.linthresh = float(linthresh)\n        self._linscale_adj = (linscale / (1.0 - self._base ** -1))\n         if vmin is not None and vmax is not None:\n             self._transform_vmin_vmax()"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "ansible_1", "problem": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:", "fixed": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                            if not os.path.isfile(os.path.join(to_text(b_search_path, errors='surrogate_or_strict'), 'MANIFEST.json')):\n                                raise AnsibleError(\n                                    message=\"Collection %s does not appear to have a MANIFEST.json. \" % collection_name +\n                                            \"A MANIFEST.json is expected if the collection has been built and installed via ansible-galaxy.\"\n                                )\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "scrapy_16", "problem": " def parse_url(url, encoding=None):", "fixed": " def parse_url(url, encoding=None):\n        Data are returned as a list of name, value pairs as bytes.\n        Arguments:\n        qs: percent-encoded query string to be parsed\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.  A\n            true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception."}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string in endwith:\n                         break\n                    elif string == ')':\n                         tokens.restore_last_token()\n                         break\n                    if string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, [','])\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':", "fixed": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            tokens.restore_last_token()\n                         break\n                    elif inside_merge and string in ['/', ',']:\n                         tokens.restore_last_token()\n                         break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n         Raises\n         ------\n         TypeError", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n        .. versionchanged:: 1.0.0\n           Returns an NA value on empty arrays\n         Raises\n         ------\n         TypeError"}
{"id": "youtube-dl_42", "problem": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n        def fix_ampersand(s):\n            return s.replace(u'& ', '&amp; ')\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_ampersand)\n         return [self._get_video_info(item) for item in idoc.findall('.//item')]", "fixed": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_xml_ampersands)\n         return [self._get_video_info(item) for item in idoc.findall('.//item')]"}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "pandas_138", "problem": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n     if dtype is not None:", "fixed": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        x = x.astype(np.int64)\n     if dtype is not None:"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')\n            quote = m.group('quote')\n            if quote is not None:\n                comparison_value = comparison_value.replace(r'\\%s' % quote, quote)\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "keras_2", "problem": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)", "fixed": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\ndef in_top_k(predictions, targets, k):\n    top_k = np.argsort(-predictions)[:, :k]\n    targets = targets.reshape(-1, 1)\n    return np.any(targets == top_k, axis=-1)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)"}
{"id": "youtube-dl_42", "problem": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_all_ampersand) \n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):", "fixed": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_ampersands)\n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_15", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n     def get(\n         self,", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n     def get(\n         self,"}
{"id": "pandas_10", "problem": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, np.ndarray) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)", "fixed": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "fastapi_6", "problem": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if field.shape in sequence_shapes and isinstance(\n                    received_body, FormData\n                ):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)", "fixed": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)"}
{"id": "pandas_53", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not self.holds_integer():\n                     self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n                     self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n     elif is_integer_dtype(dtype):\n         dtype = np.float64", "fixed": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n        is_bool = True\n     elif is_integer_dtype(dtype):\n         dtype = np.float64"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "matplotlib_25", "problem": " class EventCollection(LineCollection):\n         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py\n         segment = (lineoffset + linelength / 2.,\n                    lineoffset - linelength / 2.)\n        if positions is None or len(positions) == 0:\n             segments = []\n        elif hasattr(positions, 'ndim') and positions.ndim > 1:\n             raise ValueError('positions cannot be an array with more than '\n                              'one dimension.')\n         elif (orientation is None or orientation.lower() == 'none' or", "fixed": " class EventCollection(LineCollection):\n         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py\n        if positions is None:\n            raise ValueError('positions must be an array-like object')\n        positions = np.array(positions, copy=True)\n         segment = (lineoffset + linelength / 2.,\n                    lineoffset - linelength / 2.)\n        if positions.size == 0:\n             segments = []\n        elif positions.ndim > 1:\n             raise ValueError('positions cannot be an array with more than '\n                              'one dimension.')\n         elif (orientation is None or orientation.lower() == 'none' or"}
{"id": "spacy_7", "problem": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()", "fixed": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value):\n         return isinstance(value, self._holder)\n    def set(self, locs, values, check=False):\n         assert locs.tolist() == [0]\n        self.values = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,", "fixed": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value: ArrayLike) -> bool:\n         return isinstance(value, self._holder)\n    def set(self, locs, values):\n         assert locs.tolist() == [0]\n        self.values[:] = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n        Yields :class:`Line` objects.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            try:\n                for comment in generate_comments(node):\n                    if any_open_brackets:\n                        self.current_line.append(comment)\n                    elif comment.type == token.COMMENT:\n                        self.current_line.append(comment)\n                        yield from self.line()\n                    else:\n                        yield from self.line()\n                        self.current_line.append(comment)\n                        yield from self.line()\n            except FormatOff as f_off:\n                f_off.trim_prefix(node)\n                yield from self.line(type=UnformattedLines)\n                yield from self.visit(node)\n            except FormatOn as f_on:\n                f_on.trim_prefix(node)\n                yield from self.visit_default(node)\n            else:\n                normalize_prefix(node, inside_brackets=any_open_brackets)\n                if self.normalize_strings and node.type == token.STRING:\n                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                    normalize_string_quotes(node)\n                if node.type not in WHITESPACE:\n                    self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    self.current_line.append(comment)\n                    yield from self.line()\n                else:\n                    yield from self.line()\n                    self.current_line.append(comment)\n                    yield from self.line()\n            normalize_prefix(node, inside_brackets=any_open_brackets)\n            if self.normalize_strings and node.type == token.STRING:\n                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                normalize_string_quotes(node)\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "black_11", "problem": " def split_line(\n         return\n     line_str = str(line).strip(\"\\n\")\n    if not line.should_explode and is_line_short_enough(\n        line, line_length=line_length, line_str=line_str\n     ):\n         yield line\n         return", "fixed": " def split_line(\n         return\n     line_str = str(line).strip(\"\\n\")\n    has_special_comment = False\n    for leaf in line.leaves:\n        for comment in line.comments_after(leaf):\n            if leaf.type == token.COMMA and is_special_comment(comment):\n                has_special_comment = True\n    if (\n        not has_special_comment\n        and not line.should_explode\n        and is_line_short_enough(line, line_length=line_length, line_str=line_str)\n     ):\n         yield line\n         return"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"name": "quicksort.py", "problem": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x > pivot])\n    return lesser + [pivot] + greater", "fixed": "def quicksort(arr):\n    if not arr:\n        return []\n    pivot = arr[0]\n    lesser = quicksort([x for x in arr[1:] if x < pivot])\n    greater = quicksort([x for x in arr[1:] if x >= pivot])\n    return lesser + [pivot] + greater\n", "hint": "QuickSort\nInput:\n    arr: A list of ints", "input": [17, 0], "output": 17}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        return Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n     def _apply_to_column_groupbys(self, func):", "fixed": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        result = Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n        return self._reindex_output(result, fill_value=0)\n     def _apply_to_column_groupbys(self, func):"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(self, output):\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding", "fixed": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(\n        self, output: FrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> FrameOrSeries:\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding"}
{"id": "luigi_17", "problem": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)", "fixed": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)"}
{"id": "luigi_14", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)\n     ):\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes", "fixed": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\n     ):\n        assert is_categorical(lk) and is_categorical(rk)\n        lk = cast(Categorical, lk)\n        rk = cast(Categorical, rk)\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue", "fixed": " class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue"}
{"id": "youtube-dl_31", "problem": " def parse_duration(s):\n     m = re.match(", "fixed": " def parse_duration(s):\n     m = re.match(\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?"}
{"id": "pandas_62", "problem": "                     missing_value = StataMissingValue(um)\n                     loc = missing_loc[umissing_loc == j]\n                     replacement.iloc[loc] = missing_value\nelse:\n                 dtype = series.dtype", "fixed": "                     missing_value = StataMissingValue(um)\n                     loc = missing_loc[umissing_loc == j]\n                    if loc.ndim == 2 and loc.shape[1] == 1:\n                        loc = loc[:, 0]\n                     replacement.iloc[loc] = missing_value\nelse:\n                 dtype = series.dtype"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)"}
{"id": "pandas_87", "problem": " def crosstab(\n         **kwargs,\n     )\n     if normalize is not False:\n         table = _normalize(", "fixed": " def crosstab(\n         **kwargs,\n     )\n    if not table.empty:\n        cols_diff = df.columns.difference(original_df_cols)[0]\n        table = table[cols_diff]\n     if normalize is not False:\n         table = _normalize("}
{"id": "thefuck_22", "problem": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        commands = self._remove_duplicates(self._commands)\n        self._cached = [self._cached[0]] + sorted(\n            commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)", "fixed": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        if self._cached:\n            commands = self._remove_duplicates(self._commands)\n            self._cached = [self._cached[0]] + sorted(\n                commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "black_15", "problem": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: str) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        list(v.visit(lib2to3_parse(code)))\n KEYWORDS = set(keyword.kwlist)", "fixed": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: Union[str, Leaf, Node]) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))\n KEYWORDS = set(keyword.kwlist)"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "keras_34", "problem": " class Sequence(object):\n _SHARED_SEQUENCES = {}", "fixed": " class Sequence(object):\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item\n _SHARED_SEQUENCES = {}"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "luigi_1", "problem": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics = self._scheduler._state._metrics_collector.generate_latest()\n         if metrics:\n            metrics.configure_http_handler(self)\n             self.write(metrics)", "fixed": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n         if metrics:\n            metrics_collector.configure_http_handler(self)\n             self.write(metrics)"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, PeriodIndex)\nnew_index = self.index.to_timestamp(freq=freq, how=how)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_timestamp\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, PeriodIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_timestamp(freq=freq, how=how)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_timestamp\""}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()", "fixed": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        epsilon: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced.", "fixed": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced."}
{"id": "keras_42", "problem": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,", "fixed": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,"}
{"id": "black_20", "problem": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src.name}  (original)\"\n        dst_name = f\"{src.name}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()", "fixed": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src}  (original)\"\n        dst_name = f\"{src}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\n         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\ndef read_pickle(path, compression=\"infer\"):\n     Load pickled pandas object (or any object) from file.", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass\ndef read_pickle(\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\n):\n     Load pickled pandas object (or any object) from file."}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"id": "pandas_57", "problem": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                 categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                 else:\n                    categories[index] = value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         cat = self if inplace else self.copy()\n        if is_list_like(to_replace):\n            replace_dict = {replace_value: value for replace_value in to_replace}\n        else:\n            replace_dict = {to_replace: value}\n        for replace_value, new_value in replace_dict.items():\n            if replace_value in cat.categories:\n                if isna(new_value):\n                    cat.remove_categories(replace_value, inplace=True)\n                    continue\n                 categories = cat.categories.tolist()\n                index = categories.index(replace_value)\n                if new_value in cat.categories:\n                    value_index = categories.index(new_value)\n                     cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(replace_value, inplace=True)\n                 else:\n                    categories[index] = new_value\n                     cat.rename_categories(categories, inplace=True)\n         if not inplace:\n             return cat"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "tornado_16", "problem": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n         if self._finished:\n             self._return_result(self._finished.popleft())", "fixed": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n        self._running_future.add_done_callback(lambda f: self)\n         if self._finished:\n             self._return_result(self._finished.popleft())"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*."}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "tqdm_7", "problem": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')", "fixed": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')"}
{"id": "keras_25", "problem": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1.", "fixed": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n    x = x.astype(K.floatx())\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1."}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "luigi_9", "problem": " def _depth_first_search(set_tasks, current_task, visited):\n         for task in current_task._requires():\n             if task not in visited:\n                 _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"failed\"] or task in set_tasks[\"upstream_failure\"]:\n                 set_tasks[\"upstream_failure\"].add(current_task)\n                 upstream_failure = True\n             if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:", "fixed": " def _depth_first_search(set_tasks, current_task, visited):\n         for task in current_task._requires():\n             if task not in visited:\n                 _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"ever_failed\"] or task in set_tasks[\"upstream_failure\"]:\n                 set_tasks[\"upstream_failure\"].add(current_task)\n                 upstream_failure = True\n             if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:"}
{"id": "keras_20", "problem": " def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):\n         padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n         output_padding: Integer, amount of padding along the output dimension,\n             Can be set to `None` in which case the output length is inferred.\n         The output length (integer).", "fixed": " def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):\n         padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n         output_padding: Integer, amount of padding along the output dimension,\n             Can be set to `None` in which case the output length is inferred.\n        dilation: dilation rate, integer.\n         The output length (integer)."}
{"id": "thefuck_23", "problem": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with shelve.open(cache_path) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:", "fixed": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with closing(shelve.open(cache_path)) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_54", "problem": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, RangeIndex):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)", "fixed": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, (RangeIndex, CategoricalIndex)):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)"}
{"id": "pandas_59", "problem": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other)\n         def _get_corr(a, b):\n             a = a.rolling(", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other) if not self.is_freq_type else self.win_freq\n         def _get_corr(a, b):\n             a = a.rolling("}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "tqdm_4", "problem": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale", "fixed": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            if total:\n                total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "matplotlib_30", "problem": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    x = x * (N - 1)\n    xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n    ind = np.searchsorted(x, xind)[1:-1]\n    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n    lut = np.concatenate([\n        [y1[0]],\n        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n        [y0[-1]],\n    ])\n     return np.clip(lut, 0.0, 1.0)", "fixed": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    if N == 1:\n        lut = np.array(y0[-1])\n    else:\n        x = x * (N - 1)\n        xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n        ind = np.searchsorted(x, xind)[1:-1]\n        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n        lut = np.concatenate([\n            [y1[0]],\n            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n            [y0[-1]],\n        ])\n     return np.clip(lut, 0.0, 1.0)"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "youtube-dl_1", "problem": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: v is not None,\n        '!': lambda v: v is None,\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)", "fixed": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters,\n                                                        filter_dilation=dilation_rate)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value))\nelse:\n                 self.reset_global()", "fixed": " class Parameter(object):\n         if dest is not None:\n             value = getattr(args, dest, None)\n             if value:\n                self.set_global(self.parse_from_input(param_name, value, task_name=task_name))\nelse:\n                 self.reset_global()"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n         else:\n             left, right = other, self", "fixed": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n        elif sort is False:\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n         else:\n             left, right = other, self"}
{"id": "scrapy_33", "problem": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         extra={'spider': spider, 'failure': failure})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)", "fixed": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n        attributes.pop(\"tz\", None)\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        if m.group('strval') is not None:\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or\n            actual_value is not None and m.group('intval') is not None and\n                isinstance(actual_value, compat_str)):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "youtube-dl_30", "problem": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f", "fixed": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                    formats = list(formats)\n                    if not formats:\n                        return\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f"}
{"id": "keras_19", "problem": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN.", "fixed": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state).\n            - a `output_size` attribute. This can be a single integer or a\n                TensorShape, which represent the shape of the output. For\n                backward compatible reason, if this attribute is not available\n                for the cell, the value will be inferred by the first element\n                of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN."}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, endwith=[]):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:", "fixed": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:"}
{"id": "cookiecutter_4", "problem": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return", "fixed": " def generate_files(repo_dir, context=None, output_dir='.',\n     with work_in(repo_dir):\n        try:\n            run_hook('pre_gen_project', project_dir, context)\n        except FailedHookException:\n            shutil.rmtree(project_dir, ignore_errors=True)\n             logging.error(\"Stopping generation because pre_gen_project\"\n                           \" hook script didn't exit sucessfully\")\n             return"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "cookiecutter_4", "problem": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return EXIT_SUCCESS\n    return run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return\n    run_script_with_context(script, project_dir, context)"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = compat_urllib_request.urlopen(request).read()\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'\"fb_dtsg\":\"(.*?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<input value=\"(.*?)\" name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = compat_urllib_request.urlopen(check_req).read()\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:", "fixed": " class FacebookIE(InfoExtractor):\n             'timezone': '-60',\n             'trynum': '1',\n             }\n        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))\n         request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n         try:\n            login_results = self._download_webpage(request, None,\n                note='Logging in', errnote='unable to fetch login page')\n             if re.search(r'<form(.*)name=\"login\"(.*)</form>', login_results) is not None:\n                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')\n                 return\n             check_form = {\n                'fb_dtsg': self._search_regex(r'name=\"fb_dtsg\" value=\"(.+?)\"', login_results, 'fb_dtsg'),\n                 'nh': self._search_regex(r'name=\"nh\" value=\"(\\w*?)\"', login_results, 'nh'),\n                 'name_action_selected': 'dont_save',\n                'submit[Continue]': self._search_regex(r'<button[^>]+value=\"(.*?)\"[^>]+name=\"submit\\[Continue\\]\"', login_results, 'continue'),\n             }\n            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))\n             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')\n            check_response = self._download_webpage(check_req, None,\n                note='Confirming login')\n             if re.search(r'id=\"checkpointSubmitButton\"', check_response) is not None:\n                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')\n         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:"}
{"id": "keras_1", "problem": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.random_normal(shape, self.mean, self.stddev,\n                            dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "pandas_78", "problem": " Wild         185.0\n                     result = coerce_to_dtypes(result, self.dtypes)\n         if constructor is not None:\n            result = Series(result, index=labels)\n         return result\n     def nunique(self, axis=0, dropna=True) -> Series:", "fixed": " Wild         185.0\n                     result = coerce_to_dtypes(result, self.dtypes)\n         if constructor is not None:\n            result = self._constructor_sliced(result, index=labels)\n         return result\n     def nunique(self, axis=0, dropna=True) -> Series:"}
{"id": "ansible_10", "problem": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type", "fixed": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n        self.prev = None\n        self.next = None\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return result\n     @classmethod\n     def _add_numeric_operations(cls):", "fixed": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return self._reindex_output(result, fill_value=0)\n     @classmethod\n     def _add_numeric_operations(cls):"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "black_6", "problem": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: set(),\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,", "fixed": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n    ASYNC_IS_VALID_IDENTIFIER = 6\n    ASYNC_IS_RESERVED_KEYWORD = 7\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY35: {\n        Feature.UNICODE_LITERALS,\n        Feature.TRAILING_COMMA_IN_CALL,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n    },\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         assert_allclose(y1, y2, atol=1e-05)\n     def test_random_normal(self):\n         for mean, std in [(0., 1.), (-10., 5.)]:\n            rand = K.eval(K.random_normal((300, 200),\n                                          mean=mean, stddev=std, seed=1337))\n            assert rand.shape == (300, 200)\n             assert np.abs(np.mean(rand) - mean) < std * 0.015\n             assert np.abs(np.std(rand) - std) < std * 0.015\n            r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)\n            samples = np.array([K.eval(r) for _ in range(200)])\n            assert np.abs(np.mean(samples) - mean) < std * 0.015\n            assert np.abs(np.std(samples) - std) < std * 0.015\n     def test_random_uniform(self):\n         min_val = -1.\n         max_val = 1.\n        rand = K.eval(K.random_uniform((200, 100), min_val, max_val))\n        assert rand.shape == (200, 100)\n         assert np.abs(np.mean(rand)) < 0.015\n         assert max_val - 0.015 < np.max(rand) <= max_val\n         assert min_val + 0.015 > np.min(rand) >= min_val\n        r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples)) < 0.015\n        assert max_val - 0.015 < np.max(samples) <= max_val\n        assert min_val + 0.015 > np.min(samples) >= min_val\n     def test_random_binomial(self):\n         p = 0.5\n        rand = K.eval(K.random_binomial((200, 100), p))\n        assert rand.shape == (200, 100)\n         assert np.abs(np.mean(rand) - p) < 0.015\n         assert np.max(rand) == 1\n         assert np.min(rand) == 0\n        r = K.random_binomial((10, 10), p)\n        samples = np.array([K.eval(r) for _ in range(200)])\n        assert np.abs(np.mean(samples) - p) < 0.015\n        assert np.max(samples) == 1\n        assert np.min(samples) == 0\n     def test_truncated_normal(self):\n         mean = 0.\n         std = 1.\n         min_val = -2.\n         max_val = 2.\n        rand = K.eval(K.truncated_normal((300, 200),\n                                         mean=mean, stddev=std, seed=1337))\n        assert rand.shape == (300, 200)\n         assert np.abs(np.mean(rand) - mean) < 0.015\n         assert np.max(rand) <= max_val\n         assert np.min(rand) >= min_val", "fixed": " class TestBackend(object):\n         assert_allclose(y1, y2, atol=1e-05)\n     def test_random_normal(self):\n         for mean, std in [(0., 1.), (-10., 5.)]:\n            rand = K.eval(K.random_normal((200, 200),\n                                          mean=mean,\n                                          stddev=std))\n            assert rand.shape == (200, 200)\n             assert np.abs(np.mean(rand) - mean) < std * 0.015\n             assert np.abs(np.std(rand) - std) < std * 0.015\n     def test_random_uniform(self):\n         min_val = -1.\n         max_val = 1.\n        rand = K.eval(K.random_uniform((200, 200), min_val, max_val))\n        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand)) < 0.015\n         assert max_val - 0.015 < np.max(rand) <= max_val\n         assert min_val + 0.015 > np.min(rand) >= min_val\n     def test_random_binomial(self):\n         p = 0.5\n        rand = K.eval(K.random_binomial((200, 200), p))\n        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand) - p) < 0.015\n         assert np.max(rand) == 1\n         assert np.min(rand) == 0\n     def test_truncated_normal(self):\n         mean = 0.\n         std = 1.\n         min_val = -2.\n         max_val = 2.\n        rand = K.eval(K.truncated_normal((200, 200),\n                                         mean=mean,\n                                         stddev=std))\n        assert rand.shape == (200, 200)\n         assert np.abs(np.mean(rand) - mean) < 0.015\n         assert np.max(rand) <= max_val\n         assert np.min(rand) >= min_val"}
{"id": "pandas_125", "problem": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result", "fixed": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        filter=None,\n        regex: bool = False,\n        convert: bool = True,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = self if inplace else self.copy()\n        if filter is None:\n            result.values.replace(to_replace, value, inplace=True)\n            if convert:\n                return result.convert(numeric=False, copy=not inplace)\n            else:\n                return result\n        else:\n            if not isna(value):\n                result.values.add_categories(value, inplace=True)\n            return super(CategoricalBlock, result).replace(\n                to_replace, value, inplace, filter, regex, convert\n            )"}
{"id": "pandas_81", "problem": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):", "fixed": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype\n        dtype = pandas_dtype(dtype)\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n        elif isinstance(dtype, BooleanDtype):\n            result = self._data.astype(\"bool\", copy=False)\n            return BooleanArray(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):"}
{"id": "keras_20", "problem": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n         A tensor.", "fixed": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n         A tensor."}
{"name": "hanoi.py", "problem": "def hanoi(height, start=1, end=3):\n    steps = []\n    if height > 0:\n        helper = ({1, 2, 3} - {start} - {end}).pop()\n        steps.extend(hanoi(height - 1, start, helper))\n        steps.append((start, helper))\n        steps.extend(hanoi(height - 1, helper, end))\n    return steps", "fixed": "def hanoi(height, start=1, end=3):\n    steps = []\n    if height > 0:\n        helper = ({1, 2, 3} - {start} - {end}).pop()\n        steps.extend(hanoi(height - 1, start, helper))\n        steps.append((start, end))\n        steps.extend(hanoi(height - 1, helper, end))\n    return steps", "hint": "Towers of Hanoi\nhanoi\nAn algorithm for solving the Towers of Hanoi puzzle.  Three pegs exist, with a stack of differently-sized", "input": [1, 1, 3], "output": [[1, 3]]}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n                self.wait += 1\n     def in_cooldown(self):\n         return self.cooldown_counter > 0", "fixed": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n     def in_cooldown(self):\n         return self.cooldown_counter > 0"}
{"id": "pandas_43", "problem": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)", "fixed": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)"}
{"id": "thefuck_8", "problem": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)", "fixed": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)"}
{"id": "pandas_114", "problem": " class Index(IndexOpsMixin, PandasObject):\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n        s = extract_array(series, extract_numpy=True)\n        if isinstance(s, ExtensionArray):\n            if is_scalar(key):\n                try:\n                    iloc = self.get_loc(key)\n                    return s[iloc]\n                except KeyError:\n                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                        raise\n                    elif is_integer(key):\n                        return s[key]\n            else:\n                raise InvalidIndexError(key)\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(description_original, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):"}
{"id": "pandas_104", "problem": " class GroupBy(_GroupBy):\n            order = np.roll(list(range(result.index.nlevels)), -1)\n            result = result.reorder_levels(order)\n            result = result.reindex(q, level=-1)\n            hi = len(q) * self.ngroups\n            arr = np.arange(0, hi, self.ngroups)\n            arrays = []\n            for i in range(self.ngroups):\n                arr2 = arr + i\n                arrays.append(arr2)\n            indices = np.concatenate(arrays)\n            assert len(indices) == len(result)\n             return result.take(indices)\n     @Substitution(name=\"groupby\")", "fixed": " class GroupBy(_GroupBy):\n            order = list(range(1, result.index.nlevels)) + [0]\n            index_names = np.array(result.index.names)\n            result.index.names = np.arange(len(index_names))\n            result = result.reorder_levels(order)\n            result.index.names = index_names[order]\n            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()\n             return result.take(indices)\n     @Substitution(name=\"groupby\")"}
{"name": "knapsack.py", "problem": "def knapsack(capacity, items):\n    from collections import defaultdict\n    memo = defaultdict(int)\n    for i in range(1, len(items) + 1):\n        weight, value = items[i - 1]\n        for j in range(1, capacity + 1):\n            memo[i, j] = memo[i - 1, j]\n            if weight < j:\n                memo[i, j] = max(\n                    memo[i, j],\n                    value + memo[i - 1, j - weight]\n                )\n    return memo[len(items), capacity]", "fixed": "def knapsack(capacity, items):\n    from collections import defaultdict\n    memo = defaultdict(int)\n    for i in range(1, len(items) + 1):\n        weight, value = items[i - 1]\n        for j in range(1, capacity + 1):\n            memo[i, j] = memo[i - 1, j]\n            if weight <= j:\n                memo[i, j] = max(\n                    memo[i, j],\n                    value + memo[i - 1, j - weight]\n                )\n    return memo[len(items), capacity]", "hint": "Knapsack\nknapsack\nYou have a knapsack that can hold a maximum weight. You are given a selection of items, each with a weight and a value. You may", "input": [100, [[60, 10], [50, 8], [20, 4], [20, 4], [8, 3], [3, 2]]], "output": 19}
{"id": "keras_42", "problem": " class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n                      steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,", "fixed": " class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,"}
{"id": "thefuck_17", "problem": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)", "fixed": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)"}
{"id": "fastapi_1", "problem": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))", "fixed": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:", "fixed": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                self.wait += 1\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,", "fixed": " class APIRoute(routing.Route):\n         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n         response_model_by_alias: bool = True,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Optional[Type[Response]] = None,\n         dependency_overrides_provider: Any = None,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if is_sequence:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'", "fixed": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if use_sequence_api:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):"}
{"id": "pandas_31", "problem": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals):\n                 inference = np.int64\n            elif is_datetime64_dtype(vals):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)", "fixed": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                 inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n                             extra={'spider': spider, 'failure': download_failure})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:", "fixed": " class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n                             exc_info=failure_to_exc_info(download_failure),\n                             extra={'spider': spider})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:"}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "youtube-dl_29", "problem": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "youtube-dl_4", "problem": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]+)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')])\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)", "fixed": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)"}
{"id": "pandas_90", "problem": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    if path is None:\n        path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(path) as path:\n        pd.to_pickle(obj, path)\n        return pd.read_pickle(path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):", "fixed": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as path:\n        pd.to_pickle(obj, _path)\n        return pd.read_pickle(_path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):"}
{"id": "youtube-dl_32", "problem": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)\n def js_to_json(code):", "fixed": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(\n        r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?://[^\\n]*)*$', r'\\1', code)\n def js_to_json(code):"}
{"id": "youtube-dl_32", "problem": " class NPOIE(InfoExtractor):\n             'http://e.omroep.nl/metadata/aflevering/%s' % video_id,\n             video_id,\n            transform_source=lambda j: re.sub(r'parseMetadata\\((.*?)\\);\\n//.*$', r'\\1', j)\n         )\n         token_page = self._download_webpage(\n             'http://ida.omroep.nl/npoplayer/i.js',", "fixed": " class NPOIE(InfoExtractor):\n             'http://e.omroep.nl/metadata/aflevering/%s' % video_id,\n             video_id,\n            transform_source=strip_jsonp,\n         )\n         token_page = self._download_webpage(\n             'http://ida.omroep.nl/npoplayer/i.js',"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_17", "problem": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]", "fixed": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n    if param is None:\n        return []\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]"}
{"name": "minimum_spanning_tree.py", "problem": "def minimum_spanning_tree(weight_by_edge):\n    group_by_node = {}\n    mst_edges = set()\n    for edge in sorted(weight_by_edge, key=weight_by_edge.__getitem__):\n        u, v = edge\n        if group_by_node.setdefault(u, {u}) != group_by_node.setdefault(v, {v}):\n            mst_edges.add(edge)\n            group_by_node[u].update(group_by_node[v])\n            for node in group_by_node[v]:\n                group_by_node[node].update(group_by_node[u])\n    return mst_edges", "fixed": "def minimum_spanning_tree(weight_by_edge):\n    group_by_node = {}\n    mst_edges = set()\n    for edge in sorted(weight_by_edge, key=weight_by_edge.__getitem__):\n        u, v = edge\n        if group_by_node.setdefault(u, {u}) != group_by_node.setdefault(v, {v}):\n            mst_edges.add(edge)\n            group_by_node[u].update(group_by_node[v])\n            for node in group_by_node[v]:\n                group_by_node[node] = group_by_node[u]\n    return mst_edges", "hint": "Minimum Spanning Tree\nKruskal's algorithm implementation.\nInput:", "input": "", "output": ""}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "thefuck_31", "problem": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return '{} --staged'.format(command.script)", "fixed": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return command.script.replace(' diff', ' diff --staged')"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n    def _extract_chapters(self, webpage, description, video_id, duration):\n        return (self._extract_chapters_from_json(webpage, video_id, duration)\n                or self._extract_chapters_from_description(description, duration))\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})"}
{"id": "pandas_119", "problem": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n         result = result.append(margin_dummy)\n     except TypeError:", "fixed": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )\n         result = result.append(margin_dummy)\n     except TypeError:"}
{"id": "keras_30", "problem": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if x is None or len(x) == 0:\n                    batch_size = 1\n                elif isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n        if not self.trainable:\n            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []"}
{"id": "ansible_3", "problem": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path == '/etc/lsb-release' and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:", "fixed": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:"}
{"name": "sqrt.py", "problem": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx", "fixed": "def sqrt(x, epsilon):\n    approx = x / 2\n    while abs(x - approx ** 2) > epsilon:\n        approx = 0.5 * (approx + x / approx)\n    return approx\n", "hint": "Square Root\nNewton-Raphson method implementation.\nInput:", "input": [2, 0.01], "output": 1.4166666666666665}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"name": "next_permutation.py", "problem": "def next_permutation(perm):\n    for i in range(len(perm) - 2, -1, -1):\n        if perm[i] < perm[i + 1]:\n            for j in range(len(perm) - 1, i, -1):\n                if perm[j] < perm[i]:\n                    next_perm = list(perm)\n                    next_perm[i], next_perm[j] = perm[j], perm[i]\n                    next_perm[i + 1:] = reversed(next_perm[i + 1:])\n                    return next_perm", "fixed": "def next_permutation(perm):\n    for i in range(len(perm) - 2, -1, -1):\n        if perm[i] < perm[i + 1]:\n            for j in range(len(perm) - 1, i, -1):\n                if perm[i] < perm[j]:\n                    next_perm = list(perm)\n                    next_perm[i], next_perm[j] = perm[j], perm[i]\n                    next_perm[i + 1:] = reversed(next_perm[i + 1:])\n                    return next_perm\n", "hint": "Next Permutation\nnext-perm\nInput:", "input": [[3, 2, 4, 1]], "output": [3, 4, 1, 2]}
{"id": "black_22", "problem": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36=py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")", "fixed": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")"}
{"id": "luigi_21", "problem": " def run(cmdline_args=None, main_task_cls=None,\n     :param use_dynamic_argparse:\n     :param local_scheduler:\n     if use_dynamic_argparse:\n         interface = DynamicArgParseInterface()\n     else:", "fixed": " def run(cmdline_args=None, main_task_cls=None,\n     :param use_dynamic_argparse:\n     :param local_scheduler:\n    if cmdline_args is None:\n        cmdline_args = sys.argv[1:]\n     if use_dynamic_argparse:\n         interface = DynamicArgParseInterface()\n     else:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "black_23", "problem": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        raise AssertionError(f\"cannot parse source: {exc}\") from None\n     try:\n         dst_ast = ast.parse(dst)", "fixed": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        major, minor = sys.version_info[:2]\n        raise AssertionError(\n            f\"cannot use --safe with this file; failed to parse source file \"\n            f\"with Python {major}.{minor}'s builtin AST. Re-run with --fast \"\n            f\"or stop using deprecated Python 2 syntax. AST error message: {exc}\"\n        )\n     try:\n         dst_ast = ast.parse(dst)"}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()\n    def visit_unformatted(self, node: LN) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "tornado_5", "problem": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n         if self._next_timeout <= current_time:\n            callback_time_sec = self.callback_time / 1000.0\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec", "fixed": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n        callback_time_sec = self.callback_time / 1000.0\n         if self._next_timeout <= current_time:\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec\n        else:\n            self._next_timeout += callback_time_sec"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size", "fixed": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "luigi_16", "problem": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            if task.id not in necessary_tasks and self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)", "fixed": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            removed = self._state.prune(task, self._config)\n            if removed and task.id not in necessary_tasks:\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)\n         login_page_req.add_header('Cookie', 'locale=en_US')\n        self.report_login()\n        login_page = self._download_webpage(login_page_req, None, note=False,\n             errnote='Unable to download login page')\n         lsd = self._search_regex(\n             r'<input type=\"hidden\" name=\"lsd\" value=\"([^\"]*)\"',", "fixed": " class FacebookIE(InfoExtractor):\n         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)\n         login_page_req.add_header('Cookie', 'locale=en_US')\n        login_page = self._download_webpage(login_page_req, None,\n            note='Downloading login page',\n             errnote='Unable to download login page')\n         lsd = self._search_regex(\n             r'<input type=\"hidden\" name=\"lsd\" value=\"([^\"]*)\"',"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def predict_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or include_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    include_none=include_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )", "fixed": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or not exclude_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )"}
{"name": "rpn_eval.py", "problem": "def rpn_eval(tokens):\n    def op(symbol, a, b):\n        return {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b\n        }[symbol](a, b)\n    stack = []\n    for token in tokens:\n        if isinstance(token, float):\n            stack.append(token)\n        else:\n            a = stack.pop()\n            b = stack.pop()\n            stack.append(\n                op(token, a, b)\n            )\n    return stack.pop()", "fixed": "def rpn_eval(tokens):\n    def op(symbol, a, b):\n        return {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b\n        }[symbol](a, b)\n    stack = []\n    for token in tokens:\n        if isinstance(token, float):\n            stack.append(token)\n        else:\n            a = stack.pop()\n            b = stack.pop()\n            stack.append(\n                op(token, b, a)\n            )\n    return stack.pop()\n", "hint": "Reverse Polish Notation\nFour-function calculator with input given in Reverse Polish Notation (RPN).\nInput:", "input": [[3.0, 5.0, "+", 2.0, "/"]], "output": 4.0}
{"id": "youtube-dl_5", "problem": " def unified_timestamp(date_str, day_first=True):\n     for expression in date_formats(day_first):\n         try:\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta\n             return calendar.timegm(dt.timetuple())\n         except ValueError:\n             pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n        return calendar.timegm(timetuple.timetuple())\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_timestamp(date_str, day_first=True):\n     for expression in date_formats(day_first):\n         try:\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n             return calendar.timegm(dt.timetuple())\n         except ValueError:\n             pass\n     timetuple = email.utils.parsedate_tz(date_str)\n     if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n     return index", "fixed": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n    if copy:\n        index = index.copy()\n     return index"}
{"id": "pandas_65", "problem": " class CParserWrapper(ParserBase):\n            if isinstance(src, BufferedIOBase):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\"", "fixed": " class CParserWrapper(ParserBase):\n            if isinstance(src, (BufferedIOBase, RawIOBase)):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\""}
{"id": "PySnooper_3", "problem": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output_path, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)", "fixed": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)"}
{"id": "scrapy_20", "problem": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)", "fixed": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "luigi_5", "problem": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        @task._task_wraps(task_that_inherits)\n        class Wrapped(task_that_inherits):\n            def clone_parent(_self, **args):\n                return _self.clone(cls=self.task_to_inherit, **args)\n        return Wrapped\n class requires(object):", "fixed": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        def clone_parent(_self, **args):\n            return _self.clone(cls=self.task_to_inherit, **args)\n        task_that_inherits.clone_parent = clone_parent\n        return task_that_inherits\n class requires(object):"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"id": "luigi_11", "problem": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items())):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)", "fixed": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)"}
{"id": "scrapy_22", "problem": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        else:\n             self._xg_characters(serialized_value)\n         self.xg.endElement(name)", "fixed": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        elif isinstance(serialized_value, six.text_type):\n             self._xg_characters(serialized_value)\n        else:\n            self._xg_characters(str(serialized_value))\n         self.xg.endElement(name)"}
{"id": "cookiecutter_4", "problem": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.", "fixed": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.\n    Raised when a hook script fails"}
{"id": "scrapy_23", "problem": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)", "fixed": " class RetryTest(unittest.TestCase):\n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         r = self.mw.process_response(req, rsp, self.spider)"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "black_22", "problem": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.leaves.pop()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.leaves.pop()\n                 return True", "fixed": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.remove_trailing_comma()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.remove_trailing_comma()\n                 return True"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "luigi_3", "problem": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except ValueError:\n            return literal_eval(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))\n class NumericalParameter(Parameter):"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "pandas_136", "problem": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_int64_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_integer_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n }", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n }"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n            custom_generator(), good_batches + 1, 1,\n            workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n    workers = 4\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n            custom_generator(), good_batches * workers + 1, 1,\n            workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "keras_36", "problem": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1, 1) + strides + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1, 1) + strides\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)", "fixed": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "keras_1", "problem": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        increment = np.random.randn(3, 4)\n        x += increment\n        K.eval(K.update_add(x_var, increment))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.random.randn(3, 4)\n         x_var = K.variable(x)\n        decrement = np.random.randn(3, 4)\n        x -= decrement\n        K.eval(K.update_sub(x_var, decrement))\n        assert_allclose(x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')", "fixed": " class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n    @pytest.mark.skipif(K.backend() == 'theano',\n                        reason='theano returns tuples for update ops')\n    def test_update(self):\n        x = np.ones((3, 4))\n        x_var = K.variable(x)\n        new_x = np.random.random((3, 4))\n        op = K.update(x_var, new_x)\n        K.eval(op)\n        assert_allclose(new_x, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_add(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        increment = np.random.random((3, 4))\n        op = K.update_add(x_var, increment)\n        K.eval(op)\n        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'theano',\n                         reason='theano returns tuples for update ops')\n     def test_update_sub(self):\n        x = np.ones((3, 4))\n         x_var = K.variable(x)\n        decrement = np.random.random((3, 4))\n        op = K.update_sub(x_var, decrement)\n        K.eval(op)\n        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')"}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "pandas_87", "problem": " def crosstab(\n         kwargs = {\"aggfunc\": aggfunc}\n     table = df.pivot_table(\n        \"__dummy__\",\n         index=rownames,\n         columns=colnames,\n         margins=margins,", "fixed": " def crosstab(\n         kwargs = {\"aggfunc\": aggfunc}\n     table = df.pivot_table(\n        [\"__dummy__\"],\n         index=rownames,\n         columns=colnames,\n         margins=margins,"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if is_sequence:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'", "fixed": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if use_sequence_api:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "httpie_2", "problem": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)", "fixed": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n    requests_session.max_redirects = args.max_redirects\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n     def __init__(self,", "fixed": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n        restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n     def __init__(self,"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "matplotlib_9", "problem": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin", "fixed": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n        self._unstale_viewLim()\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading"}
{"id": "tornado_2", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and \"Transfer-Encoding\" not in headers\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and (\n                    \"Transfer-Encoding\" not in headers\n                    or headers[\"Transfer-Encoding\"] == \"chunked\"\n                )\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "thefuck_8", "problem": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read()\n     return _parse_operations(lines)", "fixed": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read().decode(\"utf-8\")\n     return _parse_operations(lines)"}
{"id": "luigi_24", "problem": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '\"{0}={1}\"'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):", "fixed": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):"}
{"id": "pandas_105", "problem": " def box_df_fail(request):\n     return request.param\n@pytest.fixture(\n    params=[\n        (pd.Index, False),\n        (pd.Series, False),\n        (pd.DataFrame, False),\n        pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),\n        (tm.to_array, False),\n    ],\n    ids=id_func,\n)\ndef box_transpose_fail(request):\n    return request.param\n @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)\n def box_with_array(request):", "fixed": " def box_df_fail(request):\n     return request.param\n @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)\n def box_with_array(request):"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n        body_param_media_types = [\n            getattr(f.schema, \"media_type\")\n            for f in flat_dependant.body_params\n            if isinstance(f.schema, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodySchema_kwargs[\"media_type\"] = body_param_media_types[0]\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,"}
{"id": "pandas_165", "problem": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):", "fixed": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n    def test_parr_add_sub_index(self):\n        pi = pd.period_range(\"2000-12-31\", periods=3)\n        parr = pi.array\n        result = parr - pi\n        expected = pi - pi\n        tm.assert_index_equal(result, expected)\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        column_margin = table.loc[:, margins_name].drop(margins_name)\n        index_margin = table.loc[margins_name, :].drop(margins_name)\n        table = table.drop(margins_name, axis=1).drop(margins_name)\n        table_index_names = table.index.names\n        table_columns_names = table.columns.names\n         table = _normalize(table, normalize=normalize, margins=False)", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        table_index = table.index\n        table_columns = table.columns\n        if (margins_name not in table.iloc[-1, :].name) | (\n            margins_name != table.iloc[:, -1].name\n        ):\n            raise ValueError(\"{} not in pivoted DataFrame\".format(margins_name))\n        column_margin = table.iloc[:-1, -1]\n        index_margin = table.iloc[-1, :-1]\n        table = table.iloc[:-1, :-1]\n         table = _normalize(table, normalize=normalize, margins=False)"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "luigi_4", "problem": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)", "fixed": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if self.columns and len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"x\", \"ymin\", \"ymax\", \"colors\"],\n                       label_namer=\"x\")\n    def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot vertical lines."}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route(", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                    route_class_override=type(route),\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route("}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"name": "bitcount.py", "problem": "def bitcount(n):\n    count = 0\n    while n:\n        n ^= n - 1\n        count += 1\n    return count", "fixed": "def bitcount(n):\n    count = 0\n    while n:\n        n &= n - 1\n        count += 1\n    return count", "hint": "Bitcount\nbitcount\nInput:", "input": [127], "output": 7}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n             result = res - lag\n             result[mask] = na\n             out_arr[res_indexer] = result\n         else:\n             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]", "fixed": " def diff(arr, n: int, axis: int = 0):\n             result = res - lag\n             result[mask] = na\n             out_arr[res_indexer] = result\n        elif is_bool:\n            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]\n         else:\n             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "pandas_6", "problem": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):", "fixed": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError, ValueError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "pandas_75", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         if isinstance(key, str):\n             try:\n                return self._get_string_slice(key)\n            except (TypeError, KeyError, ValueError, OverflowError):\n                 pass\n             try:\n                 asdt, reso = parse_time_string(key, self.freq)\n                key = asdt\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{key}' as period\")\n         elif is_integer(key):\n             raise KeyError(key)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         if isinstance(key, str):\n             try:\n                loc = self._get_string_slice(key)\n                return loc\n            except (TypeError, ValueError):\n                 pass\n             try:\n                 asdt, reso = parse_time_string(key, self.freq)\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{key}' as period\")\n            grp = resolution.Resolution.get_freq_group(reso)\n            freqn = resolution.get_freq_group(self.freq)\n            assert grp >= freqn\n            if grp == freqn:\n                key = Period(asdt, freq=self.freq)\n                loc = self.get_loc(key, method=method, tolerance=tolerance)\n                return loc\n            elif method is None:\n                raise KeyError(key)\n            else:\n                key = asdt\n         elif is_integer(key):\n             raise KeyError(key)"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "scrapy_31", "problem": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))\n     def header_items(self):\n         return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n             for k, v in self.request.headers.items()\n         ]", "fixed": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n     def header_items(self):\n         return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]"}
{"id": "black_23", "problem": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)", "fixed": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\npython_grammar_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_exec_statement.keywords[\"exec\"]\npython_grammar_no_print_statement_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"print\"]\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"exec\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)"}
{"id": "pandas_105", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n    def transpose(self, *args, **kwargs):\n        axes, kwargs = self._construct_axes_from_arguments(\n            args, kwargs, require_all=True\n        )\n        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)\n        if len(axes) != len(set(axes)):\n            raise ValueError(f\"Must specify {self._AXIS_LEN} unique axes\")\n        new_axes = self._construct_axes_dict_from(\n            self, [self._get_axis(x) for x in axes_names]\n        )\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop(\"copy\", None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n        nv.validate_transpose(tuple(), kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)\n     def swapaxes(self, axis1, axis2, copy=True):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n     def swapaxes(self, axis1, axis2, copy=True):"}
{"id": "sanic_5", "problem": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],", "fixed": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],"}
{"id": "black_18", "problem": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            sys.stdout.write(diff_contents)\n         finally:\n             if lock:\n                 lock.release()", "fixed": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff_contents)\n            f.detach()\n         finally:\n             if lock:\n                 lock.release()"}
{"id": "thefuck_27", "problem": " def match(command, settings):\n def get_new_command(command, settings):\n    return 'open http://' + command.script[5:]", "fixed": " def match(command, settings):\n def get_new_command(command, settings):\n    return command.script.replace('open ', 'open http://')"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            self.csv_file = open(self.filename, 'a' + self.file_flags)\n         else:\n            self.csv_file = open(self.filename, 'w' + self.file_flags)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}", "fixed": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            mode = 'a'\n         else:\n            mode = 'w'\n        self.csv_file = io.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}"}
{"id": "pandas_118", "problem": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(np.ravel(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\"", "fixed": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(com.flatten(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\""}
{"id": "keras_1", "problem": " def update_sub(x, decrement):\n         The variable `x` updated.\n    return tf_state_ops.assign_sub(x, decrement)\n @symbolic", "fixed": " def update_sub(x, decrement):\n         The variable `x` updated.\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n        ordinal_op = getattr(self.asi8, opname)\n         if isinstance(other, str):\n             try:", "fixed": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n         if isinstance(other, str):\n             try:"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "pandas_80", "problem": " def make_data():\n @pytest.fixture\n def dtype():\n    return pd.BooleanDtype()\n @pytest.fixture", "fixed": " def make_data():\n @pytest.fixture\n def dtype():\n    return BooleanDtype()\n @pytest.fixture"}
{"id": "keras_29", "problem": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)", "fixed": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                            self.stateful_metric_functions.append(metric_fn)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "luigi_30", "problem": " class TaskProcess(AbstractTaskProcess):\n             self.task.trigger_event(Event.START, self.task)\n             t0 = time.time()\n             status = None\n            try:\n                new_deps = self._run_get_new_deps()\n                if new_deps is None:\n                    status = RUNNING\n                else:\n                    status = SUSPENDED\n                    logger.info(\n                        '[pid %s] Worker %s new requirements      %s',\n                        os.getpid(), self.worker_id, self.task.task_id)\n                    return\n            finally:\n                if status != SUSPENDED:\n                    self.task.trigger_event(\n                        Event.PROCESSING_TIME, self.task, time.time() - t0)\n                    error_message = json.dumps(self.task.on_success())\n                    logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                                self.worker_id, self.task.task_id)\n                    self.task.trigger_event(Event.SUCCESS, self.task)\n                    status = DONE\n         except KeyboardInterrupt:\n             raise", "fixed": " class TaskProcess(AbstractTaskProcess):\n             self.task.trigger_event(Event.START, self.task)\n             t0 = time.time()\n             status = None\n            new_deps = self._run_get_new_deps()\n            if new_deps is None:\n                status = DONE\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                error_message = json.dumps(self.task.on_success())\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task.task_id)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n            else:\n                status = SUSPENDED\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task.task_id)\n         except KeyboardInterrupt:\n             raise"}
{"name": "find_in_sorted.py", "problem": "def find_in_sorted(arr, x):\n    def binsearch(start, end):\n        if start == end:\n            return -1\n        mid = start + (end - start) // 2\n        if x < arr[mid]:\n            return binsearch(start, mid)\n        elif x > arr[mid]:\n            return binsearch(mid, end)\n        else:\n            return mid\n    return binsearch(0, len(arr))", "fixed": "def find_in_sorted(arr, x):\n    def binsearch(start, end):\n        if start == end:\n            return -1\n        mid = start + (end - start) // 2\n        if x < arr[mid]:\n            return binsearch(start, mid)\n        elif x > arr[mid]:\n            return binsearch(mid + 1, end)\n        else:\n            return mid\n    return binsearch(0, len(arr))", "hint": "Binary Search\nInput:\n    arr: A sorted list of ints", "input": [[3, 4, 5, 5, 5, 5, 6], 5], "output": 3}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n         self.seed = seed", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n        self._manager = None\n         self.queue = None\n         self.seed = seed"}
{"name": "get_factors.py", "problem": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return []", "fixed": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return [n]\n", "hint": "Prime Factorization\nFactors an int using naive trial division.\nInput:", "input": [100], "output": [2, 2, 5, 5]}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.resize(ymin, x.shape)\n        ymax = np.resize(ymax, x.shape)\n        verts = [((thisx, thisymin), (thisx, thisymax))\n                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.ravel(ymin)\n        ymax = np.ravel(ymax)\n        masked_verts = np.ma.empty((len(x), 2, 2))\n        masked_verts[:, 0, 0] = x\n        masked_verts[:, 0, 1] = ymin\n        masked_verts[:, 1, 0] = x\n        masked_verts[:, 1, 1] = ymax\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "keras_39", "problem": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    current < self.target):\n                 return\n             prev_total_width = self.total_width", "fixed": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    (self.target is not None and current < self.target)):\n                 return\n             prev_total_width = self.total_width"}
{"name": "lcs_length.py", "problem": "def lcs_length(s, t):\n    from collections import Counter\n    dp = Counter()\n    for i in range(len(s)):\n        for j in range(len(t)):\n            if s[i] == t[j]:\n                dp[i, j] = dp[i - 1, j] + 1\n    return max(dp.values()) if dp else 0", "fixed": "def lcs_length(s, t):\n    from collections import Counter\n    dp = Counter()\n    for i in range(len(s)):\n        for j in range(len(t)):\n            if s[i] == t[j]:\n                dp[i, j] = dp[i - 1, j - 1] + 1\n    return max(dp.values()) if dp else 0", "hint": "Longest Common Substring\nlongest-common-substring\nInput:", "input": "", "output": ""}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "matplotlib_1", "problem": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n    if draw_disabled:\n        for meth_name in dir(RendererBase):\n            if (meth_name.startswith(\"draw_\")\n                    or meth_name in [\"open_group\", \"close_group\"]):\n                setattr(renderer, meth_name, lambda *args, **kwargs: None)\n     return renderer", "fixed": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n     return renderer"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(left.values, right.values, obj=f\"{obj} category\")", "fixed": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(\n                left.values,\n                right.values,\n                obj=f\"{obj} category\",\n                check_category_order=check_category_order,\n            )"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'", "fixed": " def fit_generator(model,\n    val_use_sequence_api = is_sequence(validation_data)\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'"}
{"id": "luigi_15", "problem": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks", "fixed": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks"}
{"id": "black_22", "problem": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:", "fixed": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n    return split_wrapper\n@dont_increase_indentation\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:"}
{"id": "pandas_67", "problem": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()", "fixed": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n    def iget(self, key):\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "keras_22", "problem": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '", "fixed": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n        self.supports_masking = True\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '"}
{"id": "pandas_153", "problem": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            values = values.astype(str)\n         else:\n             values = np.array(values, dtype=\"object\")", "fixed": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            itemsize = writers.word_len(na_rep)\n            values = values.astype(\"<U{size}\".format(size=itemsize))\n         else:\n             values = np.array(values, dtype=\"object\")"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,"}
{"id": "pandas_138", "problem": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)", "fixed": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)"}
{"id": "pandas_72", "problem": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n         ):\n             values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:", "fixed": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n         ):\n             values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n        elif exact_match:\n            values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:"}
