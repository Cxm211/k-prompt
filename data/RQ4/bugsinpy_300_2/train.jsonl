{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n             np.putmask(rlab, rmask, count)\n         count += 1\n     return llab, rlab, count", "fixed": " def _factorize_keys(lk, rk, sort=True):\n             np.putmask(rlab, rmask, count)\n         count += 1\n    if how == \"right\":\n        return rlab, llab, count\n     return llab, rlab, count"}
{"id": "ansible_13", "problem": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)", "fixed": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    requirement = None\n                    if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \\\n                            urlparse(collection_input).scheme.lower() in ['http', 'https']:\n                        name = collection_input\n                    else:\n                        name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "thefuck_12", "problem": " from difflib import get_close_matches\n from thefuck.utils import get_all_executables, \\\n    get_valid_history_without_current, get_closest\n from thefuck.specific.sudo import sudo_support\n @sudo_support\n def match(command):\n    return (command.script_parts\n             and 'not found' in command.stderr\n             and bool(get_close_matches(command.script_parts[0],\n                                        get_all_executables())))", "fixed": " from difflib import get_close_matches\n from thefuck.utils import get_all_executables, \\\n    get_valid_history_without_current, get_closest, which\n from thefuck.specific.sudo import sudo_support\n @sudo_support\n def match(command):\n    return (not which(command.script_parts[0])\n             and 'not found' in command.stderr\n             and bool(get_close_matches(command.script_parts[0],\n                                        get_all_executables())))"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     else:\n         BodySchema = params.Body\n        body_param_media_types = [\n            getattr(f.schema, \"media_type\")\n            for f in flat_dependant.body_params\n            if isinstance(f.schema, params.Body)\n        ]\n        if len(set(body_param_media_types)) == 1:\n            BodySchema_kwargs[\"media_type\"] = body_param_media_types[0]\n     field = Field(\n         name=\"body\",\n         type_=BodyModel,"}
{"id": "black_13", "problem": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token == 'def':\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            async_def = True\n                            async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],", "fixed": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token in ('def', 'for'):\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "pandas_44", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\n from pandas.core.arrays.period import (\n     PeriodArray,", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\nfrom pandas.core.dtypes.dtypes import PeriodDtype\n from pandas.core.arrays.period import (\n     PeriodArray,"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):", "fixed": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index + 1), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n     mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort)\n         for n in range(len(left_keys))\n     )\n     zipped = zip(*mapped)", "fixed": " def _get_join_indexers(\n     mapped = (\n        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)\n         for n in range(len(left_keys))\n     )\n     zipped = zip(*mapped)"}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "pandas_134", "problem": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2030, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):", "fixed": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2200, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):"}
{"id": "thefuck_1", "problem": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \\\"([a-z]+)\\\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)", "fixed": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "pandas_37", "problem": " from pandas.core.dtypes.inference import is_array_like\n from pandas import compat\n from pandas.core import ops\nfrom pandas.core.arrays import PandasArray\n from pandas.core.construction import extract_array\n from pandas.core.indexers import check_array_indexer\n from pandas.core.missing import isna", "fixed": " from pandas.core.dtypes.inference import is_array_like\n from pandas import compat\n from pandas.core import ops\nfrom pandas.core.arrays import IntegerArray, PandasArray\nfrom pandas.core.arrays.integer import _IntegerDtype\n from pandas.core.construction import extract_array\n from pandas.core.indexers import check_array_indexer\n from pandas.core.missing import isna"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "fastapi_15", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n     def get(\n         self,", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n     def get(\n         self,"}
{"id": "ansible_1", "problem": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:", "fixed": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                            if not os.path.isfile(os.path.join(to_text(b_search_path, errors='surrogate_or_strict'), 'MANIFEST.json')):\n                                raise AnsibleError(\n                                    message=\"Collection %s does not appear to have a MANIFEST.json. \" % collection_name +\n                                            \"A MANIFEST.json is expected if the collection has been built and installed via ansible-galaxy.\"\n                                )\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:"}
{"id": "keras_1", "problem": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.random_normal(shape, self.mean, self.stddev,\n                            dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "pandas_64", "problem": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df\n         self.columns = self.df.columns\n         self.float_format = float_format", "fixed": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df.reindex(columns=cols)\n         self.columns = self.df.columns\n         self.float_format = float_format"}
{"id": "ansible_5", "problem": " def test_check_mutually_exclusive_found(mutually_exclusive_terms):\n         'fox': 'red',\n         'socks': 'blue',\n     }\n    expected = \"TypeError('parameters are mutually exclusive: string1|string2, box|fox|socks',)\"\n     with pytest.raises(TypeError) as e:\n         check_mutually_exclusive(mutually_exclusive_terms, params)\n        assert e.value == expected\n def test_check_mutually_exclusive_none():", "fixed": " def test_check_mutually_exclusive_found(mutually_exclusive_terms):\n         'fox': 'red',\n         'socks': 'blue',\n     }\n    expected = \"parameters are mutually exclusive: string1|string2, box|fox|socks\"\n     with pytest.raises(TypeError) as e:\n         check_mutually_exclusive(mutually_exclusive_terms, params)\n    assert to_native(e.value) == expected\n def test_check_mutually_exclusive_none():"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "fastapi_1", "problem": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\"", "fixed": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\""}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n             return out.sort_index() if self.sort else out", "fixed": " class GroupBy(_GroupBy):\n             if not self.observed and isinstance(result_index, CategoricalIndex):\n                 out = out.reindex(result_index)\n            out = self._reindex_output(out)\n             return out.sort_index() if self.sort else out"}
{"id": "youtube-dl_17", "problem": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]", "fixed": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n    if param is None:\n        return []\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "thefuck_27", "problem": " def match(command, settings):\n def get_new_command(command, settings):\nreturn 'open http:", "fixed": " def match(command, settings):\n def get_new_command(command, settings):\nreturn command.script.replace('open ', 'open http:"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "black_15", "problem": " def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n        if pc.value in FMT_ON:\n            raise FormatOn(pc.consumed)\n        if pc.value in FMT_OFF:\n            if pc.type == STANDALONE_COMMENT:\n                raise FormatOff(pc.consumed)\n            prev = preceding_leaf(leaf)\n            if not prev or prev.type in WHITESPACE:\n                raise FormatOff(pc.consumed)\n @dataclass", "fixed": " def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n @dataclass"}
{"id": "scrapy_15", "problem": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),", "fixed": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(netloc),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "tornado_15", "problem": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root)\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)", "fixed": " class StaticFileHandler(RequestHandler):\n         .. versionadded:: 3.1\n        root = os.path.abspath(root) + os.path.sep\n         if not (absolute_path + os.path.sep).startswith(root):\n             raise HTTPError(403, \"%s is not in root static directory\",\n                             self.path)"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            self.csv_file = open(self.filename, 'a' + self.file_flags)\n         else:\n            self.csv_file = open(self.filename, 'w' + self.file_flags)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}", "fixed": " class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n            mode = 'a'\n         else:\n            mode = 'w'\n        self.csv_file = io.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "PySnooper_1", "problem": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = '\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch", "fixed": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = u'\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "luigi_9", "problem": " def _summary_format(set_tasks, worker):\n         str_output += 'Did not run any tasks'\n     smiley = \"\"\n     reason = \"\"\n    if set_tasks[\"failed\"]:\n        smiley = \":(\"\n        reason = \"there were failed tasks\"\n        if set_tasks[\"scheduling_error\"]:\n            reason += \" and tasks whose scheduling failed\"\n     elif set_tasks[\"scheduling_error\"]:\n         smiley = \":(\"\n         reason = \"there were tasks whose scheduling failed\"", "fixed": " def _summary_format(set_tasks, worker):\n         str_output += 'Did not run any tasks'\n     smiley = \"\"\n     reason = \"\"\n    if set_tasks[\"ever_failed\"]:\n        if not set_tasks[\"failed\"]:\n            smiley = \":)\"\n            reason = \"there were failed tasks but they all suceeded in a retry\"\n        else:\n            smiley = \":(\"\n            reason = \"there were failed tasks\"\n            if set_tasks[\"scheduling_error\"]:\n                reason += \" and tasks whose scheduling failed\"\n     elif set_tasks[\"scheduling_error\"]:\n         smiley = \":(\"\n         reason = \"there were tasks whose scheduling failed\""}
{"id": "fastapi_14", "problem": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[bool, SchemaBase]] = None\n class Example(BaseModel):", "fixed": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[SchemaBase, bool]] = None\n class Example(BaseModel):"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'", "fixed": " def fit_generator(model,\n    val_use_sequence_api = is_sequence(validation_data)\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'"}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n     return index", "fixed": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n    if copy:\n        index = index.copy()\n     return index"}
{"id": "pandas_53", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not self.holds_integer():\n                     self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     self._invalid_indexer(\"label\", key)\n             elif kind == \"loc\" and is_integer(key):\n                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):\n                     self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "pandas_108", "problem": " def infer_dtype_from_scalar(val, pandas_dtype: bool = False):\n         if lib.is_period(val):\n             dtype = PeriodDtype(freq=val.freq)\n             val = val.ordinal\n     return dtype, val", "fixed": " def infer_dtype_from_scalar(val, pandas_dtype: bool = False):\n         if lib.is_period(val):\n             dtype = PeriodDtype(freq=val.freq)\n             val = val.ordinal\n        elif lib.is_interval(val):\n            subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]\n            dtype = IntervalDtype(subtype=subtype)\n     return dtype, val"}
{"id": "black_14", "problem": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            for import_from_child in first_child.children[3:]:\n                if isinstance(import_from_child, Leaf):\n                    if import_from_child.type == token.NAME:\n                        imports.add(import_from_child.value)\n                else:\n                    assert import_from_child.type == syms.import_as_names\n                    for leaf in import_from_child.children:\n                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:\n                            imports.add(leaf.value)\n         else:\n             break\n     return imports", "fixed": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            imports |= set(get_imports_from_children(first_child.children[3:]))\n         else:\n             break\n     return imports"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "pandas_10", "problem": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, np.ndarray) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)", "fixed": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "youtube-dl_32", "problem": " class NPOIE(InfoExtractor):\n'http:\n             video_id,\ntransform_source=lambda j: re.sub(r'parseMetadata\\((.*?)\\);\\n\n         )\n         token_page = self._download_webpage(\n'http:", "fixed": " class NPOIE(InfoExtractor):\n'http:\n             video_id,\n            transform_source=strip_jsonp,\n         )\n         token_page = self._download_webpage(\n'http:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_41", "problem": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n     @property\n     def _holder(self):", "fixed": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n    should_store = DatetimeBlock.should_store\n     @property\n     def _holder(self):"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\nif \"\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\nif \"\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "pandas_53", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n         if takeable:\n             return self._values[label]\n        return self.index.get_value(self, label)\n     def __setitem__(self, key, value):\n         key = com.apply_if_callable(key, self)", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n         if takeable:\n             return self._values[label]\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n     def __setitem__(self, key, value):\n         key = com.apply_if_callable(key, self)"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "youtube-dl_30", "problem": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f", "fixed": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                    formats = list(formats)\n                    if not formats:\n                        return\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f"}
{"id": "youtube-dl_35", "problem": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',", "fixed": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n        '%d/%m/%Y %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',"}
{"id": "PySnooper_1", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "tornado_1", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):"}
{"id": "luigi_7", "problem": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n             if status == PENDING or status != task.status:", "fixed": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:\n             if status == PENDING or status != task.status:"}
{"id": "tornado_5", "problem": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n         if self._next_timeout <= current_time:\n            callback_time_sec = self.callback_time / 1000.0\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec", "fixed": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n        callback_time_sec = self.callback_time / 1000.0\n         if self._next_timeout <= current_time:\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec\n        else:\n            self._next_timeout += callback_time_sec"}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "black_9", "problem": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:", "fixed": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:"}
{"id": "pandas_12", "problem": " from pandas.core.dtypes.cast import (\n     validate_numeric_casting,\n )\n from pandas.core.dtypes.common import (\n    ensure_float64,\n     ensure_int64,\n     ensure_platform_int,\n     infer_dtype_from_object,", "fixed": " from pandas.core.dtypes.cast import (\n     validate_numeric_casting,\n )\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     ensure_platform_int,\n     infer_dtype_from_object,"}
{"id": "keras_20", "problem": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n         A tensor.", "fixed": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n         A tensor."}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n        actual_value = dct.get(m.group('key'))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)", "fixed": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "matplotlib_13", "problem": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n         return cls(vertices, codes)\n     def __repr__(self):", "fixed": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n        last_vert = None\n        if codes.size > 0 and codes[-1] == cls.STOP:\n            last_vert = vertices[-1]\n        vertices = vertices[codes != cls.STOP, :]\n        codes = codes[codes != cls.STOP]\n        if last_vert is not None:\n            vertices = np.append(vertices, [last_vert], axis=0)\n            codes = np.append(codes, cls.STOP)\n         return cls(vertices, codes)\n     def __repr__(self):"}
{"id": "pandas_47", "problem": " from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import (\n     is_integer,\n     is_iterator,\n     is_list_like,", "fixed": " from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import (\n    is_hashable,\n     is_integer,\n     is_iterator,\n     is_list_like,"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip()\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)", "fixed": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip('\\n')\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)"}
{"id": "keras_42", "problem": " class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n                      steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,", "fixed": " class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,"}
{"id": "pandas_119", "problem": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n         result = result.append(margin_dummy)\n     except TypeError:", "fixed": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )\n         result = result.append(margin_dummy)\n     except TypeError:"}
{"id": "keras_19", "problem": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN.", "fixed": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state).\n            - a `output_size` attribute. This can be a single integer or a\n                TensorShape, which represent the shape of the output. For\n                backward compatible reason, if this attribute is not available\n                for the cell, the value will be inferred by the first element\n                of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN."}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        bbox, info, descent = self._get_layout(self._renderer)\n        x, y = self.get_unitless_position()\n        x, y = self.get_transform().transform((x, y))\n        bbox = bbox.translated(x, y)\n        if dpi is not None:\n            self.figure.dpi = dpi_orig\n        return bbox\n     def set_backgroundcolor(self, color):", "fixed": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        with cbook._setattr_cm(self.figure, dpi=dpi):\n            bbox, info, descent = self._get_layout(self._renderer)\n            x, y = self.get_unitless_position()\n            x, y = self.get_transform().transform((x, y))\n            bbox = bbox.translated(x, y)\n            return bbox\n     def set_backgroundcolor(self, color):"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "keras_39", "problem": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    current < self.target):\n                 return\n             prev_total_width = self.total_width", "fixed": " class Progbar(object):\n         info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                    (self.target is not None and current < self.target)):\n                 return\n             prev_total_width = self.total_width"}
{"id": "luigi_14", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n        if new_status == FAILED and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "scrapy_33", "problem": " from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n logger = logging.getLogger(__name__)", "fixed": " from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "thefuck_18", "problem": " patterns = ['permission denied',\n def match(command):\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():", "fixed": " patterns = ['permission denied',\n def match(command):\n    if command.script_parts and command.script_parts[0] == 'sudo':\n        return False\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "keras_23", "problem": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape", "fixed": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"id": "scrapy_33", "problem": " from scrapy.pipelines.media import MediaPipeline\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n logger = logging.getLogger(__name__)", "fixed": " from scrapy.pipelines.media import MediaPipeline\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "luigi_3", "problem": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except ValueError:\n            return literal_eval(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))\n class NumericalParameter(Parameter):"}
{"id": "youtube-dl_40", "problem": " import platform\n import re\n import ssl\n import socket\n import subprocess\n import sys\n import traceback", "fixed": " import platform\n import re\n import ssl\n import socket\nimport struct\n import subprocess\n import sys\n import traceback"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()", "fixed": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "scrapy_17", "problem": " def get_meta_refresh(response):\n def response_status_message(status):\n    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))\n def response_httprepr(response):", "fixed": " def get_meta_refresh(response):\n def response_status_message(status):\n    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))\n def response_httprepr(response):"}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "keras_37", "problem": " class RNN(Layer):\n             self._num_constants = len(constants)\n             additional_specs += self.constants_spec\n        is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n         for tensor in additional_inputs:\n            if hasattr(tensor, '_keras_history') != is_keras_tensor:\n                 raise ValueError('The initial state or constants of an RNN'\n                                  ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors')\n         if is_keras_tensor:", "fixed": " class RNN(Layer):\n             self._num_constants = len(constants)\n             additional_specs += self.constants_spec\n        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n         for tensor in additional_inputs:\n            if K.is_keras_tensor(tensor) != is_keras_tensor:\n                 raise ValueError('The initial state or constants of an RNN'\n                                  ' layer cannot be specified with a mix of'\n                                 ' Keras tensors and non-Keras tensors'\n                                 ' (a \"Keras tensor\" is a tensor that was'\n                                 ' returned by a Keras layer, or by `Input`)')\n         if is_keras_tensor:"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "ansible_16", "problem": " class LinuxHardware(Hardware):\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):\n             i = processor_occurence", "fixed": " class LinuxHardware(Hardware):\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n             i = processor_occurence"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\ndef read_pickle(path, compression=\"infer\"):\n     Load pickled pandas object (or any object) from file.", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass\ndef read_pickle(\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\n):\n     Load pickled pandas object (or any object) from file."}
{"id": "pandas_54", "problem": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n         elif is_categorical(values):", "fixed": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n         elif is_categorical(values):"}
{"id": "thefuck_24", "problem": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\nCorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))", "fixed": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))\nclass CorrectedCommand(object):\n    def __init__(self, script, side_effect, priority):\n        self.script = script\n        self.side_effect = side_effect\n        self.priority = priority\n    def __eq__(self, other):"}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "keras_24", "problem": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:", "fixed": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +", "fixed": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +"}
{"id": "pandas_80", "problem": " class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):\n     def __len__(self) -> int:\n         return len(self._data)\n     def to_numpy(\n         self, dtype=None, copy=False, na_value: \"Scalar\" = lib.no_default,\n     ):", "fixed": " class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):\n     def __len__(self) -> int:\n         return len(self._data)\n    def __invert__(self):\n        return type(self)(~self._data, self._mask)\n     def to_numpy(\n         self, dtype=None, copy=False, na_value: \"Scalar\" = lib.no_default,\n     ):"}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"id": "pandas_13", "problem": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=False)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n     else:\n         return False"}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size", "fixed": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size"}
{"id": "pandas_123", "problem": " from pandas.core.dtypes.generic import (\n )\n from pandas.core.dtypes.missing import isna\n from pandas.core import algorithms\n import pandas.core.common as com\n from pandas.core.indexes.base import Index, InvalidIndexError, _index_shared_docs", "fixed": " from pandas.core.dtypes.generic import (\n )\n from pandas.core.dtypes.missing import isna\nfrom pandas._typing import Dtype\n from pandas.core import algorithms\n import pandas.core.common as com\n from pandas.core.indexes.base import Index, InvalidIndexError, _index_shared_docs"}
{"id": "pandas_106", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values"}
{"id": "httpie_2", "problem": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)", "fixed": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n    requests_session.max_redirects = args.max_redirects\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_42", "problem": " class Sequential(Model):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                      steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,", "fixed": " class Sequential(Model):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,"}
{"id": "black_15", "problem": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break", "fixed": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n        if parent.type == syms.file_input:\n            break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "thefuck_15", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return ('did not match any file(s) known to git.' in command.stderr\n            and \"Did you forget to 'git add'?\" in command.stderr)\n @git_support\n def get_new_command(command):\n     missing_file = re.findall(\n            r\"error: pathspec '([^']*)' \"\n            r\"did not match any file\\(s\\) known to git.\", command.stderr)[0]\n     formatme = shell.and_('git add -- {}', '{}')\n     return formatme.format(missing_file, command.script)", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return 'did not match any file(s) known to git.' in command.stderr\n @git_support\n def get_new_command(command):\n     missing_file = re.findall(\n        r\"error: pathspec '([^']*)' \"\n        r'did not match any file\\(s\\) known to git.', command.stderr)[0]\n     formatme = shell.and_('git add -- {}', '{}')\n     return formatme.format(missing_file, command.script)"}
{"id": "scrapy_5", "problem": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,", "fixed": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n     def __init__(\n             self,", "fixed": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n    Customize how values are represented as strings::\n        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))\n     def __init__(\n             self,"}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "scrapy_23", "problem": " from six.moves.urllib.parse import urlunparse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n class HttpProxyMiddleware(object):", "fixed": " from six.moves.urllib.parse import urlunparse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\nfrom scrapy.utils.python import to_bytes\n class HttpProxyMiddleware(object):"}
{"id": "ansible_4", "problem": " __metaclass__ = type\n from ansible.module_utils.six import string_types\n from ansible.playbook.attribute import FieldAttribute\n from ansible.utils.collection_loader import AnsibleCollectionLoader\n def _ensure_default_collection(collection_list=None):", "fixed": " __metaclass__ = type\n from ansible.module_utils.six import string_types\n from ansible.playbook.attribute import FieldAttribute\n from ansible.utils.collection_loader import AnsibleCollectionLoader\nfrom ansible.template import is_template, Environment\nfrom ansible.utils.display import Display\ndisplay = Display()\n def _ensure_default_collection(collection_list=None):"}
{"id": "pandas_32", "problem": "https:\n from collections import abc\n from datetime import datetime\nfrom io import BytesIO\n import struct\n import warnings", "fixed": "https:\n from collections import abc\n from datetime import datetime\n import struct\n import warnings"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        if m.group('strval') is not None:\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or\n            actual_value is not None and m.group('intval') is not None and\n                isinstance(actual_value, compat_str)):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_141", "problem": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):", "fixed": " class RangeIndex(Int64Index):\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step\n         else:\n            reverse = self._range[::-1]\n            start, stop, step = reverse.start, reverse.stop, reverse.step\n         target_array = np.asarray(target)\n         if not (is_integer_dtype(target_array) and target_array.ndim == 1):"}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "black_1", "problem": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        executor.shutdown()\n async def schedule_formatting(", "fixed": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        if executor is not None:\n            executor.shutdown()\n async def schedule_formatting("}
{"id": "pandas_135", "problem": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n            return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)", "fixed": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n        except TypeError as err:\n            if \"ndarray\" in str(err):\n                pass\n            else:\n                raise\n        return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)"}
{"id": "pandas_46", "problem": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n     def test_empty(self):\n         X = [[], [0, 1], []]", "fixed": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n     def test_empty(self):\n         X = [[], [0, 1], []]"}
{"id": "keras_42", "problem": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,", "fixed": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,"}
{"id": "scrapy_33", "problem": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         extra={'spider': spider, 'failure': failure})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)", "fixed": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n     for leaf in line.leaves:\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n     for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2", "fixed": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if is_sequence:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'", "fixed": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if use_sequence_api:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'"}
{"id": "luigi_14", "problem": " class Task(object):\n         return False\n    def can_disable(self):\n        return (self.disable_failures is not None or\n                self.disable_hard_timeout is not None)\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())", "fixed": " class Task(object):\n         return False\n     @property\n     def pretty_id(self):\n         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "youtube-dl_32", "problem": " from ..utils import (\n     unified_strdate,\n     parse_duration,\n     qualities,\n     url_basename,\n )", "fixed": " from ..utils import (\n     unified_strdate,\n     parse_duration,\n     qualities,\n    strip_jsonp,\n     url_basename,\n )"}
{"id": "tornado_8", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\",", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n        except ValueError:\n            self.handler.set_status(400)\n            log_msg = \"Missing/Invalid WebSocket headers\"\n            self.handler.finish(log_msg)\n            gen_log.debug(log_msg)\n            return\n        try:\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\","}
{"id": "pandas_92", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n             if where < start:\n                 if not is_series:", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n             if where < start:\n                 if not is_series:"}
{"id": "pandas_65", "problem": " class CParserWrapper(ParserBase):\n            if isinstance(src, BufferedIOBase):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\"", "fixed": " class CParserWrapper(ParserBase):\n            if isinstance(src, (BufferedIOBase, RawIOBase)):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\""}
{"id": "matplotlib_4", "problem": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,", "fixed": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "pandas_7", "problem": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where(", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        left_distances = np.abs(self[left_indexer] - target)\n        right_distances = np.abs(self[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where("}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "keras_41", "problem": " import sys\n import tarfile\n import threading\n import time\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool", "fixed": " import sys\n import tarfile\n import threading\n import time\nimport traceback\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool"}
{"id": "fastapi_1", "problem": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))", "fixed": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        elif tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "scrapy_28", "problem": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod", "fixed": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod"}
{"id": "ansible_3", "problem": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path == '/etc/lsb-release' and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:", "fixed": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "tqdm_9", "problem": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix", "fixed": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(left.values, right.values, obj=f\"{obj} category\")", "fixed": " def assert_series_equal(\n     if check_categorical:\n         if is_categorical_dtype(left) or is_categorical_dtype(right):\n            assert_categorical_equal(\n                left.values,\n                right.values,\n                obj=f\"{obj} category\",\n                check_category_order=check_category_order,\n            )"}
{"id": "fastapi_6", "problem": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if field.shape in sequence_shapes and isinstance(\n                    received_body, FormData\n                ):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)", "fixed": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)"}
{"id": "keras_11", "problem": " from __future__ import print_function\n import warnings\n import numpy as np\n from .training_utils import iter_sequence_infinite\n from .. import backend as K\n from ..utils.data_utils import Sequence", "fixed": " from __future__ import print_function\n import warnings\n import numpy as np\nfrom .training_utils import is_sequence\n from .training_utils import iter_sequence_infinite\n from .. import backend as K\n from ..utils.data_utils import Sequence"}
{"id": "keras_1", "problem": " class RandomUniform(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.random_uniform(shape, self.minval, self.maxval,\n                                dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class RandomUniform(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.random_uniform(shape, self.minval, self.maxval,\n                             dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "luigi_15", "problem": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks", "fixed": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks"}
{"id": "pandas_157", "problem": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetimelike(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "youtube-dl_40", "problem": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''", "fixed": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return struct_unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return struct_unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return struct_unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        column_margin = table.loc[:, margins_name].drop(margins_name)\n        index_margin = table.loc[margins_name, :].drop(margins_name)\n        table = table.drop(margins_name, axis=1).drop(margins_name)\n        table_index_names = table.index.names\n        table_columns_names = table.columns.names\n         table = _normalize(table, normalize=normalize, margins=False)", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n         table = table.fillna(0)\n     elif margins is True:\n        table_index = table.index\n        table_columns = table.columns\n        if (margins_name not in table.iloc[-1, :].name) | (\n            margins_name != table.iloc[:, -1].name\n        ):\n            raise ValueError(\"{} not in pivoted DataFrame\".format(margins_name))\n        column_margin = table.iloc[:-1, -1]\n        index_margin = table.iloc[-1, :-1]\n        table = table.iloc[:-1, :-1]\n         table = _normalize(table, normalize=normalize, margins=False)"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "pandas_27", "problem": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\nimport pandas._libs.tslibs.frequencies as libfrequencies\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import ("}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "black_6", "problem": " __credits__ = \\\n import re\n from codecs import BOM_UTF8, lookup\n from blib2to3.pgen2.token import *\n from . import token", "fixed": " __credits__ = \\\n import re\n from codecs import BOM_UTF8, lookup\nfrom attr import dataclass\n from blib2to3.pgen2.token import *\n from . import token"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        return Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n     def _apply_to_column_groupbys(self, func):", "fixed": " class SeriesGroupBy(GroupBy):\n         minlength = ngroups or 0\n         out = np.bincount(ids[mask], minlength=minlength)\n        result = Series(\n             out,\n             index=self.grouper.result_index,\n             name=self._selection_name,\n             dtype=\"int64\",\n         )\n        return self._reindex_output(result, fill_value=0)\n     def _apply_to_column_groupbys(self, func):"}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"id": "black_22", "problem": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):", "fixed": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):"}
{"id": "tornado_6", "problem": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)", "fixed": " class BaseAsyncIOLoop(IOLoop):\n         self.readers = set()\n         self.writers = set()\n         self.closing = False\n        for loop in list(IOLoop._ioloop_for_asyncio):\n            if loop.is_closed():\n                del IOLoop._ioloop_for_asyncio[loop]\n         IOLoop._ioloop_for_asyncio[asyncio_loop] = self\n         super(BaseAsyncIOLoop, self).initialize(**kwargs)"}
{"id": "keras_11", "problem": " def fit_generator(model,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def fit_generator(model,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n                 raise\n             raise IndexingError(key) from e\n    def _ensure_listlike_indexer(self, key, axis=None):\n        column_axis = 1\n        if self.ndim != 2:\n            return\n        if isinstance(key, tuple):\n            key = key[column_axis]\n            axis = column_axis\n        if (\n            axis == column_axis\n            and not isinstance(self.obj.columns, ABCMultiIndex)\n            and is_list_like_indexer(key)\n            and not com.is_bool_indexer(key)\n            and all(is_hashable(k) for k in key)\n        ):\n            for k in key:\n                try:\n                    self.obj[k]\n                except KeyError:\n                    self.obj[k] = np.nan\n     def __setitem__(self, key, value):\n         if isinstance(key, tuple):\n             key = tuple(com.apply_if_callable(x, self.obj) for x in key)"}
{"id": "keras_25", "problem": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1.", "fixed": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n    x = x.astype(K.floatx())\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1."}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "ansible_4", "problem": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)\n     def _load_collections(self, attr, ds):", "fixed": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n     def _load_collections(self, attr, ds):"}
{"id": "thefuck_20", "problem": " def _zip_file(command):\n    for c in command.script.split()[1:]:\n         if not c.startswith('-'):\n             if c.endswith('.zip'):\n                 return c", "fixed": " def _zip_file(command):\n    for c in command.split_script[1:]:\n         if not c.startswith('-'):\n             if c.endswith('.zip'):\n                 return c"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "youtube-dl_4", "problem": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]+)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')])\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)", "fixed": " class JSInterpreter(object):\n             return opfunc(x, y)\n         m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n         if m:\n             fname = m.group('func')\n             argvals = tuple([\n                 int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()\n             if fname not in self._functions:\n                 self._functions[fname] = self.extract_function(fname)\n             return self._functions[fname](argvals)"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$", "fixed": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n            (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$"}
{"id": "pandas_110", "problem": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function).", "fixed": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        if kind == \"loc\":\n            return label\n        return super()._maybe_cast_slice_bound(label, side, kind)\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function)."}
{"id": "pandas_41", "problem": " from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare\n from pandas.util._validators import validate_bool_kwarg\n from pandas.core.dtypes.cast import (", "fixed": " from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare\nfrom pandas._typing import ArrayLike\n from pandas.util._validators import validate_bool_kwarg\n from pandas.core.dtypes.cast import ("}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '", "fixed": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "spacy_7", "problem": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()", "fixed": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "pandas_167", "problem": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n     _tz = None\n     _freq = None", "fixed": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n    _supports_partial_string_indexing = True\n     _tz = None\n     _freq = None"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "luigi_5", "problem": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        @task._task_wraps(task_that_inherits)\n        class Wrapped(task_that_inherits):\n            def clone_parent(_self, **args):\n                return _self.clone(cls=self.task_to_inherit, **args)\n        return Wrapped\n class requires(object):", "fixed": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        def clone_parent(_self, **args):\n            return _self.clone(cls=self.task_to_inherit, **args)\n        task_that_inherits.clone_parent = clone_parent\n        return task_that_inherits\n class requires(object):"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "luigi_4", "problem": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)", "fixed": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if self.columns and len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)"}
{"id": "thefuck_28", "problem": " import re\n import os\nfrom thefuck.utils import memoize\n from thefuck import shells\n patterns = (\n         '^    at {file}:{line}:{col}',", "fixed": " import re\n import os\nfrom thefuck.utils import memoize, wrap_settings\n from thefuck import shells\n patterns = (\n         '^    at {file}:{line}:{col}',"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "pandas_68", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n    ABCExtensionArray,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,"}
{"id": "pandas_118", "problem": " from pandas.core.dtypes.generic import ABCMultiIndex\n from pandas.core.dtypes.missing import notna\n from pandas.core.arrays import Categorical\n from pandas.core.frame import DataFrame, _shared_docs\n from pandas.core.indexes.base import Index\n from pandas.core.reshape.concat import concat", "fixed": " from pandas.core.dtypes.generic import ABCMultiIndex\n from pandas.core.dtypes.missing import notna\n from pandas.core.arrays import Categorical\nimport pandas.core.common as com\n from pandas.core.frame import DataFrame, _shared_docs\n from pandas.core.indexes.base import Index\n from pandas.core.reshape.concat import concat"}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:https?:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "thefuck_4", "problem": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip().split('\\n')\n    for alias in alias_out:\n        name, value = alias.replace('alias ', '', 1).split(' ', 1)\n         if name not in overridden:\n             aliases[name] = value\n     return aliases", "fixed": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip()\n    if not alias_out:\n        return aliases\n    for alias in alias_out.split('\\n'):\n        for separator in (' ', '='):\n            split_alias = alias.replace('alias ', '', 1).split(separator, 1)\n            if len(split_alias) == 2:\n                name, value = split_alias\n                break\n        else:\n            continue\n         if name not in overridden:\n             aliases[name] = value\n     return aliases"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n     elif is_integer_dtype(dtype):\n         dtype = np.float64", "fixed": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n        is_bool = True\n     elif is_integer_dtype(dtype):\n         dtype = np.float64"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "tornado_10", "problem": " class RequestHandler(object):\n         self._log()\n         self._finished = True\n         self.on_finish()\n         self.ui = None", "fixed": " class RequestHandler(object):\n         self._log()\n         self._finished = True\n         self.on_finish()\n        self._break_cycles()\n    def _break_cycles(self):\n         self.ui = None"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n         self.seed = seed", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n        self._manager = None\n         self.queue = None\n         self.seed = seed"}
{"id": "pandas_66", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            new_values = self._data.fast_xs(loc)\n            if not is_list_like(new_values) or self.ndim == 1:\n                return com.maybe_box_datetimelike(new_values)\n             result = self._constructor_sliced(\n                 new_values,", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            if self.ndim == 1:\n                return self._values[loc]\n            new_values = self._data.fast_xs(loc)\n             result = self._constructor_sliced(\n                 new_values,"}
{"id": "black_22", "problem": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for comment in self.comments.values():\n             res += str(comment)\n         return res + '\\n'", "fixed": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for _, comment in self.comments:\n             res += str(comment)\n         return res + '\\n'"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "thefuck_31", "problem": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return '{} --staged'.format(command.script)", "fixed": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return command.script.replace(' diff', ' diff --staged')"}
{"id": "keras_20", "problem": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support():\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'", "fixed": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'"}
{"id": "luigi_19", "problem": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable():\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()", "fixed": " class SimpleTaskState(object):\n             elif task.scheduler_disable_time is not None:\n                 return\n        if new_status == FAILED and task.can_disable() and task.status != DISABLED:\n             task.add_failure()\n             if task.has_excessive_failures():\n                 task.scheduler_disable_time = time.time()"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_85", "problem": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n         if len(level_index):\n             grouper = level_index.take(codes)", "fixed": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n        else:\n            level_index = level_index.copy()\n         if len(level_index):\n             grouper = level_index.take(codes)"}
{"id": "scrapy_6", "problem": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')", "fixed": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')"}
{"id": "pandas_43", "problem": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)", "fixed": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)"}
{"id": "pandas_22", "problem": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type:\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n     @property\n     def trainable(self):", "fixed": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n        self.input_spec = layer.input_spec\n     @property\n     def trainable(self):"}
{"id": "pandas_44", "problem": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,", "fixed": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import DtypeObj, Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n    ensure_platform_int,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,"}
{"id": "pandas_72", "problem": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n         ):\n             values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:", "fixed": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n         ):\n             values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n        elif exact_match:\n            values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:"}
{"id": "fastapi_2", "problem": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:", "fixed": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "PySnooper_2", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_146", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False\n     def identical(self, other):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        return array_equivalent(\n            com.values_from_object(self), com.values_from_object(other)\n        )\n     def identical(self, other):"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n             result = res - lag\n             result[mask] = na\n             out_arr[res_indexer] = result\n         else:\n             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]", "fixed": " def diff(arr, n: int, axis: int = 0):\n             result = res - lag\n             result[mask] = na\n             out_arr[res_indexer] = result\n        elif is_bool:\n            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]\n         else:\n             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "scrapy_33", "problem": " from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\nfrom scrapy.utils.log import logformatter_adapter\n logger = logging.getLogger(__name__)", "fixed": " from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\nfrom scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n         if auto is not None:\n             self._autoscaleYon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n        for ax in self._shared_y_axes.get_siblings(self):\n            ax._stale_viewlim_y = False\n         if auto is not None:\n             self._autoscaleYon = bool(auto)"}
{"id": "thefuck_20", "problem": " import os\n import zipfile\n from thefuck.utils import for_app\n def _is_bad_zip(file):", "fixed": " import os\n import zipfile\n from thefuck.utils import for_app\nfrom thefuck.shells import quote\n def _is_bad_zip(file):"}
{"id": "luigi_9", "problem": " def _depth_first_search(set_tasks, current_task, visited):\n         for task in current_task._requires():\n             if task not in visited:\n                 _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"failed\"] or task in set_tasks[\"upstream_failure\"]:\n                 set_tasks[\"upstream_failure\"].add(current_task)\n                 upstream_failure = True\n             if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:", "fixed": " def _depth_first_search(set_tasks, current_task, visited):\n         for task in current_task._requires():\n             if task not in visited:\n                 _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"ever_failed\"] or task in set_tasks[\"upstream_failure\"]:\n                 set_tasks[\"upstream_failure\"].add(current_task)\n                 upstream_failure = True\n             if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:"}
{"id": "ansible_7", "problem": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n    for key in to_remove:\n        commands.append(\"no {0}\".format(key))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands", "fixed": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n    for key in to_remove:\n        if key in to_set.keys():\n            continue\n        commands.append(\"no {0}\".format(key))\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands"}
{"id": "pandas_40", "problem": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution", "fixed": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union, cast\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import ArrayLike, FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution"}
{"id": "pandas_24", "problem": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq='D')\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):", "fixed": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):"}
{"id": "thefuck_17", "problem": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)", "fixed": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\nbase_url = \"%s:\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "black_22", "problem": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:", "fixed": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n    return split_wrapper\n@dont_increase_indentation\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:"}
{"id": "pandas_83", "problem": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the", "fixed": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq"}
{"id": "black_18", "problem": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            sys.stdout.write(diff_contents)\n         finally:\n             if lock:\n                 lock.release()", "fixed": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff_contents)\n            f.detach()\n         finally:\n             if lock:\n                 lock.release()"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "scrapy_21", "problem": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        self._parsers.pop(netloc).callback(None)", "fixed": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)"}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "ansible_15", "problem": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state') and not needs_update('vrf'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':", "fixed": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):", "fixed": " class TimedeltaIndex(\n         this, other = self, other\n         if this._can_fast_union(other):\n            return this._fast_union(other, sort=sort)\n         else:\n             result = Index._union(this, other, sort=sort)\n             if isinstance(result, TimedeltaIndex):"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:", "fixed": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:"}
{"id": "pandas_21", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        if isinstance(key, list):\n            return self.loc[key]\n        return self.reindex(key)\n     def _get_values_tuple(self, key):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        return self.loc[key]\n     def _get_values_tuple(self, key):"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_86", "problem": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None", "fixed": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n    if columns is None:\n        raise TypeError(\"pivot() missing 1 required argument: 'columns'\")\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None"}
{"id": "pandas_51", "problem": " import pandas.core.indexes.base as ibase\n from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name\n from pandas.core.indexes.extension import ExtensionIndex, inherit_names\n import pandas.core.missing as missing\n _index_doc_kwargs = dict(ibase._index_doc_kwargs)\n _index_doc_kwargs.update(dict(target_klass=\"CategoricalIndex\"))", "fixed": " import pandas.core.indexes.base as ibase\n from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name\n from pandas.core.indexes.extension import ExtensionIndex, inherit_names\n import pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name\n _index_doc_kwargs = dict(ibase._index_doc_kwargs)\n _index_doc_kwargs.update(dict(target_klass=\"CategoricalIndex\"))"}
{"id": "pandas_36", "problem": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj_old(values.ravel())\n            result[:] = vec.reshape(shape)\n    elif is_datetime64_dtype(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:", "fixed": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n    elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:"}
{"id": "fastapi_7", "problem": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={\"detail\": exc.errors()}\n     )", "fixed": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY,\n        content={\"detail\": jsonable_encoder(exc.errors())},\n     )"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "pandas_65", "problem": " Module contains tools for processing files into DataFrames or other objects\n from collections import abc, defaultdict\n import csv\n import datetime\nfrom io import BufferedIOBase, StringIO, TextIOWrapper\n import re\n import sys\n from textwrap import fill", "fixed": " Module contains tools for processing files into DataFrames or other objects\n from collections import abc, defaultdict\n import csv\n import datetime\nfrom io import BufferedIOBase, RawIOBase, StringIO, TextIOWrapper\n import re\n import sys\n from textwrap import fill"}
{"id": "tqdm_6", "problem": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else self.total)\n     def __enter__(self):\n         return self", "fixed": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else getattr(self, \"total\", None))\n     def __enter__(self):\n         return self"}
{"id": "luigi_27", "problem": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_value:\n            description.append(\" [default: %s]\" % (self.value,))\n         if self.is_list:\n             action = \"append\"", "fixed": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_task_value(param_name=param_name, task_name=task_name):\n            value = self.task_value(param_name=param_name, task_name=task_name)\n            description.append(\" [default: %s]\" % (value,))\n         if self.is_list:\n             action = \"append\""}
{"id": "scrapy_7", "problem": " This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n from six.moves.urllib.parse import urljoin, urlencode\n import lxml.html\n from parsel.selector import create_root_node\nimport six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n from scrapy.utils.response import get_base_url", "fixed": " This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\nimport six\n from six.moves.urllib.parse import urljoin, urlencode\n import lxml.html\n from parsel.selector import create_root_node\nfrom w3lib.html import strip_html5_whitespace\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n from scrapy.utils.response import get_base_url"}
{"id": "matplotlib_1", "problem": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\n _log = logging.getLogger(__name__)", "fixed": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\nfrom matplotlib.cbook import _setattr_cm\n _log = logging.getLogger(__name__)"}
{"id": "ansible_11", "problem": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.connection import exec_command\nfrom ansible.module_utils.network.ios.ios import load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nimport re\n def map_obj_to_commands(updates, module):", "fixed": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.network.ios.ios import get_config, load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nfrom re import search, M\n def map_obj_to_commands(updates, module):"}
{"id": "pandas_24", "problem": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:", "fixed": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:"}
{"id": "keras_42", "problem": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.", "fixed": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training."}
{"id": "youtube-dl_41", "problem": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',',' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9:]*$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',", "fixed": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',', ' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',"}
{"id": "pandas_16", "problem": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other)\n         result._freq = new_freq\n         return result", "fixed": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other, result)\n         result._freq = new_freq\n         return result"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n        if len(weights) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())", "fixed": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n    class_sample_weight = None\n    if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if len(y.shape) == 2:\n            if y.shape[1] > 1:\n                y_classes = np.argmax(y, axis=1)\n            elif y.shape[1] == 1:\n                y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        class_sample_weight = np.asarray(\n            [class_weight[cls] for cls in y_classes if cls in class_weight])\n        if len(class_sample_weight) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "fastapi_14", "problem": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[bool, Any]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None", "fixed": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[Dict[str, Any], bool]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None"}
