{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string in endwith:\n                         break\n                    elif string == ')':\n                         tokens.restore_last_token()\n                         break\n                    if string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, [','])\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':", "fixed": " class YoutubeDL(object):\n                 elif type in [tokenize.NAME, tokenize.NUMBER]:\n                     current_selector = FormatSelector(SINGLE, string, [])\n                 elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            tokens.restore_last_token()\n                         break\n                    elif inside_merge and string in ['/', ',']:\n                         tokens.restore_last_token()\n                         break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                         selectors.append(current_selector)\n                         current_selector = None\n                     elif string == '/':\n                         first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                         current_selector = None\n                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "fastapi_6", "problem": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if field.shape in sequence_shapes and isinstance(\n                    received_body, FormData\n                ):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)", "fixed": " async def request_body_to_args(\n         for field in required_params:\n             value: Any = None\n             if received_body is not None:\n                if (\n                    field.shape in sequence_shapes or field.type_ in sequence_types\n                ) and isinstance(received_body, FormData):\n                     value = received_body.getlist(field.alias)\n                 else:\n                     value = received_body.get(field.alias)"}
{"id": "tornado_4", "problem": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if (start is not None and start >= size) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n            if start is not None and start < 0:\n                start += size\n             if end is not None and end > size:", "fixed": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if start is not None and start < 0:\n                start += size\n                if start < 0:\n                    start = 0\n            if (\n                start is not None\n                and (start >= size or (end is not None and start >= end))\n            ) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n             if end is not None and end > size:"}
{"id": "tornado_2", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and \"Transfer-Encoding\" not in headers\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and (\n                    \"Transfer-Encoding\" not in headers\n                    or headers[\"Transfer-Encoding\"] == \"chunked\"\n                )\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "pandas_146", "problem": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if np.any(left_value != right_value):\n                    return False\n         return True", "fixed": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                try:\n                    if np.any(left_value != right_value):\n                        return False\n                except TypeError as err:\n                    if \"Cannot compare tz-naive\" in str(err):\n                        return False\n                    raise\n         return True"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        epsilon: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced.", "fixed": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced."}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "scrapy_16", "problem": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)\n def parse_url(url, encoding=None):", "fixed": " def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\ndef _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    return (\n        to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n        quote(to_bytes(parts.query, encoding), _safe_chars),\n        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n    )\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n    try:\n        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n            parse_url(url), encoding=encoding)\n    except UnicodeError as e:\n        if encoding != 'utf8':\n            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n                parse_url(url), encoding='utf8')\n        else:\n            raise\n    if not six.PY2:\n        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n    else:\n        keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n    uqp = _unquotepath(path)\n    path = quote(uqp, _safe_chars) or '/'\n     fragment = '' if not keep_fragments else fragment\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n    if six.PY3:\n        return unquote_to_bytes(path)\n    else:\n        return unquote(path)\n def parse_url(url, encoding=None):"}
{"id": "luigi_23", "problem": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler()\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)", "fixed": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)"}
{"id": "keras_29", "problem": " class Model(Container):\n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n            for i, m in enumerate(self.metrics):\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "pandas_91", "problem": " class TimedeltaIndex(\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n         if isinstance(value, (np.ndarray, Index)):\n            value = np.array(value, dtype=_TD_DTYPE, copy=False)\n        else:\n            value = Timedelta(value).asm8.view(_TD_DTYPE)\n        return self.values.searchsorted(value, side=side, sorter=sorter)\n     def is_type_compatible(self, typ) -> bool:\n         return typ == self.inferred_type or typ == \"timedelta\"", "fixed": " class TimedeltaIndex(\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n         if isinstance(value, (np.ndarray, Index)):\n            if not type(self._data)._is_recognized_dtype(value):\n                raise TypeError(\n                    \"searchsorted requires compatible dtype or scalar, \"\n                    f\"not {type(value).__name__}\"\n                )\n            value = type(self._data)(value)\n            self._data._check_compatible_with(value)\n        elif isinstance(value, self._data._recognized_scalars):\n            self._data._check_compatible_with(value)\n            value = self._data._scalar_type(value)\n        elif not isinstance(value, TimedeltaArray):\n            raise TypeError(\n                \"searchsorted requires compatible dtype or scalar, \"\n                f\"not {type(value).__name__}\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n     def is_type_compatible(self, typ) -> bool:\n         return typ == self.inferred_type or typ == \"timedelta\""}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
{"id": "keras_25", "problem": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1.", "fixed": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n    x = x.astype(K.floatx())\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1."}
{"id": "thefuck_8", "problem": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)", "fixed": " def match(command):\n def _parse_operations(help_text_lines):\n    operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)\n     return operation_regex.findall(help_text_lines)"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "pandas_41", "problem": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype", "fixed": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value: ArrayLike) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "pandas_31", "problem": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals):\n                 inference = np.int64\n            elif is_datetime64_dtype(vals):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)", "fixed": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                 inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"name": "wrap.py", "problem": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    return lines", "fixed": "def wrap(text, cols):\n    lines = []\n    while len(text) > cols:\n        end = text.rfind(' ', 0, cols + 1)\n        if end == -1:\n            end = cols\n        line, text = text[:end], text[end:]\n        lines.append(line)\n    lines.append(text)\n    return lines", "hint": "Wrap Text\nGiven a long string and a column width, break the string on spaces into a list of lines such that each line is no longer than the column width.\nInput:", "input": [], "output": ""}
{"id": "tqdm_4", "problem": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale", "fixed": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            if total:\n                total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale"}
{"id": "youtube-dl_31", "problem": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = int_or_none(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?([0-9]+)\\s*s',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',", "fixed": " class MinhatecaIE(InfoExtractor):\n         filesize_approx = parse_filesize(self._html_search_regex(\n             r'<p class=\"fileSize\">(.*?)</p>',\n             webpage, 'file size approximation', fatal=False))\n        duration = parse_duration(self._html_search_regex(\n            r'(?s)<p class=\"fileLeng[ht][th]\">.*?class=\"bold\">(.*?)<',\n             webpage, 'duration', fatal=False))\n         view_count = int_or_none(self._html_search_regex(\n             r'<p class=\"downloadsCounter\">([0-9]+)</p>',"}
{"id": "pandas_58", "problem": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")"}
{"id": "luigi_30", "problem": " class TaskProcess(AbstractTaskProcess):\n             self.task.trigger_event(Event.START, self.task)\n             t0 = time.time()\n             status = None\n            try:\n                new_deps = self._run_get_new_deps()\n                if new_deps is None:\n                    status = RUNNING\n                else:\n                    status = SUSPENDED\n                    logger.info(\n                        '[pid %s] Worker %s new requirements      %s',\n                        os.getpid(), self.worker_id, self.task.task_id)\n                    return\n            finally:\n                if status != SUSPENDED:\n                    self.task.trigger_event(\n                        Event.PROCESSING_TIME, self.task, time.time() - t0)\n                    error_message = json.dumps(self.task.on_success())\n                    logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                                self.worker_id, self.task.task_id)\n                    self.task.trigger_event(Event.SUCCESS, self.task)\n                    status = DONE\n         except KeyboardInterrupt:\n             raise", "fixed": " class TaskProcess(AbstractTaskProcess):\n             self.task.trigger_event(Event.START, self.task)\n             t0 = time.time()\n             status = None\n            new_deps = self._run_get_new_deps()\n            if new_deps is None:\n                status = DONE\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                error_message = json.dumps(self.task.on_success())\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task.task_id)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n            else:\n                status = SUSPENDED\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task.task_id)\n         except KeyboardInterrupt:\n             raise"}
{"id": "scrapy_13", "problem": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 0\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'", "fixed": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 90\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'"}
{"id": "pandas_106", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values"}
{"id": "pandas_135", "problem": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n            return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)", "fixed": " class BaseGrouper:\n                 pass\n             else:\n                 raise\n        except TypeError as err:\n            if \"ndarray\" in str(err):\n                pass\n            else:\n                raise\n        return self._aggregate_series_pure_python(obj, func)\n     def _aggregate_series_fast(self, obj, func):\n         func = self._is_builtin_func(func)"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n         else:\n             left, right = other, self", "fixed": " class TimedeltaIndex(\n         if self[0] <= other[0]:\n             left, right = self, other\n        elif sort is False:\n            left, right = self, other\n            left_start = left[0]\n            loc = right.searchsorted(left_start, side=\"left\")\n            right_chunk = right.values[:loc]\n            dates = concat_compat((left.values, right_chunk))\n            return self._shallow_copy(dates)\n         else:\n             left, right = other, self"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "luigi_26", "problem": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar() or not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")", "fixed": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")"}
{"id": "black_23", "problem": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)", "fixed": " python_symbols = Symbols(python_grammar)\n python_grammar_no_print_statement = python_grammar.copy()\n del python_grammar_no_print_statement.keywords[\"print\"]\npython_grammar_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_exec_statement.keywords[\"exec\"]\npython_grammar_no_print_statement_no_exec_statement = python_grammar.copy()\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"print\"]\ndel python_grammar_no_print_statement_no_exec_statement.keywords[\"exec\"]\n pattern_grammar = driver.load_packaged_grammar(\"blib2to3\", _PATTERN_GRAMMAR_FILE)\n pattern_symbols = Symbols(pattern_grammar)"}
{"id": "scrapy_32", "problem": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(settings)\n        log_scrapy_info(settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)", "fixed": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings)\n        log_scrapy_info(self.settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "pandas_167", "problem": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n     _accessors = {\"str\"}", "fixed": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n    _supports_partial_string_indexing = False\n     _accessors = {\"str\"}"}
{"id": "pandas_162", "problem": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()", "fixed": " def _normalize(table, normalize, margins, margins_name=\"All\"):\n             column_margin = column_margin / column_margin.sum()\n             table = concat([table, column_margin], axis=1)\n             table = table.fillna(0)\n            table.columns = table_columns\n         elif normalize == \"index\":\n             index_margin = index_margin / index_margin.sum()\n             table = table.append(index_margin)\n             table = table.fillna(0)\n            table.index = table_index\n         elif normalize == \"all\" or normalize is True:\n             column_margin = column_margin / column_margin.sum()"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "tornado_6", "problem": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = weakref.WeakKeyDictionary()\n     @classmethod\n     def configure(cls, impl, **kwargs):", "fixed": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = dict()\n     @classmethod\n     def configure(cls, impl, **kwargs):"}
{"id": "black_5", "problem": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type == syms.arglist:\n                     commas += 1\n                     break", "fixed": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type in {\n                    syms.arglist,\n                    syms.typedargslist,\n                }:\n                     commas += 1\n                     break"}
{"id": "pandas_128", "problem": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(", "fixed": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n    if encoding is None:\n        encoding = \"utf-8\"\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer("}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n     for leaf in line.leaves:\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n     for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"id": "tornado_5", "problem": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n         if self._next_timeout <= current_time:\n            callback_time_sec = self.callback_time / 1000.0\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec", "fixed": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n        callback_time_sec = self.callback_time / 1000.0\n         if self._next_timeout <= current_time:\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec\n        else:\n            self._next_timeout += callback_time_sec"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing"}
{"id": "pandas_47", "problem": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]", "fixed": " class DataFrame(NDFrame):\n                 for k1, k2 in zip(key, value.columns):\n                     self[k1] = value[k2]\n             else:\n                self.loc._ensure_listlike_indexer(key, axis=1)\n                 indexer = self.loc._get_listlike_indexer(\n                     key, axis=1, raise_missing=False\n                 )[1]"}
{"id": "pandas_90", "problem": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    if path is None:\n        path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(path) as path:\n        pd.to_pickle(obj, path)\n        return pd.read_pickle(path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):", "fixed": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as path:\n        pd.to_pickle(obj, _path)\n        return pd.read_pickle(_path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:", "fixed": " class ReduceLROnPlateau(Callback):\n                 self.best = current\n                 self.wait = 0\n             elif not self.in_cooldown():\n                self.wait += 1\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr:"}
{"id": "pandas_165", "problem": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n class TestAddSubNaTMasking:", "fixed": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n    def test_tda_add_sub_index(self):\n        tdi = TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\n        tda = tdi.array\n        dti = pd.date_range(\"1999-12-31\", periods=3, freq=\"D\")\n        result = tda + dti\n        expected = tdi + dti\n        tm.assert_index_equal(result, expected)\n        result = tda + tdi\n        expected = tdi + tdi\n        tm.assert_index_equal(result, expected)\n        result = tda - tdi\n        expected = tdi - tdi\n        tm.assert_index_equal(result, expected)\n class TestAddSubNaTMasking:"}
{"id": "fastapi_14", "problem": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None", "fixed": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[str, Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None"}
{"name": "get_factors.py", "problem": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return []", "fixed": "def get_factors(n):\n    if n == 1:\n        return []\n    for i in range(2, int(n ** 0.5) + 1):\n        if n % i == 0:\n            return [i] + get_factors(n // i)\n    return [n]\n", "hint": "Prime Factorization\nFactors an int using naive trial division.\nInput:", "input": [100], "output": [2, 2, 5, 5]}
{"id": "pandas_66", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            new_values = self._data.fast_xs(loc)\n            if not is_list_like(new_values) or self.ndim == 1:\n                return com.maybe_box_datetimelike(new_values)\n             result = self._constructor_sliced(\n                 new_values,", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            if self.ndim == 1:\n                return self._values[loc]\n            new_values = self._data.fast_xs(loc)\n             result = self._constructor_sliced(\n                 new_values,"}
{"id": "fastapi_1", "problem": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))", "fixed": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))"}
{"id": "tornado_14", "problem": " class IOLoop(Configurable):\n             if IOLoop.current(instance=False) is None:\n                 self.make_current()\n         elif make_current:\n            if IOLoop.current(instance=False) is None:\n                 raise RuntimeError(\"current IOLoop already exists\")\n             self.make_current()", "fixed": " class IOLoop(Configurable):\n             if IOLoop.current(instance=False) is None:\n                 self.make_current()\n         elif make_current:\n            if IOLoop.current(instance=False) is not None:\n                 raise RuntimeError(\"current IOLoop already exists\")\n             self.make_current()"}
{"id": "tqdm_7", "problem": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')", "fixed": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "scrapy_15", "problem": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),", "fixed": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(netloc),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip()\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)", "fixed": " def map_obj_to_commands(updates, module):\n         if want['text'] and (want['text'] != have.get('text')):\n             banner_cmd = 'banner %s' % module.params['banner']\n             banner_cmd += ' @\\n'\n            banner_cmd += want['text'].strip('\\n')\n             banner_cmd += '\\n@'\n             commands.append(banner_cmd)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_38", "problem": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]", "fixed": " def read_batch_urls(batch_fd):\n     with contextlib.closing(batch_fd) as fd:\n         return [url for url in map(fixup, fd) if url]\ndef urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')"}
{"name": "levenshtein.py", "problem": "def levenshtein(source, target):\n    if source == '' or target == '':\n        return len(source) or len(target)\n    elif source[0] == target[0]:\n        return 1 + levenshtein(source[1:], target[1:])\n    else:\n        return 1 + min(\n            levenshtein(source,     target[1:]),\n            levenshtein(source[1:], target[1:]),\n            levenshtein(source[1:], target)\n        )", "fixed": "def levenshtein(source, target):\n    if source == '' or target == '':\n        return len(source) or len(target)\n    elif source[0] == target[0]:\n        return levenshtein(source[1:], target[1:])\n    else:\n        return 1 + min(\n            levenshtein(source,     target[1:]),\n            levenshtein(source[1:], target[1:]),\n            levenshtein(source[1:], target)\n        )", "hint": "Levenshtein Distance\nCalculates the Levenshtein distance between two strings.  The Levenshtein distance is defined as the minimum amount of single-character edits (either removing a character, adding a character, or changing a character) necessary to transform a source string into a target string.\nInput:", "input": ["electron", "neutron"], "output": 3}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n        /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n        /\\*.*?\\*/|//[^\\n]*|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "pandas_92", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n             if where < start:\n                 if not is_series:", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n             if where < start:\n                 if not is_series:"}
{"id": "pandas_29", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        if needs_float_conversion:\n            left = left.astype(\"float\")\n        left.values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        if needs_float_conversion:\n            right = right.astype(\"float\")\n        right.values[key] = value_right\n         self._right = right\n     def __eq__(self, other):", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n        if needs_float_conversion:\n            raise ValueError(\"Cannot set float NaN to integer-backed IntervalArray\")\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        left._values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        right._values[key] = value_right\n         self._right = right\n     def __eq__(self, other):"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_138", "problem": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n     if dtype is not None:", "fixed": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        x = x.astype(np.int64)\n     if dtype is not None:"}
{"id": "keras_1", "problem": " def update_sub(x, decrement):\n         The variable `x` updated.\n    return tf_state_ops.assign_sub(x, decrement)\n @symbolic", "fixed": " def update_sub(x, decrement):\n         The variable `x` updated.\n    op = tf_state_ops.assign_sub(x, decrement)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "youtube-dl_25", "problem": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^0[xX][0-9a-fA-F]+', 16),\n            (r'^0+[0-7]+', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(0), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v", "fixed": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^(0[xX][0-9a-fA-F]+)\\s*:?$', 16),\n            (r'^(0+[0-7]+)\\s*:?$', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(1), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v"}
{"name": "shortest_path_length.py", "problem": "from heapq import *\ndef shortest_path_length(length_by_edge, startnode, goalnode):\nunvisited_nodes = []\n    heappush(unvisited_nodes, (0, startnode))\n    visited_nodes = set()\n    while len(unvisited_nodes) > 0:\n        distance, node = heappop(unvisited_nodes)\n        if node is goalnode:\n            return distance\n        visited_nodes.add(node)\n        for nextnode in node.successors:\n            if nextnode in visited_nodes:\n                continue\n            insert_or_update(unvisited_nodes,\n                (min(\n                    get(unvisited_nodes, nextnode) or float('inf'),\n                    get(unvisited_nodes, nextnode) + length_by_edge[node, nextnode]\n                ),\n                nextnode)\n            )\n    return float('inf')\ndef get(node_heap, wanted_node):\n    for dist, node in node_heap:\n        if node == wanted_node:\n            return dist\n    return 0\ndef insert_or_update(node_heap, dist_node):\n    dist, node = dist_node\n    for i, tpl in enumerate(node_heap):\n        a, b = tpl\n        if b == node:\nnode_heap[i] = dist_node\n            return None\n    heappush(node_heap, dist_node)\n    return None", "fixed": "from heapq import *\ndef shortest_path_length(length_by_edge, startnode, goalnode):\nunvisited_nodes = []\n    heappush(unvisited_nodes, (0, startnode))\n    visited_nodes = set()\n    while len(unvisited_nodes) > 0:\n        distance, node = heappop(unvisited_nodes)\n        if node is goalnode:\n            return distance\n        visited_nodes.add(node)\n        for nextnode in node.successors:\n            if nextnode in visited_nodes:\n                continue\n            insert_or_update(unvisited_nodes,\n                (min(\n                    get(unvisited_nodes, nextnode) or float('inf'),\n                    distance + length_by_edge[node, nextnode]\n                ),\n                nextnode)\n            )\n    return float('inf')\ndef get(node_heap, wanted_node):\n    for dist, node in node_heap:\n        if node == wanted_node:\n            return dist\n    return 0\ndef insert_or_update(node_heap, dist_node):\n    dist, node = dist_node\n    for i, tpl in enumerate(node_heap):\n        a, b = tpl\n        if b == node:\nnode_heap[i] = dist_node\n            return None\n    heappush(node_heap, dist_node)\n    return None\n", "hint": "Shortest Path\ndijkstra\nImplements Dijkstra's algorithm for finding a shortest path between two nodes in a directed graph.", "input": [], "output": ""}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n    def _extract_chapters(self, webpage, description, video_id, duration):\n        return (self._extract_chapters_from_json(webpage, video_id, duration)\n                or self._extract_chapters_from_description(description, duration))\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})"}
{"name": "detect_cycle.py", "problem": "def detect_cycle(node):\n    hare = tortoise = node\n    while True:\n        if hare.successor is None:\n            return False\n        tortoise = tortoise.successor\n        hare = hare.successor.successor\n        if hare is tortoise:\n            return True", "fixed": "def detect_cycle(node):\n    hare = tortoise = node\n    while True:\n        if hare is None or hare.successor is None:\n            return False\n        tortoise = tortoise.successor\n        hare = hare.successor.successor\n        if hare is tortoise:\n            return True\n", "hint": "Linked List Cycle Detection\ntortoise-hare\nImplements the tortoise-and-hare method of cycle detection.", "input": [], "output": ""}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "pandas_118", "problem": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(np.ravel(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\"", "fixed": " def melt(\n         else:\n             id_vars = list(id_vars)\n            missing = Index(com.flatten(id_vars)).difference(cols)\n             if not missing.empty:\n                 raise KeyError(\n                     \"The following 'id_vars' are not present\""}
{"id": "scrapy_30", "problem": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).itervalues():\n             if inspect.isclass(obj) and \\\n               issubclass(obj, ScrapyCommand) and \\\n               obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):", "fixed": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).values():\n             if inspect.isclass(obj) and \\\n                    issubclass(obj, ScrapyCommand) and \\\n                    obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if is_sequence:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'", "fixed": " def predict_generator(model, generator,\n     steps_done = 0\n     all_outs = []\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps is None:\n        if use_sequence_api:\n             steps = len(generator)\n         else:\n             raise ValueError('`steps=None` is only valid for a generator'"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is", "fixed": " class Sequential(Model):\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is"}
{"id": "fastapi_1", "problem": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)", "fixed": " def get_openapi(\n     if components:\n         output[\"components\"] = components\n     output[\"paths\"] = paths\n    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n        Yields :class:`Line` objects.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            try:\n                for comment in generate_comments(node):\n                    if any_open_brackets:\n                        self.current_line.append(comment)\n                    elif comment.type == token.COMMENT:\n                        self.current_line.append(comment)\n                        yield from self.line()\n                    else:\n                        yield from self.line()\n                        self.current_line.append(comment)\n                        yield from self.line()\n            except FormatOff as f_off:\n                f_off.trim_prefix(node)\n                yield from self.line(type=UnformattedLines)\n                yield from self.visit(node)\n            except FormatOn as f_on:\n                f_on.trim_prefix(node)\n                yield from self.visit_default(node)\n            else:\n                normalize_prefix(node, inside_brackets=any_open_brackets)\n                if self.normalize_strings and node.type == token.STRING:\n                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                    normalize_string_quotes(node)\n                if node.type not in WHITESPACE:\n                    self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         If any lines were generated, set up a new current_line.\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n            for comment in generate_comments(node):\n                if any_open_brackets:\n                    self.current_line.append(comment)\n                elif comment.type == token.COMMENT:\n                    self.current_line.append(comment)\n                    yield from self.line()\n                else:\n                    yield from self.line()\n                    self.current_line.append(comment)\n                    yield from self.line()\n            normalize_prefix(node, inside_brackets=any_open_brackets)\n            if self.normalize_strings and node.type == token.STRING:\n                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n                normalize_string_quotes(node)\n            if node.type not in WHITESPACE:\n                self.current_line.append(node)\n         yield from super().visit_default(node)\n     def visit_INDENT(self, node: Node) -> Iterator[Line]:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class FastAPI(Starlette):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "pandas_81", "problem": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):", "fixed": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype\n        dtype = pandas_dtype(dtype)\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n        elif isinstance(dtype, BooleanDtype):\n            result = self._data.astype(\"bool\", copy=False)\n            return BooleanArray(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_1", "problem": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: v is not None,\n        '!': lambda v: v is None,\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)", "fixed": " def _match_one(filter_part, dct):\n         return op(actual_value, comparison_value)\n     UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n     }\n         (?P<op>%s)\\s*(?P<key>[a-z_]+)"}
{"id": "pandas_100", "problem": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        rs = rs.loc[~rs.index.duplicated()]\n        rs = rs.reindex_like(data)\n        if freq is None:\n            mask = isna(com.values_from_object(data))\n            np.putmask(rs.values, mask, np.nan)\n         return rs\n     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        if freq is not None:\n            rs = rs.loc[~rs.index.duplicated()]\n            rs = rs.reindex_like(data)\n         return rs\n     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         this, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\n         new_data = {}\n        for col, series in this.items():\n            new_data[col] = func(series.values, other.values)\n         fill_value = self._get_op_result_fill_value(other, func)", "fixed": " class SparseDataFrame(DataFrame):\n         this, other = self.align(other, join=\"outer\", axis=0, level=level, copy=False)\n         new_data = {}\n        for col in this.columns:\n            new_data[col] = func(this[col], other)\n         fill_value = self._get_op_result_fill_value(other, func)"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "fastapi_1", "problem": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}", "fixed": " client = TestClient(app)\n def test_return_defaults():\n     response = client.get(\"/\")\n     assert response.json() == {\"sub\": {}}\ndef test_return_exclude_unset():\n    response = client.get(\"/exclude_unset\")\n    assert response.json() == {\"x\": None, \"y\": \"y\"}\ndef test_return_exclude_defaults():\n    response = client.get(\"/exclude_defaults\")\n    assert response.json() == {}\ndef test_return_exclude_none():\n    response = client.get(\"/exclude_none\")\n    assert response.json() == {\"y\": \"y\", \"z\": \"z\"}\ndef test_return_exclude_unset_none():\n    response = client.get(\"/exclude_unset_none\")\n    assert response.json() == {\"y\": \"y\"}"}
{"id": "ansible_15", "problem": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state') and not needs_update('vrf'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':", "fixed": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':"}
{"id": "tornado_1", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):"}
{"id": "scrapy_31", "problem": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n     getheaders = get_all", "fixed": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]\n     getheaders = get_all"}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period):\n            if value.freq != self.freq:\n                raise raise_on_incompatible(self, value)\n            value = value.ordinal\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq).ordinal\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period) or value is NaT:\n            self._data._check_compatible_with(value)\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq)\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        elif not isinstance(value, PeriodArray):\n            raise TypeError(\n                \"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:"}
{"id": "pandas_14", "problem": " def id_func(x):\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])", "fixed": " def id_func(x):\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n    ]\n)\ndef names(request):\n    return request.param\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])"}
{"id": "pandas_160", "problem": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\"):\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif isinstance(o, np.ndarray):\n                     dtypes |= {o.dtype.name}", "fixed": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif hasattr(o, \"dtype\"):\n                     dtypes |= {o.dtype.name}"}
{"id": "pandas_165", "problem": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):", "fixed": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n    def test_parr_add_sub_index(self):\n        pi = pd.period_range(\"2000-12-31\", periods=3)\n        parr = pi.array\n        result = parr - pi\n        expected = pi - pi\n        tm.assert_index_equal(result, expected)\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):"}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s was created successfully\" % obj_name)\n     def execute_info(self):", "fixed": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))\n     def execute_info(self):"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "pandas_105", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n    def transpose(self, *args, **kwargs):\n        axes, kwargs = self._construct_axes_from_arguments(\n            args, kwargs, require_all=True\n        )\n        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)\n        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)\n        if len(axes) != len(set(axes)):\n            raise ValueError(f\"Must specify {self._AXIS_LEN} unique axes\")\n        new_axes = self._construct_axes_dict_from(\n            self, [self._get_axis(x) for x in axes_names]\n        )\n        new_values = self.values.transpose(axes_numbers)\n        if kwargs.pop(\"copy\", None) or (len(args) and args[-1]):\n            new_values = new_values.copy()\n        nv.validate_transpose(tuple(), kwargs)\n        return self._constructor(new_values, **new_axes).__finalize__(self)\n     def swapaxes(self, axis1, axis2, copy=True):", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         self._data.set_axis(axis, labels)\n         self._clear_item_cache()\n     def swapaxes(self, axis1, axis2, copy=True):"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n     def __init__(\n             self,", "fixed": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n    Customize how values are represented as strings::\n        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))\n     def __init__(\n             self,"}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"id": "luigi_1", "problem": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics = self._scheduler._state._metrics_collector.generate_latest()\n         if metrics:\n            metrics.configure_http_handler(self)\n             self.write(metrics)", "fixed": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n         if metrics:\n            metrics_collector.configure_http_handler(self)\n             self.write(metrics)"}
{"id": "pandas_123", "problem": " class RangeIndex(Int64Index):\n    @staticmethod\n    def _validate_dtype(dtype):", "fixed": " class RangeIndex(Int64Index):"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "pandas_2", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n         if is_setter:\n             return list(key)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)\n         if is_setter:\n             return list(key)"}
{"id": "pandas_2", "problem": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n        key = list(self._convert_key(key, is_setter=True))\n         self.obj._set_value(*key, value=value, takeable=self._takeable)", "fixed": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key, is_setter=True))\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n         self.obj._set_value(*key, value=value, takeable=self._takeable)"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "scrapy_16", "problem": " def parse_url(url, encoding=None):", "fixed": " def parse_url(url, encoding=None):\n        Data are returned as a list of name, value pairs as bytes.\n        Arguments:\n        qs: percent-encoded query string to be parsed\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.  A\n            true value indicates that blanks should be retained as blank\n            strings.  The default false value indicates that blank values\n            are to be ignored and treated as if they were  not included.\n        strict_parsing: flag indicating what to do with parsing errors. If\n            false (the default), errors are silently ignored. If true,\n            errors raise a ValueError exception."}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n             if not ax.is_unique:\n                 return False", "fixed": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                return False\n             if not ax.is_unique:\n                 return False"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "ansible_11", "problem": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n    if text:\n        text = str(text).strip()\n     return {\n         'banner': module.params['banner'],\n         'text': text,", "fixed": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n     return {\n         'banner': module.params['banner'],\n         'text': text,"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "cookiecutter_4", "problem": " def run_script_with_context(script_path, cwd, context):\n     ) as temp:\n         temp.write(Template(contents).render(**context))\n    return run_script(temp.name, cwd)\n def run_hook(hook_name, project_dir, context):", "fixed": " def run_script_with_context(script_path, cwd, context):\n     ) as temp:\n         temp.write(Template(contents).render(**context))\n    run_script(temp.name, cwd)\n def run_hook(hook_name, project_dir, context):"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return result\n     @classmethod\n     def _add_numeric_operations(cls):", "fixed": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return self._reindex_output(result, fill_value=0)\n     @classmethod\n     def _add_numeric_operations(cls):"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "pandas_79", "problem": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError):\n                 return False", "fixed": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError, InvalidIndexError):\n                 return False"}
{"id": "pandas_114", "problem": " class Index(IndexOpsMixin, PandasObject):\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n        s = extract_array(series, extract_numpy=True)\n        if isinstance(s, ExtensionArray):\n            if is_scalar(key):\n                try:\n                    iloc = self.get_loc(key)\n                    return s[iloc]\n                except KeyError:\n                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                        raise\n                    elif is_integer(key):\n                        return s[key]\n            else:\n                raise InvalidIndexError(key)\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds", "fixed": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds"}
{"id": "black_22", "problem": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines):\n         yield line\n         return\n     line_str = str(line).strip('\\n')\n    if len(line_str) <= line_length and '\\n' not in line_str:\n         yield line\n         return\n     if line.is_def:\n         split_funcs = [left_hand_split]\n     elif line.inside_brackets:\n        split_funcs = [delimiter_split]\n        if '\\n' not in line_str:\n            split_funcs.append(right_hand_split)\n     else:\n         split_funcs = [right_hand_split]\n     for split_func in split_funcs:", "fixed": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines) or line.is_comment:\n         yield line\n         return\n     line_str = str(line).strip('\\n')\n    if (\n        len(line_str) <= line_length\n        and '\\n' not in line_str\n        and not line.contains_standalone_comments\n    ):\n         yield line\n         return\n    split_funcs: List[SplitFunc]\n     if line.is_def:\n         split_funcs = [left_hand_split]\n     elif line.inside_brackets:\n        split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]\n     else:\n         split_funcs = [right_hand_split]\n     for split_func in split_funcs:"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "pandas_9", "problem": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_scalar(key) and isna(key):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)", "fixed": " class Categorical(NDArrayBackedExtensionArray, PandasObject):\n         Returns True if `key` is in this Categorical.\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.isna().any()\n         return contains(self, key, container=self._codes)"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "scrapy_30", "problem": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()\n        return comm[0].strip()\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\", "fixed": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\"}
{"id": "pandas_16", "problem": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other)\n         result._freq = new_freq\n         return result", "fixed": " def _make_wrapped_arith_op_with_freq(opname: str):\n         if result is NotImplemented:\n             return NotImplemented\n        new_freq = self._get_addsub_freq(other, result)\n         result._freq = new_freq\n         return result"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 16,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 16\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 8,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 8\n         },\n     },\n     {"}
{"id": "pandas_151", "problem": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        values = self._ndarray\n        t = np.result_type(value, values)\n        if t != self._ndarray.dtype:\n            values = values.astype(t, casting=\"safe\")\n            values[key] = value\n            self._dtype = PandasDtype(t)\n            self._ndarray = values\n        else:\n            self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)", "fixed": " class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):\n         if not lib.is_scalar(value):\n             value = np.asarray(value)\n        value = np.asarray(value, dtype=self._ndarray.dtype)\n        self._ndarray[key] = value\n     def __len__(self) -> int:\n         return len(self._ndarray)"}
{"id": "keras_42", "problem": " class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n                      steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,", "fixed": " class Model(Container):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self,\n                       generator,\n                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 try:\n                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                         generator_output = next(self._generator)\n                        self.queue.put(generator_output)\n                     else:\n                         time.sleep(self.wait_time)\n                 except StopIteration:\n                     break\n                except Exception:\n                     self._stop_event.set()\n                    raise\n         try:\n             if self._use_multiprocessing:\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n                 self.queue = queue.Queue()", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n                 try:\n                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                         generator_output = next(self._generator)\n                        self.queue.put((True, generator_output))\n                     else:\n                         time.sleep(self.wait_time)\n                 except StopIteration:\n                     break\n                except Exception as e:\n                    if self._use_multiprocessing:\n                        traceback.print_exc()\n                        setattr(e, '__traceback__', None)\n                    elif not hasattr(e, '__traceback__'):\n                        setattr(e, '__traceback__', sys.exc_info()[2])\n                    self.queue.put((False, e))\n                     self._stop_event.set()\n                    break\n         try:\n             if self._use_multiprocessing:\n                self._manager = multiprocessing.Manager()\n                self.queue = self._manager.Queue(maxsize=max_queue_size)\n                 self._stop_event = multiprocessing.Event()\n             else:\n                 self.queue = queue.Queue()"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n    _responseMatcher = re.compile('HTTP/1\\.. 200')\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     for it.\n    _responseMatcher = re.compile(b'HTTP/1\\.. 200')\n     def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                  timeout=30, bindAddress=None):"}
{"id": "pandas_41", "problem": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )", "fixed": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )"}
{"id": "black_22", "problem": " Depth = int\n NodeType = int\n LeafID = int\n Priority = int\n LN = Union[Leaf, Node]\n out = partial(click.secho, bold=True, err=True)\n err = partial(click.secho, fg='red', err=True)", "fixed": " Depth = int\n NodeType = int\n LeafID = int\n Priority = int\nIndex = int\n LN = Union[Leaf, Node]\nSplitFunc = Callable[['Line', bool], Iterator['Line']]\n out = partial(click.secho, bold=True, err=True)\n err = partial(click.secho, fg='red', err=True)"}
{"id": "luigi_6", "problem": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x))\n         except ValueError:\nreturn literal_eval(x)\n    def serialize(self, x):\n        return json.dumps(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n         except ValueError:\nreturn literal_eval(x)\n class NumericalParameter(Parameter):"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a same-typed block.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block.", "fixed": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block."}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "keras_42", "problem": " class Model(Container):\n         if do_validation:\n             self._make_test_function()\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n        if val_gen and not validation_steps:\n            raise ValueError('When using a generator for validation data, '\n                             'you must specify a value for '\n                             '`validation_steps`.')\n         out_labels = self._get_deduped_metrics_names()", "fixed": " class Model(Container):\n         if do_validation:\n             self._make_test_function()\n        is_sequence = isinstance(generator, Sequence)\n        if not is_sequence and use_multiprocessing and workers > 1:\n            warnings.warn(\n                UserWarning('Using a generator with `use_multiprocessing=True`'\n                            ' and multiple workers may duplicate your data.'\n                            ' Please consider using the`keras.utils.Sequence'\n                            ' class.'))\n        if steps_per_epoch is None:\n            if is_sequence:\n                steps_per_epoch = len(generator)\n            else:\n                raise ValueError('`steps_per_epoch=None` is only valid for a'\n                                 ' generator based on the `keras.utils.Sequence`'\n                                 ' class. Please specify `steps_per_epoch` or use'\n                                 ' the `keras.utils.Sequence` class.')\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__') or\n                    isinstance(validation_data, Sequence))\n        if (val_gen and not isinstance(validation_data, Sequence) and\n                not validation_steps):\n            raise ValueError('`validation_steps=None` is only valid for a'\n                             ' generator based on the `keras.utils.Sequence`'\n                             ' class. Please specify `validation_steps` or use'\n                             ' the `keras.utils.Sequence` class.')\n         out_labels = self._get_deduped_metrics_names()"}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = df.loc[\"a\", \"A\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             df.at[\"a\", 0]\n        with pytest.raises(TypeError, match=msg):\n             df.loc[\"a\", 0]\n     def test_series_at_raises_key_error(self):", "fixed": " class TestScalar2:\n         result = df.loc[\"a\", \"A\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             df.at[\"a\", 0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             df.loc[\"a\", 0]\n     def test_series_at_raises_key_error(self):"}
{"id": "PySnooper_3", "problem": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output_path, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)", "fixed": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)"}
{"id": "black_22", "problem": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36=py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")", "fixed": " def split_line(\n         result: List[Line] = []\n         try:\n            for l in split_func(line, py36):\n                 if str(l).strip('\\n') == line_str:\n                     raise CannotSplit(\"Split function returned an unchanged result\")"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "black_23", "problem": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n   return None\nasync def coroutine(arg):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n  exec(\"new-style exec\", {}, {})\n   return None\nasync def coroutine(arg, exec=False):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "luigi_13", "problem": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.fs.mkdir(d)\n         os.rename(old_path, new_path)", "fixed": " class LocalFileSystem(FileSystem):\n             raise RuntimeError('Destination exists: %s' % new_path)\n         d = os.path.dirname(new_path)\n         if d and not os.path.exists(d):\n            self.mkdir(d)\n         os.rename(old_path, new_path)"}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"name": "powerset.py", "problem": "def powerset(arr):\n    if arr:\nfirst, *rest = arr\n        rest_subsets = powerset(rest)\n        return [[first] + subset for subset in rest_subsets]\n    else:\n        return [[]]", "fixed": "def powerset(arr):\n    if arr:\n        first, *rest = arr\n        rest_subsets = powerset(rest)\n        return rest_subsets + [[first] + subset for subset in rest_subsets]\n    else:\n        return [[]]\n", "hint": "Power Set\nInput:\n    arr: A list", "input": [["a", "b", "c"]], "output": [[], ["c"], ["b"], ["b", "c"], ["a"], ["a", "c"], ["a", "b"], ["a", "b", "c"]]}
{"id": "pandas_76", "problem": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError):\n                 pass", "fixed": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError, OverflowError):\n                 pass"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n     >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n     'http://example.com/foo?a=b&c=d&c=d2'\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "tqdm_9", "problem": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix", "fixed": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix"}
{"id": "scrapy_31", "problem": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))\n     def header_items(self):\n         return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n             for k, v in self.request.headers.items()\n         ]", "fixed": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n     def header_items(self):\n         return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "thefuck_30", "problem": " def _search(stderr):\n def match(command, settings):\n    return 'EDITOR' in os.environ and _search(command.stderr)\n def get_new_command(command, settings):", "fixed": " def _search(stderr):\n def match(command, settings):\n    if 'EDITOR' not in os.environ:\n        return False\n    m = _search(command.stderr)\n    return m and os.path.isfile(m.group('file'))\n def get_new_command(command, settings):"}
{"id": "keras_20", "problem": " def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):\n         padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n         output_padding: Integer, amount of padding along the output dimension,\n             Can be set to `None` in which case the output length is inferred.\n         The output length (integer).", "fixed": " def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):\n         padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n         output_padding: Integer, amount of padding along the output dimension,\n             Can be set to `None` in which case the output length is inferred.\n        dilation: dilation rate, integer.\n         The output length (integer)."}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n     }, {\n         'url': 'https://www.facebook.com/video.php?v=10204634152394104',\n         'only_matching': True,", "fixed": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n    }, {\n        'note': 'Video without discernible title',\n        'url': 'https://www.facebook.com/video.php?v=274175099429670',\n        'info_dict': {\n            'id': '274175099429670',\n            'ext': 'mp4',\n            'title': 'Facebook video\n        }\n     }, {\n         'url': 'https://www.facebook.com/video.php?v=10204634152394104',\n         'only_matching': True,"}
{"id": "keras_19", "problem": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state). In this case, the first entry\n                (`state_size[0]`) should be the same as\n                the size of the cell output.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN.", "fixed": " class RNN(Layer):\n                 the size of the recurrent state\n                 (which should be the same as the size of the cell output).\n                 This can also be a list/tuple of integers\n                (one size per state).\n            - a `output_size` attribute. This can be a single integer or a\n                TensorShape, which represent the shape of the output. For\n                backward compatible reason, if this attribute is not available\n                for the cell, the value will be inferred by the first element\n                of the `state_size`.\n             It is also possible for `cell` to be a list of RNN cell instances,\n             in which cases the cells get stacked on after the other in the RNN,\n             implementing an efficient stacked RNN."}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")", "fixed": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n    is_bool = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")"}
{"id": "youtube-dl_31", "problem": " def parse_duration(s):\n     m = re.match(", "fixed": " def parse_duration(s):\n     m = re.match(\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "pandas_43", "problem": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n     if not isinstance(right, ABCDataFrame):\n         return False", "fixed": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n    if op is operator.pow or op is rpow:\n        return False\n     if not isinstance(right, ABCDataFrame):\n         return False"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_43", "problem": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?\n     if not m:\n         return u''\n     return m.group(1)", "fixed": " def remove_start(s, start):\n def url_basename(url):\n    m = re.match(r'(?:https?:|)//[^/]+/(?:[^?\n     if not m:\n         return u''\n     return m.group(1)"}
{"id": "cookiecutter_3", "problem": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default\n     )\n     return choice_map[user_choice]", "fixed": " def read_user_choice(var_name, options):\n     ))\n     user_choice = click.prompt(\n        prompt, type=click.Choice(choices), default=default, show_choices=False\n     )\n     return choice_map[user_choice]"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        elif tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit"}
{"id": "youtube-dl_33", "problem": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()", "fixed": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'(\\.[0-9]+)?(?:Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "scrapy_18", "problem": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:", "fixed": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:"}
{"id": "youtube-dl_11", "problem": " def str_or_none(v, default=None):\n def str_to_int(int_str):\n    if int_str is None:\n        return None\n     int_str = re.sub(r'[,\\.\\+]', '', int_str)\n     return int(int_str)", "fixed": " def str_or_none(v, default=None):\n def str_to_int(int_str):\n    if not isinstance(int_str, compat_str):\n        return int_str\n     int_str = re.sub(r'[,\\.\\+]', '', int_str)\n     return int(int_str)"}
{"id": "keras_19", "problem": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "black_23", "problem": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        raise AssertionError(f\"cannot parse source: {exc}\") from None\n     try:\n         dst_ast = ast.parse(dst)", "fixed": " def assert_equivalent(src: str, dst: str) -> None:\n     try:\n         src_ast = ast.parse(src)\n     except Exception as exc:\n        major, minor = sys.version_info[:2]\n        raise AssertionError(\n            f\"cannot use --safe with this file; failed to parse source file \"\n            f\"with Python {major}.{minor}'s builtin AST. Re-run with --fast \"\n            f\"or stop using deprecated Python 2 syntax. AST error message: {exc}\"\n        )\n     try:\n         dst_ast = ast.parse(dst)"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or include_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    include_none=include_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )", "fixed": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or not exclude_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        bbox, info, descent = self._get_layout(self._renderer)\n        x, y = self.get_unitless_position()\n        x, y = self.get_transform().transform((x, y))\n        bbox = bbox.translated(x, y)\n        if dpi is not None:\n            self.figure.dpi = dpi_orig\n        return bbox\n     def set_backgroundcolor(self, color):", "fixed": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        with cbook._setattr_cm(self.figure, dpi=dpi):\n            bbox, info, descent = self._get_layout(self._renderer)\n            x, y = self.get_unitless_position()\n            x, y = self.get_transform().transform((x, y))\n            bbox = bbox.translated(x, y)\n            return bbox\n     def set_backgroundcolor(self, color):"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n             False,\n             padding,\n             padding],\n        output_shape=output_shape)\n     return _postprocess_conv2d_output(x, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n             False,\n             padding,\n             padding],\n        output_shape=output_shape,\n        dilation=dilation_rate)\n     return _postprocess_conv2d_output(x, data_format)"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "pandas_59", "problem": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other)\n         def _get_corr(a, b):\n             a = a.rolling(", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other) if not self.is_freq_type else self.win_freq\n         def _get_corr(a, b):\n             a = a.rolling("}
{"id": "ansible_13", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "pandas_112", "problem": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)", "fixed": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical(target_as_index):\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)"}
{"id": "pandas_13", "problem": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=False)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n     else:\n         return False"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "pandas_24", "problem": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq='D')\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:", "fixed": " default 'raise'\n         >>> tz_aware.tz_localize(None)\n         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n                        '2018-03-03 09:00:00'],\n                      dtype='datetime64[ns]', freq=None)\n         Be careful with DST changes. When there is sequential data, pandas can\n         infer the DST time:"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))", "fixed": " class Tracer:\n             thread_global.depth -= 1\n             if not ended_by_exception:\n                return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)\n                 self.write('{indent}Return value:.. {return_value_repr}'.\n                            format(**locals()))"}
{"id": "black_22", "problem": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: Dict[LeafID, Leaf] = Factory(dict)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False", "fixed": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: List[Tuple[Index, Leaf]] = Factory(list)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False"}
{"id": "keras_22", "problem": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '", "fixed": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n        self.supports_masking = True\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '"}
{"id": "sanic_3", "problem": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:", "fixed": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = host or self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    path : str\n        File path where the pickled object will be loaded.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        For on-the-fly decompression of on-disk data. If 'infer', then use\n        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',\n        or '.zip' respectively, and no decompression otherwise.\n        Set to None for no decompression.\n     Returns\n     -------", "fixed": " def read_pickle(path, compression=\"infer\"):\n     Parameters\n     ----------\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be loaded from.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL is not limited to S3 and GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     Returns\n     -------"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n     def get_config(self):\n        config = []\n         for layer in self.layers:\n            config.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n        return copy.deepcopy(config)\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n        model = cls()\n        for conf in config:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n         return model", "fixed": " class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n     def get_config(self):\n        layer_configs = []\n         for layer in self.layers:\n            layer_configs.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n        config = {\n            'name': self.name,\n            'layers': copy.deepcopy(layer_configs)\n        }\n        if self._build_input_shape:\n            config['build_input_shape'] = self._build_input_shape\n        return config\n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n        if 'name' in config:\n            name = config['name']\n            build_input_shape = config.get('build_input_shape')\n            layer_configs = config['layers']\n        model = cls(name=name)\n        for conf in layer_configs:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n        if not model.inputs and build_input_shape:\n            model.build(build_input_shape)\n         return model"}
{"id": "pandas_60", "problem": " class _Rolling_and_Expanding(_Rolling):\n             raise ValueError(\"engine must be either 'numba' or 'cython'\")\n         return self._apply(\n             apply_func,\n             center=False,\n             floor=0,\n             name=func,\n             use_numba_cache=engine == \"numba\",\n         )\n     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             raise ValueError(\"engine must be either 'numba' or 'cython'\")\n         return self._apply(\n             apply_func,\n             center=False,\n             floor=0,\n             name=func,\n             use_numba_cache=engine == \"numba\",\n            raw=raw,\n         )\n     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):"}
{"id": "pandas_41", "problem": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values", "fixed": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values"}
{"id": "luigi_23", "problem": " class CentralPlannerScheduler(Scheduler):\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})", "fixed": " class CentralPlannerScheduler(Scheduler):\n        if self._config.prune_on_get_work:\n            self.prune()\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})"}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "pandas_154", "problem": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(obj.values)\n             if not cython_dtype:\n                cython_dtype = obj.values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = obj.values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:", "fixed": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n            values = obj._data._values\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(values)\n             if not cython_dtype:\n                cython_dtype = values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:"}
{"id": "pandas_37", "problem": " class StringArray(PandasArray):\n             if copy:\n                 return self.copy()\n             return self\n         return super().astype(dtype, copy)\n     def _reduce(self, name, skipna=True, **kwargs):", "fixed": " class StringArray(PandasArray):\n             if copy:\n                 return self.copy()\n             return self\n        elif isinstance(dtype, _IntegerDtype):\n            arr = self._ndarray.copy()\n            mask = self.isna()\n            arr[mask] = 0\n            values = arr.astype(dtype.numpy_dtype)\n            return IntegerArray(values, mask, copy=False)\n         return super().astype(dtype, copy)\n     def _reduce(self, name, skipna=True, **kwargs):"}
{"id": "youtube-dl_42", "problem": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n        def fix_ampersand(s):\n            return s.replace(u'& ', '&amp; ')\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_ampersand)\n         return [self._get_video_info(item) for item in idoc.findall('.//item')]", "fixed": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_xml_ampersands)\n         return [self._get_video_info(item) for item in idoc.findall('.//item')]"}
{"id": "spacy_7", "problem": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n     return result", "fixed": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n     return result"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"id": "luigi_29", "problem": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n            if cls.run == NotImplemented:\n                continue\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\", "fixed": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                 return self._constructor(values, **attributes)\n             except (TypeError, ValueError):\n                 pass\n        attributes.pop(\"tz\", None)\n         return Index(values, **attributes)\n     def _update_inplace(self, result, **kwargs):"}
{"id": "luigi_9", "problem": " def _depth_first_search(set_tasks, current_task, visited):\n         for task in current_task._requires():\n             if task not in visited:\n                 _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"failed\"] or task in set_tasks[\"upstream_failure\"]:\n                 set_tasks[\"upstream_failure\"].add(current_task)\n                 upstream_failure = True\n             if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:", "fixed": " def _depth_first_search(set_tasks, current_task, visited):\n         for task in current_task._requires():\n             if task not in visited:\n                 _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"ever_failed\"] or task in set_tasks[\"upstream_failure\"]:\n                 set_tasks[\"upstream_failure\"].add(current_task)\n                 upstream_failure = True\n             if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role related operations.\",\n         )", "fixed": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n         )"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        if m.group('strval') is not None:\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))", "fixed": " def _match_one(filter_part, dct):\n     m = operator_rex.search(filter_part)\n     if m:\n         op = COMPARISON_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if (m.group('strval') is not None or\n            actual_value is not None and m.group('intval') is not None and\n                isinstance(actual_value, compat_str)):\n             if m.group('op') not in ('=', '!='):\n                 raise ValueError(\n                     'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('strval') or m.group('intval')\n         else:\n             try:\n                 comparison_value = int(m.group('intval'))"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "youtube-dl_34", "problem": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n         if not expected:\n             msg = msg + u'; please report this issue on https://yt-dl.org/bug . Be sure to call youtube-dl with the --verbose flag and include its complete output. Make sure you are using the latest version; type  youtube-dl -U  to update.'\n         super(ExtractorError, self).__init__(msg)", "fixed": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n        if cause:\n            msg += u' (caused by %r)' % cause\n         if not expected:\n             msg = msg + u'; please report this issue on https://yt-dl.org/bug . Be sure to call youtube-dl with the --verbose flag and include its complete output. Make sure you are using the latest version; type  youtube-dl -U  to update.'\n         super(ExtractorError, self).__init__(msg)"}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"id": "black_22", "problem": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:", "fixed": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n    return split_wrapper\n@dont_increase_indentation\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(description_original, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):"}
{"id": "matplotlib_1", "problem": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n    if draw_disabled:\n        for meth_name in dir(RendererBase):\n            if (meth_name.startswith(\"draw_\")\n                    or meth_name in [\"open_group\", \"close_group\"]):\n                setattr(renderer, meth_name, lambda *args, **kwargs: None)\n     return renderer", "fixed": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n     return renderer"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "black_22", "problem": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for comment in self.comments.values():\n             res += str(comment)\n         return res + '\\n'", "fixed": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for _, comment in self.comments:\n             res += str(comment)\n         return res + '\\n'"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):", "fixed": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "keras_8", "problem": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:", "fixed": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    raise LookupError\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:"}
{"id": "httpie_2", "problem": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)", "fixed": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n    requests_session.max_redirects = args.max_redirects\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)"}
{"id": "black_15", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n def normalize_fmt_off(node: Node) -> None:", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n def normalize_fmt_off(node: Node) -> None:\n    Returns True if a pair was converted."}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "pandas_45", "problem": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)", "fixed": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n    elif isinstance(data, abc.Set):\n        raise TypeError(\"Set type is unordered\")\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "keras_29", "problem": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:", "fixed": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:"}
{"id": "pandas_70", "problem": "                cls = dtype.construct_array_type()\n                result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)", "fixed": "                if len(result) and isinstance(result[0], dtype.type):\n                    cls = dtype.construct_array_type()\n                    result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)"}
{"id": "luigi_25", "problem": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path()\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):", "fixed": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):"}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)\n         yield current_line", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        yield current_line\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n    if current_line:\n         yield current_line"}
{"id": "luigi_6", "problem": " class DictParameter(Parameter):\n         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)\n     def serialize(self, x):\n        return json.dumps(x, cls=DictParameter._DictParamEncoder)\n class ListParameter(Parameter):", "fixed": " class DictParameter(Parameter):\n         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)\n     def serialize(self, x):\n        return json.dumps(x, cls=_DictParamEncoder)\n class ListParameter(Parameter):"}
{"name": "rpn_eval.py", "problem": "def rpn_eval(tokens):\n    def op(symbol, a, b):\n        return {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b\n        }[symbol](a, b)\n    stack = []\n    for token in tokens:\n        if isinstance(token, float):\n            stack.append(token)\n        else:\n            a = stack.pop()\n            b = stack.pop()\n            stack.append(\n                op(token, a, b)\n            )\n    return stack.pop()", "fixed": "def rpn_eval(tokens):\n    def op(symbol, a, b):\n        return {\n            '+': lambda a, b: a + b,\n            '-': lambda a, b: a - b,\n            '*': lambda a, b: a * b,\n            '/': lambda a, b: a / b\n        }[symbol](a, b)\n    stack = []\n    for token in tokens:\n        if isinstance(token, float):\n            stack.append(token)\n        else:\n            a = stack.pop()\n            b = stack.pop()\n            stack.append(\n                op(token, b, a)\n            )\n    return stack.pop()\n", "hint": "Reverse Polish Notation\nFour-function calculator with input given in Reverse Polish Notation (RPN).\nInput:", "input": [[3.0, 5.0, "+", 2.0, "/"]], "output": 4.0}
{"id": "matplotlib_10", "problem": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])", "fixed": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n            if 'label1On' in kwtrans or 'label2On' in kwtrans:\n                self.offsetText.set_visible(\n                    self._major_tick_kw.get('label1On', False)\n                    or self._major_tick_kw.get('label2On', False))\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])"}
{"name": "is_valid_parenthesization.py", "problem": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return True", "fixed": "def is_valid_parenthesization(parens):\n    depth = 0\n    for paren in parens:\n        if paren == '(':\n            depth += 1\n        else:\n            depth -= 1\n            if depth < 0:\n                return False\n    return depth == 0\n", "hint": "Nested Parens\nInput:\n    parens: A string of parentheses", "input": ["((()()))()"], "output": "True"}
{"name": "minimum_spanning_tree.py", "problem": "def minimum_spanning_tree(weight_by_edge):\n    group_by_node = {}\n    mst_edges = set()\n    for edge in sorted(weight_by_edge, key=weight_by_edge.__getitem__):\n        u, v = edge\n        if group_by_node.setdefault(u, {u}) != group_by_node.setdefault(v, {v}):\n            mst_edges.add(edge)\n            group_by_node[u].update(group_by_node[v])\n            for node in group_by_node[v]:\n                group_by_node[node].update(group_by_node[u])\n    return mst_edges", "fixed": "def minimum_spanning_tree(weight_by_edge):\n    group_by_node = {}\n    mst_edges = set()\n    for edge in sorted(weight_by_edge, key=weight_by_edge.__getitem__):\n        u, v = edge\n        if group_by_node.setdefault(u, {u}) != group_by_node.setdefault(v, {v}):\n            mst_edges.add(edge)\n            group_by_node[u].update(group_by_node[v])\n            for node in group_by_node[v]:\n                group_by_node[node] = group_by_node[u]\n    return mst_edges", "hint": "Minimum Spanning Tree\nKruskal's algorithm implementation.\nInput:", "input": "", "output": ""}
{"id": "thefuck_27", "problem": " def match(command, settings):\n def get_new_command(command, settings):\n    return 'open http://' + command.script[5:]", "fixed": " def match(command, settings):\n def get_new_command(command, settings):\n    return command.script.replace('open ', 'open http://')"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "pandas_16", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            return self.freq\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:\n         if is_period_dtype(self.dtype):\n            if is_period_dtype(result.dtype):\n                return self.freq\n            return None\n         elif self.freq is None:\n             return None\n         elif lib.is_scalar(other) and isna(other):"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     @staticmethod\n     def get_target_path(target):\n         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n             return target.path\n         elif isinstance(target, bigquery.BigQueryTarget):\n            \"{}:{}.{}\".format(target.project_id, target.dataset_id, target.table_id)\n         else:\n            raise ValueError(\"Target not supported\")", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     @staticmethod\n     def get_target_path(target):\n         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):\n             return target.path\n         elif isinstance(target, bigquery.BigQueryTarget):\n            return \"{}:{}.{}\".format(target.table.project_id, target.table.dataset_id, target.table.table_id)\n         else:\n            raise ValueError(\"Target %s not supported\" % target)"}
{"id": "black_6", "problem": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n             result = drv.parse_string(src_txt, True)\n             break", "fixed": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n     stashed = None\n     async_def = False", "fixed": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n    async_is_reserved_keyword = config.async_is_reserved_keyword\n     stashed = None\n     async_def = False"}
{"id": "httpie_1", "problem": " def filename_from_url(url, content_type):\n     return fn\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        if not exists(filename + suffix):\n            return filename + suffix\n         attempt += 1", "fixed": " def filename_from_url(url, content_type):\n     return fn\ndef trim_filename(filename, max_len):\n    if len(filename) > max_len:\n        trim_by = len(filename) - max_len\n        name, ext = os.path.splitext(filename)\n        if trim_by >= len(name):\n            filename = filename[:-trim_by]\n        else:\n            filename = name[:-trim_by] + ext\n    return filename\ndef get_filename_max_length(directory):\n    try:\n        max_len = os.pathconf(directory, 'PC_NAME_MAX')\n    except OSError as e:\n        if e.errno == errno.EINVAL:\n            max_len = 255\n        else:\n            raise\n    return max_len\ndef trim_filename_if_needed(filename, directory='.', extra=0):\n    max_len = get_filename_max_length(directory) - extra\n    if len(filename) > max_len:\n        filename = trim_filename(filename, max_len)\n    return filename\n def get_unique_filename(filename, exists=os.path.exists):\n     attempt = 0\n     while True:\n         suffix = '-' + str(attempt) if attempt > 0 else ''\n        try_filename = trim_filename_if_needed(filename, extra=len(suffix))\n        try_filename += suffix\n        if not exists(try_filename):\n            return try_filename\n         attempt += 1"}
{"id": "fastapi_12", "problem": " class HTTPBearer(HTTPBase):\n             else:\n                 return None\n         if scheme.lower() != \"bearer\":\n            raise HTTPException(\n                status_code=HTTP_403_FORBIDDEN,\n                detail=\"Invalid authentication credentials\",\n            )\n         return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)", "fixed": " class HTTPBearer(HTTPBase):\n             else:\n                 return None\n         if scheme.lower() != \"bearer\":\n            if self.auto_error:\n                raise HTTPException(\n                    status_code=HTTP_403_FORBIDDEN,\n                    detail=\"Invalid authentication credentials\",\n                )\n            else:\n                return None\n         return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)"}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         with np.errstate(invalid=\"ignore\"):\n             masked = np.abs(a) > self.linthresh\n         sign = np.sign(a[masked])\n        log = (self._linscale_adj +\n               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)\n         log *= sign * self.linthresh\n         a[masked] = log\n         a[~masked] *= self._linscale_adj"}
{"id": "scrapy_22", "problem": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        else:\n             self._xg_characters(serialized_value)\n         self.xg.endElement(name)", "fixed": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        elif isinstance(serialized_value, six.text_type):\n             self._xg_characters(serialized_value)\n        else:\n            self._xg_characters(str(serialized_value))\n         self.xg.endElement(name)"}
{"name": "max_sublist_sum.py", "problem": "def max_sublist_sum(arr):\n    max_ending_here = 0\n    max_so_far = 0\n    for x in arr:\n        max_ending_here = max_ending_here + x\n        max_so_far = max(max_so_far, max_ending_here)\n    return max_so_far", "fixed": "def max_sublist_sum(arr):\n    max_ending_here = 0\n    max_so_far = 0\n    for x in arr:\n        max_ending_here = max(0, max_ending_here + x)\n        max_so_far = max(max_so_far, max_ending_here)\n    return max_so_far\n", "hint": "Max Sublist Sum\nmax-sublist-sum\nEfficient equivalent to max(sum(arr[i:j]) for 0 <= i <= j <= len(arr))", "input": [[4, -5, 2, 1, -1, 3]], "output": 5}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:", "fixed": " def fit_generator(model,\n                 val_enqueuer_gen = val_enqueuer.get()\n             elif val_gen:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer_gen = iter_sequence_infinite(val_data)\n                     validation_steps = validation_steps or len(val_data)\n                 else:"}
{"id": "pandas_70", "problem": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})", "fixed": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n         Raises\n         ------\n         TypeError", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n        .. versionchanged:: 1.0.0\n           Returns an NA value on empty arrays\n         Raises\n         ------\n         TypeError"}
{"id": "pandas_56", "problem": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._iget_item_cache(col)\n            return com.maybe_box_datetimelike(series._values[index])\n         series = self._get_item_cache(col)\n         engine = self.index._engine", "fixed": " class DataFrame(NDFrame):\n         scalar\n         if takeable:\n            series = self._ixs(col, axis=1)\n            return series._values[index]\n         series = self._get_item_cache(col)\n         engine = self.index._engine"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
