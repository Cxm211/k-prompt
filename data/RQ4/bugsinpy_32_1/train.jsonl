{"id": "fastapi_16", "problem": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        if not obj.Config.json_encoders:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n            )\n        else:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n                custom_encoder=obj.Config.json_encoders,\n            )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):", "fixed": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        encoder = getattr(obj.Config, \"json_encoders\", custom_encoder)\n        return jsonable_encoder(\n            obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n            include_none=include_none,\n            custom_encoder=encoder,\n        )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):"}
{"id": "ansible_12", "problem": "import os\n from ansible.plugins.lookup import LookupBase\n class LookupModule(LookupBase):", "fixed": " from ansible.plugins.lookup import LookupBase\nfrom ansible.utils import py3compat\n class LookupModule(LookupBase):"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "keras_27", "problem": " class Bidirectional(Wrapper):\n             return self.forward_layer.updates + self.backward_layer.updates\n         return []\n     @property\n     def losses(self):\n         if hasattr(self.forward_layer, 'losses'):\n             return self.forward_layer.losses + self.backward_layer.losses\n         return []\n     @property\n     def constraints(self):\n         constraints = {}", "fixed": " class Bidirectional(Wrapper):\n             return self.forward_layer.updates + self.backward_layer.updates\n         return []\n    def get_updates_for(self, inputs=None):\n        forward_updates = self.forward_layer.get_updates_for(inputs)\n        backward_updates = self.backward_layer.get_updates_for(inputs)\n        return (super(Wrapper, self).get_updates_for(inputs) +\n                forward_updates + backward_updates)\n     @property\n     def losses(self):\n         if hasattr(self.forward_layer, 'losses'):\n             return self.forward_layer.losses + self.backward_layer.losses\n         return []\n    def get_losses_for(self, inputs=None):\n        forward_losses = self.forward_layer.get_losses_for(inputs)\n        backward_losses = self.backward_layer.get_losses_for(inputs)\n        return (super(Wrapper, self).get_losses_for(inputs) +\n                forward_losses + backward_losses)\n     @property\n     def constraints(self):\n         constraints = {}"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "pandas_71", "problem": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_integer,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,", "fixed": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n    is_extension_array_dtype,\n     is_integer,\n    is_integer_dtype,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "youtube-dl_30", "problem": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f", "fixed": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                    formats = list(formats)\n                    if not formats:\n                        return\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f"}
{"id": "pandas_113", "problem": " def any_int_dtype(request):\n     return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):", "fixed": " def any_int_dtype(request):\n     return request.param\n@pytest.fixture(params=ALL_EA_INT_DTYPES)\ndef any_nullable_int_dtype(request):\n    return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,"}
{"id": "keras_8", "problem": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:", "fixed": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    raise LookupError\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "pandas_65", "problem": " class CParserWrapper(ParserBase):\n            if isinstance(src, BufferedIOBase):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\"", "fixed": " class CParserWrapper(ParserBase):\n            if isinstance(src, (BufferedIOBase, RawIOBase)):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\""}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "scrapy_25", "problem": " from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n class FormRequest(Request):", "fixed": " from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\nfrom scrapy.utils.response import get_base_url\n class FormRequest(Request):"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "pandas_44", "problem": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n     def get_loc(self, key, method=None, tolerance=None):", "fixed": " class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):\n             other = TimedeltaIndex(other)\n         return self, other\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        return is_timedelta64_dtype(dtype)\n     def get_loc(self, key, method=None, tolerance=None):"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "youtube-dl_19", "problem": " import re\n import shutil\n import subprocess\n import socket\n import sys\n import time\n import tokenize", "fixed": " import re\n import shutil\n import subprocess\n import socket\nimport string\n import sys\n import time\n import tokenize"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "pandas_149", "problem": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_s3_url\n def get_engine(engine):", "fixed": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url\n def get_engine(engine):"}
{"id": "PySnooper_2", "problem": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')\ndef get_local_reprs(frame, watch=()):\n     code = frame.f_code\n     vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())\n    result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]\n     result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))\n     result = collections.OrderedDict(result_items)", "fixed": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\nif pycompat.PY2:\n    from io import open\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')\ndef get_local_reprs(frame, watch=(), custom_repr=()):\n     code = frame.f_code\n     vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())\n    result_items = [(key, utils.get_shortish_repr(value, custom_repr=custom_repr)) for key, value in frame.f_locals.items()]\n     result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))\n     result = collections.OrderedDict(result_items)"}
{"id": "pandas_40", "problem": " from pandas.core.dtypes.common import (\n     is_array_like,\n     is_bool,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_datetime64tz_dtype,\n     is_dtype_equal,", "fixed": " from pandas.core.dtypes.common import (\n     is_array_like,\n     is_bool,\n     is_bool_dtype,\n    is_categorical,\n     is_categorical_dtype,\n     is_datetime64tz_dtype,\n     is_dtype_equal,"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "matplotlib_1", "problem": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None, *, draw_disabled=False):", "fixed": " class KeyEvent(LocationEvent):\n         self.key = key\ndef _get_renderer(figure, print_method=None):"}
{"id": "pandas_31", "problem": " from pandas.util._decorators import Appender, Substitution, cache_readonly, doc\n from pandas.core.dtypes.cast import maybe_cast_result\n from pandas.core.dtypes.common import (\n     ensure_float,\n     is_datetime64_dtype,\n     is_integer_dtype,\n     is_numeric_dtype,\n     is_object_dtype,", "fixed": " from pandas.util._decorators import Appender, Substitution, cache_readonly, doc\n from pandas.core.dtypes.cast import maybe_cast_result\n from pandas.core.dtypes.common import (\n     ensure_float,\n    is_bool_dtype,\n     is_datetime64_dtype,\n    is_extension_array_dtype,\n     is_integer_dtype,\n     is_numeric_dtype,\n     is_object_dtype,"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue", "fixed": " class Sequential(Model):\n                 at the end of every epoch. It should typically\n                 be equal to the number of samples of your\n                 validation dataset divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Maximum size for the generator queue"}
{"id": "black_11", "problem": " def is_import(leaf: Leaf) -> bool:\n     )\n def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:", "fixed": " def is_import(leaf: Leaf) -> bool:\n     )\ndef is_special_comment(leaf: Leaf) -> bool:\n    t = leaf.type\n    v = leaf.value\n    return bool(\n        (t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith(\"\n    )\n def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:"}
