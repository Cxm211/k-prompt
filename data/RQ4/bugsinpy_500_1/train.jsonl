{"id": "pandas_32", "problem": "https:\n from collections import abc\n from datetime import datetime\nfrom io import BytesIO\n import struct\n import warnings", "fixed": "https:\n from collections import abc\n from datetime import datetime\n import struct\n import warnings"}
{"id": "pandas_92", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n             if where < start:\n                 if not is_series:", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n             if where < start:\n                 if not is_series:"}
{"id": "spacy_7", "problem": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()", "fixed": " def filter_spans(spans):\n     spans (iterable): The spans to filter.\n     RETURNS (list): The filtered spans.\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()"}
{"id": "matplotlib_26", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "youtube-dl_39", "problem": " from ..utils import (\n     compat_urllib_parse,\n     compat_urllib_request,\n     urlencode_postdata,\n     ExtractorError,\n )", "fixed": " from ..utils import (\n     compat_urllib_parse,\n     compat_urllib_request,\n     urlencode_postdata,\n     ExtractorError,\n    limit_length,\n )"}
{"id": "pandas_70", "problem": "                cls = dtype.construct_array_type()\n                result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)", "fixed": "                if len(result) and isinstance(result[0], dtype.type):\n                    cls = dtype.construct_array_type()\n                    result = try_cast_to_ea(cls, result, dtype=dtype)\n             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:\n                 result = maybe_downcast_to_dtype(result, dtype)"}
{"id": "scrapy_3", "problem": " import logging\nfrom six.moves.urllib.parse import urljoin\n from w3lib.url import safe_url_string", "fixed": " import logging\nfrom six.moves.urllib.parse import urljoin, urlparse\n from w3lib.url import safe_url_string"}
{"id": "pandas_44", "problem": " from pandas._libs import NaT, Timedelta, index as libindex\nfrom pandas._typing import Label\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs import NaT, Timedelta, index as libindex\nfrom pandas._typing import DtypeObj, Label\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import ("}
{"id": "luigi_3", "problem": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except ValueError:\n            return literal_eval(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(ListParameter):\n         try:\n             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n        except (ValueError, TypeError):\n            return tuple(literal_eval(x))\n class NumericalParameter(Parameter):"}
{"id": "pandas_165", "problem": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n class TestAddSubNaTMasking:", "fixed": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n    def test_tda_add_sub_index(self):\n        tdi = TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\n        tda = tdi.array\n        dti = pd.date_range(\"1999-12-31\", periods=3, freq=\"D\")\n        result = tda + dti\n        expected = tdi + dti\n        tm.assert_index_equal(result, expected)\n        result = tda + tdi\n        expected = tdi + tdi\n        tm.assert_index_equal(result, expected)\n        result = tda - tdi\n        expected = tdi - tdi\n        tm.assert_index_equal(result, expected)\n class TestAddSubNaTMasking:"}
{"id": "keras_42", "problem": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.", "fixed": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training."}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "luigi_26", "problem": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar() or not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")", "fixed": " class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):\n             arglist.append('{}@{}'.format(username, host))\n         else:\n             arglist = []\n            if not job.jar():\n                raise HadoopJarJobError(\"Jar not defined\")\n            if not os.path.exists(job.jar()):\n                 logger.error(\"Can't find jar: %s, full path %s\", job.jar(), os.path.abspath(job.jar()))\n                 raise HadoopJarJobError(\"job jar does not exist\")"}
{"id": "pandas_41", "problem": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a a maybe different typed block.\n         Parameters\n         ----------", "fixed": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         Parameters\n         ----------"}
{"id": "pandas_25", "problem": " default 'raise'\n         from pandas import DataFrame\n        sarray = fields.build_isocalendar_sarray(self.asi8)\n         iso_calendar_df = DataFrame(\n             sarray, columns=[\"year\", \"week\", \"day\"], dtype=\"UInt32\"\n         )", "fixed": " default 'raise'\n         from pandas import DataFrame\n        if self.tz is not None and not timezones.is_utc(self.tz):\n            values = self._local_timestamps()\n        else:\n            values = self.asi8\n        sarray = fields.build_isocalendar_sarray(values)\n         iso_calendar_df = DataFrame(\n             sarray, columns=[\"year\", \"week\", \"day\"], dtype=\"UInt32\"\n         )"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        epsilon: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced.", "fixed": " class ReduceLROnPlateau(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n             to only focus on significant changes.\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced."}
{"id": "scrapy_33", "problem": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         extra={'spider': spider, 'failure': failure})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)", "fixed": " class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)"}
{"id": "PySnooper_2", "problem": " import functools\n import inspect\n import opcode\n import sys\n import re\n import collections", "fixed": " import functools\n import inspect\n import opcode\nimport os\n import sys\n import re\n import collections"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n         self.seed = seed", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n        self._manager = None\n         self.queue = None\n         self.seed = seed"}
{"id": "black_14", "problem": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            for import_from_child in first_child.children[3:]:\n                if isinstance(import_from_child, Leaf):\n                    if import_from_child.type == token.NAME:\n                        imports.add(import_from_child.value)\n                else:\n                    assert import_from_child.type == syms.import_as_names\n                    for leaf in import_from_child.children:\n                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:\n                            imports.add(leaf.value)\n         else:\n             break\n     return imports", "fixed": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            imports |= set(get_imports_from_children(first_child.children[3:]))\n         else:\n             break\n     return imports"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"id": "black_10", "problem": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char == ' ':\n                 current_column += 1\n            elif char == '\\t':\n                current_column += 4\n             elif char == '\\n':\n                 current_column = 0", "fixed": " class Driver(object):\n                     current_line = \"\"\n                     current_column = 0\n                     wait_for_nl = False\n            elif char in ' \\t':\n                 current_column += 1\n             elif char == '\\n':\n                 current_column = 0"}
{"id": "pandas_131", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.generic import ABCSeries\n from pandas.core.accessor import PandasDelegate, delegate_names\nfrom pandas.core.algorithms import take_1d\n from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\n from pandas.core.base import NoNewAttributesMixin, PandasObject\n from pandas.core.indexes.datetimes import DatetimeIndex", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.generic import ABCSeries\n from pandas.core.accessor import PandasDelegate, delegate_names\n from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray\n from pandas.core.base import NoNewAttributesMixin, PandasObject\n from pandas.core.indexes.datetimes import DatetimeIndex"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n         raise raise_on_incompatible(self, None)\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not isinstance(dtype, PeriodDtype):\n            return False\n        return dtype.freq == self.freq"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "scrapy_32", "problem": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(settings)\n        log_scrapy_info(settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)", "fixed": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings)\n        log_scrapy_info(self.settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "matplotlib_3", "problem": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = True\n         self._marker_function()\n     def __bool__(self):", "fixed": " class MarkerStyle:\n         self._snap_threshold = None\n         self._joinstyle = 'round'\n         self._capstyle = 'butt'\n        self._filled = self._fillstyle != 'none'\n         self._marker_function()\n     def __bool__(self):"}
{"id": "luigi_23", "problem": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active\nself.started = time.time()\nself.tasks = set()\n         self.info = {}", "fixed": " class Worker(object):\n     def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\n        self.last_active = last_active or time.time()\nself.started = time.time()\nself.tasks = set()\n         self.info = {}"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result\n def logical_op(", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n                     f\"and scalar of type [{typ}]\"\n                 )\n    return result.reshape(x.shape)\n def logical_op("}
{"id": "youtube-dl_40", "problem": " import base64\n import io\n import itertools\n import os\nfrom struct import unpack, pack\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,", "fixed": " import base64\n import io\n import itertools\n import os\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n    struct_pack,\n    struct_unpack,\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,"}
{"id": "youtube-dl_31", "problem": " def parse_duration(s):\n     m = re.match(", "fixed": " def parse_duration(s):\n     m = re.match(\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?"}
{"id": "keras_29", "problem": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:", "fixed": " class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:"}
{"id": "ansible_14", "problem": " from ansible.module_utils.urls import open_url\n from ansible.utils.display import Display\n from ansible.utils.hashing import secure_hash_s\n display = Display()", "fixed": " from ansible.module_utils.urls import open_url\n from ansible.utils.display import Display\n from ansible.utils.hashing import secure_hash_s\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n display = Display()"}
{"id": "thefuck_17", "problem": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)", "fixed": " class Zsh(Generic):\n     @memoize\n     def get_aliases(self):\n        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\\n')\n         return dict(self._parse_alias(alias)\n                     for alias in raw_aliases if alias and '=' in alias)"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "pandas_40", "problem": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution", "fixed": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union, cast\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import ArrayLike, FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution"}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 48,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 48\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '23', 'POWER8 (architected), altivec supported',\n             ],\n             'processor_cores': 1,\n            'processor_count': 24,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 24\n         },\n     },\n     {"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):", "fixed": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "pandas_71", "problem": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_integer,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,", "fixed": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n    is_extension_array_dtype,\n     is_integer,\n    is_integer_dtype,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "luigi_24", "problem": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '\"{0}={1}\"'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):", "fixed": " class SparkSubmitTask(luigi.Task):\n         command = []\n         if value and isinstance(value, dict):\n             for prop, value in value.items():\n                command += [name, '{0}={1}'.format(prop, value)]\n         return command\n     def _flag_arg(self, name, value):"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                if responses is None:\n                    responses = {}\n                responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,", "fixed": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        if responses is None:\n            responses = {}\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n        if not self.trainable:\n            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "tornado_12", "problem": " import hmac\n import time\n import uuid\nfrom tornado.concurrent import TracebackFuture, return_future\n from tornado import gen\n from tornado import httpclient\n from tornado import escape", "fixed": " import hmac\n import time\n import uuid\nfrom tornado.concurrent import TracebackFuture, return_future, chain_future\n from tornado import gen\n from tornado import httpclient\n from tornado import escape"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "thefuck_8", "problem": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read()\n     return _parse_operations(lines)", "fixed": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read().decode(\"utf-8\")\n     return _parse_operations(lines)"}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             ser.at[0]\n        with pytest.raises(TypeError, match=msg):\n             ser.loc[0]\n    def test_frame_raises_type_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]", "fixed": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.at[0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.loc[0]\n    def test_frame_raises_key_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]"}
{"id": "pandas_68", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n    ABCExtensionArray,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,"}
{"id": "pandas_146", "problem": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if np.any(left_value != right_value):\n                    return False\n         return True", "fixed": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                try:\n                    if np.any(left_value != right_value):\n                        return False\n                except TypeError as err:\n                    if \"Cannot compare tz-naive\" in str(err):\n                        return False\n                    raise\n         return True"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "scrapy_30", "problem": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()\n        return comm[0].strip()\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\", "fixed": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "pandas_70", "problem": " def test_resample_integerarray():\n     result = ts.resample(\"3T\").mean()\n     expected = Series(\n        [1, 4, 7], index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"), dtype=\"Int64\"\n     )\n     tm.assert_series_equal(result, expected)", "fixed": " def test_resample_integerarray():\n     result = ts.resample(\"3T\").mean()\n     expected = Series(\n        [1, 4, 7],\n        index=pd.date_range(\"1/1/2000\", periods=3, freq=\"3T\"),\n        dtype=\"float64\",\n     )\n     tm.assert_series_equal(result, expected)"}
{"id": "luigi_6", "problem": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x))\n         except ValueError:\nreturn literal_eval(x)\n    def serialize(self, x):\n        return json.dumps(x)\n class NumericalParameter(Parameter):", "fixed": " class TupleParameter(Parameter):\n         try:\n            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))\n         except ValueError:\nreturn literal_eval(x)\n class NumericalParameter(Parameter):"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "pandas_131", "problem": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n            result = take_1d(result, self.orig.cat.codes)\n             index = self.orig.index\n         else:\n             index = self._parent.index", "fixed": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n             index = self.orig.index\n         else:\n             index = self._parent.index"}
{"id": "spacy_8", "problem": " class Errors(object):\n\"details: https:\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n @add_codes", "fixed": " class Errors(object):\n\"details: https:\n     E174 = (\"Architecture '{name}' not found in registry. Available \"\n             \"names: {names}\")\n    E175 = (\"Can't remove rule for unknown match pattern ID: {key}\")\n @add_codes"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "pandas_41", "problem": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values", "fixed": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values"}
{"id": "luigi_33", "problem": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if p.significant]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))", "fixed": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if not p.is_global]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))"}
{"id": "matplotlib_9", "problem": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin", "fixed": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n        self._unstale_viewLim()\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin"}
{"id": "youtube-dl_37", "problem": " import calendar\n import contextlib\n import ctypes\n import datetime", "fixed": " import calendar\nimport codecs\n import contextlib\n import ctypes\n import datetime"}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"id": "fastapi_1", "problem": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))", "fixed": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n         else:\n             output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output", "fixed": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n         else:\n             output = None\n    else:\n        output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):", "fixed": " def assert_series_equal(\n             check_dtype=check_dtype,\n             obj=str(obj),\n         )\n    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):\n         assert_extension_array_equal(left._values, right._values)\n     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):"}
{"id": "black_22", "problem": " class Line:\n             self.bracket_tracker.mark(leaf)\n             self.maybe_remove_trailing_comma(leaf)\n             self.maybe_increment_for_loop_variable(leaf)\n            if self.maybe_adapt_standalone_comment(leaf):\n                return\n         if not self.append_comment(leaf):\n             self.leaves.append(leaf)\n     @property\n     def is_comment(self) -> bool:", "fixed": " class Line:\n             self.bracket_tracker.mark(leaf)\n             self.maybe_remove_trailing_comma(leaf)\n             self.maybe_increment_for_loop_variable(leaf)\n         if not self.append_comment(leaf):\n             self.leaves.append(leaf)\n    def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:\n        if self.bracket_tracker.depth == 0:\n            if self.is_comment:\n                raise ValueError(\"cannot append to standalone comments\")\n            if self.leaves and leaf.type == STANDALONE_COMMENT:\n                raise ValueError(\n                    \"cannot append standalone comments to a populated line\"\n                )\n        self.append(leaf, preformatted=preformatted)\n     @property\n     def is_comment(self) -> bool:"}
{"id": "ansible_4", "problem": " __metaclass__ = type\n from ansible.module_utils.six import string_types\n from ansible.playbook.attribute import FieldAttribute\n from ansible.utils.collection_loader import AnsibleCollectionLoader\n def _ensure_default_collection(collection_list=None):", "fixed": " __metaclass__ = type\n from ansible.module_utils.six import string_types\n from ansible.playbook.attribute import FieldAttribute\n from ansible.utils.collection_loader import AnsibleCollectionLoader\nfrom ansible.template import is_template, Environment\nfrom ansible.utils.display import Display\ndisplay = Display()\n def _ensure_default_collection(collection_list=None):"}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n        elif isinstance(other, int):\n            other = Period(other, freq=self.freq)\n            result = ordinal_op(other.ordinal)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)", "fixed": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)"}
{"id": "pandas_114", "problem": " class Index(IndexOpsMixin, PandasObject):\n        s = getattr(series, \"_values\", series)\n        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):\n            try:\n                iloc = self.get_loc(key)\n                return s[iloc]\n            except KeyError:\n                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                    raise\n                elif is_integer(key):\n                    return s[key]\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n        s = extract_array(series, extract_numpy=True)\n        if isinstance(s, ExtensionArray):\n            if is_scalar(key):\n                try:\n                    iloc = self.get_loc(key)\n                    return s[iloc]\n                except KeyError:\n                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):\n                        raise\n                    elif is_integer(key):\n                        return s[key]\n            else:\n                raise InvalidIndexError(key)\n         s = com.values_from_object(series)\n         k = com.values_from_object(key)"}
{"id": "fastapi_16", "problem": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        if not obj.Config.json_encoders:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n            )\n        else:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n                custom_encoder=obj.Config.json_encoders,\n            )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):", "fixed": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        encoder = getattr(obj.Config, \"json_encoders\", custom_encoder)\n        return jsonable_encoder(\n            obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n            include_none=include_none,\n            custom_encoder=encoder,\n        )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n         elif (\n             is_datetime64_any_dtype(data)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n         elif (\n             is_datetime64_any_dtype(data)"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "ansible_8", "problem": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = []\n        for arg in args:\n            arg = self._unquote(arg).replace('/', '\\\\')\n            parts.extend([a for a in arg.split('\\\\') if a])\n        path = '\\\\'.join(parts)\n        if path.startswith('~'):\n            return path\n        return path\n     def get_remote_filename(self, pathname):", "fixed": " class ShellModule(ShellBase):\n         return \"\"\n     def join_path(self, *args):\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n     def get_remote_filename(self, pathname):"}
{"id": "youtube-dl_30", "problem": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f", "fixed": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                    formats = list(formats)\n                    if not formats:\n                        return\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f"}
{"id": "pandas_94", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        freq = self.freq if is_period_dtype(self) else None\n        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):\n     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n     def repeat(self, repeats, axis=None):\n         nv.validate_repeat(tuple(), dict(axis=axis))\n        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)\n        return self._shallow_copy(result)\n     @Appender(_index_shared_docs[\"where\"] % _index_doc_kwargs)\n     def where(self, cond, other=None):"}
{"id": "fastapi_3", "problem": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        if exclude_unset and isinstance(response_content, BaseModel):\n            if PYDANTIC_1:\n                response_content = response_content.dict(exclude_unset=exclude_unset)\n            else:\n                response_content = response_content.dict(\n                    skip_defaults=exclude_unset\n                )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:", "fixed": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n        )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:"}
{"id": "black_5", "problem": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type == syms.arglist:\n                     commas += 1\n                     break", "fixed": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type in {\n                    syms.arglist,\n                    syms.typedargslist,\n                }:\n                     commas += 1\n                     break"}
{"id": "pandas_12", "problem": " from pandas.core.dtypes.cast import (\n     validate_numeric_casting,\n )\n from pandas.core.dtypes.common import (\n    ensure_float64,\n     ensure_int64,\n     ensure_platform_int,\n     infer_dtype_from_object,", "fixed": " from pandas.core.dtypes.cast import (\n     validate_numeric_casting,\n )\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     ensure_platform_int,\n     infer_dtype_from_object,"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(description_original, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n                     errnote='Unable to download video annotations', fatal=False,\n                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)\n         if self._downloader.params.get('youtube_include_dash_manifest', True):"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)\n         yield current_line", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        yield current_line\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n    if current_line:\n         yield current_line"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, extra={'spider': info.spider, 'failure': f})\n         )\nreturn dfd.addBoth(lambda _: wad)", "fixed": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\nreturn dfd.addBoth(lambda _: wad)"}
{"id": "keras_40", "problem": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            output_dim = self.cell.state_size[0]\n         else:\n            output_dim = self.cell.state_size\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            state_size = self.cell.state_size\n         else:\n            state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "httpie_3", "problem": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue", "fixed": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n            if value is None:\n                continue\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\nif \"\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\nif \"\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "keras_29", "problem": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:", "fixed": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:"}
{"id": "youtube-dl_38", "problem": " class FacebookIE(InfoExtractor):\n         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)\n         login_page_req.add_header('Cookie', 'locale=en_US')\n        self.report_login()\n        login_page = self._download_webpage(login_page_req, None, note=False,\n             errnote='Unable to download login page')\n         lsd = self._search_regex(\n             r'<input type=\"hidden\" name=\"lsd\" value=\"([^\"]*)\"',", "fixed": " class FacebookIE(InfoExtractor):\n         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)\n         login_page_req.add_header('Cookie', 'locale=en_US')\n        login_page = self._download_webpage(login_page_req, None,\n            note='Downloading login page',\n             errnote='Unable to download login page')\n         lsd = self._search_regex(\n             r'<input type=\"hidden\" name=\"lsd\" value=\"([^\"]*)\"',"}
{"id": "pandas_155", "problem": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            return self.obj.index\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value):\n         return isinstance(value, self._holder)\n    def set(self, locs, values, check=False):\n         assert locs.tolist() == [0]\n        self.values = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,", "fixed": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value: ArrayLike) -> bool:\n         return isinstance(value, self._holder)\n    def set(self, locs, values):\n         assert locs.tolist() == [0]\n        self.values[:] = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,"}
{"id": "youtube-dl_25", "problem": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^0[xX][0-9a-fA-F]+', 16),\n            (r'^0+[0-7]+', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(0), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v", "fixed": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^(0[xX][0-9a-fA-F]+)\\s*:?$', 16),\n            (r'^(0+[0-7]+)\\s*:?$', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(1), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v"}
{"id": "black_6", "problem": " from . import grammar, parse, token, tokenize, pgen\n class Driver(object):\n    def __init__(self, grammar, convert=None, logger=None):\n         self.grammar = grammar\n         if logger is None:\n             logger = logging.getLogger(__name__)\n         self.logger = logger\n         self.convert = convert\n     def parse_tokens(self, tokens, debug=False):", "fixed": " from . import grammar, parse, token, tokenize, pgen\n class Driver(object):\n    def __init__(\n        self,\n        grammar,\n        convert=None,\n        logger=None,\n        tokenizer_config=tokenize.TokenizerConfig(),\n    ):\n         self.grammar = grammar\n         if logger is None:\n             logger = logging.getLogger(__name__)\n         self.logger = logger\n         self.convert = convert\n        self.tokenizer_config = tokenizer_config\n     def parse_tokens(self, tokens, debug=False):"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "black_22", "problem": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for comment in self.comments.values():\n             res += str(comment)\n         return res + '\\n'", "fixed": " class Line:\n         res = f'{first.prefix}{indent}{first.value}'\n         for leaf in leaves:\n             res += str(leaf)\n        for _, comment in self.comments:\n             res += str(comment)\n         return res + '\\n'"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "scrapy_5", "problem": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,", "fixed": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "pandas_123", "problem": " class RangeIndex(Int64Index):\n    @staticmethod\n    def _validate_dtype(dtype):", "fixed": " class RangeIndex(Int64Index):"}
{"id": "pandas_112", "problem": " import re\n import numpy as np\n import pytest\nfrom pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm", "fixed": " import re\n import numpy as np\n import pytest\nfrom pandas import (\n    CategoricalIndex,\n    Interval,\n    IntervalIndex,\n    Timedelta,\n    date_range,\n    timedelta_range,\n)\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm"}
{"id": "keras_16", "problem": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n         if layers:", "fixed": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n        self._build_input_shape = None\n         if layers:"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "pandas_112", "problem": " from pandas.core.dtypes.cast import (\n )\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_dtype_equal,", "fixed": " from pandas.core.dtypes.cast import (\n )\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n    is_categorical,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_dtype_equal,"}
{"id": "youtube-dl_28", "problem": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        return compat_chr(int(numstr, base))\n     return ('&%s;' % entity)", "fixed": " def _htmlentity_transform(entity):\n             numstr = '0%s' % numstr\n         else:\n             base = 10\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass\n     return ('&%s;' % entity)"}
{"id": "luigi_2", "problem": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n     @abstractmethod\n     def dataflow_executable(self):", "fixed": " class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):\n     def __init__(self):\n         if not isinstance(self.dataflow_params, DataflowParamKeys):\n             raise ValueError(\"dataflow_params must be of type DataflowParamKeys\")\n        super(BeamDataflowJobTask, self).__init__()\n     @abstractmethod\n     def dataflow_executable(self):"}
{"id": "youtube-dl_40", "problem": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''", "fixed": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return struct_unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return struct_unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return struct_unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "pandas_2", "problem": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n        key = list(self._convert_key(key, is_setter=True))\n         self.obj._set_value(*key, value=value, takeable=self._takeable)", "fixed": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key, is_setter=True))\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n         self.obj._set_value(*key, value=value, takeable=self._takeable)"}
{"id": "scrapy_21", "problem": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        self._parsers.pop(netloc).callback(None)", "fixed": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)"}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "cookiecutter_2", "problem": " def run_hook(hook_name, project_dir, context):\n     :param project_dir: The directory to execute the script from.\n     :param context: Cookiecutter project context.\n    script = find_hook(hook_name)\n    if script is None:\n         logger.debug('No %s hook found', hook_name)\n         return\n     logger.debug('Running hook %s', hook_name)\n    run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     :param project_dir: The directory to execute the script from.\n     :param context: Cookiecutter project context.\n    scripts = find_hook(hook_name)\n    if not scripts:\n         logger.debug('No %s hook found', hook_name)\n         return\n     logger.debug('Running hook %s', hook_name)\n    for script in scripts:\n        run_script_with_context(script, project_dir, context)"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "cookiecutter_4", "problem": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.", "fixed": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.\n    Raised when a hook script fails"}
{"id": "pandas_2", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n         if is_setter:\n             return list(key)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)\n         if is_setter:\n             return list(key)"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()\n    def visit_unformatted(self, node: LN) -> Iterator[Line]:", "fixed": " class LineGenerator(Visitor[Line]):\n         yield from self.visit_default(leaf)\n         yield from self.line()"}
{"id": "ansible_16", "problem": " class LinuxHardware(Hardware):\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):\n             i = processor_occurence", "fixed": " class LinuxHardware(Hardware):\n        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n             i = processor_occurence"}
{"id": "pandas_108", "problem": " from .common import (\n     is_unsigned_integer_dtype,\n     pandas_dtype,\n )\nfrom .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype\n from .generic import (\n     ABCDataFrame,\n     ABCDatetimeArray,", "fixed": " from .common import (\n     is_unsigned_integer_dtype,\n     pandas_dtype,\n )\nfrom .dtypes import DatetimeTZDtype, ExtensionDtype, IntervalDtype, PeriodDtype\n from .generic import (\n     ABCDataFrame,\n     ABCDatetimeArray,"}
{"id": "keras_2", "problem": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)", "fixed": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\ndef in_top_k(predictions, targets, k):\n    top_k = np.argsort(-predictions)[:, :k]\n    targets = targets.reshape(-1, 1)\n    return np.any(targets == top_k, axis=-1)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)"}
{"id": "black_22", "problem": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: Dict[LeafID, Leaf] = Factory(dict)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False", "fixed": " class Line:\n     depth: int = 0\n     leaves: List[Leaf] = Factory(list)\n    comments: List[Tuple[Index, Leaf]] = Factory(list)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n     has_for: bool = False"}
{"id": "pandas_123", "problem": " from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n     ensure_python_int,\n    is_int64_dtype,\n     is_integer,\n     is_integer_dtype,\n     is_list_like,", "fixed": " from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n     ensure_python_int,\n     is_integer,\n     is_integer_dtype,\n     is_list_like,"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "youtube-dl_40", "problem": " import platform\n import re\n import ssl\n import socket\n import subprocess\n import sys\n import traceback", "fixed": " import platform\n import re\n import ssl\n import socket\nimport struct\n import subprocess\n import sys\n import traceback"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "matplotlib_1", "problem": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n    if draw_disabled:\n        for meth_name in dir(RendererBase):\n            if (meth_name.startswith(\"draw_\")\n                    or meth_name in [\"open_group\", \"close_group\"]):\n                setattr(renderer, meth_name, lambda *args, **kwargs: None)\n     return renderer", "fixed": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n     return renderer"}
{"id": "youtube-dl_42", "problem": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n'http:\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_all_ampersand) \n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):", "fixed": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n'http:\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_ampersands)\n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,", "fixed": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n            dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,"}
{"id": "thefuck_1", "problem": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \\\"([a-z]+)\\\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)", "fixed": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "black_1", "problem": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        executor.shutdown()\n async def schedule_formatting(", "fixed": " def reformat_many(\n         )\n     finally:\n         shutdown(loop)\n        if executor is not None:\n            executor.shutdown()\n async def schedule_formatting("}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )", "fixed": " class APIRoute(routing.Route):\n             response_model_exclude=self.response_model_exclude,\n             response_model_by_alias=self.response_model_by_alias,\n             response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n             dependency_overrides_provider=self.dependency_overrides_provider,\n         )"}
{"id": "pandas_64", "problem": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df\n         self.columns = self.df.columns\n         self.float_format = float_format", "fixed": " class ExcelFormatter:\n                 raise KeyError(\"Not all names specified in 'columns' are found\")\n            self.df = df.reindex(columns=cols)\n         self.columns = self.df.columns\n         self.float_format = float_format"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)"}
{"id": "keras_1", "problem": " def update(x, new_x):\n         The variable `x` updated.\n    return tf_state_ops.assign(x, new_x)\n @symbolic", "fixed": " def update(x, new_x):\n         The variable `x` updated.\n    op = tf_state_ops.assign(x, new_x)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "black_14", "problem": "from __future__ import unicode_literals\n u'hello'\n U\"hello\"", "fixed": "from __future__ import unicode_literals as _unicode_literals\nfrom __future__ import absolute_import\nfrom __future__ import print_function as lol, with_function\n u'hello'\n U\"hello\""}
{"id": "pandas_6", "problem": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):", "fixed": " def get_grouper(\n             return False\n         try:\n             return gpr is obj[gpr.name]\n        except (KeyError, IndexError, ValueError):\n             return False\n     for i, (gpr, level) in enumerate(zip(keys, levels)):"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n     def __init__(self,", "fixed": " class EarlyStopping(Callback):\n         baseline: Baseline value for the monitored quantity to reach.\n             Training will stop if the model doesn't show improvement\n             over the baseline.\n        restore_best_weights: whether to restore model weights from\n            the epoch with the best value of the monitored quantity.\n            If False, the model weights obtained at the last step of\n            training are used.\n     def __init__(self,"}
{"id": "keras_1", "problem": " def get_variable_shape(x):\n     return int_shape(x)\n def print_tensor(x, message=''):", "fixed": " def get_variable_shape(x):\n     return int_shape(x)\n@symbolic\n def print_tensor(x, message=''):"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.non_trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def non_trainable_weights(self):\n         if isinstance(self.cell, Layer):\n            if not self.trainable:\n                return self.cell.weights\n             return self.cell.non_trainable_weights\n         return []"}
{"id": "tornado_16", "problem": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n         if self._finished:\n             self._return_result(self._finished.popleft())", "fixed": " class WaitIterator(object):\n         the inputs.\n         self._running_future = TracebackFuture()\n        self._running_future.add_done_callback(lambda f: self)\n         if self._finished:\n             self._return_result(self._finished.popleft())"}
{"id": "pandas_65", "problem": " def get_handle(\n     try:\n         from s3fs import S3File\n        need_text_wrapping = (BufferedIOBase, S3File)\n     except ImportError:\n        need_text_wrapping = BufferedIOBase\n     handles: List[IO] = list()\n     f = path_or_buf", "fixed": " def get_handle(\n     try:\n         from s3fs import S3File\n        need_text_wrapping = (BufferedIOBase, RawIOBase, S3File)\n     except ImportError:\n        need_text_wrapping = (BufferedIOBase, RawIOBase)\n     handles: List[IO] = list()\n     f = path_or_buf"}
{"id": "pandas_48", "problem": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except ValueError:\n                         result = result.reshape(1, -1)", "fixed": " class DataFrameGroupBy(GroupBy):\n                         result = type(block.values)._from_sequence(\n                             result.ravel(), dtype=block.values.dtype\n                         )\n                    except (ValueError, TypeError):\n                         result = result.reshape(1, -1)"}
{"id": "scrapy_33", "problem": " from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\nfrom scrapy.utils.log import logformatter_adapter\n logger = logging.getLogger(__name__)", "fixed": " from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\nfrom scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "luigi_29", "problem": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n            if cls.run == NotImplemented:\n                continue\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\", "fixed": " class Register(abc.ABCMeta):\n         reg = OrderedDict()\n         for cls in cls._reg:\n             name = cls.task_family\n             if name in reg and reg[name] != cls and \\"}
{"id": "pandas_92", "problem": " class TimeGrouper(Grouper):\n         rng += freq_mult\n         rng -= bin_shift\n        bins = memb.searchsorted(rng, side=\"left\")\n         if nat_count > 0:", "fixed": " class TimeGrouper(Grouper):\n         rng += freq_mult\n         rng -= bin_shift\n        prng = type(memb._data)(rng, dtype=memb.dtype)\n        bins = memb.searchsorted(prng, side=\"left\")\n         if nat_count > 0:"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "scrapy_33", "problem": " from six.moves.urllib import robotparser\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n logger = logging.getLogger(__name__)", "fixed": " from six.moves.urllib import robotparser\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "pandas_86", "problem": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None", "fixed": " def _convert_by(by):\n @Substitution(\"\\ndata : DataFrame\")\n @Appender(_shared_docs[\"pivot\"], indents=1)\n def pivot(data: \"DataFrame\", index=None, columns=None, values=None) -> \"DataFrame\":\n    if columns is None:\n        raise TypeError(\"pivot() missing 1 required argument: 'columns'\")\n     if values is None:\n         cols = [columns] if index is None else [index, columns]\n         append = index is None"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    include_none: bool = True,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:", "fixed": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters,\n                                                        filter_dilation=dilation_rate)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):", "fixed": " class StackedRNNCells(Layer):\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "scrapy_33", "problem": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n         return failure\n     dont_log = named.pop('dont_log', None)", "fixed": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n         return failure\n     dont_log = named.pop('dont_log', None)"}
{"id": "fastapi_3", "problem": "except ImportError:\nfrom pydantic.fields import Field as ModelField\n async def serialize_response(\n     *,\n     field: ModelField = None,", "fixed": "except ImportError:\nfrom pydantic.fields import Field as ModelField\ndef _prepare_response_content(\n    res: Any, *, by_alias: bool = True, exclude_unset: bool\n) -> Any:\n    if isinstance(res, BaseModel):\n        if PYDANTIC_1:\n            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)\n        else:\n            return res.dict(\n                by_alias=by_alias, skip_defaults=exclude_unset\n            )\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(v, exclude_unset=exclude_unset)\n            for k, v in res.items()\n        }\n    return res\n async def serialize_response(\n     *,\n     field: ModelField = None,"}
{"id": "pandas_153", "problem": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            values = values.astype(str)\n         else:\n             values = np.array(values, dtype=\"object\")", "fixed": " class Block(PandasObject):\n         mask = isna(values)\n         if not self.is_object and not quoting:\n            itemsize = writers.word_len(na_rep)\n            values = values.astype(\"<U{size}\".format(size=itemsize))\n         else:\n             values = np.array(values, dtype=\"object\")"}
{"id": "youtube-dl_17", "problem": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]", "fixed": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n    if param is None:\n        return []\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)"}
{"id": "tornado_1", "problem": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__(", "fixed": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__("}
{"id": "pandas_41", "problem": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )", "fixed": " class BoolBlock(NumericBlock):\n             return issubclass(tipo.type, np.bool_)\n         return isinstance(element, (bool, np.bool_))\n    def should_store(self, value: ArrayLike) -> bool:\n         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(\n             value\n         )"}
{"id": "pandas_70", "problem": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})", "fixed": " def test_aggregate_mixed_types():\n     tm.assert_frame_equal(result, expected)\n@pytest.mark.xfail(reason=\"Not implemented.\")\ndef test_aggregate_udf_na_extension_type():\n    def aggfunc(x):\n        if all(x > 2):\n            return 1\n        else:\n            return pd.NA\n    df = pd.DataFrame({\"A\": pd.array([1, 2, 3])})\n    result = df.groupby([1, 1, 2]).agg(aggfunc)\n    expected = pd.DataFrame({\"A\": pd.array([1, pd.NA], dtype=\"Int64\")}, index=[1, 2])\n    tm.assert_frame_equal(result, expected)\n class TestLambdaMangling:\n     def test_basic(self):\n         df = pd.DataFrame({\"A\": [0, 0, 1, 1], \"B\": [1, 2, 3, 4]})"}
{"id": "pandas_44", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if isinstance(target, PeriodIndex):\n            if target.freq != self.freq:\n                no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n                return no_matches, no_matches\n            target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     def get_indexer_non_unique(self, target):\n         target = ensure_index(target)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        target = target.asi8\n         indexer, missing = self._int64index.get_indexer_non_unique(target)\n         return ensure_platform_int(indexer), missing"}
{"id": "scrapy_25", "problem": " from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n class FormRequest(Request):", "fixed": " from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\nfrom scrapy.utils.response import get_base_url\n class FormRequest(Request):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "scrapy_12", "problem": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n         if _root is not None:", "fixed": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n         st = _st(response, type or self._default_type)\n         if _root is not None:"}
{"id": "pandas_138", "problem": " from pandas._libs.lib import infer_dtype\n from pandas.core.dtypes.common import (\n     _NS_DTYPE,\n     ensure_int64,\n     is_categorical_dtype,\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,", "fixed": " from pandas._libs.lib import infer_dtype\n from pandas.core.dtypes.common import (\n     _NS_DTYPE,\n     ensure_int64,\n    is_bool_dtype,\n     is_categorical_dtype,\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n         if fill_value is None:", "fixed": " class DataFrame(NDFrame):\n    def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n         if fill_value is None:"}
{"id": "sanic_3", "problem": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:", "fixed": " class Sanic:\n         netloc = kwargs.pop(\"_server\", None)\n         if netloc is None and external:\n            netloc = host or self.config.get(\"SERVER_NAME\", \"\")\n         if external:\n             if not scheme:"}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "pandas_42", "problem": " def assert_series_equal(\n                 f\"is not equal to {right._values}.\"\n             )\n             raise AssertionError(msg)\n    elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):\n         assert_interval_array_equal(left.array, right.array)\n     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):\n         _testing.assert_almost_equal(", "fixed": " def assert_series_equal(\n                 f\"is not equal to {right._values}.\"\n             )\n             raise AssertionError(msg)\n    elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):\n         assert_interval_array_equal(left.array, right.array)\n     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):\n         _testing.assert_almost_equal("}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n     obj=\"Series\",\n ):", "fixed": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n    check_category_order=True,\n     obj=\"Series\",\n ):"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output: Series or DataFrame\n             Object resulting from grouping and applying an operation.\n         Returns\n         -------", "fixed": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output : Series or DataFrame\n             Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n         Returns\n         -------"}
{"id": "black_8", "problem": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                if leaves[-1].type != token.COMMA:\n                    leaves.append(Leaf(token.COMMA, \",\"))\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)", "fixed": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                for i in range(len(leaves) - 1, -1, -1):\n                    if leaves[i].type == STANDALONE_COMMENT:\n                        continue\n                    elif leaves[i].type == token.COMMA:\n                        break\n                    else:\n                        leaves.insert(i + 1, Leaf(token.COMMA, \",\"))\n                        break\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "black_22", "problem": " Depth = int\n NodeType = int\n LeafID = int\n Priority = int\n LN = Union[Leaf, Node]\n out = partial(click.secho, bold=True, err=True)\n err = partial(click.secho, fg='red', err=True)", "fixed": " Depth = int\n NodeType = int\n LeafID = int\n Priority = int\nIndex = int\n LN = Union[Leaf, Node]\nSplitFunc = Callable[['Line', bool], Iterator['Line']]\n out = partial(click.secho, bold=True, err=True)\n err = partial(click.secho, fg='red', err=True)"}
{"id": "pandas_59", "problem": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other)\n         def _get_corr(a, b):\n             a = a.rolling(", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             pairwise = True if pairwise is None else pairwise\n         other = self._shallow_copy(other)\n        window = self._get_window(other) if not self.is_freq_type else self.win_freq\n         def _get_corr(a, b):\n             a = a.rolling("}
{"id": "scrapy_15", "problem": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),", "fixed": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(netloc),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),"}
{"id": "pandas_54", "problem": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n         elif is_categorical(values):", "fixed": " class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):\n                 raise ValueError(\n                     \"Cannot specify `categories` or `ordered` together with `dtype`.\"\n                 )\n            elif not isinstance(dtype, CategoricalDtype):\n                raise ValueError(f\"Cannot not construct CategoricalDtype from {dtype}\")\n         elif is_categorical(values):"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        include_none=include_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )", "fixed": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             while steps_done < steps:\n                 generator_output = next(output_generator)"}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"id": "tornado_3", "problem": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            if self._instance_cache.get(self.io_loop) is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n            del self._instance_cache[self.io_loop]\n     def fetch(\n         self,", "fixed": " class AsyncHTTPClient(Configurable):\n             return\n         self._closed = True\n         if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            if cached_val is not None and cached_val is not self:\n                 raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")\n     def fetch(\n         self,"}
{"id": "tqdm_9", "problem": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable)\n     def __iter__(self):", "fixed": " class tqdm(object):\n         self.n = 0\n     def __len__(self):\n        return len(self.iterable) if self.iterable else self.total\n     def __iter__(self):"}
{"id": "matplotlib_1", "problem": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\n _log = logging.getLogger(__name__)", "fixed": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\nfrom matplotlib.cbook import _setattr_cm\n _log = logging.getLogger(__name__)"}
{"id": "pandas_110", "problem": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not self.is_integer()\n         if kind == \"getitem\":", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         is_null_slicer = start is None and stop is None\n         is_index_slice = is_int(start) and is_int(stop)\n        is_positional = is_index_slice and not (\n            self.is_integer() or self.is_categorical()\n        )\n         if kind == \"getitem\":"}
{"id": "PySnooper_1", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False"}
{"id": "pandas_120", "problem": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        return Series(res, index=ri, name=self._selection_name)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):", "fixed": " class SeriesGroupBy(GroupBy):\n             res, out = np.zeros(len(ri), dtype=out.dtype), res\n             res[ids[idx]] = out\n        result = Series(res, index=ri, name=self._selection_name)\n        return self._reindex_output(result, fill_value=0)\n     @Appender(Series.describe.__doc__)\n     def describe(self, **kwargs):"}
{"id": "youtube-dl_38", "problem": " from ..utils import (\n     compat_urllib_error,\n     compat_urllib_parse,\n     compat_urllib_request,\n     ExtractorError,\n )", "fixed": " from ..utils import (\n     compat_urllib_error,\n     compat_urllib_parse,\n     compat_urllib_request,\n    urlencode_postdata,\n     ExtractorError,\n )"}
{"id": "black_6", "problem": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: set(),\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,", "fixed": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n    ASYNC_IS_VALID_IDENTIFIER = 6\n    ASYNC_IS_RESERVED_KEYWORD = 7\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY35: {\n        Feature.UNICODE_LITERALS,\n        Feature.TRAILING_COMMA_IN_CALL,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n    },\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "tqdm_6", "problem": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else self.total)\n     def __enter__(self):\n         return self", "fixed": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else getattr(self, \"total\", None))\n     def __enter__(self):\n         return self"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "scrapy_33", "problem": " from twisted.python.failure import Failure\n from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n     getAllReceivers, disconnect\n from scrapy.xlib.pydispatch.robustapply import robustApply\n logger = logging.getLogger(__name__)", "fixed": " from twisted.python.failure import Failure\n from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n     getAllReceivers, disconnect\n from scrapy.xlib.pydispatch.robustapply import robustApply\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             else:\n                 return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            return PeriodIndex(data, copy=copy, name=name, **kwargs)\n         elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             else:\n                 return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n        elif is_period_dtype(data) or is_period_dtype(dtype):\n            if is_dtype_equal(_o_dtype, dtype):\n                return PeriodIndex(data, copy=False, name=name, **kwargs).astype(object)\n            return PeriodIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n         elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n             )\nelse:\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,", "fixed": " def jsonable_encoder(\n                 exclude=exclude,\n                 by_alias=by_alias,\n                 exclude_unset=bool(exclude_unset or skip_defaults),\n                exclude_none=exclude_none,\n                exclude_defaults=exclude_defaults,\n             )\nelse:\n            if exclude_defaults:\n                raise ValueError(\"Cannot use exclude_defaults\")\n             obj_dict = obj.dict(\n                 include=include,\n                 exclude=exclude,"}
{"id": "pandas_148", "problem": " class FrameApply:\n         from pandas import Series\n         if not should_reduce:\n            EMPTY_SERIES = Series([])\n             try:\n                r = self.f(EMPTY_SERIES, *self.args, **self.kwds)\n             except Exception:\n                 pass\n             else:\n                 should_reduce = not isinstance(r, Series)\n         if should_reduce:\n            return self.obj._constructor_sliced(np.nan, index=self.agg_axis)\n         else:\n             return self.obj.copy()", "fixed": " class FrameApply:\n         from pandas import Series\n         if not should_reduce:\n             try:\n                r = self.f(Series([]))\n             except Exception:\n                 pass\n             else:\n                 should_reduce = not isinstance(r, Series)\n         if should_reduce:\n            if len(self.agg_axis):\n                r = self.f(Series([]))\n            else:\n                r = np.nan\n            return self.obj._constructor_sliced(r, index=self.agg_axis)\n         else:\n             return self.obj.copy()"}
{"id": "pandas_78", "problem": " Wild         185.0\n                     result = coerce_to_dtypes(result, self.dtypes)\n         if constructor is not None:\n            result = Series(result, index=labels)\n         return result\n     def nunique(self, axis=0, dropna=True) -> Series:", "fixed": " Wild         185.0\n                     result = coerce_to_dtypes(result, self.dtypes)\n         if constructor is not None:\n            result = self._constructor_sliced(result, index=labels)\n         return result\n     def nunique(self, axis=0, dropna=True) -> Series:"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return", "fixed": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return"}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n     for leaf in line.leaves:\n        current_line.append(leaf, preformatted=True)\n        comment_after = line.comments.get(id(leaf))\n        if comment_after:\n            current_line.append(comment_after, preformatted=True)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n     lowest_depth = sys.maxsize\n     trailing_comma_safe = True\n    def append_to_line(leaf: Leaf) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n     for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth"}
{"id": "youtube-dl_9", "problem": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, endwith=[]):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:", "fixed": " class YoutubeDL(object):\n                 else:\n                     filter_parts.append(string)\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n             selectors = []\n             current_selector = None\n             for type, string, start, _, _ in tokens:"}
{"id": "fastapi_15", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n     def get(\n         self,", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n     def get(\n         self,"}
{"id": "cookiecutter_1", "problem": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file) as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:", "fixed": " def generate_context(\n     context = OrderedDict([])\n     try:\n        with open(context_file, encoding='utf-8') as file_handle:\n             obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n     except ValueError as e:"}
{"id": "scrapy_30", "problem": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).itervalues():\n             if inspect.isclass(obj) and \\\n               issubclass(obj, ScrapyCommand) and \\\n               obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):", "fixed": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).values():\n             if inspect.isclass(obj) and \\\n                    issubclass(obj, ScrapyCommand) and \\\n                    obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route(", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     response_class=route.response_class or default_response_class,\n                     name=route.name,\n                    route_class_override=type(route),\n                 )\n             elif isinstance(route, routing.Route):\n                 self.add_route("}
{"id": "tqdm_7", "problem": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')", "fixed": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "matplotlib_7", "problem": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if hasattr(intensity, 'mask'):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]", "fixed": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if np.ma.is_masked(intensity):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]"}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:https?:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "tornado_4", "problem": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if (start is not None and start >= size) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n            if start is not None and start < 0:\n                start += size\n             if end is not None and end > size:", "fixed": " class StaticFileHandler(RequestHandler):\n         size = self.get_content_size()\n         if request_range:\n             start, end = request_range\n            if start is not None and start < 0:\n                start += size\n                if start < 0:\n                    start = 0\n            if (\n                start is not None\n                and (start >= size or (end is not None and start >= end))\n            ) or end == 0:\nself.set_status(416)\n                 self.set_header(\"Content-Type\", \"text/plain\")\n                 self.set_header(\"Content-Range\", \"bytes */%s\" % (size,))\n                 return\n             if end is not None and end > size:"}
{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_business_getitem(self):\n         rng = pd.bdate_range(START, END)\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq", "fixed": " class TestGetItem:\n     def test_dti_business_getitem(self):\n         rng = pd.bdate_range(START, END)\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"B\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq"}
{"id": "pandas_31", "problem": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals):\n                 inference = np.int64\n            elif is_datetime64_dtype(vals):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)", "fixed": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                 inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)"}
{"id": "black_22", "problem": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):", "fixed": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):"}
{"id": "luigi_6", "problem": " class DictParameter(Parameter):\n         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)\n     def serialize(self, x):\n        return json.dumps(x, cls=DictParameter._DictParamEncoder)\n class ListParameter(Parameter):", "fixed": " class DictParameter(Parameter):\n         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)\n     def serialize(self, x):\n        return json.dumps(x, cls=_DictParamEncoder)\n class ListParameter(Parameter):"}
{"id": "scrapy_34", "problem": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = {}\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)", "fixed": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)"}
{"id": "pandas_102", "problem": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n         if columns is None:\n            columns = [0]\n        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)", "fixed": " def init_ndarray(values, index, columns, dtype=None, copy=False):\n         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)\n     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):\n        if isinstance(values, np.ndarray) and values.ndim > 1:\n            values = [values[:, n] for n in range(values.shape[1])]\n        else:\n            values = [values]\n         if columns is None:\n            columns = list(range(len(values)))\n        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)"}
{"id": "scrapy_6", "problem": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')", "fixed": " class ImagesPipeline(FilesPipeline):\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n         elif image.mode != 'RGB':\n             image = image.convert('RGB')"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "black_6", "problem": " __credits__ = \\\n import re\n from codecs import BOM_UTF8, lookup\n from blib2to3.pgen2.token import *\n from . import token", "fixed": " __credits__ = \\\n import re\n from codecs import BOM_UTF8, lookup\nfrom attr import dataclass\n from blib2to3.pgen2.token import *\n from . import token"}
{"id": "scrapy_40", "problem": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        if self.binary:\n            return to_bytes(value, encoding=self.encoding)\n        else:\n            return to_unicode(value, encoding=self.encoding)\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):", "fixed": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):"}
{"id": "youtube-dl_19", "problem": " import re\n import shutil\n import subprocess\n import socket\n import sys\n import time\n import tokenize", "fixed": " import re\n import shutil\n import subprocess\n import socket\nimport string\n import sys\n import time\n import tokenize"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()", "fixed": " def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n                if hidden_value.endswith(\"\\n\"):\n                    hidden_value = hidden_value[:-1]\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()"}
{"id": "scrapy_19", "problem": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n     def has_header(self, name):\n         return name in self.request.headers", "fixed": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n    @property\n    def full_url(self):\n        return self.get_full_url()\n    @property\n    def host(self):\n        return self.get_host()\n    @property\n    def type(self):\n        return self.get_type()\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n     def has_header(self, name):\n         return name in self.request.headers"}
{"id": "black_6", "problem": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the", "fixed": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the"}
{"id": "pandas_125", "problem": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result", "fixed": " class CategoricalBlock(ExtensionBlock):\n             )\n         return result\n    def replace(\n        self,\n        to_replace,\n        value,\n        inplace: bool = False,\n        filter=None,\n        regex: bool = False,\n        convert: bool = True,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        result = self if inplace else self.copy()\n        if filter is None:\n            result.values.replace(to_replace, value, inplace=True)\n            if convert:\n                return result.convert(numeric=False, copy=not inplace)\n            else:\n                return result\n        else:\n            if not isna(value):\n                result.values.add_categories(value, inplace=True)\n            return super(CategoricalBlock, result).replace(\n                to_replace, value, inplace, filter, regex, convert\n            )"}
{"id": "fastapi_1", "problem": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\"", "fixed": " def get_request_handler(\n     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     response_model_by_alias: bool = True,\n     response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n     dependency_overrides_provider: Any = None,\n ) -> Callable:\n     assert dependant.call is not None, \"dependant.call must be a function\""}
{"id": "pandas_118", "problem": " from pandas.core.dtypes.generic import ABCMultiIndex\n from pandas.core.dtypes.missing import notna\n from pandas.core.arrays import Categorical\n from pandas.core.frame import DataFrame, _shared_docs\n from pandas.core.indexes.base import Index\n from pandas.core.reshape.concat import concat", "fixed": " from pandas.core.dtypes.generic import ABCMultiIndex\n from pandas.core.dtypes.missing import notna\n from pandas.core.arrays import Categorical\nimport pandas.core.common as com\n from pandas.core.frame import DataFrame, _shared_docs\n from pandas.core.indexes.base import Index\n from pandas.core.reshape.concat import concat"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "pandas_72", "problem": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n         ):\n             values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:", "fixed": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n         ):\n             values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n        elif exact_match:\n            values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:"}
{"id": "pandas_24", "problem": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq='D')\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):", "fixed": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):"}
{"id": "youtube-dl_35", "problem": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }", "fixed": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n        upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(upload_date_str),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "youtube-dl_34", "problem": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n         if not expected:\nmsg = msg + u'; please report this issue on https:\n         super(ExtractorError, self).__init__(msg)", "fixed": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n        if cause:\n            msg += u' (caused by %r)' % cause\n         if not expected:\nmsg = msg + u'; please report this issue on https:\n         super(ExtractorError, self).__init__(msg)"}
{"id": "thefuck_14", "problem": " from .generic import Generic\n class Fish(Generic):\n     def _get_overridden_aliases(self):\n        overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()\n        if overridden_aliases:\n            return [alias.strip() for alias in overridden_aliases.split(',')]\n        else:\n            return ['cd', 'grep', 'ls', 'man', 'open']\n     def app_alias(self, fuck):", "fixed": " from .generic import Generic\n class Fish(Generic):\n     def _get_overridden_aliases(self):\n        default = {'cd', 'grep', 'ls', 'man', 'open'}\n        for alias in os.environ.get('TF_OVERRIDDEN_ALIASES', '').split(','):\n            default.add(alias.strip())\n        return default\n     def app_alias(self, fuck):"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '", "fixed": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "black_15", "problem": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: str) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        list(v.visit(lib2to3_parse(code)))\n KEYWORDS = set(keyword.kwlist)", "fixed": " class DebugVisitor(Visitor[T]):\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n     @classmethod\n    def show(cls, code: Union[str, Leaf, Node]) -> None:\n         v: DebugVisitor[None] = DebugVisitor()\n        if isinstance(code, str):\n            code = lib2to3_parse(code)\n        list(v.visit(code))\n KEYWORDS = set(keyword.kwlist)"}
{"id": "luigi_21", "problem": " def run(cmdline_args=None, main_task_cls=None,\n     :param use_dynamic_argparse:\n     :param local_scheduler:\n     if use_dynamic_argparse:\n         interface = DynamicArgParseInterface()\n     else:", "fixed": " def run(cmdline_args=None, main_task_cls=None,\n     :param use_dynamic_argparse:\n     :param local_scheduler:\n    if cmdline_args is None:\n        cmdline_args = sys.argv[1:]\n     if use_dynamic_argparse:\n         interface = DynamicArgParseInterface()\n     else:"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size", "fixed": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "pandas_127", "problem": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n         rs = rs.reindex_like(data)\n         if freq is None:\n             mask = isna(com.values_from_object(data))", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n             data = self.fillna(method=fill_method, limit=limit, axis=axis)\n         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1\n        rs = rs.loc[~rs.index.duplicated()]\n         rs = rs.reindex_like(data)\n         if freq is None:\n             mask = isna(com.values_from_object(data))"}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "matplotlib_30", "problem": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    x = x * (N - 1)\n    xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n    ind = np.searchsorted(x, xind)[1:-1]\n    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n    lut = np.concatenate([\n        [y1[0]],\n        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n        [y0[-1]],\n    ])\n     return np.clip(lut, 0.0, 1.0)", "fixed": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    if N == 1:\n        lut = np.array(y0[-1])\n    else:\n        x = x * (N - 1)\n        xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n        ind = np.searchsorted(x, xind)[1:-1]\n        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n        lut = np.concatenate([\n            [y1[0]],\n            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n            [y0[-1]],\n        ])\n     return np.clip(lut, 0.0, 1.0)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_90", "problem": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    if path is None:\n        path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(path) as path:\n        pd.to_pickle(obj, path)\n        return pd.read_pickle(path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):", "fixed": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as path:\n        pd.to_pickle(obj, _path)\n        return pd.read_pickle(_path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n        ordinal_op = getattr(self.asi8, opname)\n         if isinstance(other, str):\n             try:", "fixed": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n         if isinstance(other, str):\n             try:"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"id": "pandas_112", "problem": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         [", "fixed": " class TestGetIndexer:\n         expected = np.array([0] * size, dtype=\"intp\")\n         tm.assert_numpy_array_equal(result, expected)\n    @pytest.mark.parametrize(\n        \"target\",\n        [\n            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),\n            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=\"both\"),\n            [-1, 0, 0.5, 1, 2, 2.5, np.nan],\n            [\"foo\", \"foo\", \"bar\", \"baz\"],\n        ],\n    )\n    def test_get_indexer_categorical(self, target, ordered_fixture):\n        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])\n        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)\n        result = index.get_indexer(categorical_target)\n        expected = index.get_indexer(target)\n        tm.assert_numpy_array_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"tuples, closed\",\n         ["}
{"id": "matplotlib_19", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "youtube-dl_29", "problem": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1.ordinal, side=\"left\"),\n            self.searchsorted(t2.ordinal, side=\"right\"),\n         )\n     def _convert_tolerance(self, tolerance, target):", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n         t1, t2 = self._parsed_string_to_bounds(reso, parsed)\n         return slice(\n            self.searchsorted(t1, side=\"left\"), self.searchsorted(t2, side=\"right\")\n         )\n     def _convert_tolerance(self, tolerance, target):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "youtube-dl_32", "problem": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)\n def js_to_json(code):", "fixed": " def parse_age_limit(s):\n def strip_jsonp(code):\n    return re.sub(\nr'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?(?:\n def js_to_json(code):"}
{"id": "ansible_13", "problem": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)", "fixed": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    requirement = None\n                    if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \\\n                            urlparse(collection_input).scheme.lower() in ['http', 'https']:\n                        name = collection_input\n                    else:\n                        name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)"}
{"id": "pandas_5", "problem": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)", "fixed": " def test_join_multi_wrong_order():\n     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"b\", \"a\"])\n    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)\n     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)\n     tm.assert_index_equal(midx1, join_idx)\n     assert lidx is None\n     tm.assert_numpy_array_equal(ridx, exp_ridx)\ndef test_join_multi_return_indexers():\n    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[\"a\", \"b\", \"c\"])\n    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    result = midx1.join(midx2, return_indexers=False)\n    tm.assert_index_equal(result, midx1)"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 16,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 16\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 8,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 8\n         },\n     },\n     {"}
{"id": "luigi_23", "problem": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler()\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)", "fixed": " class core(task.Config):\n class WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, host, port):\n         return rpc.RemoteScheduler(host=host, port=port)"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "youtube-dl_33", "problem": " class DRTVIE(SubtitlesInfoExtractor):\n     }\n     def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n         programcard = self._download_json(\n'http:", "fixed": " class DRTVIE(SubtitlesInfoExtractor):\n     }\n     def _real_extract(self, url):\n        video_id = self._match_id(url)\n         programcard = self._download_json(\n'http:"}
{"id": "pandas_108", "problem": " def infer_dtype_from_scalar(val, pandas_dtype: bool = False):\n         if lib.is_period(val):\n             dtype = PeriodDtype(freq=val.freq)\n             val = val.ordinal\n     return dtype, val", "fixed": " def infer_dtype_from_scalar(val, pandas_dtype: bool = False):\n         if lib.is_period(val):\n             dtype = PeriodDtype(freq=val.freq)\n             val = val.ordinal\n        elif lib.is_interval(val):\n            subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]\n            dtype = IntervalDtype(subtype=subtype)\n     return dtype, val"}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if is_sequence:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'", "fixed": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if use_sequence_api:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "ansible_7", "problem": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n    for key in to_remove:\n        commands.append(\"no {0}\".format(key))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands", "fixed": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n    for key in to_remove:\n        if key in to_set.keys():\n            continue\n        commands.append(\"no {0}\".format(key))\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands"}
{"id": "tornado_11", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\") == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)"}
{"id": "black_22", "problem": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines):\n         yield line\n         return\n     line_str = str(line).strip('\\n')\n    if len(line_str) <= line_length and '\\n' not in line_str:\n         yield line\n         return\n     if line.is_def:\n         split_funcs = [left_hand_split]\n     elif line.inside_brackets:\n        split_funcs = [delimiter_split]\n        if '\\n' not in line_str:\n            split_funcs.append(right_hand_split)\n     else:\n         split_funcs = [right_hand_split]\n     for split_func in split_funcs:", "fixed": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines) or line.is_comment:\n         yield line\n         return\n     line_str = str(line).strip('\\n')\n    if (\n        len(line_str) <= line_length\n        and '\\n' not in line_str\n        and not line.contains_standalone_comments\n    ):\n         yield line\n         return\n    split_funcs: List[SplitFunc]\n     if line.is_def:\n         split_funcs = [left_hand_split]\n     elif line.inside_brackets:\n        split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]\n     else:\n         split_funcs = [right_hand_split]\n     for split_func in split_funcs:"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')", "fixed": " class Tracer:\n         old_local_reprs = self.frame_to_local_reprs.get(frame, {})\n         self.frame_to_local_reprs[frame] = local_reprs = \\\n                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)\n         newish_string = ('Starting var:.. ' if event == 'call' else\n                                                             'New var:....... ')"}
{"id": "pandas_18", "problem": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            self.min_periods or 1,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,", "fixed": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(self.window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            window_indexer.window_size,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n     def __init__(\n             self,", "fixed": " class Tracer:\n         @pysnooper.snoop(thread_info=True)\n    Customize how values are represented as strings::\n        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))\n     def __init__(\n             self,"}
{"id": "fastapi_14", "problem": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None", "fixed": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[str, Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a same-typed block.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block.", "fixed": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block."}
{"id": "keras_42", "problem": " class Sequential(Model):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                      steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,", "fixed": " class Sequential(Model):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,"}
{"id": "scrapy_13", "problem": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 0\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'", "fixed": " class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n    EXPIRES = 90\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'"}
{"id": "thefuck_20", "problem": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])\n def side_effect(old_cmd, command):", "fixed": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))\n def side_effect(old_cmd, command):"}
{"id": "pandas_55", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            lower = self._take_with_is_copy(item, axis=self._info_axis_number)\n         return lower\n     def _box_item_values(self, key, values):", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item: int):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            return self._ixs(item, axis=1)\n         return lower\n     def _box_item_values(self, key, values):"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "black_23", "problem": " def func_no_args():\n         print(i)\n         continue\n     return None\nasync def coroutine(arg):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n         print(i)\n         continue\n    exec(\"new-style exec\", {}, {})\n     return None\nasync def coroutine(arg, exec=False):\n     \"Single-line docstring. Multiline is harder to reformat.\"\n     async with some_connection() as conn:\n         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "matplotlib_15", "problem": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z1,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')", "fixed": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z1,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')"}
{"id": "cookiecutter_4", "problem": " def run_script_with_context(script_path, cwd, context):\n     ) as temp:\n         temp.write(Template(contents).render(**context))\n    return run_script(temp.name, cwd)\n def run_hook(hook_name, project_dir, context):", "fixed": " def run_script_with_context(script_path, cwd, context):\n     ) as temp:\n         temp.write(Template(contents).render(**context))\n    run_script(temp.name, cwd)\n def run_hook(hook_name, project_dir, context):"}
{"id": "keras_30", "problem": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if x is None or len(x) == 0:\n                        batch_size = 1\n                    elif isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]"}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        bbox, info, descent = self._get_layout(self._renderer)\n        x, y = self.get_unitless_position()\n        x, y = self.get_transform().transform((x, y))\n        bbox = bbox.translated(x, y)\n        if dpi is not None:\n            self.figure.dpi = dpi_orig\n        return bbox\n     def set_backgroundcolor(self, color):", "fixed": " class Text(Artist):\n         if self._renderer is None:\n             raise RuntimeError('Cannot get window extent w/o renderer')\n        with cbook._setattr_cm(self.figure, dpi=dpi):\n            bbox, info, descent = self._get_layout(self._renderer)\n            x, y = self.get_unitless_position()\n            x, y = self.get_transform().transform((x, y))\n            bbox = bbox.translated(x, y)\n            return bbox\n     def set_backgroundcolor(self, color):"}
{"id": "matplotlib_4", "problem": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,", "fixed": " def violinplot(\n @_copy_docstring_and_deprecators(Axes.vlines)\n def vlines(\n        x, ymin, ymax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().vlines(\n         x, ymin, ymax, colors=colors, linestyles=linestyles,"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:", "fixed": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:"}
{"id": "thefuck_5", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n     return ('push' in command.script_parts\n            and 'set-upstream' in command.output)\n def _get_upstream_option_index(command_parts):", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n     return ('push' in command.script_parts\n            and 'git push --set-upstream' in command.output)\n def _get_upstream_option_index(command_parts):"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n            custom_generator(), good_batches + 1, 1,\n            workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n    workers = 4\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n            custom_generator(), good_batches * workers + 1, 1,\n            workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "pandas_79", "problem": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError):\n                 return False", "fixed": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError, InvalidIndexError):\n                 return False"}
{"id": "pandas_27", "problem": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\nimport pandas._libs.tslibs.frequencies as libfrequencies\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import ("}
{"id": "pandas_165", "problem": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):", "fixed": " class TestPeriodIndexArithmetic:\n         with pytest.raises(TypeError):\n             other - obj\n    def test_parr_add_sub_index(self):\n        pi = pd.period_range(\"2000-12-31\", periods=3)\n        parr = pi.array\n        result = parr - pi\n        expected = pi - pi\n        tm.assert_index_equal(result, expected)\n class TestPeriodSeriesArithmetic:\n     def test_ops_series_timedelta(self):"}
{"id": "spacy_7", "problem": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n            seen_tokens.update(range(span.start, span.end))\n     return result", "fixed": " def main(model=\"en_core_web_sm\"):\n def filter_spans(spans):\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n     result = []\n     seen_tokens = set()\n     for span in sorted_spans:\n         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n             result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n     return result"}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "luigi_4", "problem": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)", "fixed": " class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):\n         logger.info(\"Inserting file: %s\", f)\n         colnames = ''\n        if self.columns and len(self.columns) > 0:\n             colnames = \",\".join([x[0] for x in self.columns])\n             colnames = '({})'.format(colnames)"}
{"id": "pandas_22", "problem": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type:\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)\n     ):\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes", "fixed": " def _factorize_keys(lk, rk, sort=True):\n         rk, _ = rk._values_for_factorize()\n     elif (\n        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)\n     ):\n        assert is_categorical(lk) and is_categorical(rk)\n        lk = cast(Categorical, lk)\n        rk = cast(Categorical, rk)\n         if lk.categories.equals(rk.categories):\n             rk = rk.codes"}
{"id": "ansible_4", "problem": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)\n     def _load_collections(self, attr, ds):", "fixed": " def _ensure_default_collection(collection_list=None):\n class CollectionSearch:\n    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,\n                                  always_post_validate=True, static=True)\n     def _load_collections(self, attr, ds):"}
{"id": "black_15", "problem": " def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n        if pc.value in FMT_ON:\n            raise FormatOn(pc.consumed)\n        if pc.value in FMT_OFF:\n            if pc.type == STANDALONE_COMMENT:\n                raise FormatOff(pc.consumed)\n            prev = preceding_leaf(leaf)\n            if not prev or prev.type in WHITESPACE:\n                raise FormatOff(pc.consumed)\n @dataclass", "fixed": " def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n @dataclass"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "pandas_44", "problem": " from pandas._libs.lib import no_default\n from pandas._libs.tslibs import frequencies as libfrequencies, resolution\n from pandas._libs.tslibs.parsing import parse_time_string\n from pandas._libs.tslibs.period import Period\nfrom pandas._typing import Label\n from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.lib import no_default\n from pandas._libs.tslibs import frequencies as libfrequencies, resolution\n from pandas._libs.tslibs.parsing import parse_time_string\n from pandas._libs.tslibs.period import Period\nfrom pandas._typing import DtypeObj, Label\n from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import ("}
{"id": "keras_23", "problem": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape", "fixed": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,", "fixed": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            self._build_input_shape = input_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,"}
{"id": "black_22", "problem": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n         if not (", "fixed": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n    @property\n    def contains_standalone_comments(self) -> bool:\n         if not ("}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n        /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n/\\*.*?\\*/|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "scrapy_28", "problem": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod", "fixed": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_58", "problem": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")"}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "pandas_65", "problem": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, BufferedIOBase):\n             handles.append(g)\n         f = g", "fixed": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, (BufferedIOBase, RawIOBase)):\n             handles.append(g)\n         f = g"}
{"id": "pandas_40", "problem": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":", "fixed": " def _get_join_indexers(\n    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n     kwargs = copy.copy(kwargs)\n     if how == \"left\":"}
{"id": "pandas_67", "problem": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()", "fixed": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n    def iget(self, key):\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()"}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "pandas_43", "problem": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)", "fixed": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)"}
{"id": "luigi_25", "problem": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path()\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):", "fixed": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()", "fixed": " def test_multiprocessing_fit_error():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()"}
{"id": "spacy_9", "problem": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n @add_codes", "fixed": " class Warnings(object):\n             \"loaded. (Shape: {shape})\")\n     W021 = (\"Unexpected hash collision in PhraseMatcher. Matches may be \"\n             \"incorrect. Modify PhraseMatcher._terminal_hash to fix.\")\n    W022 = (\"Training a new part-of-speech tagger using a model with no \"\n            \"lemmatization rules or data. This means that the trained model \"\n            \"may not be able to lemmatize correctly. If this is intentional \"\n            \"or the language you're using doesn't have lemmatization data, \"\n            \"you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. \"\n            \"If this is surprising, make sure you have the spacy-lookups-data \"\n            \"package installed.\")\n @add_codes"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "pandas_9", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d"}
{"id": "ansible_10", "problem": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type", "fixed": " class PamdRule(PamdLine):\n     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']\n     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):\n        self.prev = None\n        self.next = None\n         self._control = None\n         self._args = None\n         self.rule_type = rule_type"}
{"id": "scrapy_3", "problem": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['location'])\n         redirected_url = urljoin(request.url, location)", "fixed": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['Location'])\nif response.headers['Location'].startswith(b'\n            request_scheme = urlparse(request.url).scheme\nlocation = request_scheme + ':\n         redirected_url = urljoin(request.url, location)"}
{"id": "thefuck_22", "problem": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        commands = self._remove_duplicates(self._commands)\n        self._cached = [self._cached[0]] + sorted(\n            commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)", "fixed": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        if self._cached:\n            commands = self._remove_duplicates(self._commands)\n            self._cached = [self._cached[0]] + sorted(\n                commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)"}
{"id": "thefuck_24", "problem": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\nCorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))", "fixed": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))\nclass CorrectedCommand(object):\n    def __init__(self, script, side_effect, priority):\n        self.script = script\n        self.side_effect = side_effect\n        self.priority = priority\n    def __eq__(self, other):"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n             result = res - lag\n             result[mask] = na\n             out_arr[res_indexer] = result\n         else:\n             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]", "fixed": " def diff(arr, n: int, axis: int = 0):\n             result = res - lag\n             result[mask] = na\n             out_arr[res_indexer] = result\n        elif is_bool:\n            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]\n         else:\n             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n     samples = batch_size * (good_batches + 1)\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.fit_generator(\n             custom_generator(), samples, 1,\n             use_multiprocessing=False,"}
{"id": "ansible_2", "problem": " class _Alpha:\n         raise ValueError\n    def __gt__(self, other):\n        return not self.__lt__(other)\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n     def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n class _Numeric:", "fixed": " class _Alpha:\n         raise ValueError\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n    def __gt__(self, other):\n        return not self.__le__(other)\n     def __ge__(self, other):\n        return not self.__lt__(other)\n class _Numeric:"}
{"id": "ansible_1", "problem": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:", "fixed": " def verify_collections(collections, search_paths, apis, validate_certs, ignore_e\n                     for search_path in search_paths:\n                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                         if os.path.isdir(b_search_path):\n                            if not os.path.isfile(os.path.join(to_text(b_search_path, errors='surrogate_or_strict'), 'MANIFEST.json')):\n                                raise AnsibleError(\n                                    message=\"Collection %s does not appear to have a MANIFEST.json. \" % collection_name +\n                                            \"A MANIFEST.json is expected if the collection has been built and installed via ansible-galaxy.\"\n                                )\n                             local_collection = CollectionRequirement.from_path(b_search_path, False)\n                             break\n                     if local_collection is None:"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "pandas_79", "problem": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,", "fixed": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,"}
{"id": "keras_25", "problem": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1.", "fixed": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n    x = x.astype(K.floatx())\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1."}
{"id": "pandas_159", "problem": " class DataFrame(NDFrame):\n             return ops.dispatch_to_series(this, other, _arith_op)\n         else:\n            result = _arith_op(this.values, other.values)\n             return self._constructor(\n                 result, index=new_index, columns=new_columns, copy=False\n             )", "fixed": " class DataFrame(NDFrame):\n             return ops.dispatch_to_series(this, other, _arith_op)\n         else:\n            with np.errstate(all=\"ignore\"):\n                result = _arith_op(this.values, other.values)\n            result = dispatch_fill_zeros(func, this.values, other.values, result)\n             return self._constructor(\n                 result, index=new_index, columns=new_columns, copy=False\n             )"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "matplotlib_10", "problem": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])", "fixed": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n            if 'label1On' in kwtrans or 'label2On' in kwtrans:\n                self.offsetText.set_visible(\n                    self._major_tick_kw.get('label1On', False)\n                    or self._major_tick_kw.get('label2On', False))\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])"}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "keras_20", "problem": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support():\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'", "fixed": " def _preprocess_conv2d_input(x, data_format):\n         x = tf.cast(x, 'float32')\n     tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n        if not _has_nchw_support() or force_transpose:\nx = tf.transpose(x, (0, 2, 3, 1))\n         else:\n             tf_data_format = 'NCHW'"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         return result\n     def __rsub__(self, other):\n        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):\n             if not isinstance(other, DatetimeLikeArrayMixin):"}
{"id": "pandas_41", "problem": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype", "fixed": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value: ArrayLike) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype"}
{"id": "pandas_41", "problem": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n     @property\n     def _holder(self):", "fixed": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n    should_store = DatetimeBlock.should_store\n     @property\n     def _holder(self):"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"id": "pandas_99", "problem": " def _convert_listlike_datetimes(\n     elif unit is not None:\n         if format is not None:\n             raise ValueError(\"cannot specify both format and unit\")\n        arg = getattr(arg, \"values\", arg)\n        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n         if errors == \"ignore\":\n             from pandas import Index", "fixed": " def _convert_listlike_datetimes(\n     elif unit is not None:\n         if format is not None:\n             raise ValueError(\"cannot specify both format and unit\")\n        arg = getattr(arg, \"_values\", arg)\n        if isinstance(arg, IntegerArray):\n            mask = arg.isna()\n            arg = arg._ndarray_values\n        else:\n            mask = None\n        result, tz_parsed = tslib.array_with_unit_to_datetime(\n            arg, mask, unit, errors=errors\n        )\n         if errors == \"ignore\":\n             from pandas import Index"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*."}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n            edgecolors = 'face'\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):", "fixed": " default: :rc:`scatter.edgecolors`\n         path = marker_obj.get_path().transformed(\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             if linewidths is None:\n                 linewidths = rcParams['lines.linewidth']\n             elif np.iterable(linewidths):"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "youtube-dl_16", "problem": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:", "fixed": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with open(dfxp_file, 'rb') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "scrapy_33", "problem": " from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\nfrom scrapy.utils.log import logformatter_adapter\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response", "fixed": " from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\nfrom scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_31", "problem": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length))\n    input_length = tf.to_int32(tf.squeeze(input_length))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())", "fixed": " def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n         Tensor with shape (samples,1) containing the\n             CTC loss of each element.\n    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))\n    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))\n     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())"}
{"id": "scrapy_23", "problem": " from six.moves.urllib.parse import urlunparse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n class HttpProxyMiddleware(object):", "fixed": " from six.moves.urllib.parse import urlunparse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\nfrom scrapy.utils.python import to_bytes\n class HttpProxyMiddleware(object):"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "matplotlib_1", "problem": " def get_renderer(fig):\n             return canvas.get_renderer()\n         else:\n             from . import backend_bases\n            return backend_bases._get_renderer(fig, draw_disabled=True)\n def get_subplotspec_list(axes_list, grid_spec=None):", "fixed": " def get_renderer(fig):\n             return canvas.get_renderer()\n         else:\n             from . import backend_bases\n            return backend_bases._get_renderer(fig)\n def get_subplotspec_list(axes_list, grid_spec=None):"}
{"id": "black_22", "problem": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:", "fixed": " def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None\n             )\ndef dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:\n    @wraps(split_func)\n    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:\n        for l in split_func(line, py36):\n            normalize_prefix(l.leaves[0], inside_brackets=True)\n            yield l\n    return split_wrapper\n@dont_increase_indentation\n def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "pandas_43", "problem": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n     if not isinstance(right, ABCDataFrame):\n         return False", "fixed": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n    if op is operator.pow or op is rpow:\n        return False\n     if not isinstance(right, ABCDataFrame):\n         return False"}
{"id": "luigi_27", "problem": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_value:\n            description.append(\" [default: %s]\" % (self.value,))\n         if self.is_list:\n             action = \"append\"", "fixed": " class Parameter(object):\n             description.append('for all instances of class %s' % task_name)\n         elif self.description:\n             description.append(self.description)\n        if self.has_task_value(param_name=param_name, task_name=task_name):\n            value = self.task_value(param_name=param_name, task_name=task_name)\n            description.append(\" [default: %s]\" % (value,))\n         if self.is_list:\n             action = \"append\""}
{"id": "pandas_44", "problem": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,", "fixed": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import DtypeObj, Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n    ensure_platform_int,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,"}
{"id": "black_13", "problem": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token == 'def':\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            async_def = True\n                            async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],", "fixed": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token in ('def', 'for'):\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "keras_22", "problem": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '", "fixed": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n        self.supports_masking = True\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '"}
{"id": "pandas_13", "problem": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=False)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n     else:\n         return False"}
{"id": "pandas_153", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2", "fixed": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s was created successfully\" % obj_name)\n     def execute_info(self):", "fixed": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))\n     def execute_info(self):"}
{"id": "youtube-dl_41", "problem": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',',' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9:]*$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',", "fixed": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',', ' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n }", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n }"}
{"id": "youtube-dl_27", "problem": " def parse_dfxp_time_expr(time_expr):\n     if mobj:\n         return float(mobj.group('time_offset'))\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:\\.\\d+)?)$', time_expr)\n     if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))\n def srt_subtitles_timecode(seconds):", "fixed": " def parse_dfxp_time_expr(time_expr):\n     if mobj:\n         return float(mobj.group('time_offset'))\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n     if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n def srt_subtitles_timecode(seconds):"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "pandas_46", "problem": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n     def test_empty(self):\n         X = [[], [0, 1], []]", "fixed": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n     def test_empty(self):\n         X = [[], [0, 1], []]"}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.resize(xmin, y.shape)\n        xmax = np.resize(xmax, y.shape)\n        verts = [((thisxmin, thisy), (thisxmax, thisy))\n                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(xmax):\n             xmax = [xmax]\n        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)\n         y = np.ravel(y)\n        xmin = np.ravel(xmin)\n        xmax = np.ravel(xmax)\n        masked_verts = np.ma.empty((len(y), 2, 2))\n        masked_verts[:, 0, 0] = xmin\n        masked_verts[:, 0, 1] = y\n        masked_verts[:, 1, 0] = xmax\n        masked_verts[:, 1, 1] = y\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "thefuck_23", "problem": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with shelve.open(cache_path) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:", "fixed": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with closing(shelve.open(cache_path)) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:"}
{"id": "black_18", "problem": " def format_str(\n     return dst_contents\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,", "fixed": " def format_str(\n     return dst_contents\ndef prepare_input(src: bytes) -> Tuple[str, str, str]:\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,"}
{"id": "black_18", "problem": " from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\n from enum import Enum, Flag\n from functools import partial, wraps\n import keyword\n import logging\n from multiprocessing import Manager", "fixed": " from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\n from enum import Enum, Flag\n from functools import partial, wraps\nimport io\n import keyword\n import logging\n from multiprocessing import Manager"}
{"id": "youtube-dl_39", "problem": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output", "fixed": " except AttributeError:\n         if ret:\n             raise subprocess.CalledProcessError(ret, p.args, output=output)\n         return output\ndef limit_length(s, length):\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"id": "scrapy_22", "problem": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        else:\n             self._xg_characters(serialized_value)\n         self.xg.endElement(name)", "fixed": " class XmlItemExporter(BaseItemExporter):\n         elif is_listlike(serialized_value):\n             for value in serialized_value:\n                 self._export_xml_field('value', value)\n        elif isinstance(serialized_value, six.text_type):\n             self._xg_characters(serialized_value)\n        else:\n            self._xg_characters(str(serialized_value))\n         self.xg.endElement(name)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )"}
{"id": "pandas_125", "problem": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)\n    def replace(self, to_replace, value, inplace: bool = False):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                else:\n                    categories[index] = value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat"}
{"id": "luigi_23", "problem": " class CentralPlannerScheduler(Scheduler):\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})", "fixed": " class CentralPlannerScheduler(Scheduler):\n        if self._config.prune_on_get_work:\n            self.prune()\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})"}
{"id": "thefuck_7", "problem": " from thefuck.utils import replace_argument, for_app\n @for_app('php')\n def match(command):\n    return \"php -s\" in command.script\n def get_new_command(command):\n     return replace_argument(command.script, \"-s\", \"-S\")\nrequires_output = False", "fixed": " from thefuck.utils import replace_argument, for_app\n @for_app('php')\n def match(command):\n    return \" -s \" in command.script\n def get_new_command(command):\n     return replace_argument(command.script, \"-s\", \"-S\")"}
{"id": "thefuck_20", "problem": " def _zip_file(command):\n    for c in command.script.split()[1:]:\n         if not c.startswith('-'):\n             if c.endswith('.zip'):\n                 return c", "fixed": " def _zip_file(command):\n    for c in command.split_script[1:]:\n         if not c.startswith('-'):\n             if c.endswith('.zip'):\n                 return c"}
{"id": "pandas_41", "problem": " from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare\n from pandas.util._validators import validate_bool_kwarg\n from pandas.core.dtypes.cast import (", "fixed": " from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare\nfrom pandas._typing import ArrayLike\n from pandas.util._validators import validate_bool_kwarg\n from pandas.core.dtypes.cast import ("}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "keras_41", "problem": " import sys\n import tarfile\n import threading\n import time\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool", "fixed": " import sys\n import tarfile\n import threading\n import time\nimport traceback\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(self, output):\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding", "fixed": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(\n        self, output: FrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> FrameOrSeries:\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding"}
{"id": "scrapy_38", "problem": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:", "fixed": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:"}
{"id": "tornado_2", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and \"Transfer-Encoding\" not in headers\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and (\n                    \"Transfer-Encoding\" not in headers\n                    or headers[\"Transfer-Encoding\"] == \"chunked\"\n                )\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "fastapi_8", "problem": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n     ) -> None:\n        route = self.route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,", "fixed": " class APIRouter(routing.Router):\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n     ) -> None:\n        route_class = route_class_override or self.route_class\n        route = route_class(\n             path,\n             endpoint=endpoint,\n             response_model=response_model,"}
{"id": "youtube-dl_35", "problem": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',", "fixed": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n        '%d/%m/%Y %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "black_23", "problem": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n   return None\nasync def coroutine(arg):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)", "fixed": " def func_no_args():\n   for i in range(10):\n     print(i)\n     continue\n  exec(\"new-style exec\", {}, {})\n   return None\nasync def coroutine(arg, exec=False):\n  \"Single-line docstring. Multiline is harder to reformat.\"\n  async with some_connection() as conn:\n      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)"}
{"id": "pandas_113", "problem": " def any_int_dtype(request):\n     return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):", "fixed": " def any_int_dtype(request):\n     return request.param\n@pytest.fixture(params=ALL_EA_INT_DTYPES)\ndef any_nullable_int_dtype(request):\n    return request.param\n @pytest.fixture(params=ALL_REAL_DTYPES)\n def any_real_dtype(request):"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading"}
{"id": "pandas_137", "problem": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort", "fixed": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import array, extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort"}
{"id": "tqdm_2", "problem": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            if ncols:\n                return disp_trim(res, ncols)\n         else:\n             return ((prefix + \": \") if prefix else '') + \\", "fixed": " class tqdm(Comparable):\n                 if ncols else 10,\n                 charset=Bar.BLANK)\n             res = bar_format.format(bar=full_bar, **format_dict)\n            return disp_trim(res, ncols) if ncols else res\n         else:\n             return ((prefix + \": \") if prefix else '') + \\"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n        if len(weights) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())", "fixed": " def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n    class_sample_weight = None\n    if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n        if len(y.shape) == 2:\n            if y.shape[1] > 1:\n                y_classes = np.argmax(y, axis=1)\n            elif y.shape[1] == 1:\n                y_classes = np.reshape(y, y.shape[0])\n         else:\n             y_classes = y\n        class_sample_weight = np.asarray(\n            [class_weight[cls] for cls in y_classes if cls in class_weight])\n        if len(class_sample_weight) != len(y_classes):\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())"}
{"id": "thefuck_32", "problem": " def match(command, settings):\n    return 'ls' in command.script and not ('ls -' in command.script)\n def get_new_command(command, settings):", "fixed": " def match(command, settings):\n    return (command.script == 'ls'\n            or command.script.startswith('ls ')\n            and not ('ls -' in command.script))\n def get_new_command(command, settings):"}
{"id": "httpie_2", "problem": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)", "fixed": " def get_response(args, config_dir):\n     requests_session = get_requests_session()\n    requests_session.max_redirects = args.max_redirects\n     if not args.session and not args.session_read_only:\n         kwargs = get_requests_kwargs(args)"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "keras_15", "problem": " import numpy as np\n import time\n import json\n import warnings\n from collections import deque\n from collections import OrderedDict", "fixed": " import numpy as np\n import time\n import json\n import warnings\nimport io\nimport sys\n from collections import deque\n from collections import OrderedDict"}
{"id": "pandas_13", "problem": " def _use_inf_as_na(key):\n         globals()[\"_isna\"] = _isna_new\ndef _isna_ndarraylike(obj):\n    values = getattr(obj, \"_values\", obj)\n    dtype = values.dtype\n    if is_extension_array_dtype(dtype):\n        result = values.isna()\n    elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n    elif needs_i8_conversion(dtype):\n        result = values.view(\"i8\") == iNaT\n    else:\n        result = np.isnan(values)\n    if isinstance(obj, ABCSeries):\n        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)\n    return result\n     values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n    if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n     elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:\n        result = ~np.isfinite(values)\n     if isinstance(obj, ABCSeries):", "fixed": " def _use_inf_as_na(key):\n         globals()[\"_isna\"] = _isna_new\ndef _isna_ndarraylike(obj, old: bool = False):\n     values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n    if is_extension_array_dtype(dtype):\n        if old:\n            result = values.isna() | (values == -np.inf) | (values == np.inf)\n        else:\n            result = values.isna()\n    elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=old)\n     elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:\n        if old:\n            result = ~np.isfinite(values)\n        else:\n            result = np.isnan(values)\n     if isinstance(obj, ABCSeries):"}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n     return index", "fixed": " def _get_combined_index(\n             index = index.sort_values()\n         except TypeError:\n             pass\n    if copy:\n        index = index.copy()\n     return index"}
{"id": "spacy_10", "problem": " class Errors(object):\n     E168 = (\"Unknown field: {field}\")\n     E169 = (\"Can't find module: {module}\")\n     E170 = (\"Cannot apply transition {name}: invalid for the current state.\")\n @add_codes", "fixed": " class Errors(object):\n     E168 = (\"Unknown field: {field}\")\n     E169 = (\"Can't find module: {module}\")\n     E170 = (\"Cannot apply transition {name}: invalid for the current state.\")\n    E171 = (\"Matcher.add received invalid on_match callback argument: expected \"\n            \"callable or None, but got: {arg_type}\")\n @add_codes"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'", "fixed": " def fit_generator(model,\n    val_use_sequence_api = is_sequence(validation_data)\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'"}
{"id": "pandas_80", "problem": "from .interface import BaseInterfaceTests\nfrom .io import BaseParsingTests\nfrom .methods import BaseMethodsTests\nfrom .missing import BaseMissingTests\nfrom .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil\nfrom .printing import BasePrintingTests\nfrom .reduce import (\n     BaseBooleanReduceTests,", "fixed": "from .interface import BaseInterfaceTests\nfrom .io import BaseParsingTests\nfrom .methods import BaseMethodsTests\nfrom .missing import BaseMissingTests\nfrom .ops import (\n    BaseArithmeticOpsTests,\n    BaseComparisonOpsTests,\n    BaseOpsUtil,\n    BaseUnaryOpsTests,\n)\nfrom .printing import BasePrintingTests\nfrom .reduce import (\n     BaseBooleanReduceTests,"}
{"id": "pandas_111", "problem": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    return self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    return self._invalid_indexer(\"label\", key)\n         return key", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                     \"unicode\",\n                     \"mixed\",\n                 ]:\n                    self._invalid_indexer(\"label\", key)\n             elif kind in [\"loc\"] and is_integer(key):\n                 if not self.holds_integer():\n                    self._invalid_indexer(\"label\", key)\n         return key"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        elif tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit"}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "scrapy_24", "problem": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                  self._tunneledPort)\n         if self._proxyAuthHeader:\n            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse", "fixed": " class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n     def requestTunnel(self, protocol):\n        tunnelReq = (\n            b'CONNECT ' +\n            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n            to_bytes(str(self._tunneledPort)) +\n            b' HTTP/1.1\\r\\n')\n         if self._proxyAuthHeader:\n            tunnelReq += \\\n                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n        tunnelReq += b'\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse"}
{"id": "ansible_11", "problem": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.connection import exec_command\nfrom ansible.module_utils.network.ios.ios import load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nimport re\n def map_obj_to_commands(updates, module):", "fixed": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.network.ios.ios import get_config, load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nfrom re import search, M\n def map_obj_to_commands(updates, module):"}
{"id": "sanic_5", "problem": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],", "fixed": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],"}
{"id": "thefuck_30", "problem": " def _search(stderr):\n def match(command, settings):\n    return 'EDITOR' in os.environ and _search(command.stderr)\n def get_new_command(command, settings):", "fixed": " def _search(stderr):\n def match(command, settings):\n    if 'EDITOR' not in os.environ:\n        return False\n    m = _search(command.stderr)\n    return m and os.path.isfile(m.group('file'))\n def get_new_command(command, settings):"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with numpy.\n         Returns\n         -------", "fixed": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n         Returns\n         -------"}
{"id": "youtube-dl_23", "problem": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\n        elif v.startswith('/*') or v == ',':\n             return \"\"\n         if v[0] in (\"'\", '\"'):", "fixed": " def js_to_json(code):\n         v = m.group(0)\n         if v in ('true', 'false', 'null'):\n             return v\nelif v.startswith('/*') or v.startswith('\n             return \"\"\n         if v[0] in (\"'\", '\"'):"}
{"id": "ansible_12", "problem": "import os\n from ansible.plugins.lookup import LookupBase\n class LookupModule(LookupBase):", "fixed": " from ansible.plugins.lookup import LookupBase\nfrom ansible.utils import py3compat\n class LookupModule(LookupBase):"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "pandas_105", "problem": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested", "fixed": " class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):\n         super().test_merge_on_extension_array_duplicates(data)\n    @skip_nested\n    def test_transpose(self, data):\n        super().test_transpose(data)\n class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):\n     @skip_nested"}
{"id": "black_22", "problem": " def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):", "fixed": " def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):"}
{"id": "keras_41", "problem": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_predict_error():\n     model.add(Dense(1, input_shape=(5,)))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches * workers + 1, 1,\n             workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.predict_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "scrapy_26", "problem": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n            compsett.update(self[name])\n             return compsett\n        else:\n            return self[name]\n     def getpriority(self, name):", "fixed": " class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n            compsett = BaseSettings(self[basename], priority='default')\n            for k in self[name]:\n                prio = self[name].getpriority(k)\n                if prio > get_settings_priority('default'):\n                    compsett.set(k, self[name][k], prio)\n             return compsett\n        return self[name]\n     def getpriority(self, name):"}
{"id": "pandas_161", "problem": " class Categorical(ExtensionArray, PandasObject):\n                     raise ValueError(\"fill value must be in categories\")\n                 values_codes = _get_codes_for_values(value, self.categories)\n                indexer = np.where(values_codes != -1)\n                codes[indexer] = values_codes[values_codes != -1]\n             elif is_hashable(value):", "fixed": " class Categorical(ExtensionArray, PandasObject):\n                     raise ValueError(\"fill value must be in categories\")\n                 values_codes = _get_codes_for_values(value, self.categories)\n                indexer = np.where(codes == -1)\n                codes[indexer] = values_codes[indexer]\n             elif is_hashable(value):"}
{"id": "pandas_157", "problem": " from pandas.core.dtypes.common import (\n     is_bool,\n     is_bool_dtype,\n     is_categorical_dtype,\n    is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetimelike,\n     is_dtype_equal,", "fixed": " from pandas.core.dtypes.common import (\n     is_bool,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_datetime64tz_dtype,\n     is_datetimelike,\n     is_dtype_equal,"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"id": "luigi_29", "problem": " class AmbiguousClass(luigi.Task):\n     pass\nclass NonAmbiguousClass(luigi.ExternalTask):\n    pass\nclass NonAmbiguousClass(luigi.Task):\n    def run(self):\n        NonAmbiguousClass.has_run = True\n class TaskWithSameName(luigi.Task):\n     def run(self):", "fixed": " class AmbiguousClass(luigi.Task):\n     pass\n class TaskWithSameName(luigi.Task):\n     def run(self):"}
{"id": "scrapy_18", "problem": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:", "fixed": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:"}
{"id": "pandas_83", "problem": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort\n         )\n     def _get_concat_axis(self) -> Index:", "fixed": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,\n         )\n     def _get_concat_axis(self) -> Index:"}
{"id": "ansible_13", "problem": " from ansible.galaxy.role import GalaxyRole\n from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel\n from ansible.module_utils.ansible_release import __version__ as ansible_version\n from ansible.module_utils._text import to_bytes, to_native, to_text\n from ansible.parsing.yaml.loader import AnsibleLoader\n from ansible.playbook.role.requirement import RoleRequirement\n from ansible.utils.display import Display\n from ansible.utils.plugin_docs import get_versioned_doclink\n display = Display()\n class GalaxyCLI(CLI):", "fixed": " from ansible.galaxy.role import GalaxyRole\n from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel\n from ansible.module_utils.ansible_release import __version__ as ansible_version\n from ansible.module_utils._text import to_bytes, to_native, to_text\nfrom ansible.module_utils import six\n from ansible.parsing.yaml.loader import AnsibleLoader\n from ansible.playbook.role.requirement import RoleRequirement\n from ansible.utils.display import Display\n from ansible.utils.plugin_docs import get_versioned_doclink\n display = Display()\nurlparse = six.moves.urllib.parse.urlparse\n class GalaxyCLI(CLI):"}
{"id": "pandas_139", "problem": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    )\n                 )", "fixed": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    ),\n                    name=self.name,\n                 )"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         dest = self.parser_dest(param_name, task_name, glob=False)\n         if dest is not None:\n             value = getattr(args, dest, None)\n            params[param_name] = self.parse_from_input(param_name, value)\n     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):", "fixed": " class Parameter(object):\n         dest = self.parser_dest(param_name, task_name, glob=False)\n         if dest is not None:\n             value = getattr(args, dest, None)\n            params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)\n     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):"}
{"id": "black_18", "problem": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            sys.stdout.write(diff_contents)\n         finally:\n             if lock:\n                 lock.release()", "fixed": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff_contents)\n            f.detach()\n         finally:\n             if lock:\n                 lock.release()"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "black_11", "problem": " def split_line(\n         return\n     line_str = str(line).strip(\"\\n\")\n    if not line.should_explode and is_line_short_enough(\n        line, line_length=line_length, line_str=line_str\n     ):\n         yield line\n         return", "fixed": " def split_line(\n         return\n     line_str = str(line).strip(\"\\n\")\n    has_special_comment = False\n    for leaf in line.leaves:\n        for comment in line.comments_after(leaf):\n            if leaf.type == token.COMMA and is_special_comment(comment):\n                has_special_comment = True\n    if (\n        not has_special_comment\n        and not line.should_explode\n        and is_line_short_enough(line, line_length=line_length, line_str=line_str)\n     ):\n         yield line\n         return"}
{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]", "fixed": " class TestGetItem:\n     def test_dti_custom_getitem(self):\n         rng = pd.bdate_range(START, END, freq=\"C\")\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"C\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq\n         sliced = rng[::5]"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n         zdelta = 0.1\n        def line_props_with_rcdefaults(subkey, explicit, zdelta=0):\n             d = {k.split('.')[-1]: v for k, v in rcParams.items()\n                  if k.startswith(f'boxplot.{subkey}')}\n             d['zorder'] = zorder + zdelta\n             if explicit is not None:\n                 d.update(\n                     cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))", "fixed": " class Axes(_AxesBase):\n         zdelta = 0.1\n        def line_props_with_rcdefaults(subkey, explicit, zdelta=0,\n                                       use_marker=True):\n             d = {k.split('.')[-1]: v for k, v in rcParams.items()\n                  if k.startswith(f'boxplot.{subkey}')}\n             d['zorder'] = zorder + zdelta\n            if not use_marker:\n                d['marker'] = ''\n             if explicit is not None:\n                 d.update(\n                     cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))"}
{"id": "matplotlib_5", "problem": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            linewidths = rcParams['lines.linewidth']\n         offsets = np.ma.column_stack([x, y])", "fixed": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            if linewidths is None:\n                linewidths = rcParams['lines.linewidth']\n            elif np.iterable(linewidths):\n                linewidths = [\n                    lw if lw is not None else rcParams['lines.linewidth']\n                    for lw in linewidths]\n         offsets = np.ma.column_stack([x, y])"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds", "fixed": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds"}
{"id": "youtube-dl_31", "problem": " from ..compat import (\n )\n from ..utils import (\n     int_or_none,\n     parse_filesize,\n )", "fixed": " from ..compat import (\n )\n from ..utils import (\n     int_or_none,\n    parse_duration,\n     parse_filesize,\n )"}
{"id": "pandas_44", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\n from pandas.core.arrays.period import (\n     PeriodArray,", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\nfrom pandas.core.dtypes.dtypes import PeriodDtype\n from pandas.core.arrays.period import (\n     PeriodArray,"}
