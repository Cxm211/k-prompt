{"id": "tornado_9", "problem": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "keras_34", "problem": " class Sequence(object):\n _SHARED_SEQUENCES = {}", "fixed": " class Sequence(object):\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item\n _SHARED_SEQUENCES = {}"}
{"id": "pandas_46", "problem": " import numpy as np\n from pandas.core.dtypes.common import is_list_like\nimport pandas.core.common as com\n def cartesian_product(X):", "fixed": " import numpy as np\n from pandas.core.dtypes.common import is_list_like\n def cartesian_product(X):"}
{"id": "pandas_27", "problem": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\nimport pandas._libs.tslibs.frequencies as libfrequencies\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import ("}
{"id": "fastapi_15", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n     def get(\n         self,", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n     def get(\n         self,"}
{"id": "pandas_2", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n         if is_setter:\n             return list(key)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         Require they keys to be the same type as the index. (so we don't\n         fallback)\n        if self.ndim == 1 and len(key) > 1:\n            key = (key,)\n         if is_setter:\n             return list(key)"}
{"id": "pandas_20", "problem": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)\n class MonthEnd(MonthOffset):", "fixed": " class MonthOffset(SingleConstructorOffset):\n     @apply_index_wraps\n     def apply_index(self, i):\n         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)\n        return type(i)._simple_new(shifted, dtype=i.dtype)\n class MonthEnd(MonthOffset):"}
{"id": "thefuck_16", "problem": " from .generic import Generic\n class Zsh(Generic):\n     def app_alias(self, alias_name):\n        alias = \"alias {0}='TF_ALIAS={0}\" \\\n                 \" PYTHONIOENCODING=utf-8\" \\\n                ' TF_SHELL_ALIASES=$(alias)' \\\n                \" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&\" \\\n                 \" eval $TF_CMD\".format(alias_name)\n         if settings.alter_history:", "fixed": " from .generic import Generic\n class Zsh(Generic):\n     def app_alias(self, alias_name):\n        alias = \"alias {0}='TF_CMD=$(TF_ALIAS={0}\" \\\n                 \" PYTHONIOENCODING=utf-8\" \\\n                \" TF_SHELL_ALIASES=$(alias)\" \\\n                \" thefuck $(fc -ln -1 | tail -n 1)) &&\" \\\n                 \" eval $TF_CMD\".format(alias_name)\n         if settings.alter_history:"}
{"id": "thefuck_10", "problem": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n    last_arg = command.script_parts[-1]\n     return [\n        last_arg + ' --help',\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n     ]", "fixed": " def get_new_command(command):\n     if '2' in command.script:\n         return command.script.replace(\"2\", \"3\")\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n    if command.stderr.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n     split_cmd2 = command.script_parts\n     split_cmd3 = split_cmd2[:]\n     split_cmd2.insert(1, ' 2 ')\n     split_cmd3.insert(1, ' 3 ')\n     return [\n         \"\".join(split_cmd3),\n         \"\".join(split_cmd2),\n        help_command,\n     ]"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "black_6", "problem": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n     stashed = None\n     async_def = False", "fixed": " def generate_tokens(readline):\n     contline = None\n     indents = [0]\n    async_is_reserved_keyword = config.async_is_reserved_keyword\n     stashed = None\n     async_def = False"}
{"id": "pandas_37", "problem": " from pandas.core.dtypes.inference import is_array_like\n from pandas import compat\n from pandas.core import ops\nfrom pandas.core.arrays import PandasArray\n from pandas.core.construction import extract_array\n from pandas.core.indexers import check_array_indexer\n from pandas.core.missing import isna", "fixed": " from pandas.core.dtypes.inference import is_array_like\n from pandas import compat\n from pandas.core import ops\nfrom pandas.core.arrays import IntegerArray, PandasArray\nfrom pandas.core.arrays.integer import _IntegerDtype\n from pandas.core.construction import extract_array\n from pandas.core.indexers import check_array_indexer\n from pandas.core.missing import isna"}
{"id": "matplotlib_22", "problem": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')", "fixed": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n        if not cbook.is_scalar_or_string(bins):\n            bins = self.convert_xunits(bins)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n                 index = child.remove() or 0\n                prefix = child.prefix\n                child.prefix = \"\"\n                new_child = Node(syms.atom, [lpar, child, rpar])\n                new_child.prefix = prefix\n                node.insert_child(index, new_child)\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*.", "fixed": " class Axes(_AxesBase):\n     @_preprocess_data(replace_names=[\"y\", \"xmin\", \"xmax\", \"colors\"],\n                       label_namer=\"y\")\n    def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',\n                label='', **kwargs):\n         Plot horizontal lines at each *y* from *xmin* to *xmax*."}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "pandas_138", "problem": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)", "fixed": " def test_timedelta_cut_roundtrip():\n         [\"0 days 23:57:07.200000\", \"2 days 00:00:00\", \"3 days 00:00:00\"]\n     )\n     tm.assert_index_equal(result_bins, expected_bins)\n@pytest.mark.parametrize(\"bins\", [6, 7])\n@pytest.mark.parametrize(\n    \"box, compare\",\n    [\n        (Series, tm.assert_series_equal),\n        (np.array, tm.assert_categorical_equal),\n        (list, tm.assert_equal),\n    ],\n)\ndef test_cut_bool_coercion_to_int(bins, box, compare):\n    data_expected = box([0, 1, 1, 0, 1] * 10)\n    data_result = box([False, True, True, False, True] * 10)\n    expected = cut(data_expected, bins, duplicates=\"drop\")\n    result = cut(data_result, bins, duplicates=\"drop\")\n    compare(result, expected)"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n     }, {\n'url': 'https:\n         'only_matching': True,", "fixed": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n    }, {\n        'note': 'Video without discernible title',\n'url': 'https:\n        'info_dict': {\n            'id': '274175099429670',\n            'ext': 'mp4',\n            'title': 'Facebook video\n        }\n     }, {\n'url': 'https:\n         'only_matching': True,"}
{"id": "pandas_44", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n            tgt_values = target.asi8\n         else:\n             tgt_values = target._get_engine_target()", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if pself is not self or ptarget is not target:\n             return pself.get_indexer_non_unique(ptarget)\n        if is_categorical_dtype(target.dtype):\n             tgt_values = np.asarray(target)\n         else:\n             tgt_values = target._get_engine_target()"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n        elif isinstance(other, int):\n            other = Period(other, freq=self.freq)\n            result = ordinal_op(other.ordinal)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)", "fixed": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "keras_2", "problem": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)", "fixed": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\ndef in_top_k(predictions, targets, k):\n    top_k = np.argsort(-predictions)[:, :k]\n    targets = targets.reshape(-1, 1)\n    return np.any(targets == top_k, axis=-1)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n                self.wait += 1\n     def in_cooldown(self):\n         return self.cooldown_counter > 0", "fixed": " class ReduceLROnPlateau(Callback):\n                                   'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n     def in_cooldown(self):\n         return self.cooldown_counter > 0"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "ansible_13", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)"}
{"id": "youtube-dl_30", "problem": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f", "fixed": " class YoutubeDL(object):\n                 format_spec = selector.selector\n                 def selector_function(formats):\n                    formats = list(formats)\n                    if not formats:\n                        return\n                     if format_spec == 'all':\n                         for f in formats:\n                             yield f"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):", "fixed": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index + 1), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):"}
{"id": "thefuck_31", "problem": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return '{} --staged'.format(command.script)", "fixed": " def match(command, settings):\n @utils.git_support\n def get_new_command(command, settings):\n    return command.script.replace(' diff', ' diff --staged')"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_143", "problem": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if not (method is None and tolerance is None and is_list_like(target)):\n            return super().get_indexer(target, method=method, tolerance=tolerance)\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step", "fixed": " class RangeIndex(Int64Index):\n     @Appender(_index_shared_docs[\"get_indexer\"])\n     def get_indexer(self, target, method=None, limit=None, tolerance=None):\n        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):\n            return super().get_indexer(\n                target, method=method, tolerance=tolerance, limit=limit\n            )\n         if self.step > 0:\n             start, stop, step = self.start, self.stop, self.step"}
{"id": "scrapy_31", "problem": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]\n     getheaders = get_all", "fixed": " class WrappedResponse(object):\n     def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]\n     getheaders = get_all"}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "keras_25", "problem": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1.", "fixed": " def _preprocess_numpy_input(x, data_format, mode):\n         Preprocessed Numpy array.\n    x = x.astype(K.floatx())\n     if mode == 'tf':\n         x /= 127.5\n         x -= 1."}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "pandas_29", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        if needs_float_conversion:\n            left = left.astype(\"float\")\n        left.values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        if needs_float_conversion:\n            right = right.astype(\"float\")\n        right.values[key] = value_right\n         self._right = right\n     def __eq__(self, other):", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n                 msg = f\"'value' should be an interval type, got {type(value)} instead.\"\n                 raise TypeError(msg) from err\n        if needs_float_conversion:\n            raise ValueError(\"Cannot set float NaN to integer-backed IntervalArray\")\n         key = check_array_indexer(self, key)\n         left = self.left.copy(deep=True)\n        left._values[key] = value_left\n         self._left = left\n         right = self.right.copy(deep=True)\n        right._values[key] = value_right\n         self._right = right\n     def __eq__(self, other):"}
{"id": "scrapy_30", "problem": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).itervalues():\n             if inspect.isclass(obj) and \\\n               issubclass(obj, ScrapyCommand) and \\\n               obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):", "fixed": " def _iter_command_classes(module_name):\n     for module in walk_modules(module_name):\n        for obj in vars(module).values():\n             if inspect.isclass(obj) and \\\n                    issubclass(obj, ScrapyCommand) and \\\n                    obj.__module__ == module.__name__:\n                 yield obj\n def _get_commands_from_module(module, inproject):"}
{"id": "pandas_45", "problem": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)", "fixed": " def sanitize_array(\n         arr = np.arange(data.start, data.stop, data.step, dtype=\"int64\")\n         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)\n    elif isinstance(data, abc.Set):\n        raise TypeError(\"Set type is unordered\")\n     else:\n         subarr = _try_cast(data, dtype, copy, raise_cast_failure)"}
{"id": "fastapi_14", "problem": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[bool, Any]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None", "fixed": " class SchemaBase(BaseModel):\nnot_: Optional[List[Any]] = PSchema(None, alias=\"not\")\n     items: Optional[Any] = None\n     properties: Optional[Dict[str, Any]] = None\n    additionalProperties: Optional[Union[Dict[str, Any], bool]] = None\n     description: Optional[str] = None\n     format: Optional[str] = None\n     default: Optional[Any] = None"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"id": "luigi_11", "problem": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items())):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)", "fixed": " class Scheduler(object):\n             if (best_task and batched_params and task.family == best_task.family and\n                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(\n                    task.params.get(name) == value for name, value in unbatched_params.items()) and\n                    self._schedulable(task)):\n                 for name, params in batched_params.items():\n                     params.append(task.params.get(name))\n                 batched_tasks.append(task)"}
{"id": "pandas_72", "problem": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            len(arr_value.shape)\n            and arr_value.shape[0] == values.shape[0]\n            and arr_value.size == values.size\n         ):\n             values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:", "fixed": " class Block(PandasObject):\n             values[indexer] = value\n         elif (\n            exact_match\n            and is_categorical_dtype(arr_value.dtype)\n            and not is_categorical_dtype(values)\n         ):\n             values[indexer] = value\n            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))\n        elif exact_match:\n            values[indexer] = value\n             try:\n                 values = values.astype(arr_value.dtype)\n             except ValueError:"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                                                         kshp=kernel_shape,\n                                                         subsample=strides,\n                                                         border_mode=th_padding,\n                                                        filter_flip=not flip_filters,\n                                                        filter_dilation=dilation_rate)\n     conv_out = op(kernel, x, output_shape[2:])\n     conv_out = _postprocess_conv2d_output(conv_out, x, padding,\n                                           kernel_shape, strides, data_format)"}
{"id": "black_6", "problem": " from blib2to3 import pygram, pytree\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\n __version__ = \"19.3b0\"", "fixed": " from blib2to3 import pygram, pytree\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenizerConfig\n __version__ = \"19.3b0\""}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "youtube-dl_34", "problem": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n         if not expected:\nmsg = msg + u'; please report this issue on https:\n         super(ExtractorError, self).__init__(msg)", "fixed": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n        if cause:\n            msg += u' (caused by %r)' % cause\n         if not expected:\nmsg = msg + u'; please report this issue on https:\n         super(ExtractorError, self).__init__(msg)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_85", "problem": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n         if len(level_index):\n             grouper = level_index.take(codes)", "fixed": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n        else:\n            level_index = level_index.copy()\n         if len(level_index):\n             grouper = level_index.take(codes)"}
{"id": "fastapi_2", "problem": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:", "fixed": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "youtube-dl_26", "problem": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)", "fixed": " def js_to_json(code):\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         /\\*.*?\\*/|,(?=\\s*[\\]}])|\n         [a-zA-Z_][.a-zA-Z_0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\\s*:)?|\n         [0-9]+(?=\\s*:)"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()", "fixed": " def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n                if hidden_value.endswith(\"\\n\"):\n                    hidden_value = hidden_value[:-1]\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "pandas_117", "problem": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))", "fixed": " def _isna_old(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCGeneric):\n         return obj._constructor(obj._data.isna(func=_isna_old))"}
{"id": "pandas_123", "problem": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \"", "fixed": " class NumericIndex(Index):\n     _is_numeric_dtype = True\n     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):\n        cls._validate_dtype(dtype)\n         if fastpath is not None:\n             warnings.warn(\n                 \"The 'fastpath' keyword is deprecated, and will be \""}
{"id": "keras_30", "problem": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if x is None or len(x) == 0:\n                    batch_size = 1\n                elif isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]"}
{"id": "youtube-dl_31", "problem": " def parse_duration(s):\n     m = re.match(", "fixed": " def parse_duration(s):\n     m = re.match(\n            (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?"}
{"id": "pandas_40", "problem": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution", "fixed": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union, cast\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import ArrayLike, FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, extra={'spider': info.spider, 'failure': f})\n         )\nreturn dfd.addBoth(lambda _: wad)", "fixed": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\nreturn dfd.addBoth(lambda _: wad)"}
{"id": "keras_29", "problem": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)", "fixed": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                            self.stateful_metric_functions.append(metric_fn)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)"}
{"id": "pandas_20", "problem": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):", "fixed": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):"}
{"id": "pandas_154", "problem": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(obj.values)\n             if not cython_dtype:\n                cython_dtype = obj.values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = obj.values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(obj.values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:", "fixed": " class GroupBy(_GroupBy):\n         base_func = getattr(libgroupby, how)\n         for name, obj in self._iterate_slices():\n            values = obj._data._values\n             if aggregate:\n                 result_sz = ngroups\n             else:\n                result_sz = len(values)\n             if not cython_dtype:\n                cython_dtype = values.dtype\n             result = np.zeros(result_sz, dtype=cython_dtype)\n             func = partial(base_func, result, labels)\n             inferences = None\n             if needs_values:\n                vals = values\n                 if pre_processing:\n                     vals, inferences = pre_processing(vals)\n                 func = partial(func, vals)\n             if needs_mask:\n                mask = isna(values).view(np.uint8)\n                 func = partial(func, mask)\n             if needs_ngroups:"}
{"id": "pandas_23", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[])\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left.values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         start = right[0]\n         if end < start:\n            return type(self)(data=[], dtype=self.dtype, freq=self.freq)\n         else:\n             lslice = slice(*left.slice_locs(start, end))\n            left_chunk = left._values[lslice]\n             return self._shallow_copy(left_chunk)\n     def _can_fast_union(self, other) -> bool:"}
{"id": "scrapy_21", "problem": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        self._parsers.pop(netloc).callback(None)", "fixed": " class RobotsTxtMiddleware(object):\n         rp_dfd.callback(rp)\n     def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)"}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "black_6", "problem": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the", "fixed": " def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\ndef generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the"}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "pandas_110", "problem": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function).", "fixed": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     take_nd = take\n    @Appender(_index_shared_docs[\"_maybe_cast_slice_bound\"])\n    def _maybe_cast_slice_bound(self, label, side, kind):\n        if kind == \"loc\":\n            return label\n        return super()._maybe_cast_slice_bound(label, side, kind)\n     def map(self, mapper):\n         Map values using input correspondence (a dict, Series, or function)."}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "pandas_53", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n         if takeable:\n             return self._values[label]\n        return self.index.get_value(self, label)\n     def __setitem__(self, key, value):\n         key = com.apply_if_callable(key, self)", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n         if takeable:\n             return self._values[label]\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n     def __setitem__(self, key, value):\n         key = com.apply_if_callable(key, self)"}
{"id": "pandas_149", "problem": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_s3_url\n def get_engine(engine):", "fixed": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url\n def get_engine(engine):"}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"id": "keras_29", "problem": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:", "fixed": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:"}
{"id": "youtube-dl_25", "problem": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^0[xX][0-9a-fA-F]+', 16),\n            (r'^0+[0-7]+', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(0), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v", "fixed": " def js_to_json(code):\n             }.get(m.group(0), m.group(0)), v[1:-1])\n         INTEGER_TABLE = (\n            (r'^(0[xX][0-9a-fA-F]+)\\s*:?$', 16),\n            (r'^(0+[0-7]+)\\s*:?$', 8),\n         )\n         for regex, base in INTEGER_TABLE:\n             im = re.match(regex, v)\n             if im:\n                i = int(im.group(1), base)\n                 return '\"%d\":' % i if v.endswith(':') else '%d' % i\n         return '\"%s\"' % v"}
{"id": "pandas_90", "problem": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    if path is None:\n        path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(path) as path:\n        pd.to_pickle(obj, path)\n        return pd.read_pickle(path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):", "fixed": " def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr\n     pandas object\n         The original object that was pickled and then re-read.\n    _path = path\n    if _path is None:\n        _path = f\"__{rands(10)}__.pickle\"\n    with ensure_clean(_path) as path:\n        pd.to_pickle(obj, _path)\n        return pd.read_pickle(_path)\n def round_trip_pathlib(writer, reader, path: Optional[str] = None):"}
{"id": "black_6", "problem": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n }", "fixed": " VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n }"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n    def _extract_chapters(self, webpage, description, video_id, duration):\n        return (self._extract_chapters_from_json(webpage, video_id, duration)\n                or self._extract_chapters_from_description(description, duration))\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n         else:\n             output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output", "fixed": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n         else:\n             output = None\n    else:\n        output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output: Series or DataFrame\n             Object resulting from grouping and applying an operation.\n         Returns\n         -------", "fixed": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output : Series or DataFrame\n             Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n         Returns\n         -------"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "ansible_16", "problem": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 16,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 16\n         },\n     },\n     {", "fixed": " CPU_INFO_TEST_SCENARIOS = [\n                 '7', 'POWER7 (architected), altivec supported'\n             ],\n             'processor_cores': 1,\n            'processor_count': 8,\n             'processor_threads_per_core': 1,\n            'processor_vcpus': 8\n         },\n     },\n     {"}
{"id": "luigi_5", "problem": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        @task._task_wraps(task_that_inherits)\n        class Wrapped(task_that_inherits):\n            def clone_parent(_self, **args):\n                return _self.clone(cls=self.task_to_inherit, **args)\n        return Wrapped\n class requires(object):", "fixed": " class inherits(object):\n         self.task_to_inherit = task_to_inherit\n     def __call__(self, task_that_inherits):\n         for param_name, param_obj in self.task_to_inherit.get_params():\n             if not hasattr(task_that_inherits, param_name):\n                 setattr(task_that_inherits, param_name, param_obj)\n        def clone_parent(_self, **args):\n            return _self.clone(cls=self.task_to_inherit, **args)\n        task_that_inherits.clone_parent = clone_parent\n        return task_that_inherits\n class requires(object):"}
{"id": "black_18", "problem": " from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\n from enum import Enum, Flag\n from functools import partial, wraps\n import keyword\n import logging\n from multiprocessing import Manager", "fixed": " from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\n from enum import Enum, Flag\n from functools import partial, wraps\nimport io\n import keyword\n import logging\n from multiprocessing import Manager"}
{"id": "ansible_7", "problem": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n    for key in to_remove:\n        commands.append(\"no {0}\".format(key))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands", "fixed": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n    for key in to_remove:\n        if key in to_set.keys():\n            continue\n        commands.append(\"no {0}\".format(key))\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands"}
{"id": "ansible_12", "problem": "import os\n from ansible.plugins.lookup import LookupBase\n class LookupModule(LookupBase):", "fixed": " from ansible.plugins.lookup import LookupBase\nfrom ansible.utils import py3compat\n class LookupModule(LookupBase):"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
