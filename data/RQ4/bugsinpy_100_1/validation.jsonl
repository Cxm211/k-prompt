{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n     >>> import os\n     >>> os.remove(\"./dummy.pkl\")\n    path = stringify_path(path)\n    f, fh = get_handle(path, \"rb\", compression=compression, is_text=False)", "fixed": " def read_pickle(path, compression=\"infer\"):\n     >>> import os\n     >>> os.remove(\"./dummy.pkl\")\n    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n        filepath_or_buffer, compression=compression\n    )\n    if not isinstance(fp_or_buf, str) and compression == \"infer\":\n        compression = None\n    f, fh = get_handle(fp_or_buf, \"rb\", compression=compression, is_text=False)"}
{"id": "pandas_87", "problem": " def crosstab(\n     from pandas import DataFrame\n     df = DataFrame(data, index=common_idx)\n     if values is None:\n         df[\"__dummy__\"] = 0\n         kwargs = {\"aggfunc\": len, \"fill_value\": 0}", "fixed": " def crosstab(\n     from pandas import DataFrame\n     df = DataFrame(data, index=common_idx)\n    original_df_cols = df.columns\n     if values is None:\n         df[\"__dummy__\"] = 0\n         kwargs = {\"aggfunc\": len, \"fill_value\": 0}"}
{"id": "black_23", "problem": "def whitespace(leaf: Leaf) -> str:\n         ):\n             return NO\n     elif prev.type in OPENING_BRACKETS:\n         return NO", "fixed": "def whitespace(leaf: Leaf) -> str:\n         ):\n             return NO\n        elif (\n            prevp.type == token.RIGHTSHIFT\n            and prevp.parent\n            and prevp.parent.type == syms.shift_expr\n            and prevp.prev_sibling\n            and prevp.prev_sibling.type == token.NAME\n            and prevp.prev_sibling.value == 'print'\n        ):\n            return NO\n     elif prev.type in OPENING_BRACKETS:\n         return NO"}
{"id": "pandas_81", "problem": " from pandas.core.dtypes.common import (\n     is_list_like,\n     is_object_dtype,\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import register_extension_dtype\n from pandas.core.dtypes.missing import isna", "fixed": " from pandas.core.dtypes.common import (\n     is_list_like,\n     is_object_dtype,\n     is_scalar,\n    pandas_dtype,\n )\n from pandas.core.dtypes.dtypes import register_extension_dtype\n from pandas.core.dtypes.missing import isna"}
{"id": "black_14", "problem": " def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf\n def get_future_imports(node: Node) -> Set[str]:\n    imports = set()\n     for child in node.children:\n         if child.type != syms.simple_stmt:\n             break", "fixed": " def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf\n def get_future_imports(node: Node) -> Set[str]:\n    imports: Set[str] = set()\n    def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:\n        for child in children:\n            if isinstance(child, Leaf):\n                if child.type == token.NAME:\n                    yield child.value\n            elif child.type == syms.import_as_name:\n                orig_name = child.children[0]\n                assert isinstance(orig_name, Leaf), \"Invalid syntax parsing imports\"\n                assert orig_name.type == token.NAME, \"Invalid syntax parsing imports\"\n                yield orig_name.value\n            elif child.type == syms.import_as_names:\n                yield from get_imports_from_children(child.children)\n            else:\n                assert False, \"Invalid syntax parsing imports\"\n     for child in node.children:\n         if child.type != syms.simple_stmt:\n             break"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n         Compare datetime-like which is comparable ignoring dtype.\n     check_categorical : bool, default True\n         Whether to compare internal Categorical exactly.\n     obj : str, default 'Series'\n         Specify object name being compared, internally used to show appropriate\n         assertion message.", "fixed": " def assert_series_equal(\n         Compare datetime-like which is comparable ignoring dtype.\n     check_categorical : bool, default True\n         Whether to compare internal Categorical exactly.\n    check_category_order : bool, default True\n        Whether to compare category order of internal Categoricals\n        .. versionadded:: 1.0.2\n     obj : str, default 'Series'\n         Specify object name being compared, internally used to show appropriate\n         assertion message."}
{"id": "pandas_35", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     @cache_readonly\n     def _engine(self):\n        period = weakref.ref(self)\n         return self._engine_type(period, len(self))\n     @doc(Index.__contains__)", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):\n     @cache_readonly\n     def _engine(self):\n        period = weakref.ref(self._values)\n         return self._engine_type(period, len(self))\n     @doc(Index.__contains__)"}
{"id": "pandas_145", "problem": " def dispatch_to_series(left, right, func, str_rep=None, axis=None):\n         assert right.index.equals(left.columns)\n        def column_op(a, b):\n            return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}\n     elif isinstance(right, ABCSeries):\nassert right.index.equals(left.index)", "fixed": " def dispatch_to_series(left, right, func, str_rep=None, axis=None):\n         assert right.index.equals(left.columns)\n        if right.dtype == \"timedelta64[ns]\":\n            right = np.asarray(right)\n            def column_op(a, b):\n                return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}\n        else:\n            def column_op(a, b):\n                return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}\n     elif isinstance(right, ABCSeries):\nassert right.index.equals(left.index)"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n         out_height = conv_utils.deconv_length(height,\n                                               stride_h, kernel_h,\n                                               self.padding,\n                                              out_pad_h)\n         out_width = conv_utils.deconv_length(width,\n                                              stride_w, kernel_w,\n                                              self.padding,\n                                             out_pad_w)\n         if self.data_format == 'channels_first':\n             output_shape = (batch_size, self.filters, out_height, out_width)\n         else:", "fixed": " class Conv2DTranspose(Conv2D):\n         out_height = conv_utils.deconv_length(height,\n                                               stride_h, kernel_h,\n                                               self.padding,\n                                              out_pad_h,\n                                              self.dilation_rate[0])\n         out_width = conv_utils.deconv_length(width,\n                                              stride_w, kernel_w,\n                                              self.padding,\n                                             out_pad_w,\n                                             self.dilation_rate[1])\n         if self.data_format == 'channels_first':\n             output_shape = (batch_size, self.filters, out_height, out_width)\n         else:"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\n         inject_data = dict(\n            description='your description',\n             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\n         )\n         if galaxy_type == 'role':", "fixed": " class GalaxyCLI(CLI):\n         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\n         inject_data = dict(\n            description='your {0} description'.format(galaxy_type),\n             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\n         )\n         if galaxy_type == 'role':"}
{"id": "black_15", "problem": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines) or line.is_comment:\n         yield line\n         return", "fixed": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if line.is_comment:\n         yield line\n         return"}
{"id": "tornado_7", "problem": " import time\n import traceback\n import math\nfrom tornado.concurrent import TracebackFuture, is_future\n from tornado.log import app_log, gen_log\n from tornado.platform.auto import set_close_exec, Waker\n from tornado import stack_context", "fixed": " import time\n import traceback\n import math\nfrom tornado.concurrent import TracebackFuture, is_future, chain_future\n from tornado.log import app_log, gen_log\n from tornado.platform.auto import set_close_exec, Waker\n from tornado import stack_context"}
{"id": "luigi_9", "problem": " class RetcodesTest(LuigiTestCase):\n         with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):\n             self.run_and_expect('RequiringTask', 0)\n             self.run_and_expect('RequiringTask --retcode-not-run 5', 5)", "fixed": " class RetcodesTest(LuigiTestCase):\n         with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):\n             self.run_and_expect('RequiringTask', 0)\n             self.run_and_expect('RequiringTask --retcode-not-run 5', 5)\n    def test_retry_sucess_task(self):\n        class Foo(luigi.Task):\n            run_count = 0\n            def run(self):\n                self.run_count += 1\n                if self.run_count == 1:\n                    raise ValueError()\n            def complete(self):\n                return self.run_count > 0\n        self.run_and_expect('Foo --scheduler-retry-delay=0', 0)\n        self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)\n        self.run_with_config(dict(task_failed='3'), 'Foo', 0)"}
{"id": "tornado_13", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n             return connection_header != \"close\"\n         elif (\"Content-Length\" in headers\n               or headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\"\n              or start_line.method in (\"HEAD\", \"GET\")):\n             return connection_header == \"keep-alive\"\n         return False", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n             return connection_header != \"close\"\n         elif (\"Content-Length\" in headers\n               or headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\"\n              or getattr(start_line, 'method', None) in (\"HEAD\", \"GET\")):\n             return connection_header == \"keep-alive\"\n         return False"}
{"id": "keras_1", "problem": " def update_add(x, increment):\n         The variable `x` updated.\n    return tf_state_ops.assign_add(x, increment)\n @symbolic", "fixed": " def update_add(x, increment):\n         The variable `x` updated.\n    op = tf_state_ops.assign_add(x, increment)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                              use_multiprocessing=use_multiprocessing)\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10, workers=1,\n                           use_multiprocessing=False, verbose=0):", "fixed": " class Sequential(Model):\n                                              use_multiprocessing=use_multiprocessing)\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10, workers=1,\n                           use_multiprocessing=False, verbose=0):"}
{"id": "black_17", "problem": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n     srcbuf = io.BytesIO(src)\n     encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n     newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n     srcbuf.seek(0)\n     with io.TextIOWrapper(srcbuf, encoding) as tiow:", "fixed": " def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n     srcbuf = io.BytesIO(src)\n     encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    if not lines:\n        return \"\", encoding, \"\\n\"\n     newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n     srcbuf.seek(0)\n     with io.TextIOWrapper(srcbuf, encoding) as tiow:"}
{"id": "scrapy_33", "problem": " from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\n logger = logging.getLogger(__name__)", "fixed": " from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "keras_9", "problem": " def count_leading_spaces(s):\n def process_list_block(docstring, starting_point, section_end,\n                        leading_spaces, marker):\n     ending_point = docstring.find('\\n\\n', starting_point)\n    block = docstring[starting_point:(None if ending_point == -1 else\n                                      ending_point - 1)]\n     docstring_slice = docstring[starting_point:section_end].replace(block, marker)\n     docstring = (docstring[:starting_point]", "fixed": " def count_leading_spaces(s):\n def process_list_block(docstring, starting_point, section_end,\n                        leading_spaces, marker):\n     ending_point = docstring.find('\\n\\n', starting_point)\n    block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else\n                                      section_end)]\n     docstring_slice = docstring[starting_point:section_end].replace(block, marker)\n     docstring = (docstring[:starting_point]"}
{"id": "pandas_121", "problem": " class BlockManager(PandasObject):\n                         convert=convert,\n                         regex=regex,\n                     )\n                    if m.any():\n                         new_rb = _extend_blocks(result, new_rb)\n                     else:\n                         new_rb.append(b)", "fixed": " class BlockManager(PandasObject):\n                         convert=convert,\n                         regex=regex,\n                     )\n                    if m.any() or convert:\n                         new_rb = _extend_blocks(result, new_rb)\n                     else:\n                         new_rb.append(b)"}
{"id": "youtube-dl_19", "problem": " class YoutubeDL(object):\n                         FORMAT_RE.format(numeric_field),\n                         r'%({0})s'.format(numeric_field), outtmpl)\n            filename = expand_path(outtmpl % template_dict)", "fixed": " class YoutubeDL(object):\n                         FORMAT_RE.format(numeric_field),\n                         r'%({0})s'.format(numeric_field), outtmpl)\n            sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])\n            outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))\n            filename = expand_path(outtmpl).replace(sep, '') % template_dict"}
{"id": "pandas_126", "problem": " class DataFrame(NDFrame):\n             other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n        elif isinstance(other, list) and not isinstance(other[0], DataFrame):\n            other = DataFrame(other)\n            if (self.columns.get_indexer(other.columns) >= 0).all():\n                other = other.reindex(columns=self.columns)\n         from pandas.core.reshape.concat import concat", "fixed": " class DataFrame(NDFrame):\n             other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n        elif isinstance(other, list):\n            if not other:\n                pass\n            elif not isinstance(other[0], DataFrame):\n                other = DataFrame(other)\n                if (self.columns.get_indexer(other.columns) >= 0).all():\n                    other = other.reindex(columns=self.columns)\n         from pandas.core.reshape.concat import concat"}
{"id": "youtube-dl_3", "problem": " def unescapeHTML(s):\n     assert type(s) == compat_str\n     return re.sub(\n        r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n def get_subprocess_encoding():", "fixed": " def unescapeHTML(s):\n     assert type(s) == compat_str\n     return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n def get_subprocess_encoding():"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         else:\n             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n    def test_print_tensor(self):\n         check_single_tensor_operation('print_tensor', (), WITH_NP)\n         check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n        check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)\n        check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), WITH_NP)", "fixed": " class TestBackend(object):\n         else:\n             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)\n    def test_print_tensor(self, capsys):\n        for k in [KTH, KTF]:\n            x = k.placeholder((1, 1))\n            y = k.print_tensor(x, 'msg')\n            fn = k.function([x], [y])\n            _ = fn([np.ones((1, 1))])\n            out, err = capsys.readouterr()\n            assert out.replace('__str__ = ', '') == 'msg [[1.]]\\n'\n         check_single_tensor_operation('print_tensor', (), WITH_NP)\n         check_single_tensor_operation('print_tensor', (2,), WITH_NP)\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2), WITH_NP)"}
{"id": "luigi_23", "problem": " class scheduler(Config):\n     visualization_graph = parameter.Parameter(default=\"svg\", config_path=dict(section='scheduler', name='visualization-graph'))\n def fix_time(x):", "fixed": " class scheduler(Config):\n     visualization_graph = parameter.Parameter(default=\"svg\", config_path=dict(section='scheduler', name='visualization-graph'))\n    prune_on_get_work = parameter.BoolParameter(default=False)\n def fix_time(x):"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n             )\n         return jsonable_encoder(\n             obj_dict,\n            include_none=include_none,\n             custom_encoder=encoder,\n             sqlalchemy_safe=sqlalchemy_safe,\n         )", "fixed": " def jsonable_encoder(\n             )\n         return jsonable_encoder(\n             obj_dict,\n            exclude_none=exclude_none,\n            exclude_defaults=exclude_defaults,\n             custom_encoder=encoder,\n             sqlalchemy_safe=sqlalchemy_safe,\n         )"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n         )\n        msg = \"cannot insert TimedeltaIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, pd.Timestamp(\"2012-01-01\"))\n        msg = \"cannot insert TimedeltaIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n         )\n        msg = \"cannot insert TimedeltaArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, pd.Timestamp(\"2012-01-01\"))\n        msg = \"cannot insert TimedeltaArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "matplotlib_15", "problem": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')", "fixed": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')"}
{"id": "youtube-dl_34", "problem": " def js_to_json(code):\n             ([{,]\\s*)\n             (\"[^\"]*\"|\\'[^\\']*\\'|[a-z0-9A-Z]+)\n             (:\\s*)\n            ([0-9.]+|true|false|\"[^\"]*\"|\\'[^\\']*\\'|\\[|\\{)\n     res = re.sub(r',(\\s*\\])', lambda m: m.group(1), res)\n     return res", "fixed": " def js_to_json(code):\n             ([{,]\\s*)\n             (\"[^\"]*\"|\\'[^\\']*\\'|[a-z0-9A-Z]+)\n             (:\\s*)\n            ([0-9.]+|true|false|\"[^\"]*\"|\\'[^\\']*\\'|\n                (?=\\[|\\{)\n            )\n     res = re.sub(r',(\\s*\\])', lambda m: m.group(1), res)\n     return res"}
{"id": "thefuck_11", "problem": " def match(command):\n @git_support\n def get_new_command(command):\n     push_upstream = command.stderr.split('\\n')[-3].strip().partition('git ')[2]\n    return replace_argument(command.script, 'push', push_upstream)", "fixed": " def match(command):\n @git_support\n def get_new_command(command):\n    upstream_option_index = -1\n    try:\n        upstream_option_index = command.script_parts.index('--set-upstream')\n    except ValueError:\n        pass\n    try:\n        upstream_option_index = command.script_parts.index('-u')\n    except ValueError:\n        pass\n    if upstream_option_index is not -1:\n        command.script_parts.pop(upstream_option_index)\n        command.script_parts.pop(upstream_option_index)\n     push_upstream = command.stderr.split('\\n')[-3].strip().partition('git ')[2]\n    return replace_argument(\" \".join(command.script_parts), 'push', push_upstream)"}
{"id": "scrapy_33", "problem": " class FeedExporter(object):\n         d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                             extra={'spider': spider}))\n         d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            extra={'spider': spider, 'failure': f}))\n         return d\n     def item_scraped(self, item, spider):", "fixed": " class FeedExporter(object):\n         d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                             extra={'spider': spider}))\n         d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n         return d\n     def item_scraped(self, item, spider):"}
{"id": "pandas_70", "problem": " class BaseGrouper:\n             if mask.any():\n                 result = result.astype(\"float64\")\n                 result[mask] = np.nan\n         if kind == \"aggregate\" and self._filter_empty_groups and not counts.all():\n             assert result.ndim != 2", "fixed": " class BaseGrouper:\n             if mask.any():\n                 result = result.astype(\"float64\")\n                 result[mask] = np.nan\n        elif (\n            how == \"add\"\n            and is_integer_dtype(orig_values.dtype)\n            and is_extension_array_dtype(orig_values.dtype)\n        ):\n            result = result.astype(\"int64\")\n         if kind == \"aggregate\" and self._filter_empty_groups and not counts.all():\n             assert result.ndim != 2"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "thefuck_16", "problem": " from .generic import Generic\n class Bash(Generic):\n     def app_alias(self, fuck):\n        alias = \"TF_ALIAS={0}\" \\\n                \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " from .generic import Generic\n class Bash(Generic):\n     def app_alias(self, fuck):\n        alias = \"alias {0}='TF_CMD=$(TF_ALIAS={0}\" \\\n                \" PYTHONIOENCODING=utf-8\" \\\n                \" TF_SHELL_ALIASES=$(alias)\" \\\n                \" thefuck $(fc -ln -1)) &&\" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "fastapi_1", "problem": " def get_request_handler(\n                 exclude=response_model_exclude,\n                 by_alias=response_model_by_alias,\n                 exclude_unset=response_model_exclude_unset,\n                 is_coroutine=is_coroutine,\n             )\n             response = response_class(", "fixed": " def get_request_handler(\n                 exclude=response_model_exclude,\n                 by_alias=response_model_by_alias,\n                 exclude_unset=response_model_exclude_unset,\n                exclude_defaults=response_model_exclude_defaults,\n                exclude_none=response_model_exclude_none,\n                 is_coroutine=is_coroutine,\n             )\n             response = response_class("}
{"id": "keras_10", "problem": " def standardize_weights(y,\n     Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array.\n         y: Numpy array of model targets to be weighted.", "fixed": " def standardize_weights(y,\n     Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array. If both `sample_weights` and `class_weights` are provided,\n    the weights are multiplied together.\n         y: Numpy array of model targets to be weighted."}
{"id": "pandas_61", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 indexer = self.index.get_indexer_for(key)\n                 return self.iloc[indexer]\n             else:\n                return self._get_values(key)\n         if isinstance(key, (list, tuple)):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 indexer = self.index.get_indexer_for(key)\n                 return self.iloc[indexer]\n             else:\n                return self.iloc[key]\n         if isinstance(key, (list, tuple)):"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n         pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n         maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n         for fields in mtab_entries:\n             device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]", "fixed": " class LinuxHardware(Hardware):\n         pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n         maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n         for fields in mtab_entries:\n            fields = [self._replace_octal_escapes(field) for field in fields]\n             device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]"}
{"id": "keras_11", "problem": " def fit_generator(model,\n             if val_gen and workers > 0:\n                 val_data = validation_data\n                if isinstance(val_data, Sequence):\n                     val_enqueuer = OrderedEnqueuer(\n                         val_data,\n                         use_multiprocessing=use_multiprocessing)", "fixed": " def fit_generator(model,\n             if val_gen and workers > 0:\n                 val_data = validation_data\n                if is_sequence(val_data):\n                     val_enqueuer = OrderedEnqueuer(\n                         val_data,\n                         use_multiprocessing=use_multiprocessing)"}
{"id": "youtube-dl_42", "problem": " import re\n from .common import InfoExtractor\n from ..utils import (\n    fix_xml_all_ampersand,\n )", "fixed": " import re\n from .common import InfoExtractor\n from ..utils import (\n    fix_xml_ampersands,\n )"}
{"id": "pandas_79", "problem": " class MultiIndex(Index):\n                     indexer = self._get_level_indexer(key, level=level)\n                     new_index = maybe_mi_droplevels(indexer, [0], drop_level)\n                     return indexer, new_index\n            except TypeError:\n                 pass\n             if not any(isinstance(k, slice) for k in key):", "fixed": " class MultiIndex(Index):\n                     indexer = self._get_level_indexer(key, level=level)\n                     new_index = maybe_mi_droplevels(indexer, [0], drop_level)\n                     return indexer, new_index\n            except (TypeError, InvalidIndexError):\n                 pass\n             if not any(isinstance(k, slice) for k in key):"}
{"id": "keras_42", "problem": " class Model(Container):\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Integer. Maximum size for the generator queue.", "fixed": " class Model(Container):\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(validation_data)` as a number of steps.\n             class_weight: Dictionary mapping class indices to a weight\n                 for the class.\n             max_queue_size: Integer. Maximum size for the generator queue."}
{"id": "black_6", "problem": " single_quoted = (\n tabsize = 8\n class TokenError(Exception): pass\n class StopTokenizing(Exception): pass", "fixed": " single_quoted = (\n tabsize = 8\n@dataclass(frozen=True)\nclass TokenizerConfig:\n    async_is_reserved_keyword: bool = False\n class TokenError(Exception): pass\n class StopTokenizing(Exception): pass"}
{"id": "keras_34", "problem": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                output_generator = generator\n             callback_model.stop_training = False", "fixed": " class Model(Container):\n                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                 output_generator = enqueuer.get()\n             else:\n                if is_sequence:\n                    output_generator = iter(generator)\n                else:\n                    output_generator = generator\n             callback_model.stop_training = False"}
{"id": "keras_15", "problem": " class CSVLogger(Callback):\n         if not self.writer:\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n             self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)\n             if self.append_header:\n                 self.writer.writeheader()", "fixed": " class CSVLogger(Callback):\n         if not self.writer:\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n            fieldnames = ['epoch'] + self.keys\n            if six.PY2:\n                fieldnames = [unicode(x) for x in fieldnames]\n             self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=fieldnames,\n                                         dialect=CustomDialect)\n             if self.append_header:\n                 self.writer.writeheader()"}
{"id": "fastapi_1", "problem": " class APIRoute(routing.Route):\n         self.response_model_exclude = response_model_exclude\n         self.response_model_by_alias = response_model_by_alias\n         self.response_model_exclude_unset = response_model_exclude_unset\n         self.include_in_schema = include_in_schema\n         self.response_class = response_class", "fixed": " class APIRoute(routing.Route):\n         self.response_model_exclude = response_model_exclude\n         self.response_model_by_alias = response_model_by_alias\n         self.response_model_exclude_unset = response_model_exclude_unset\n        self.response_model_exclude_defaults = response_model_exclude_defaults\n        self.response_model_exclude_none = response_model_exclude_none\n         self.include_in_schema = include_in_schema\n         self.response_class = response_class"}
{"id": "pandas_36", "problem": " def _isna_old(obj):\n     elif hasattr(obj, \"__array__\"):\n         return _isna_ndarraylike_old(np.asarray(obj))\n     else:\n        return obj is None\n _isna = _isna_new", "fixed": " def _isna_old(obj):\n     elif hasattr(obj, \"__array__\"):\n         return _isna_ndarraylike_old(np.asarray(obj))\n     else:\n        return False\n _isna = _isna_new"}
{"id": "scrapy_1", "problem": " class OffsiteMiddleware(object):\n         if not allowed_domains:\nreturn re.compile('')\nurl_pattern = re.compile(\"^https?:\n         for domain in allowed_domains:\n            if url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                 warnings.warn(message, URLWarning)\n        domains = [re.escape(d) for d in allowed_domains if d is not None]\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)", "fixed": " class OffsiteMiddleware(object):\n         if not allowed_domains:\nreturn re.compile('')\nurl_pattern = re.compile(\"^https?:\n        domains = []\n         for domain in allowed_domains:\n            if domain is None:\n                continue\n            elif url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                 warnings.warn(message, URLWarning)\n            else:\n                domains.append(re.escape(domain))\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)"}
{"id": "keras_20", "problem": " def in_top_k(predictions, targets, k):\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):\n     data_format = normalize_data_format(data_format)\n     x = _preprocess_conv2d_input(x, data_format)", "fixed": " def in_top_k(predictions, targets, k):\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):\n     data_format = normalize_data_format(data_format)\n     x = _preprocess_conv2d_input(x, data_format)"}
{"id": "ansible_5", "problem": " def test_check_mutually_exclusive_none():\n def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):\n     with pytest.raises(TypeError) as te:\n         check_mutually_exclusive(mutually_exclusive_terms, None)\n        assert \"TypeError: 'NoneType' object is not iterable\" in to_native(te.error)", "fixed": " def test_check_mutually_exclusive_none():\n def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):\n     with pytest.raises(TypeError) as te:\n         check_mutually_exclusive(mutually_exclusive_terms, None)\n    assert \"'NoneType' object is not iterable\" in to_native(te.value)"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n                     spider=spider, exception=output.value)\n             else:\n                 logger.error('Error processing %(item)s', {'item': item},\n                             extra={'spider': spider, 'failure': output})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})", "fixed": " class Scraper(object):\n                     spider=spider, exception=output.value)\n             else:\n                 logger.error('Error processing %(item)s', {'item': item},\n                             exc_info=failure_to_exc_info(output),\n                             extra={'spider': spider})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})"}
{"id": "youtube-dl_16", "problem": " def srt_subtitles_timecode(seconds):\n def dfxp2srt(dfxp_data):\n     LEGACY_NAMESPACES = (\n('http:\n'http:\n'http:\n'http:\n         ]),\n('http:\n'http:\n         ]),\n     )", "fixed": " def srt_subtitles_timecode(seconds):\n def dfxp2srt(dfxp_data):\n     LEGACY_NAMESPACES = (\n(b'http:\nb'http:\nb'http:\nb'http:\n         ]),\n(b'http:\nb'http:\n         ]),\n     )"}
{"id": "ansible_6", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\n     if existing and not collection_info.force:\n        existing[0].add_requirement(to_text(collection_info), requirement)\n         collection_info = existing[0]\n     dep_map[to_text(collection_info)] = collection_info", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\n     if existing and not collection_info.force:\n        existing[0].add_requirement(parent, requirement)\n         collection_info = existing[0]\n     dep_map[to_text(collection_info)] = collection_info"}
{"id": "tornado_15", "problem": " setup(\n             \"options_test.cfg\",\n             \"static/robots.txt\",\n             \"static/dir/index.html\",\n             \"templates/utf8.html\",\n             \"test.crt\",\n             \"test.key\",", "fixed": " setup(\n             \"options_test.cfg\",\n             \"static/robots.txt\",\n             \"static/dir/index.html\",\n            \"static_foo.txt\",\n             \"templates/utf8.html\",\n             \"test.crt\",\n             \"test.key\","}
{"id": "black_15", "problem": " class CannotSplit(Exception):\n    It holds the number of bytes of the prefix consumed before the format\n    control comment appeared.\n        unformatted_prefix = leaf.prefix[: self.consumed]\n        return Leaf(token.NEWLINE, unformatted_prefix)\nclass FormatOn(FormatError):\n class WriteBack(Enum):\n     NO = 0\n     YES = 1", "fixed": " class CannotSplit(Exception):\n class WriteBack(Enum):\n     NO = 0\n     YES = 1"}
{"id": "ansible_6", "problem": " class CollectionRequirement:\n                 requirement = req\n                 op = operator.eq\n                if parent and version == '*' and requirement != '*':\n                    break\n                elif requirement == '*' or version == '*':\n                    continue\n             if not op(LooseVersion(version), LooseVersion(requirement)):\n                 break", "fixed": " class CollectionRequirement:\n                 requirement = req\n                 op = operator.eq\n            if parent and version == '*' and requirement != '*':\n                display.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\n                                \"install does not have a version set, the collection may not work.\"\n                                % (to_text(self), req, parent))\n                continue\n            elif requirement == '*' or version == '*':\n                continue\n             if not op(LooseVersion(version), LooseVersion(requirement)):\n                 break"}
{"id": "pandas_84", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n     index = data.index\n     clocs = [index._get_level_number(i) for i in clocs]\n     rlocs = [i for i in range(index.nlevels) if i not in clocs]", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n     index = data.index\n    if clocs in index.names:\n        clocs = [clocs]\n     clocs = [index._get_level_number(i) for i in clocs]\n     rlocs = [i for i in range(index.nlevels) if i not in clocs]"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n             output_shape,\n             self.strides,\n             padding=self.padding,\n            data_format=self.data_format)\n         if self.use_bias:\n             outputs = K.bias_add(", "fixed": " class Conv2DTranspose(Conv2D):\n             output_shape,\n             self.strides,\n             padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate)\n         if self.use_bias:\n             outputs = K.bias_add("}
{"id": "pandas_80", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             return self\n        arr = operator.inv(com.values_from_object(self))\n        return self.__array_wrap__(arr)\n     def __nonzero__(self):\n         raise ValueError(", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             return self\n        new_data = self._data.apply(operator.invert)\n        result = self._constructor(new_data).__finalize__(self)\n        return result\n     def __nonzero__(self):\n         raise ValueError("}
{"id": "pandas_36", "problem": " def na_value_for_dtype(dtype, compat: bool = True):\n     if is_extension_array_dtype(dtype):\n         return dtype.na_value\n    if (\n        is_datetime64_dtype(dtype)\n        or is_datetime64tz_dtype(dtype)\n        or is_timedelta64_dtype(dtype)\n        or is_period_dtype(dtype)\n    ):\n         return NaT\n     elif is_float_dtype(dtype):\n         return np.nan", "fixed": " def na_value_for_dtype(dtype, compat: bool = True):\n     if is_extension_array_dtype(dtype):\n         return dtype.na_value\n    if needs_i8_conversion(dtype):\n         return NaT\n     elif is_float_dtype(dtype):\n         return np.nan"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_6", "problem": " def match_filter_func(filter_str):\n def parse_dfxp_time_expr(time_expr):\n     if not time_expr:\n        return 0.0\n     mobj = re.match(r'^(?P<time_offset>\\d+(?:\\.\\d+)?)s?$', time_expr)\n     if mobj:", "fixed": " def match_filter_func(filter_str):\n def parse_dfxp_time_expr(time_expr):\n     if not time_expr:\n        return\n     mobj = re.match(r'^(?P<time_offset>\\d+(?:\\.\\d+)?)s?$', time_expr)\n     if mobj:"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n             - 'none': No patch boundary will be drawn.\n             - A color or sequence of colors.\n            For non-filled markers, the *edgecolors* kwarg is ignored and\n            forced to 'face' internally.\n         plotnonfinite : bool, default: False\n             Set to plot points with nonfinite *c*, in conjunction with", "fixed": " default: :rc:`scatter.edgecolors`\n             - 'none': No patch boundary will be drawn.\n             - A color or sequence of colors.\n            For non-filled markers, *edgecolors* is ignored. Instead, the color\n            is determined like with 'face', i.e. from *c*, *colors*, or\n            *facecolors*.\n         plotnonfinite : bool, default: False\n             Set to plot points with nonfinite *c*, in conjunction with"}
{"id": "pandas_89", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n             result = data\n             for i in range(len(clocs)):\n                 val = clocs[i]\n                result = result.unstack(val)\n                 clocs = [v if i > v else v - 1 for v in clocs]\n             return result", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n             result = data\n             for i in range(len(clocs)):\n                 val = clocs[i]\n                result = result.unstack(val, fill_value=fill_value)\n                 clocs = [v if i > v else v - 1 for v in clocs]\n             return result"}
{"id": "pandas_96", "problem": " class BusinessHourMixin(BusinessMixin):\n             if bd != 0:\n                skip_bd = BusinessDay(n=bd)\n                 if not self.next_bday.is_on_offset(other):\n                     prev_open = self._prev_opening_time(other)", "fixed": " class BusinessHourMixin(BusinessMixin):\n             if bd != 0:\n                if isinstance(self, _CustomMixin):\n                    skip_bd = CustomBusinessDay(\n                        n=bd,\n                        weekmask=self.weekmask,\n                        holidays=self.holidays,\n                        calendar=self.calendar,\n                    )\n                else:\n                    skip_bd = BusinessDay(n=bd)\n                 if not self.next_bday.is_on_offset(other):\n                     prev_open = self._prev_opening_time(other)"}
{"id": "keras_18", "problem": " class Function(object):\n                                 feed_symbols,\n                                 symbol_vals,\n                                 session)\n        fetched = self._callable_fn(*array_vals)\n         return fetched[:len(self.outputs)]\n     def _legacy_call(self, inputs):", "fixed": " class Function(object):\n                                 feed_symbols,\n                                 symbol_vals,\n                                 session)\n        if self.run_metadata:\n            fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\n        else:\n            fetched = self._callable_fn(*array_vals)\n         return fetched[:len(self.outputs)]\n     def _legacy_call(self, inputs):"}
{"id": "pandas_137", "problem": " class Categorical(ExtensionArray, PandasObject):\n             if dtype == self.dtype:\n                 return self\n             return self._set_dtype(dtype)\n         if is_integer_dtype(dtype) and self.isna().any():\n             msg = \"Cannot convert float NaN to integer\"\n             raise ValueError(msg)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             if dtype == self.dtype:\n                 return self\n             return self._set_dtype(dtype)\n        if is_extension_array_dtype(dtype):\n            return array(self, dtype=dtype, copy=copy)\n         if is_integer_dtype(dtype) and self.isna().any():\n             msg = \"Cannot convert float NaN to integer\"\n             raise ValueError(msg)"}
{"id": "pandas_97", "problem": " class TimedeltaIndex(\n                     result._set_freq(\"infer\")\n             return result\n    def _fast_union(self, other):\n         if len(other) == 0:\n             return self.view(type(self))", "fixed": " class TimedeltaIndex(\n                     result._set_freq(\"infer\")\n             return result\n    def _fast_union(self, other, sort=None):\n         if len(other) == 0:\n             return self.view(type(self))"}
{"id": "tornado_6", "problem": " import threading\n import time\n import traceback\n import math\nimport weakref\nfrom tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback\n from tornado.log import app_log, gen_log", "fixed": " import threading\n import time\n import traceback\n import math\nfrom tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback\n from tornado.log import app_log, gen_log"}
{"id": "thefuck_3", "problem": " class Fish(Generic):\n     def info(self):\n        proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],\n                      stdout=PIPE, stderr=DEVNULL)\n        version = proc.stdout.read().decode('utf-8').strip()\n         return u'Fish Shell {}'.format(version)\n     def put_to_history(self, command):", "fixed": " class Fish(Generic):\n     def info(self):\n        proc = Popen(['fish', '--version'],\n                      stdout=PIPE, stderr=DEVNULL)\n        version = proc.stdout.read().decode('utf-8').split()[-1]\n         return u'Fish Shell {}'.format(version)\n     def put_to_history(self, command):"}
{"id": "luigi_20", "problem": " class Task(object):\n         params_str = {}\n         params = dict(self.get_params())\n         for param_name, param_value in six.iteritems(self.param_kwargs):\n            if params[param_name].significant:\n                params_str[param_name] = params[param_name].serialize(param_value)\n         return params_str", "fixed": " class Task(object):\n         params_str = {}\n         params = dict(self.get_params())\n         for param_name, param_value in six.iteritems(self.param_kwargs):\n            params_str[param_name] = params[param_name].serialize(param_value)\n         return params_str"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         self.target_codes = set()\n         self.target_frames = set()\n         self.thread_local = threading.local()\n     def __call__(self, function):\n         self.target_codes.add(function.__code__)\n         @functools.wraps(function)", "fixed": " class Tracer:\n         self.target_codes = set()\n         self.target_frames = set()\n         self.thread_local = threading.local()\n        if len(custom_repr) == 2 and not all(isinstance(x,\n                      pycompat.collections_abc.Iterable) for x in custom_repr):\n            custom_repr = (custom_repr,)\n        self.custom_repr = custom_repr\n     def __call__(self, function):\n        if DISABLED:\n            return function\n         self.target_codes.add(function.__code__)\n         @functools.wraps(function)"}
{"id": "matplotlib_27", "problem": " class ColorbarBase(_ColorbarMappableDummy):\n     def set_label(self, label, **kw):\n        self._label = str(label)\n         self._labelkw = kw\n         self._set_label()", "fixed": " class ColorbarBase(_ColorbarMappableDummy):\n     def set_label(self, label, **kw):\n        self._label = label\n         self._labelkw = kw\n         self._set_label()"}
{"id": "luigi_32", "problem": " except ImportError:\n     from ordereddict import OrderedDict\n from luigi import six\n class TaskClassException(Exception):", "fixed": " except ImportError:\n     from ordereddict import OrderedDict\n from luigi import six\nimport logging\nlogger = logging.getLogger('luigi-interface')\n class TaskClassException(Exception):"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         left, right = sorted([left, right], reverse=bool(reverse))\n         self._viewLim.intervalx = (left, right)\n         if auto is not None:\n             self._autoscaleXon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         left, right = sorted([left, right], reverse=bool(reverse))\n         self._viewLim.intervalx = (left, right)\n        for ax in self._shared_x_axes.get_siblings(self):\n            ax._stale_viewlim_x = False\n         if auto is not None:\n             self._autoscaleXon = bool(auto)"}
{"id": "thefuck_28", "problem": " patterns = (\n         '^lua: {file}:{line}:',\n        '^{file} \\(line {line}\\):',\n         '^{file}: line {line}: ',\n        '^{file}:{line}:',\n         '^{file}:{line}:{col}',\n         'at {file} line {line}',\n     )", "fixed": " patterns = (\n         '^lua: {file}:{line}:',\n        '^{file} \\\\(line {line}\\\\):',\n         '^{file}: line {line}: ',\n         '^{file}:{line}:{col}',\n        '^{file}:{line}:',\n         'at {file} line {line}',\n     )"}
{"id": "keras_11", "problem": " from keras.utils.data_utils import validate_file\n from keras import backend as K\n pytestmark = pytest.mark.skipif(\n    K.backend() == 'tensorflow',\n     reason='Temporarily disabled until the use_multiprocessing problem is solved')\n if sys.version_info < (3,):", "fixed": " from keras.utils.data_utils import validate_file\n from keras import backend as K\n pytestmark = pytest.mark.skipif(\n    K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,\n     reason='Temporarily disabled until the use_multiprocessing problem is solved')\n if sys.version_info < (3,):"}
{"id": "matplotlib_2", "problem": " default: :rc:`scatter.edgecolors`\n         collection = mcoll.PathCollection(\n                 (path,), scales,\n                facecolors=colors,\n                edgecolors=edgecolors,\n                 linewidths=linewidths,\n                 offsets=offsets,\n                 transOffset=kwargs.pop('transform', self.transData),", "fixed": " default: :rc:`scatter.edgecolors`\n         collection = mcoll.PathCollection(\n                 (path,), scales,\n                facecolors=colors if marker_obj.is_filled() else 'none',\n                edgecolors=edgecolors if marker_obj.is_filled() else colors,\n                 linewidths=linewidths,\n                 offsets=offsets,\n                 transOffset=kwargs.pop('transform', self.transData),"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "PySnooper_2", "problem": " class Tracer:\n         self._write(s)\n     def __enter__(self):\n         calling_frame = inspect.currentframe().f_back\n         if not self._is_internal_frame(calling_frame):\n             calling_frame.f_trace = self.trace\n             self.target_frames.add(calling_frame)\n        stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])\n         stack.append(sys.gettrace())\n         sys.settrace(self.trace)\n     def __exit__(self, exc_type, exc_value, exc_traceback):\n         stack = self.thread_local.original_trace_functions\n         sys.settrace(stack.pop())\n         calling_frame = inspect.currentframe().f_back", "fixed": " class Tracer:\n         self._write(s)\n     def __enter__(self):\n        if DISABLED:\n            return\n         calling_frame = inspect.currentframe().f_back\n         if not self._is_internal_frame(calling_frame):\n             calling_frame.f_trace = self.trace\n             self.target_frames.add(calling_frame)\n        stack = self.thread_local.__dict__.setdefault(\n            'original_trace_functions', []\n        )\n         stack.append(sys.gettrace())\n         sys.settrace(self.trace)\n     def __exit__(self, exc_type, exc_value, exc_traceback):\n        if DISABLED:\n            return\n         stack = self.thread_local.original_trace_functions\n         sys.settrace(stack.pop())\n         calling_frame = inspect.currentframe().f_back"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         from .tight_layout import (\n             get_renderer, get_subplotspec_list, get_tight_layout_figure)\n         subplotspec_list = get_subplotspec_list(self.axes)\n         if None in subplotspec_list:", "fixed": " default: 'top'\n         from .tight_layout import (\n             get_renderer, get_subplotspec_list, get_tight_layout_figure)\n        from .cbook import _setattr_cm\n        from .backend_bases import RendererBase\n         subplotspec_list = get_subplotspec_list(self.axes)\n         if None in subplotspec_list:"}
{"id": "pandas_111", "problem": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     @Appender(_index_shared_docs[\"_convert_scalar_indexer\"])\n     def _convert_scalar_indexer(self, key, kind=None):\n        if self.categories._defer_to_indexing:\n            return self.categories._convert_scalar_indexer(key, kind=kind)\n         return super()._convert_scalar_indexer(key, kind=kind)\n     @Appender(_index_shared_docs[\"_convert_list_indexer\"])", "fixed": " class CategoricalIndex(Index, accessor.PandasDelegate):\n     @Appender(_index_shared_docs[\"_convert_scalar_indexer\"])\n     def _convert_scalar_indexer(self, key, kind=None):\n        if kind == \"loc\":\n            try:\n                return self.categories._convert_scalar_indexer(key, kind=kind)\n            except TypeError:\n                self._invalid_indexer(\"label\", key)\n         return super()._convert_scalar_indexer(key, kind=kind)\n     @Appender(_index_shared_docs[\"_convert_list_indexer\"])"}
{"id": "black_18", "problem": " def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n     if src_txt[-1] != \"\\n\":\n        nl = \"\\r\\n\" if \"\\r\\n\" in src_txt[:1024] else \"\\n\"\n        src_txt += nl\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)\n         try:", "fixed": " def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n     if src_txt[-1] != \"\\n\":\n        src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)\n         try:"}
{"id": "youtube-dl_20", "problem": " def get_elements_by_attribute(attribute, value, html, escape_value=True):\n     retlist = []\n         <([a-zA-Z0-9:._-]+)\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'))*?\n          \\s+%s=['\"]?%s['\"]?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'))*?\n         \\s*>\n         (?P<content>.*?)\n         </\\1>", "fixed": " def get_elements_by_attribute(attribute, value, html, escape_value=True):\n     retlist = []\n         <([a-zA-Z0-9:._-]+)\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n          \\s+%s=['\"]?%s['\"]?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n         \\s*>\n         (?P<content>.*?)\n         </\\1>"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if method == \"pearson\":\n            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)\n         elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)\n         elif method == \"kendall\" or callable(method):\n             if min_periods is None:\n                 min_periods = 1\n            mat = ensure_float64(mat).T\n             corrf = nanops.get_corr_func(method)\n             K = len(cols)\n             correl = np.empty((K, K), dtype=float)", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if method == \"pearson\":\n            correl = libalgos.nancorr(mat, minp=min_periods)\n         elif method == \"spearman\":\n            correl = libalgos.nancorr_spearman(mat, minp=min_periods)\n         elif method == \"kendall\" or callable(method):\n             if min_periods is None:\n                 min_periods = 1\n            mat = mat.T\n             corrf = nanops.get_corr_func(method)\n             K = len(cols)\n             correl = np.empty((K, K), dtype=float)"}
{"id": "pandas_70", "problem": " def test_resample_categorical_data_with_timedeltaindex():\n         index=pd.to_timedelta([0, 10], unit=\"s\"),\n     )\n     expected = expected.reindex([\"Group_obj\", \"Group\"], axis=1)\n    expected[\"Group\"] = expected[\"Group_obj\"].astype(\"category\")\n     tm.assert_frame_equal(result, expected)", "fixed": " def test_resample_categorical_data_with_timedeltaindex():\n         index=pd.to_timedelta([0, 10], unit=\"s\"),\n     )\n     expected = expected.reindex([\"Group_obj\", \"Group\"], axis=1)\n    expected[\"Group\"] = expected[\"Group_obj\"]\n     tm.assert_frame_equal(result, expected)"}
{"id": "tornado_10", "problem": " class WebSocketHandler(tornado.web.RequestHandler):\n         if not self._on_close_called:\n             self._on_close_called = True\n             self.on_close()\n     def send_error(self, *args, **kwargs):\n         if self.stream is None:", "fixed": " class WebSocketHandler(tornado.web.RequestHandler):\n         if not self._on_close_called:\n             self._on_close_called = True\n             self.on_close()\n            self._break_cycles()\n    def _break_cycles(self):\n        if self.get_status() != 101 or self._on_close_called:\n            super(WebSocketHandler, self)._break_cycles()\n     def send_error(self, *args, **kwargs):\n         if self.stream is None:"}
{"id": "pandas_150", "problem": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if left_value != right_value:\n                     return False\n         return True", "fixed": " def array_equivalent(left, right, strict_nan=False):\n                 if not isinstance(right_value, float) or not np.isnan(right_value):\n                     return False\n             else:\n                if np.any(left_value != right_value):\n                     return False\n         return True"}
{"id": "ansible_5", "problem": " def check_required_arguments(argument_spec, module_parameters):\n             missing.append(k)\n     if missing:\n        msg = \"missing required arguments: %s\" % \", \".join(missing)\n         raise TypeError(to_native(msg))\n     return missing", "fixed": " def check_required_arguments(argument_spec, module_parameters):\n             missing.append(k)\n     if missing:\n        msg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\n         raise TypeError(to_native(msg))\n     return missing"}
{"id": "matplotlib_15", "problem": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')", "fixed": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')"}
{"id": "tornado_13", "problem": " TEST_MODULES = [\n     'tornado.test.curl_httpclient_test',\n     'tornado.test.escape_test',\n     'tornado.test.gen_test',\n     'tornado.test.httpclient_test',\n     'tornado.test.httpserver_test',\n     'tornado.test.httputil_test',", "fixed": " TEST_MODULES = [\n     'tornado.test.curl_httpclient_test',\n     'tornado.test.escape_test',\n     'tornado.test.gen_test',\n    'tornado.test.http1connection_test',\n     'tornado.test.httpclient_test',\n     'tornado.test.httpserver_test',\n     'tornado.test.httputil_test',"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to\n     :func:`format_file_contents`.\n    src = sys.stdin.read()\n     dst = src\n     try:\n         dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)", "fixed": " def format_stdin_to_stdout(\n     `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to\n     :func:`format_file_contents`.\n    newline, encoding, src = prepare_input(sys.stdin.buffer.read())\n     dst = src\n     try:\n         dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)"}
{"id": "pandas_112", "problem": " from pandas.core.dtypes.generic import ABCSeries\n from pandas.core.dtypes.missing import isna\n from pandas._typing import AnyArrayLike\n from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs\n import pandas.core.common as com\n import pandas.core.indexes.base as ibase", "fixed": " from pandas.core.dtypes.generic import ABCSeries\n from pandas.core.dtypes.missing import isna\n from pandas._typing import AnyArrayLike\nfrom pandas.core.algorithms import take_1d\n from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs\n import pandas.core.common as com\n import pandas.core.indexes.base as ibase"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def predict_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "luigi_6", "problem": " class DictParameter(Parameter):\n     tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related\n     values (like a database connection config).\n        JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n         Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.", "fixed": " class DictParameter(Parameter):\n     tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related\n     values (like a database connection config).\n         Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed."}
{"id": "pandas_13", "problem": " def _isna_old(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike_old(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike_old(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_old(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=True)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=True)\n     else:\n         return False"}
{"id": "matplotlib_28", "problem": " class _AxesBase(martist.Artist):\n             if right is None:\n                 right = old_right\n        if self.get_xscale() == 'log':\n             if left <= 0:\n                 cbook._warn_external(\n                     'Attempted to set non-positive left xlim on a '", "fixed": " class _AxesBase(martist.Artist):\n             if right is None:\n                 right = old_right\n        if self.get_xscale() == 'log' and (left <= 0 or right <= 0):\n            old_left, old_right = self.get_xlim()\n             if left <= 0:\n                 cbook._warn_external(\n                     'Attempted to set non-positive left xlim on a '"}
{"id": "tqdm_8", "problem": " class tqdm(object):\n                     l_bar_user, r_bar_user = bar_format.split('{bar}')\n                    l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)\n                 else:\n                     return bar_format.format(**bar_args)", "fixed": " class tqdm(object):\n                     l_bar_user, r_bar_user = bar_format.split('{bar}')\n                    l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)\n                 else:\n                     return bar_format.format(**bar_args)"}
{"id": "keras_19", "problem": " class SimpleRNNCell(Layer):\n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class SimpleRNNCell(Layer):\n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
