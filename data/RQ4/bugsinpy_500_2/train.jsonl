{"id": "pandas_23", "problem": " class TestGetItem:\n     def test_dti_business_getitem(self):\n         rng = pd.bdate_range(START, END)\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5])\n         tm.assert_index_equal(smaller, exp)\n         assert smaller.freq == rng.freq", "fixed": " class TestGetItem:\n     def test_dti_business_getitem(self):\n         rng = pd.bdate_range(START, END)\n         smaller = rng[:5]\n        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=\"B\")\n         tm.assert_index_equal(smaller, exp)\n        assert smaller.freq == exp.freq\n         assert smaller.freq == rng.freq"}
{"id": "pandas_167", "problem": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n     _tz = None\n     _freq = None", "fixed": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n    _supports_partial_string_indexing = True\n     _tz = None\n     _freq = None"}
{"id": "black_11", "problem": " def is_import(leaf: Leaf) -> bool:\n     )\n def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:", "fixed": " def is_import(leaf: Leaf) -> bool:\n     )\ndef is_special_comment(leaf: Leaf) -> bool:\n    t = leaf.type\n    v = leaf.value\n    return bool(\n        (t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith(\"\n    )\n def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:"}
{"id": "thefuck_20", "problem": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])\n def side_effect(old_cmd, command):", "fixed": " def match(command):\n def get_new_command(command):\n    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))\n def side_effect(old_cmd, command):"}
{"id": "keras_42", "problem": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):", "fixed": " class Model(Container):\n         return self.history\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10,\n                            workers=1,\n                            use_multiprocessing=False):"}
{"id": "keras_18", "problem": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):", "fixed": " class Function(object):\n                         'supported with sparse inputs.')\n                 return self._legacy_call(inputs)\n            if (self.run_metadata and\n                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'In order to feed symbolic tensors to a Keras model and set '\n                        '`run_metadata`, you need tensorflow 1.10 or higher.')\n                return self._legacy_call(inputs)\n             return self._call(inputs)\n         else:\n             if py_any(is_tensor(x) for x in inputs):"}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n             if not ax.is_unique:\n                 return False", "fixed": " class _LocIndexer(_LocationIndexer):\n             if isinstance(ax, MultiIndex):\n                 return False\n            if isinstance(k, str) and ax._supports_partial_string_indexing:\n                return False\n             if not ax.is_unique:\n                 return False"}
{"id": "ansible_9", "problem": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, 1\n        pool_ids[pool_id] = str(quantity)\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]", "fixed": " def main():\n                 module.fail_json(msg='Unable to parse pool_ids option.')\n             pool_id, quantity = list(value.items())[0]\n         else:\n            pool_id, quantity = value, None\n        pool_ids[pool_id] = quantity\n     consumer_type = module.params[\"consumer_type\"]\n     consumer_name = module.params[\"consumer_name\"]\n     consumer_id = module.params[\"consumer_id\"]"}
{"id": "thefuck_27", "problem": " def match(command, settings):\n def get_new_command(command, settings):\nreturn 'open http:", "fixed": " def match(command, settings):\n def get_new_command(command, settings):\nreturn command.script.replace('open ', 'open http:"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n        elif isinstance(other, int):\n            other = Period(other, freq=self.freq)\n            result = ordinal_op(other.ordinal)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)", "fixed": " def _period_array_cmp(cls, op):\n             except ValueError:\n                 return invalid_comparison(self, other, op)\n         if isinstance(other, self._recognized_scalars) or other is NaT:\n             other = self._scalar_type(other)"}
{"id": "keras_36", "problem": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1, 1) + strides + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1, 1) + strides\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)", "fixed": " def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     padding = _preprocess_padding(padding)\n     if tf_data_format == 'NHWC':\n         spatial_start_dim = 1\n        strides = (1,) + strides * 2 + (1,)\n     else:\n         spatial_start_dim = 2\n        strides = (1, 1) + strides * 2\n     x = tf.expand_dims(x, spatial_start_dim)\n     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)"}
{"id": "pandas_112", "problem": " from pandas.core.dtypes.cast import (\n )\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_dtype_equal,", "fixed": " from pandas.core.dtypes.cast import (\n )\n from pandas.core.dtypes.common import (\n     ensure_platform_int,\n    is_categorical,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_dtype_equal,"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n     for f in flat_dependant.body_params:\n         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)\n     required = any(True for f in flat_dependant.body_params if f.required)\n    BodySchema_kwargs: Dict[str, Any] = dict(default=None)\n     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):\n         BodySchema: Type[params.Body] = params.File\n     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):"}
{"id": "fastapi_2", "problem": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:", "fixed": " class APIRouter(routing.Router):\n     def add_api_websocket_route(\n         self, path: str, endpoint: Callable, name: str = None\n     ) -> None:\n        route = APIWebSocketRoute(\n            path,\n            endpoint=endpoint,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n         self.routes.append(route)\n     def websocket(self, path: str, name: str = None) -> Callable:"}
{"id": "pandas_157", "problem": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 )\n             )\n            if is_datetimelike(lt):\n                 if not isinstance(self.tolerance, Timedelta):\n                     raise MergeError(msg)\n                 if self.tolerance < Timedelta(0):"}
{"id": "pandas_83", "problem": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort\n         )\n     def _get_concat_axis(self) -> Index:", "fixed": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,\n         )\n     def _get_concat_axis(self) -> Index:"}
{"id": "scrapy_3", "problem": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['location'])\n         redirected_url = urljoin(request.url, location)", "fixed": " class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n        location = safe_url_string(response.headers['Location'])\nif response.headers['Location'].startswith(b'\n            request_scheme = urlparse(request.url).scheme\nlocation = request_scheme + ':\n         redirected_url = urljoin(request.url, location)"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_9", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n )\n from pandas.core.dtypes.dtypes import CategoricalDtype\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna\n from pandas.core import accessor\n from pandas.core.algorithms import take_1d"}
{"id": "luigi_33", "problem": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if p.significant]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))", "fixed": " class Task(object):\n         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)\n        positional_params = [(n, p) for n, p in params if not p.is_global]\n         for i, arg in enumerate(args):\n             if i >= len(positional_params):\n                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))"}
{"id": "pandas_80", "problem": "from .interface import BaseInterfaceTests\nfrom .io import BaseParsingTests\nfrom .methods import BaseMethodsTests\nfrom .missing import BaseMissingTests\nfrom .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil\nfrom .printing import BasePrintingTests\nfrom .reduce import (\n     BaseBooleanReduceTests,", "fixed": "from .interface import BaseInterfaceTests\nfrom .io import BaseParsingTests\nfrom .methods import BaseMethodsTests\nfrom .missing import BaseMissingTests\nfrom .ops import (\n    BaseArithmeticOpsTests,\n    BaseComparisonOpsTests,\n    BaseOpsUtil,\n    BaseUnaryOpsTests,\n)\nfrom .printing import BasePrintingTests\nfrom .reduce import (\n     BaseBooleanReduceTests,"}
{"id": "keras_2", "problem": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)", "fixed": " def l2_normalize(x, axis=-1):\n     return x / np.sqrt(y)\ndef in_top_k(predictions, targets, k):\n    top_k = np.argsort(-predictions)[:, :k]\n    targets = targets.reshape(-1, 1)\n    return np.any(targets == top_k, axis=-1)\n def binary_crossentropy(target, output, from_logits=False):\n     if not from_logits:\n         output = np.clip(output, 1e-7, 1 - 1e-7)"}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "pandas_27", "problem": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.tslibs import (\n     timezones,\n     tzconversion,\n )\nimport pandas._libs.tslibs.frequencies as libfrequencies\n from pandas.errors import PerformanceWarning\n from pandas.core.dtypes.common import ("}
{"id": "youtube-dl_41", "problem": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',',' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9:]*$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',", "fixed": " def unified_strdate(date_str):\n     upload_date = None\n    date_str = date_str.replace(',', ' ')\n    date_str = re.sub(r' ?(\\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)\n     format_expressions = [\n         '%d %B %Y',\n         '%B %d %Y',"}
{"id": "black_8", "problem": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                if leaves[-1].type != token.COMMA:\n                    leaves.append(Leaf(token.COMMA, \",\"))\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)", "fixed": " def bracket_split_build_line(\n         if leaves:\n             normalize_prefix(leaves[0], inside_brackets=True)\n             if original.is_import:\n                for i in range(len(leaves) - 1, -1, -1):\n                    if leaves[i].type == STANDALONE_COMMENT:\n                        continue\n                    elif leaves[i].type == token.COMMA:\n                        break\n                    else:\n                        leaves.insert(i + 1, Leaf(token.COMMA, \",\"))\n                        break\n     for leaf in leaves:\n         result.append(leaf, preformatted=True)"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n                 when using process based threading"}
{"id": "black_15", "problem": " from typing import (\n     Sequence,\n     Set,\n     Tuple,\n    Type,\n     TypeVar,\n     Union,\n     cast,", "fixed": " from typing import (\n     Sequence,\n     Set,\n     Tuple,\n     TypeVar,\n     Union,\n     cast,"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n     @property\n     def trainable(self):", "fixed": " class Bidirectional(Wrapper):\n         self.supports_masking = True\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n        self.input_spec = layer.input_spec\n     @property\n     def trainable(self):"}
{"id": "tqdm_3", "problem": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")", "fixed": " class tqdm(Comparable):\n         self.start_t = self.last_print_t\n    def __bool__(self):\n        if self.total is not None:\n            return self.total > 0\n        if self.iterable is None:\n            raise TypeError('Boolean cast is undefined'\n                            ' for tqdm objects that have no iterable or total')\n        return bool(self.iterable)\n    def __nonzero__(self):\n        return self.__bool__()\n     def __len__(self):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")"}
{"id": "pandas_65", "problem": " Module contains tools for processing files into DataFrames or other objects\n from collections import abc, defaultdict\n import csv\n import datetime\nfrom io import BufferedIOBase, StringIO, TextIOWrapper\n import re\n import sys\n from textwrap import fill", "fixed": " Module contains tools for processing files into DataFrames or other objects\n from collections import abc, defaultdict\n import csv\n import datetime\nfrom io import BufferedIOBase, RawIOBase, StringIO, TextIOWrapper\n import re\n import sys\n from textwrap import fill"}
{"id": "pandas_80", "problem": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n     @pytest.mark.parametrize(\n         \"df\",\n         [", "fixed": " class TestDataFrameUnaryOperators:\n         tm.assert_frame_equal(-(df < 0), ~(df < 0))\n    def test_invert_mixed(self):\n        shape = (10, 5)\n        df = pd.concat(\n            [\n                pd.DataFrame(np.zeros(shape, dtype=\"bool\")),\n                pd.DataFrame(np.zeros(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        result = ~df\n        expected = pd.concat(\n            [\n                pd.DataFrame(np.ones(shape, dtype=\"bool\")),\n                pd.DataFrame(-np.ones(shape, dtype=int)),\n            ],\n            axis=1,\n            ignore_index=True,\n        )\n        tm.assert_frame_equal(result, expected)\n     @pytest.mark.parametrize(\n         \"df\",\n         ["}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, extra={'spider': info.spider, 'failure': f})\n         )\nreturn dfd.addBoth(lambda _: wad)", "fixed": " class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\nreturn dfd.addBoth(lambda _: wad)"}
{"id": "luigi_6", "problem": " def _recursively_freeze(value):\n     Parameter whose value is a ``dict``.", "fixed": " def _recursively_freeze(value):\n    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.\n     Parameter whose value is a ``dict``."}
{"id": "thefuck_12", "problem": " from difflib import get_close_matches\n from thefuck.utils import get_all_executables, \\\n    get_valid_history_without_current, get_closest\n from thefuck.specific.sudo import sudo_support\n @sudo_support\n def match(command):\n    return (command.script_parts\n             and 'not found' in command.stderr\n             and bool(get_close_matches(command.script_parts[0],\n                                        get_all_executables())))", "fixed": " from difflib import get_close_matches\n from thefuck.utils import get_all_executables, \\\n    get_valid_history_without_current, get_closest, which\n from thefuck.specific.sudo import sudo_support\n @sudo_support\n def match(command):\n    return (not which(command.script_parts[0])\n             and 'not found' in command.stderr\n             and bool(get_close_matches(command.script_parts[0],\n                                        get_all_executables())))"}
{"id": "pandas_129", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):\n             if not isinstance(other, DatetimeLikeArrayMixin):\n                 from pandas.core.arrays import DatetimeArray", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n         if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):\n            if lib.is_scalar(other):\n                return Timestamp(other) - self\n             if not isinstance(other, DatetimeLikeArrayMixin):\n                 from pandas.core.arrays import DatetimeArray"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n    if sample_weight is not None and class_weight is not None:\n        warnings.warn('Found both `sample_weight` and `class_weight`: '\n                      '`class_weight` argument will be ignored.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +", "fixed": " def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +"}
{"id": "keras_23", "problem": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape", "fixed": " class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     >>> import os\n     >>> os.remove(\"./dummy.pkl\")\n    path = stringify_path(path)\n    f, fh = get_handle(path, \"wb\", compression=compression, is_text=False)\n     if protocol < 0:\n         protocol = pickle.HIGHEST_PROTOCOL\n     try:", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     >>> import os\n     >>> os.remove(\"./dummy.pkl\")\n    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n        filepath_or_buffer, compression=compression, mode=\"wb\"\n    )\n    if not isinstance(fp_or_buf, str) and compression == \"infer\":\n        compression = None\n    f, fh = get_handle(fp_or_buf, \"wb\", compression=compression, is_text=False)\n     if protocol < 0:\n         protocol = pickle.HIGHEST_PROTOCOL\n     try:"}
{"id": "luigi_5", "problem": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        @task._task_wraps(task_that_requires)\n        class Wrapped(task_that_requires):\n            def requires(_self):\n                return _self.clone_parent()\n        return Wrapped\n class copies(object):", "fixed": " class requires(object):\n     def __call__(self, task_that_requires):\n         task_that_requires = self.inherit_decorator(task_that_requires)\n        def requires(_self):\n            return _self.clone_parent()\n        task_that_requires.requires = requires\n        return task_that_requires\n class copies(object):"}
{"id": "ansible_11", "problem": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.connection import exec_command\nfrom ansible.module_utils.network.ios.ios import load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nimport re\n def map_obj_to_commands(updates, module):", "fixed": " commands:\n     - string\n from ansible.module_utils.basic import AnsibleModule\nfrom ansible.module_utils.network.ios.ios import get_config, load_config\n from ansible.module_utils.network.ios.ios import ios_argument_spec\nfrom re import search, M\n def map_obj_to_commands(updates, module):"}
{"id": "ansible_12", "problem": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(os.getenv(var, ''))\n         return ret", "fixed": " class LookupModule(LookupBase):\n         ret = []\n         for term in terms:\n             var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n         return ret"}
{"id": "youtube-dl_38", "problem": " from ..utils import (\n     compat_urllib_error,\n     compat_urllib_parse,\n     compat_urllib_request,\n     ExtractorError,\n )", "fixed": " from ..utils import (\n     compat_urllib_error,\n     compat_urllib_parse,\n     compat_urllib_request,\n    urlencode_postdata,\n     ExtractorError,\n )"}
{"id": "matplotlib_18", "problem": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:", "fixed": " class RadialLocator(mticker.Locator):\n         return self.base.refresh()\n    def nonsingular(self, vmin, vmax):\n        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)\n                else self.base.nonsingular(vmin, vmax))\n     def view_limits(self, vmin, vmax):\n         vmin, vmax = self.base.view_limits(vmin, vmax)\n         if vmax > vmin:"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:", "fixed": " class EarlyStopping(Callback):\n         if self.monitor_op(current - self.min_delta, self.best):\n             self.best = current\n             self.wait = 0\n            if self.restore_best_weights:\n                self.best_weights = self.model.get_weights()\n         else:\n             self.wait += 1\n             if self.wait >= self.patience:\n                 self.stopped_epoch = epoch\n                 self.model.stop_training = True\n                if self.restore_best_weights:\n                    if self.verbose > 0:\n                        print(\"Restoring model weights from the end of the best epoch\")\n                    self.model.set_weights(self.best_weights)\n     def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n         good = self._codes != -1\n         if not good.all():\n             if skipna:", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         max : the maximum of this `Categorical`\n         self.check_for_ordered(\"max\")\n        if not len(self._codes):\n            return self.dtype.na_value\n         good = self._codes != -1\n         if not good.all():\n             if skipna:"}
{"id": "fastapi_9", "problem": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(None),\n     )\n     return field", "fixed": " def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:\n         model_config=BaseConfig,\n         class_validators={},\n         alias=\"body\",\n        schema=BodySchema(**BodySchema_kwargs),\n     )\n     return field"}
{"id": "pandas_95", "problem": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n        ordinal_op = getattr(self.asi8, opname)\n         if isinstance(other, str):\n             try:", "fixed": " def _period_array_cmp(cls, op):\n     @unpack_zerodim_and_defer(opname)\n     def wrapper(self, other):\n         if isinstance(other, str):\n             try:"}
{"id": "fastapi_7", "problem": " from fastapi.exceptions import RequestValidationError\n from starlette.exceptions import HTTPException\n from starlette.requests import Request", "fixed": "from fastapi.encoders import jsonable_encoder\n from fastapi.exceptions import RequestValidationError\n from starlette.exceptions import HTTPException\n from starlette.requests import Request"}
{"id": "keras_19", "problem": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class LSTMCell(Layer):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n         self.state_size = (self.units, self.units)\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "matplotlib_9", "problem": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin", "fixed": " class PolarAxes(Axes):\n     @cbook._delete_parameter(\"3.3\", \"args\")\n     @cbook._delete_parameter(\"3.3\", \"kwargs\")\n     def draw(self, renderer, *args, **kwargs):\n        self._unstale_viewLim()\n         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)\n         if thetamin > thetamax:\n             thetamin, thetamax = thetamax, thetamin"}
{"id": "pandas_83", "problem": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the", "fixed": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the"}
{"id": "youtube-dl_42", "problem": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\ninfo = self._download_xml('http:\n            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []", "fixed": " class MetacriticIE(InfoExtractor):\n         webpage = self._download_webpage(url, video_id)\ninfo = self._download_xml('http:\n            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)\n         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)\n         formats = []"}
{"id": "pandas_128", "problem": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(", "fixed": " def read_json(\n         dtype = True\n     if convert_axes is None and orient != \"table\":\n         convert_axes = True\n    if encoding is None:\n        encoding = \"utf-8\"\n     compression = _infer_compression(path_or_buf, compression)\n     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer("}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n     def inaxes(self, xy):\n        Check if a point is in an axes.\n         Parameters\n         ----------", "fixed": " class FigureCanvasBase:\n     def inaxes(self, xy):\n        Return the topmost visible `~.axes.Axes` containing the point *xy*.\n         Parameters\n         ----------"}
{"id": "pandas_44", "problem": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,", "fixed": " import numpy as np\n from pandas._libs import NaT, iNaT, join as libjoin, lib\n from pandas._libs.tslibs import timezones\nfrom pandas._typing import DtypeObj, Label\n from pandas.compat.numpy import function as nv\n from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender, cache_readonly, doc\n from pandas.core.dtypes.common import (\n     ensure_int64,\n    ensure_platform_int,\n     is_bool_dtype,\n     is_categorical_dtype,\n     is_dtype_equal,"}
{"id": "thefuck_13", "problem": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)", "fixed": " def get_new_command(command):\n     branch_name = re.findall(\n         r\"fatal: A branch named '([^']*)' already exists.\", command.stderr)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],\n                              ['git checkout {0}']]\n     for new_command_template in new_command_templates:\n         yield shell.and_(*new_command_template).format(branch_name)"}
{"id": "pandas_28", "problem": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=others)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:", "fixed": " class StringMethods(NoNewAttributesMixin):\n         if isinstance(others, ABCSeries):\n             return [others]\n         elif isinstance(others, ABCIndexClass):\n            return [Series(others._values, index=idx)]\n         elif isinstance(others, ABCDataFrame):\n             return [others[x] for x in others]\n         elif isinstance(others, np.ndarray) and others.ndim == 2:"}
{"id": "pandas_149", "problem": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_s3_url\n def get_engine(engine):", "fixed": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url\n def get_engine(engine):"}
{"id": "pandas_40", "problem": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(lk, rk, sort=True):\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)", "fixed": " def _right_outer_join(x, y, max_groups):\n     return left_indexer, right_indexer\ndef _factorize_keys(\n    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n) -> Tuple[np.array, np.array, int]:\n     lk = extract_array(lk, extract_numpy=True)\n     rk = extract_array(rk, extract_numpy=True)"}
{"id": "pandas_131", "problem": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n            result = take_1d(result, self.orig.cat.codes)\n             index = self.orig.index\n         else:\n             index = self._parent.index", "fixed": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n             index = self.orig.index\n         else:\n             index = self._parent.index"}
{"id": "sanic_5", "problem": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],", "fixed": " LOGGING_CONFIG_DEFAULTS = dict(\n     version=1,\n     disable_existing_loggers=False,\n     loggers={\n        \"sanic.root\": {\"level\": \"INFO\", \"handlers\": [\"console\"]},\n         \"sanic.error\": {\n             \"level\": \"INFO\",\n             \"handlers\": [\"error_console\"],"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "pandas_92", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period):\n            if value.freq != self.freq:\n                raise raise_on_incompatible(self, value)\n            value = value.ordinal\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq).ordinal\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     @Substitution(klass=\"PeriodIndex\")\n     @Appender(_shared_docs[\"searchsorted\"])\n     def searchsorted(self, value, side=\"left\", sorter=None):\n        if isinstance(value, Period) or value is NaT:\n            self._data._check_compatible_with(value)\n         elif isinstance(value, str):\n             try:\n                value = Period(value, freq=self.freq)\n             except DateParseError:\n                 raise KeyError(f\"Cannot interpret '{value}' as period\")\n        elif not isinstance(value, PeriodArray):\n            raise TypeError(\n                \"PeriodIndex.searchsorted requires either a Period or PeriodArray\"\n            )\n        return self._data.searchsorted(value, side=side, sorter=sorter)\n     @property\n     def is_full(self) -> bool:"}
{"id": "youtube-dl_40", "problem": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''", "fixed": " class FlvReader(io.BytesIO):\n     def read_unsigned_long_long(self):\n        return struct_unpack('!Q', self.read(8))[0]\n     def read_unsigned_int(self):\n        return struct_unpack('!I', self.read(4))[0]\n     def read_unsigned_char(self):\n        return struct_unpack('!B', self.read(1))[0]\n     def read_string(self):\n         res = b''"}
{"id": "thefuck_30", "problem": " def _search(stderr):\n def match(command, settings):\n    return 'EDITOR' in os.environ and _search(command.stderr)\n def get_new_command(command, settings):", "fixed": " def _search(stderr):\n def match(command, settings):\n    if 'EDITOR' not in os.environ:\n        return False\n    m = _search(command.stderr)\n    return m and os.path.isfile(m.group('file'))\n def get_new_command(command, settings):"}
{"id": "pandas_7", "problem": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where(", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n         right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n        left_distances = np.abs(self[left_indexer] - target)\n        right_distances = np.abs(self[right_indexer] - target)\n         op = operator.lt if self.is_monotonic_increasing else operator.le\n         indexer = np.where("}
{"id": "luigi_7", "problem": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:\n             if status == PENDING or status != task.status:", "fixed": " class Scheduler(object):\n                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):\n                     batch_task.expl = expl\n        if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:\n             if status == PENDING or status != task.status:"}
{"id": "pandas_3", "problem": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        assert isinstance(self.index, DatetimeIndex)\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\"", "fixed": " Name: Max Speed, dtype: float64\n         if copy:\n             new_values = new_values.copy()\n        if not isinstance(self.index, DatetimeIndex):\n            raise TypeError(f\"unsupported Type {type(self.index).__name__}\")\nnew_index = self.index.to_period(freq=freq)\n         return self._constructor(new_values, index=new_index).__finalize__(\n             self, method=\"to_period\""}
{"id": "scrapy_11", "problem": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf\n                 finally:\n                     break\n             else:", "fixed": " def gunzip(data):\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n                                   extra={'spider': spider, 'failure': f}))\n         self._scrape_next(spider, slot)\n         return dfd", "fixed": " class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n         self._scrape_next(spider, slot)\n         return dfd"}
{"id": "pandas_62", "problem": " class Block(PandasObject):\n         transpose = self.ndim == 2\n         if value is None:\n             if self.is_numeric:", "fixed": " class Block(PandasObject):\n         transpose = self.ndim == 2\n        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:\n            raise ValueError(f\"Cannot set values with ndim > {self.ndim}\")\n         if value is None:\n             if self.is_numeric:"}
{"id": "pandas_85", "problem": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n         if len(level_index):\n             grouper = level_index.take(codes)", "fixed": " class MultiIndex(Index):\n         if len(uniques) < len(level_index):\n             level_index = level_index.take(uniques)\n        else:\n            level_index = level_index.copy()\n         if len(level_index):\n             grouper = level_index.take(codes)"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n     __add__ = make_wrapped_arith_op(\"__add__\")", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n             return (lhs_mask & rhs_mask).nonzero()[0]\n    @Appender(Index.get_indexer_non_unique.__doc__)\n    def get_indexer_non_unique(self, target):\n        target = ensure_index(target)\n        pself, ptarget = self._maybe_promote(target)\n        if pself is not self or ptarget is not target:\n            return pself.get_indexer_non_unique(ptarget)\n        if not self._is_comparable_dtype(target.dtype):\n            no_matches = -1 * np.ones(self.shape, dtype=np.intp)\n            return no_matches, no_matches\n        tgt_values = target.asi8\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n     __add__ = make_wrapped_arith_op(\"__add__\")"}
{"id": "luigi_27", "problem": " class Parameter(object):\n             return [str(v) for v in x]\n         return str(x)\n    def parse_from_input(self, param_name, x):\n         Parses the parameter value from input ``x``, handling defaults and is_list.", "fixed": " class Parameter(object):\n             return [str(v) for v in x]\n         return str(x)\n    def parse_from_input(self, param_name, x, task_name=None):\n         Parses the parameter value from input ``x``, handling defaults and is_list."}
{"id": "PySnooper_2", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\n class Tracer:", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False\n thread_global = threading.local()\nDISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))\n class Tracer:"}
{"id": "pandas_79", "problem": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,", "fixed": " from pandas.core.arrays.datetimes import (\n     validate_tz_from_dtype,\n )\n import pandas.core.common as com\nfrom pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name\n from pandas.core.indexes.datetimelike import (\n     DatetimelikeDelegateMixin,\n     DatetimeTimedeltaMixin,"}
{"id": "keras_40", "problem": " class RNN(Layer):\n             output_shape = (input_shape[0], output_dim)\n         if self.return_state:\n            state_shape = [(input_shape[0], output_dim) for _ in self.states]\n             return [output_shape] + state_shape\n         else:\n             return output_shape", "fixed": " class RNN(Layer):\n             output_shape = (input_shape[0], output_dim)\n         if self.return_state:\n            state_shape = [(input_shape[0], dim) for dim in state_size]\n             return [output_shape] + state_shape\n         else:\n             return output_shape"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n         self.seed = seed", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n        self._manager = None\n         self.queue = None\n         self.seed = seed"}
{"id": "fastapi_3", "problem": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        if exclude_unset and isinstance(response_content, BaseModel):\n            if PYDANTIC_1:\n                response_content = response_content.dict(exclude_unset=exclude_unset)\n            else:\n                response_content = response_content.dict(\n                    skip_defaults=exclude_unset\n                )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:", "fixed": " async def serialize_response(\n ) -> Any:\n     if field:\n         errors = []\n        response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n        )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n         else:"}
{"id": "black_17", "problem": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)", "fixed": " GRAMMARS = [\n def lib2to3_parse(src_txt: str) -> Node:\n     grammar = pygram.python_grammar_no_print_statement\n    if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n     for grammar in GRAMMARS:\n         drv = driver.Driver(grammar, pytree.convert)"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n     }, {\n'url': 'https:\n         'only_matching': True,", "fixed": " class FacebookIE(InfoExtractor):\n             'duration': 38,\n             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',\n         }\n    }, {\n        'note': 'Video without discernible title',\n'url': 'https:\n        'info_dict': {\n            'id': '274175099429670',\n            'ext': 'mp4',\n            'title': 'Facebook video\n        }\n     }, {\n'url': 'https:\n         'only_matching': True,"}
{"id": "pandas_37", "problem": " from pandas.core.dtypes.inference import is_array_like\n from pandas import compat\n from pandas.core import ops\nfrom pandas.core.arrays import PandasArray\n from pandas.core.construction import extract_array\n from pandas.core.indexers import check_array_indexer\n from pandas.core.missing import isna", "fixed": " from pandas.core.dtypes.inference import is_array_like\n from pandas import compat\n from pandas.core import ops\nfrom pandas.core.arrays import IntegerArray, PandasArray\nfrom pandas.core.arrays.integer import _IntegerDtype\n from pandas.core.construction import extract_array\n from pandas.core.indexers import check_array_indexer\n from pandas.core.missing import isna"}
{"id": "pandas_114", "problem": " import pandas.core.algorithms as algos\n from pandas.core.arrays import ExtensionArray\n from pandas.core.base import IndexOpsMixin, PandasObject\n import pandas.core.common as com\n from pandas.core.indexers import maybe_convert_indices\n from pandas.core.indexes.frozen import FrozenList\n import pandas.core.missing as missing", "fixed": " import pandas.core.algorithms as algos\n from pandas.core.arrays import ExtensionArray\n from pandas.core.base import IndexOpsMixin, PandasObject\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array\n from pandas.core.indexers import maybe_convert_indices\n from pandas.core.indexes.frozen import FrozenList\n import pandas.core.missing as missing"}
{"id": "keras_44", "problem": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []", "fixed": " class RNN(Layer):\n     @property\n     def trainable_weights(self):\n        if not self.trainable:\n            return []\n         if isinstance(self.cell, Layer):\n             return self.cell.trainable_weights\n         return []"}
{"id": "matplotlib_22", "problem": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')", "fixed": " optional.\n         if bin_range is not None:\n             bin_range = self.convert_xunits(bin_range)\n        if not cbook.is_scalar_or_string(bins):\n            bins = self.convert_xunits(bins)\n         if weights is not None:\n             w = cbook._reshape_2D(weights, 'weights')"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_94", "problem": " from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\n from pandas.core import algorithms\n from pandas.core.accessor import PandasDelegate\nfrom pandas.core.arrays import ExtensionArray, ExtensionOpsMixin\n from pandas.core.arrays.datetimelike import (\n     DatetimeLikeArrayMixin,\n     _ensure_datetimelike_to_i8,", "fixed": " from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries\n from pandas.core import algorithms\n from pandas.core.accessor import PandasDelegate\nfrom pandas.core.arrays import (\n    DatetimeArray,\n    ExtensionArray,\n    ExtensionOpsMixin,\n    TimedeltaArray,\n)\n from pandas.core.arrays.datetimelike import (\n     DatetimeLikeArrayMixin,\n     _ensure_datetimelike_to_i8,"}
{"id": "spacy_2", "problem": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)", "fixed": " def load_model_from_path(model_path, meta=False, **overrides):\n     for name in pipeline:\n         if name not in disable:\n             config = meta.get(\"pipeline_args\", {}).get(name, {})\n            config.update(overrides)\n             factory = factories.get(name, name)\n             component = nlp.create_pipe(factory, config=config)\n             nlp.add_pipe(component, name=name)"}
{"id": "pandas_22", "problem": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type:\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")"}
{"id": "pandas_123", "problem": " class RangeIndex(Int64Index):\n    @staticmethod\n    def _validate_dtype(dtype):", "fixed": " class RangeIndex(Int64Index):"}
{"id": "black_6", "problem": " from . import grammar, parse, token, tokenize, pgen\n class Driver(object):\n    def __init__(self, grammar, convert=None, logger=None):\n         self.grammar = grammar\n         if logger is None:\n             logger = logging.getLogger(__name__)\n         self.logger = logger\n         self.convert = convert\n     def parse_tokens(self, tokens, debug=False):", "fixed": " from . import grammar, parse, token, tokenize, pgen\n class Driver(object):\n    def __init__(\n        self,\n        grammar,\n        convert=None,\n        logger=None,\n        tokenizer_config=tokenize.TokenizerConfig(),\n    ):\n         self.grammar = grammar\n         if logger is None:\n             logger = logging.getLogger(__name__)\n         self.logger = logger\n         self.convert = convert\n        self.tokenizer_config = tokenizer_config\n     def parse_tokens(self, tokens, debug=False):"}
{"id": "keras_16", "problem": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,", "fixed": " class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n            self._build_input_shape = input_shape\n         if self.inputs:\n             self._init_graph_network(self.inputs,"}
{"id": "pandas_139", "problem": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    )\n                 )", "fixed": " class Grouping:\n                 self._group_index = CategoricalIndex(\n                     Categorical.from_codes(\n                         codes=codes, categories=categories, ordered=self.grouper.ordered\n                    ),\n                    name=self.name,\n                 )"}
{"id": "keras_41", "problem": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)", "fixed": " class GeneratorEnqueuer(SequenceEnqueuer):\n         while self.is_running():\n             if not self.queue.empty():\n                success, value = self.queue.get()\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                if value is not None:\n                    yield value\n             else:\n                 all_finished = all([not thread.is_alive() for thread in self._threads])\n                 if all_finished and self.queue.empty():\n                     raise StopIteration()\n                 else:\n                     time.sleep(self.wait_time)\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)"}
{"id": "youtube-dl_42", "problem": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n        def fix_ampersand(s):\n            return s.replace(u'& ', '&amp; ')\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_ampersand)\nreturn [self._get_video_info(item) for item in idoc.findall('.", "fixed": " class MTVServicesInfoExtractor(InfoExtractor):\n         video_id = self._id_from_uri(uri)\n         data = compat_urllib_parse.urlencode({'uri': uri})\n         idoc = self._download_xml(\n             self._FEED_URL + '?' + data, video_id,\n            u'Downloading info', transform_source=fix_xml_ampersands)\nreturn [self._get_video_info(item) for item in idoc.findall('."}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = df.loc[\"a\", \"A\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             df.at[\"a\", 0]\n        with pytest.raises(TypeError, match=msg):\n             df.loc[\"a\", 0]\n     def test_series_at_raises_key_error(self):", "fixed": " class TestScalar2:\n         result = df.loc[\"a\", \"A\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             df.at[\"a\", 0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             df.loc[\"a\", 0]\n     def test_series_at_raises_key_error(self):"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "thefuck_24", "problem": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\nCorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))", "fixed": " from .logs import debug\n Command = namedtuple('Command', ('script', 'stdout', 'stderr'))\n Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',\n                            'enabled_by_default', 'side_effect',\n                            'priority', 'requires_output'))\nclass CorrectedCommand(object):\n    def __init__(self, script, side_effect, priority):\n        self.script = script\n        self.side_effect = side_effect\n        self.priority = priority\n    def __eq__(self, other):"}
{"id": "youtube-dl_35", "problem": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',", "fixed": " def unified_strdate(date_str):\n         '%d/%m/%Y',\n         '%d/%m/%y',\n         '%Y/%m/%d %H:%M:%S',\n        '%d/%m/%Y %H:%M:%S',\n         '%Y-%m-%d %H:%M:%S',\n         '%d.%m.%Y %H:%M',\n         '%d.%m.%Y %H.%M',"}
{"id": "pandas_90", "problem": "from pandas._config.localization import (\n )\n import pandas._libs.testing as _testing\nfrom pandas._typing import FrameOrSeries\n from pandas.compat import _get_lzma_file, _import_lzma\n from pandas.core.dtypes.common import (", "fixed": "from pandas._config.localization import (\n )\n import pandas._libs.testing as _testing\nfrom pandas._typing import FilePathOrBuffer, FrameOrSeries\n from pandas.compat import _get_lzma_file, _import_lzma\n from pandas.core.dtypes.common import ("}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)", "fixed": " class GroupBy(_GroupBy):\n         output = output.drop(labels=list(g_names), axis=1)\n        output = output.set_index(self.grouper.result_index).reindex(\n            index, copy=False, fill_value=fill_value\n        )"}
{"id": "black_18", "problem": " def format_str(\n     return dst_contents\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,", "fixed": " def format_str(\n     return dst_contents\ndef prepare_input(src: bytes) -> Tuple[str, str, str]:\n    srcbuf = io.BytesIO(src)\n    encoding, lines = tokenize.detect_encoding(srcbuf.readline)\n    newline = \"\\r\\n\" if b\"\\r\\n\" == lines[0][-2:] else \"\\n\"\n    srcbuf.seek(0)\n    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()\n GRAMMARS = [\n     pygram.python_grammar_no_print_statement_no_exec_statement,\n     pygram.python_grammar_no_print_statement,"}
{"id": "keras_32", "problem": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.epsilon = epsilon\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown", "fixed": " class ReduceLROnPlateau(Callback):\n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n         super(ReduceLROnPlateau, self).__init__()\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError('ReduceLROnPlateau '\n                              'does not support a factor >= 1.0.')\n        if 'epsilon' in kwargs:\n            min_delta = kwargs.pop('epsilon')\n            warnings.warn('`epsilon` argument is deprecated and '\n                          'will be removed, use `min_delta` insted.')\n         self.factor = factor\n         self.min_lr = min_lr\n        self.min_delta = min_delta\n         self.patience = patience\n         self.verbose = verbose\n         self.cooldown = cooldown"}
{"id": "tornado_9", "problem": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)", "fixed": " def url_concat(url, args):\n>>> url_concat(\"http:\n'http:\n    if args is None:\n        return url\n     parsed_url = urlparse(url)\n     if isinstance(args, dict):\n         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)"}
{"id": "keras_20", "problem": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n         A tensor, result of transposed 2D convolution.", "fixed": " def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n             Whether to use Theano or TensorFlow/CNTK data format\n             for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n         A tensor, result of transposed 2D convolution."}
{"id": "matplotlib_16", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "black_22", "problem": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.leaves.pop()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.leaves.pop()\n                 return True", "fixed": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.remove_trailing_comma()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.remove_trailing_comma()\n                 return True"}
{"id": "pandas_41", "problem": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a a maybe different typed block.\n         Parameters\n         ----------", "fixed": " class Block(PandasObject):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         Parameters\n         ----------"}
{"id": "pandas_46", "problem": " def cartesian_product(X):\n         b = np.zeros_like(cumprodX)\n    return [\n        np.tile(\n            np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])\n        )\n        for i, x in enumerate(X)\n    ]", "fixed": " def cartesian_product(X):\n         b = np.zeros_like(cumprodX)\n    return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]\ndef _tile_compat(arr, num: int):\n    if isinstance(arr, np.ndarray):\n        return np.tile(arr, num)\n    taker = np.tile(np.arange(len(arr)), num)\n    return arr.take(taker)"}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "ansible_12", "problem": "import os\n from ansible.plugins.lookup import LookupBase\n class LookupModule(LookupBase):", "fixed": " from ansible.plugins.lookup import LookupBase\nfrom ansible.utils import py3compat\n class LookupModule(LookupBase):"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        normalize_prefix(current_line.leaves[0], inside_brackets=True)\n         yield current_line", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             and trailing_comma_safe\n         ):\n             current_line.append(Leaf(token.COMMA, ','))\n        yield current_line\n@dont_increase_indentation\ndef standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:\n        nonlocal current_line\n        try:\n            current_line.append_safe(leaf, preformatted=True)\n        except ValueError as ve:\n            yield current_line\n            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)\n            current_line.append(leaf)\n    for leaf in line.leaves:\n        yield from append_to_line(leaf)\n        for comment_after in line.comments_after(leaf):\n            yield from append_to_line(comment_after)\n    if current_line:\n         yield current_line"}
{"id": "ansible_13", "problem": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)", "fixed": " class GalaxyCLI(CLI):\n             else:\n                 requirements = []\n                 for collection_input in collections:\n                    requirement = None\n                    if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \\\n                            urlparse(collection_input).scheme.lower() in ['http', 'https']:\n                        name = collection_input\n                    else:\n                        name, dummy, requirement = collection_input.partition(':')\n                     requirements.append((name, requirement or '*', None))\n             output_path = GalaxyCLI._resolve_path(output_path)"}
{"id": "black_22", "problem": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            comment_after = line.comments.get(id(leaf))\n            if comment_after:\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):", "fixed": " def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:\n     ):\n         for leaf in leaves:\n             result.append(leaf, preformatted=True)\n            for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n     bracket_split_succeeded_or_raise(head, body, tail)\n     for result in (head, body, tail):"}
{"id": "scrapy_25", "problem": " from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n class FormRequest(Request):", "fixed": " from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\nfrom scrapy.utils.response import get_base_url\n class FormRequest(Request):"}
{"id": "black_15", "problem": " class Line:\n         return bool(self.leaves or self.comments)\nclass UnformattedLines(Line):\n        The `preformatted` argument is ignored.\n        Keeps track of indentation `depth`, which is useful when the user\n        says `\n        `depth` is not used for indentation in this case.\n        raise NotImplementedError(\"Unformatted lines don't store comments separately.\")\n    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class Line:\n         return bool(self.leaves or self.comments)\n @dataclass\n class EmptyLineTracker:"}
{"id": "matplotlib_1", "problem": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation),\n                        draw_disabled=True)\n                    self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:", "fixed": " class FigureCanvasBase:\n                     renderer = _get_renderer(\n                         self.figure,\n                         functools.partial(\n                            print_method, orientation=orientation)\n                    )\n                    no_ops = {\n                        meth_name: lambda *args, **kwargs: None\n                        for meth_name in dir(RendererBase)\n                        if (meth_name.startswith(\"draw_\")\n                            or meth_name in [\"open_group\", \"close_group\"])\n                    }\n                    with _setattr_cm(renderer, **no_ops):\n                        self.figure.draw(renderer)\n                     bbox_inches = self.figure.get_tightbbox(\n                         renderer, bbox_extra_artists=bbox_extra_artists)\n                     if pad_inches is None:"}
{"id": "matplotlib_29", "problem": " class YAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposy\n     def set_default_intervals(self):\n         ymin, ymax = 0., 1.", "fixed": " class YAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposy\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         ymin, ymax = 0., 1."}
{"id": "pandas_51", "problem": " import pandas.core.indexes.base as ibase\n from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name\n from pandas.core.indexes.extension import ExtensionIndex, inherit_names\n import pandas.core.missing as missing\n _index_doc_kwargs = dict(ibase._index_doc_kwargs)\n _index_doc_kwargs.update(dict(target_klass=\"CategoricalIndex\"))", "fixed": " import pandas.core.indexes.base as ibase\n from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name\n from pandas.core.indexes.extension import ExtensionIndex, inherit_names\n import pandas.core.missing as missing\nfrom pandas.core.ops import get_op_result_name\n _index_doc_kwargs = dict(ibase._index_doc_kwargs)\n _index_doc_kwargs.update(dict(target_klass=\"CategoricalIndex\"))"}
{"id": "black_4", "problem": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before -= self.previous_after\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after", "fixed": " class EmptyLineTracker:\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n        before = (\n            0\n            if self.previous_line is None\n            else before - self.previous_after\n        )\n         self.previous_after = after\n         self.previous_line = current_line\n         return before, after"}
{"id": "pandas_153", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare"}
{"id": "pandas_138", "problem": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n     if dtype is not None:", "fixed": " def _coerce_to_type(x):\n     elif is_timedelta64_dtype(x):\n         x = to_timedelta(x)\n         dtype = np.dtype(\"timedelta64[ns]\")\n    elif is_bool_dtype(x):\n        x = x.astype(np.int64)\n     if dtype is not None:"}
{"id": "tornado_5", "problem": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n         if self._next_timeout <= current_time:\n            callback_time_sec = self.callback_time / 1000.0\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec", "fixed": " class PeriodicCallback(object):\n             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)\n     def _update_next(self, current_time):\n        callback_time_sec = self.callback_time / 1000.0\n         if self._next_timeout <= current_time:\n             self._next_timeout += (math.floor((current_time - self._next_timeout) /\n                                               callback_time_sec) + 1) * callback_time_sec\n        else:\n            self._next_timeout += callback_time_sec"}
{"id": "pandas_47", "problem": " from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import (\n     is_integer,\n     is_iterator,\n     is_list_like,", "fixed": " from pandas.errors import AbstractMethodError\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import (\n    is_hashable,\n     is_integer,\n     is_iterator,\n     is_list_like,"}
{"id": "pandas_71", "problem": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n     is_integer,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,", "fixed": " from pandas.core.dtypes.common import (\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,\n     is_datetime_or_timedelta_dtype,\n    is_extension_array_dtype,\n     is_integer,\n    is_integer_dtype,\n     is_list_like,\n     is_scalar,\n     is_timedelta64_dtype,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_133", "problem": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n        else:\n            _maybe_transposed_self = self\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:", "fixed": " class NDFrame(PandasObject, SelectionMixin):\n         inplace = validate_bool_kwarg(inplace, \"inplace\")\n        axis = self._get_axis_number(axis)\n         if axis == 0:\n             ax = self._info_axis_name\n             _maybe_transposed_self = self\n         elif axis == 1:\n             _maybe_transposed_self = self.T\n             ax = 1\n         ax = _maybe_transposed_self._get_axis_number(ax)\n         if _maybe_transposed_self.ndim == 2:"}
{"id": "scrapy_19", "problem": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n     def has_header(self, name):\n         return name in self.request.headers", "fixed": " class WrappedRequest(object):\n         return self.request.meta.get('is_unverifiable', False)\n    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname\n    @property\n    def full_url(self):\n        return self.get_full_url()\n    @property\n    def host(self):\n        return self.get_host()\n    @property\n    def type(self):\n        return self.get_type()\n     @property\n     def unverifiable(self):\n         return self.is_unverifiable()\n    @property\n    def origin_req_host(self):\n        return self.get_origin_req_host()\n     def has_header(self, name):\n         return name in self.request.headers"}
{"id": "pandas_167", "problem": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine", "fixed": " class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):\n     _data = None\n     _engine_type = libindex.PeriodEngine\n    _supports_partial_string_indexing = True"}
{"id": "black_13", "problem": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token == 'def':\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            async_def = True\n                            async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],", "fixed": " def generate_tokens(readline):\n                         stashed = tok\n                         continue\n                    if token in ('def', 'for'):\n                         if (stashed\n                                 and stashed[0] == NAME\n                                 and stashed[1] == 'async'):\n                            if token == 'def':\n                                async_def = True\n                                async_def_indent = indents[-1]\n                             yield (ASYNC, stashed[1],\n                                    stashed[2], stashed[3],"}
{"id": "tqdm_9", "problem": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1000.0:\n            if abs(num) < 100.0:\n                if abs(num) < 10.0:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix", "fixed": " def format_sizeof(num, suffix=''):\n         Number with Order of Magnitude SI unit postfix.\n     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.95:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                     return '{0:1.2f}'.format(num) + unit + suffix\n                 return '{0:2.1f}'.format(num) + unit + suffix\n             return '{0:3.0f}'.format(num) + unit + suffix"}
{"id": "pandas_66", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            new_values = self._data.fast_xs(loc)\n            if not is_list_like(new_values) or self.ndim == 1:\n                return com.maybe_box_datetimelike(new_values)\n             result = self._constructor_sliced(\n                 new_values,", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n                 new_index = self.index[loc]\n         if is_scalar(loc):\n            if self.ndim == 1:\n                return self._values[loc]\n            new_values = self._data.fast_xs(loc)\n             result = self._constructor_sliced(\n                 new_values,"}
{"id": "pandas_20", "problem": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(\n            shifted, freq=dtindex.freq, dtype=dtindex.dtype\n        )\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):", "fixed": " class YearOffset(DateOffset):\n         shifted = liboffsets.shift_quarters(\n             dtindex.asi8, self.n, self.month, self._day_opt, modby=12\n         )\n        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)\n     def is_on_offset(self, dt: datetime) -> bool:\n         if self.normalize and not _is_normalized(dt):"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n         elif (\n             is_datetime64_any_dtype(data)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n         elif (\n             is_datetime64_any_dtype(data)"}
{"id": "pandas_54", "problem": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, RangeIndex):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)", "fixed": " class Base:\n         assert not indices.equals(np.array(indices))\n        if not isinstance(indices, (RangeIndex, CategoricalIndex)):\n             same_values = Index(indices, dtype=object)\n             assert indices.equals(same_values)\n             assert same_values.equals(indices)"}
{"id": "pandas_58", "problem": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")", "fixed": " class Categorical(ExtensionArray, PandasObject):\n             )\n             raise ValueError(msg)\n        if is_extension_array_dtype(codes) and is_integer_dtype(codes):\n            if isna(codes).any():\n                raise ValueError(\"codes cannot contain NA values\")\n            codes = codes.to_numpy(dtype=np.int64)\n        else:\n            codes = np.asarray(codes)\n         if len(codes) and not is_integer_dtype(codes):\n             raise ValueError(\"codes need to be array-like integers\")"}
{"id": "luigi_17", "problem": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)", "fixed": " class core(task.Config):\n class _WorkerSchedulerFactory(object):\n     def create_local_scheduler(self):\n        return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)\n     def create_remote_scheduler(self, url):\n         return rpc.RemoteScheduler(url)"}
{"id": "pandas_43", "problem": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n     if not isinstance(right, ABCDataFrame):\n         return False", "fixed": " def _align_method_FRAME(\n def _should_reindex_frame_op(\n    left: \"DataFrame\", right, op, axis, default_axis: int, fill_value, level\n ) -> bool:\n     assert isinstance(left, ABCDataFrame)\n    if op is operator.pow or op is rpow:\n        return False\n     if not isinstance(right, ABCDataFrame):\n         return False"}
{"id": "scrapy_28", "problem": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod", "fixed": " class RFPDupeFilter(BaseDupeFilter):\n         self.logger = logging.getLogger(__name__)\n         if path:\n             self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n     @classmethod"}
{"id": "matplotlib_13", "problem": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n         return cls(vertices, codes)\n     def __repr__(self):", "fixed": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n        last_vert = None\n        if codes.size > 0 and codes[-1] == cls.STOP:\n            last_vert = vertices[-1]\n        vertices = vertices[codes != cls.STOP, :]\n        codes = codes[codes != cls.STOP]\n        if last_vert is not None:\n            vertices = np.append(vertices, [last_vert], axis=0)\n            codes = np.append(codes, cls.STOP)\n         return cls(vertices, codes)\n     def __repr__(self):"}
{"id": "fastapi_14", "problem": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[Union[str], Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None", "fixed": " class Operation(BaseModel):\n     operationId: Optional[str] = None\n     parameters: Optional[List[Union[Parameter, Reference]]] = None\n     requestBody: Optional[Union[RequestBody, Reference]] = None\n    responses: Union[Responses, Dict[str, Response]]\n     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None\n     deprecated: Optional[bool] = None"}
{"id": "scrapy_20", "problem": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.body):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)", "fixed": " class SitemapSpider(Spider):\n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role related operations.\",\n         )", "fixed": " class GalaxyCLI(CLI):\n         super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n         )"}
{"id": "pandas_13", "problem": " def _use_inf_as_na(key):\n         globals()[\"_isna\"] = _isna_new\ndef _isna_ndarraylike(obj):\n    values = getattr(obj, \"_values\", obj)\n    dtype = values.dtype\n    if is_extension_array_dtype(dtype):\n        result = values.isna()\n    elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n    elif needs_i8_conversion(dtype):\n        result = values.view(\"i8\") == iNaT\n    else:\n        result = np.isnan(values)\n    if isinstance(obj, ABCSeries):\n        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)\n    return result\n     values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n    if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n     elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:\n        result = ~np.isfinite(values)\n     if isinstance(obj, ABCSeries):", "fixed": " def _use_inf_as_na(key):\n         globals()[\"_isna\"] = _isna_new\ndef _isna_ndarraylike(obj, old: bool = False):\n     values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n    if is_extension_array_dtype(dtype):\n        if old:\n            result = values.isna() | (values == -np.inf) | (values == np.inf)\n        else:\n            result = values.isna()\n    elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=old)\n     elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:\n        if old:\n            result = ~np.isfinite(values)\n        else:\n            result = np.isnan(values)\n     if isinstance(obj, ABCSeries):"}
{"id": "keras_40", "problem": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            output_dim = self.cell.state_size[0]\n         else:\n            output_dim = self.cell.state_size\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             input_shape = input_shape[0]\n         if hasattr(self.cell.state_size, '__len__'):\n            state_size = self.cell.state_size\n         else:\n            state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "tornado_1", "problem": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__(", "fixed": " class WebSocketProtocol(abc.ABC):\n     async def _receive_frame_loop(self) -> None:\n         raise NotImplementedError()\n    @abc.abstractmethod\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n class _PerMessageDeflateCompressor(object):\n     def __init__("}
{"id": "pandas_169", "problem": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )", "fixed": " class DataFrame(NDFrame):\n         if is_transposed:\n             data = data.T\n        if len(data.columns) == 0:\n            cols = Index([], name=self.columns.name)\n            if is_list_like(q):\n                return self._constructor([], index=q, columns=cols)\n            return self._constructor_sliced([], index=cols, name=q)\n         result = data._data.quantile(\n             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed\n         )"}
{"id": "cookiecutter_4", "problem": " from binaryornot.check import is_binary\n from .exceptions import (\n     NonTemplatedInputDirException,\n     ContextDecodingException,\n     OutputDirExistsException\n )\n from .find import find_template\n from .utils import make_sure_path_exists, work_in\nfrom .hooks import run_hook, EXIT_SUCCESS\n def copy_without_render(path, context):", "fixed": " from binaryornot.check import is_binary\n from .exceptions import (\n     NonTemplatedInputDirException,\n     ContextDecodingException,\n    FailedHookException,\n     OutputDirExistsException\n )\n from .find import find_template\n from .utils import make_sure_path_exists, work_in\nfrom .hooks import run_hook\n def copy_without_render(path, context):"}
{"id": "matplotlib_7", "problem": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if hasattr(intensity, 'mask'):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]", "fixed": " class LightSource:\n                                  .format(lookup.keys)) from err\n        if np.ma.is_masked(intensity):\n             mask = intensity.mask[..., 0]\n             for i in range(3):\n                 blend[..., i][mask] = rgb[..., i][mask]"}
{"id": "matplotlib_1", "problem": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\n _log = logging.getLogger(__name__)", "fixed": " from matplotlib._pylab_helpers import Gcf\n from matplotlib.backend_managers import ToolManager\n from matplotlib.transforms import Affine2D\n from matplotlib.path import Path\nfrom matplotlib.cbook import _setattr_cm\n _log = logging.getLogger(__name__)"}
{"id": "PySnooper_3", "problem": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output_path, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)", "fixed": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)"}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n         if isinstance(string, str):\n             msg = \"Could not construct DatetimeTZDtype from '{}'\"\n            try:\n                match = cls._match.match(string)\n                if match:\n                    d = match.groupdict()\n                     return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n            except Exception:\n                pass\n             raise TypeError(msg.format(string))\n         raise TypeError(\"Could not construct DatetimeTZDtype\")", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n         if isinstance(string, str):\n             msg = \"Could not construct DatetimeTZDtype from '{}'\"\n            match = cls._match.match(string)\n            if match:\n                d = match.groupdict()\n                try:\n                     return cls(unit=d[\"unit\"], tz=d[\"tz\"])\n                except (KeyError, TypeError, ValueError) as err:\n                    raise TypeError(msg.format(string)) from err\n             raise TypeError(msg.format(string))\n         raise TypeError(\"Could not construct DatetimeTZDtype\")"}
{"id": "pandas_83", "problem": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index], intersect: bool = False, sort: bool = False\n ) -> Index:\n     Return the union or intersection of indexes.", "fixed": " def _get_distinct_objs(objs: List[Index]) -> List[Index]:\n def _get_combined_index(\n    indexes: List[Index],\n    intersect: bool = False,\n    sort: bool = False,\n    copy: bool = False,\n ) -> Index:\n     Return the union or intersection of indexes."}
{"id": "tornado_10", "problem": " class RequestHandler(object):\n         self._log()\n         self._finished = True\n         self.on_finish()\n         self.ui = None", "fixed": " class RequestHandler(object):\n         self._log()\n         self._finished = True\n         self.on_finish()\n        self._break_cycles()\n    def _break_cycles(self):\n         self.ui = None"}
{"id": "pandas_79", "problem": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError):\n                 return False", "fixed": " def get_grouper(\n             items = obj._data.items\n             try:\n                 items.get_loc(key)\n            except (KeyError, TypeError, InvalidIndexError):\n                 return False"}
{"id": "pandas_44", "problem": " from pandas._libs import NaT, Timedelta, index as libindex\nfrom pandas._typing import Label\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs import NaT, Timedelta, index as libindex\nfrom pandas._typing import DtypeObj, Label\n from pandas.util._decorators import Appender\n from pandas.core.dtypes.common import ("}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "keras_1", "problem": " def get_variable_shape(x):\n     return int_shape(x)\n def print_tensor(x, message=''):", "fixed": " def get_variable_shape(x):\n     return int_shape(x)\n@symbolic\n def print_tensor(x, message=''):"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '", "fixed": " class EarlyStopping(Callback):\n         self.min_delta = min_delta\n         self.wait = 0\n         self.stopped_epoch = 0\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('EarlyStopping mode %s is unknown, '"}
{"id": "black_22", "problem": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n         if not (", "fixed": " class Line:\n             and self.leaves[0].value == 'yield'\n         )\n    @property\n    def contains_standalone_comments(self) -> bool:\n         if not ("}
{"id": "black_9", "problem": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:", "fixed": " def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n     if not target_versions:\n         return GRAMMARS\n     elif all(not version.is_python2() for version in target_versions):\n         return [\n             pygram.python_grammar_no_print_statement_no_exec_statement,\n             pygram.python_grammar_no_print_statement,\n         ]\n     else:\n        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:"}
{"id": "luigi_22", "problem": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=None):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active", "fixed": " class Worker(object):\n     Structure for tracking worker activity and keeping their references.\n    def __init__(self, worker_id, last_active=time.time()):\n         self.id = worker_id\nself.reference = None\nself.last_active = last_active"}
{"id": "pandas_17", "problem": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)", "fixed": " class TestInsertIndexCoercion(CoercionBase):\n             with pytest.raises(TypeError, match=msg):\n                 obj.insert(1, pd.Timestamp(\"2012-01-01\", tz=\"Asia/Tokyo\"))\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             obj.insert(1, 1)"}
{"id": "cookiecutter_4", "problem": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return EXIT_SUCCESS\n    return run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     script = find_hooks().get(hook_name)\n     if script is None:\n         logging.debug('No hooks found')\n        return\n    run_script_with_context(script, project_dir, context)"}
{"id": "pandas_9", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import CategoricalDtype\n from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries\n from pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import isna, notna\n from pandas.core import ops\n from pandas.core.accessor import PandasDelegate, delegate_names", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import CategoricalDtype\n from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries\n from pandas.core.dtypes.inference import is_hashable\nfrom pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna, notna\n from pandas.core import ops\n from pandas.core.accessor import PandasDelegate, delegate_names"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         -------\n         loc : int\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if is_valid_nat_for_dtype(key, self.dtype):\n             key = NaT"}
{"id": "scrapy_33", "problem": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         extra={'spider': spider, 'failure': failure})\n         return failure\n     dont_log = named.pop('dont_log', None)", "fixed": " def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n         return failure\n     dont_log = named.pop('dont_log', None)"}
{"id": "fastapi_16", "problem": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        if not obj.Config.json_encoders:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n            )\n        else:\n            return jsonable_encoder(\n                obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n                include_none=include_none,\n                custom_encoder=obj.Config.json_encoders,\n            )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):", "fixed": " def jsonable_encoder(\n     custom_encoder: dict = {},\n ) -> Any:\n     if isinstance(obj, BaseModel):\n        encoder = getattr(obj.Config, \"json_encoders\", custom_encoder)\n        return jsonable_encoder(\n            obj.dict(include=include, exclude=exclude, by_alias=by_alias),\n            include_none=include_none,\n            custom_encoder=encoder,\n        )\n     if isinstance(obj, Enum):\n         return obj.value\n     if isinstance(obj, (str, int, float, type(None))):"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    include_none: bool = True,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:", "fixed": " def jsonable_encoder(\n     by_alias: bool = True,\n     skip_defaults: bool = None,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     custom_encoder: dict = {},\n     sqlalchemy_safe: bool = True,\n ) -> Any:"}
{"id": "cookiecutter_4", "problem": " import tempfile\n from jinja2 import Template\n from cookiecutter import utils\n _HOOKS = [", "fixed": " import tempfile\n from jinja2 import Template\n from cookiecutter import utils\nfrom .exceptions import FailedHookException\n _HOOKS = ["}
{"id": "matplotlib_30", "problem": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    x = x * (N - 1)\n    xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n    ind = np.searchsorted(x, xind)[1:-1]\n    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n    lut = np.concatenate([\n        [y1[0]],\n        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n        [y0[-1]],\n    ])\n     return np.clip(lut, 0.0, 1.0)", "fixed": " def makeMappingArray(N, data, gamma=1.0):\n     if (np.diff(x) < 0).any():\n         raise ValueError(\"data mapping points must have x in increasing order\")\n    if N == 1:\n        lut = np.array(y0[-1])\n    else:\n        x = x * (N - 1)\n        xind = (N - 1) * np.linspace(0, 1, N) ** gamma\n        ind = np.searchsorted(x, xind)[1:-1]\n        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])\n        lut = np.concatenate([\n            [y1[0]],\n            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],\n            [y0[-1]],\n        ])\n     return np.clip(lut, 0.0, 1.0)"}
{"id": "pandas_65", "problem": " class CParserWrapper(ParserBase):\n            if isinstance(src, BufferedIOBase):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\"", "fixed": " class CParserWrapper(ParserBase):\n            if isinstance(src, (BufferedIOBase, RawIOBase)):\n                 src = TextIOWrapper(src, encoding=encoding, newline=\"\")\n             kwds[\"encoding\"] = \"utf-8\""}
{"id": "youtube-dl_42", "problem": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n'http:\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_all_ampersand) \n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):", "fixed": " class ClipsyndicateIE(InfoExtractor):\n         pdoc = self._download_xml(\n'http:\n             video_id, u'Downloading video info',\n            transform_source=fix_xml_ampersands)\n         track_doc = pdoc.find('trackList/track')\n         def find_param(name):"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,", "fixed": " class APIRouter(routing.Router):\n                     summary=route.summary,\n                     description=route.description,\n                     response_description=route.response_description,\n                    responses=combined_responses,\n                     deprecated=route.deprecated,\n                     methods=route.methods,\n                     operation_id=route.operation_id,"}
{"id": "cookiecutter_4", "problem": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.", "fixed": " class InvalidModeException(CookiecutterException):\n     Raised when cookiecutter is called with both `no_input==True` and\n     `replay==True` at the same time.\n    Raised when a hook script fails"}
{"id": "fastapi_4", "problem": " def get_openapi_path(\n             operation_parameters = get_openapi_operation_parameters(all_route_params)\n             parameters.extend(operation_parameters)\n             if parameters:\n                operation[\"parameters\"] = parameters\n             if method in METHODS_WITH_BODY:\n                 request_body_oai = get_openapi_operation_request_body(\n                     body_field=route.body_field, model_name_map=model_name_map", "fixed": " def get_openapi_path(\n             operation_parameters = get_openapi_operation_parameters(all_route_params)\n             parameters.extend(operation_parameters)\n             if parameters:\n                operation[\"parameters\"] = list(\n                    {param[\"name\"]: param for param in parameters}.values()\n                )\n             if method in METHODS_WITH_BODY:\n                 request_body_oai = get_openapi_operation_request_body(\n                     body_field=route.body_field, model_name_map=model_name_map"}
{"id": "pandas_122", "problem": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.dtype.name, block.mgr_locs.as_array.tolist())\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)", "fixed": " class BlockManager(PandasObject):\n         if len(self.blocks) != len(other.blocks):\n             return False\n         def canonicalize(block):\n            return (block.mgr_locs.as_array.tolist(), block.dtype.name)\n         self_blocks = sorted(self.blocks, key=canonicalize)\n         other_blocks = sorted(other.blocks, key=canonicalize)"}
{"id": "scrapy_33", "problem": " class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n                             extra={'spider': spider, 'failure': download_failure})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:", "fixed": " class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n                             exc_info=failure_to_exc_info(download_failure),\n                             extra={'spider': spider})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:"}
{"id": "ansible_7", "problem": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n    for key in to_remove:\n        commands.append(\"no {0}\".format(key))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands", "fixed": " def generate_commands(vlan_id, to_set, to_remove):\n     if \"vlan_id\" in to_remove:\n         return [\"no vlan {0}\".format(vlan_id)]\n    for key in to_remove:\n        if key in to_set.keys():\n            continue\n        commands.append(\"no {0}\".format(key))\n     for key, value in to_set.items():\n         if key == \"vlan_id\" or value is None:\n             continue\n         commands.append(\"{0} {1}\".format(key, value))\n     if commands:\n         commands.insert(0, \"vlan {0}\".format(vlan_id))\n     return commands"}
{"id": "tornado_1", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n         self.write_ping(b\"\")\n         self.last_ping = now\n    def set_nodelay(self, x: bool) -> None:\n        self.stream.set_nodelay(x)\n class WebSocketClientConnection(simple_httpclient._HTTPConnection):"}
{"id": "keras_1", "problem": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow',\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2", "fixed": " class TestBackend(object):\n         assert output == [21.]\n         assert K.get_session().run(fetches=[x, y]) == [30., 40.]\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='Uses the `options` and `run_metadata` arguments.')\n     def test_function_tf_run_options_with_run_metadata(self):\n         from tensorflow.core.protobuf import config_pb2"}
{"id": "ansible_14", "problem": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n             while not done:\n                url = _urljoin(self.api_server, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.vvvv(\"Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                         % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])", "fixed": " class GalaxyAPI:\n             data = self._call_galaxy(url)\n             results = data['results']\n             done = (data.get('next_link', None) is None)\n            url_info = urlparse(self.api_server)\nbase_url = \"%s:\n             while not done:\n                url = _urljoin(base_url, data['next_link'])\n                 data = self._call_galaxy(url)\n                 results += data['results']\n                 done = (data.get('next_link', None) is None)\n         except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n         return results\n     @g_connect(['v1'])"}
{"id": "pandas_62", "problem": "                     missing_value = StataMissingValue(um)\n                     loc = missing_loc[umissing_loc == j]\n                     replacement.iloc[loc] = missing_value\nelse:\n                 dtype = series.dtype", "fixed": "                     missing_value = StataMissingValue(um)\n                     loc = missing_loc[umissing_loc == j]\n                    if loc.ndim == 2 and loc.shape[1] == 1:\n                        loc = loc[:, 0]\n                     replacement.iloc[loc] = missing_value\nelse:\n                 dtype = series.dtype"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "keras_41", "problem": " import sys\n import tarfile\n import threading\n import time\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool", "fixed": " import sys\n import tarfile\n import threading\n import time\nimport traceback\n import zipfile\n from abc import abstractmethod\n from multiprocessing.pool import ThreadPool"}
{"id": "scrapy_33", "problem": " from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n logger = logging.getLogger(__name__)", "fixed": " from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\nfrom scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)"}
{"id": "matplotlib_11", "problem": " class Text(Artist):\n         if not self.get_visible():\n             return Bbox.unit()\n        if dpi is not None:\n            dpi_orig = self.figure.dpi\n            self.figure.dpi = dpi\n         if self.get_text() == '':\n            tx, ty = self._get_xy_display()\n            return Bbox.from_bounds(tx, ty, 0, 0)\n         if renderer is not None:\n             self._renderer = renderer", "fixed": " class Text(Artist):\n         if not self.get_visible():\n             return Bbox.unit()\n        if dpi is None:\n            dpi = self.figure.dpi\n         if self.get_text() == '':\n            with cbook._setattr_cm(self.figure, dpi=dpi):\n                tx, ty = self._get_xy_display()\n                return Bbox.from_bounds(tx, ty, 0, 0)\n         if renderer is not None:\n             self._renderer = renderer"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    path : str\n        File path where the pickled object will be stored.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        A string representing the compression to use in the output file. By\n        default, infers from the file extension in specified path.\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n     ----------\n     obj : any object\n         Any python object.\n    filepath_or_buffer : str, path object or file-like object\n        File path, URL, or buffer where the pickled object will be stored.\n        .. versionchanged:: 1.0.0\n           Accept URL. URL has to be of S3 or GCS.\n     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n        If 'infer' and 'path_or_url' is path-like, then detect compression from\n        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n        compression) If 'infer' and 'path_or_url' is not path-like, then use\n        None (= no decompression).\n     protocol : int\n         Int which indicates which protocol should be used by the pickler,\n         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible"}
{"id": "pandas_5", "problem": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            return multi_join_idx, lidx, ridx\n         jl = list(overlap)[0]", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             multi_join_idx = multi_join_idx.remove_unused_levels()\n            if return_indexers:\n                return multi_join_idx, lidx, ridx\n            else:\n                return multi_join_idx\n         jl = list(overlap)[0]"}
{"id": "pandas_168", "problem": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)", "fixed": " def _get_grouper(\nelif is_in_axis(gpr):\n             if gpr in obj:\n                 if validate:\n                    obj._check_label_or_level_ambiguity(gpr, axis=axis)\n                 in_axis, name, gpr = True, gpr, obj[gpr]\n                 exclusions.append(name)\n            elif obj._is_level_reference(gpr, axis=axis):\n                 in_axis, name, level, gpr = False, None, gpr, None\n             else:\n                 raise KeyError(gpr)"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "pandas_113", "problem": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    result = op(self._data, other)\n             if mask is None:", "fixed": " class IntegerArray(ExtensionArray, ExtensionOpsMixin):\n             with warnings.catch_warnings():\n                 warnings.filterwarnings(\"ignore\", \"elementwise\", FutureWarning)\n                 with np.errstate(all=\"ignore\"):\n                    method = getattr(self._data, f\"__{op_name}__\")\n                    result = method(other)\n                    if result is NotImplemented:\n                        result = invalid_comparison(self._data, other, op)\n             if mask is None:"}
{"id": "keras_1", "problem": " def update(x, new_x):\n         The variable `x` updated.\n    return tf_state_ops.assign(x, new_x)\n @symbolic", "fixed": " def update(x, new_x):\n         The variable `x` updated.\n    op = tf_state_ops.assign(x, new_x)\n    with tf.control_dependencies([op]):\n        return tf.identity(x)\n @symbolic"}
{"id": "keras_11", "problem": " def predict_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def predict_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "black_15", "problem": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break", "fixed": " def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n        if parent.type == syms.file_input:\n            break\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break"}
{"id": "pandas_65", "problem": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, BufferedIOBase):\n             handles.append(g)\n         f = g", "fixed": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, (BufferedIOBase, RawIOBase)):\n             handles.append(g)\n         f = g"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "black_6", "problem": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for grammar in get_grammars(set(target_versions)):\n        drv = driver.Driver(grammar, pytree.convert)\n         try:\n             result = drv.parse_string(src_txt, True)\n             break", "fixed": " def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n    for parser_config in get_parser_configs(set(target_versions)):\n        drv = driver.Driver(\n            parser_config.grammar,\n            pytree.convert,\n            tokenizer_config=parser_config.tokenizer_config,\n        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"}
{"id": "black_21", "problem": " def dump_to_file(*output: str) -> str:\n     import tempfile\n     with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False\n     ) as f:\n         for lines in output:\n             f.write(lines)", "fixed": " def dump_to_file(*output: str) -> str:\n     import tempfile\n     with tempfile.NamedTemporaryFile(\n        mode=\"w\", prefix=\"blk_\", suffix=\".log\", delete=False, encoding=\"utf8\"\n     ) as f:\n         for lines in output:\n             f.write(lines)"}
{"id": "pandas_12", "problem": " from pandas.core.dtypes.cast import (\n     validate_numeric_casting,\n )\n from pandas.core.dtypes.common import (\n    ensure_float64,\n     ensure_int64,\n     ensure_platform_int,\n     infer_dtype_from_object,", "fixed": " from pandas.core.dtypes.cast import (\n     validate_numeric_casting,\n )\n from pandas.core.dtypes.common import (\n     ensure_int64,\n     ensure_platform_int,\n     infer_dtype_from_object,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "fastapi_10", "problem": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n         return jsonable_encoder(\n             value,\n             include=include,", "fixed": " def serialize_response(\n             errors.extend(errors_)\n         if errors:\n             raise ValidationError(errors)\n        if skip_defaults and isinstance(response, BaseModel):\n            value = response.dict(skip_defaults=skip_defaults)\n         return jsonable_encoder(\n             value,\n             include=include,"}
{"id": "matplotlib_10", "problem": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])", "fixed": " class Axis(martist.Artist):\n                 self._minor_tick_kw.update(kwtrans)\n                 for tick in self.minorTicks:\n                     tick._apply_params(**kwtrans)\n            if 'label1On' in kwtrans or 'label2On' in kwtrans:\n                self.offsetText.set_visible(\n                    self._major_tick_kw.get('label1On', False)\n                    or self._major_tick_kw.get('label2On', False))\n             if 'labelcolor' in kwtrans:\n                 self.offsetText.set_color(kwtrans['labelcolor'])"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "keras_24", "problem": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    tf.summary.histogram('{}_out'.format(layer.name),\n                                         layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:", "fixed": " class TensorBoard(Callback):\n                         tf.summary.image(mapped_weight_name, w_img)\n                 if hasattr(layer, 'output'):\n                    if isinstance(layer.output, list):\n                        for i, output in enumerate(layer.output):\n                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)\n                    else:\n                        tf.summary.histogram('{}_out'.format(layer.name),\n                                             layer.output)\n         self.merged = tf.summary.merge_all()\n         if self.write_graph:"}
{"id": "scrapy_33", "problem": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(msg, extra={'spider': spider, 'failure': failure})\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())", "fixed": " class ExecutionEngine(object):\n         def log_failure(msg):\n             def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n             return errback\n         dfd.addBoth(lambda _: self.downloader.close())"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n        states = []\n        for cell_states in new_nested_states[::-1]:\n            states += cell_states\n        return inputs, states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):", "fixed": " class StackedRNNCells(Layer):\n        new_states = []\n        if self.reverse_state_order:\n            new_nested_states = new_nested_states[::-1]\n        for cell_states in new_nested_states:\n            new_states += cell_states\n        return inputs, new_states\n     def build(self, input_shape):\n         if isinstance(input_shape, list):"}
{"id": "fastapi_1", "problem": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content, by_alias=by_alias, exclude_unset=exclude_unset\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))", "fixed": " async def serialize_response(\n     exclude: Union[SetIntStr, DictIntStrAny] = set(),\n     by_alias: bool = True,\n     exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n     is_coroutine: bool = True,\n ) -> Any:\n     if field:\n         errors = []\n         response_content = _prepare_response_content(\n            response_content,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n         )\n         if is_coroutine:\n             value, errors_ = field.validate(response_content, {}, loc=(\"response\",))"}
{"id": "ansible_8", "problem": " import re\n import shlex\n import pkgutil\n import xml.etree.ElementTree as ET\n from ansible.errors import AnsibleError\n from ansible.module_utils._text import to_bytes, to_text", "fixed": " import re\n import shlex\n import pkgutil\n import xml.etree.ElementTree as ET\nimport ntpath\n from ansible.errors import AnsibleError\n from ansible.module_utils._text import to_bytes, to_text"}
{"id": "scrapy_29", "problem": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\"", "fixed": " def request_httprepr(request):\n     parsed = urlparse_cached(request)\n     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n     s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n     if request.headers:\n         s += request.headers.to_string() + b\"\\r\\n\"\n     s += b\"\\r\\n\""}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "black_14", "problem": " from typing import (\n     Callable,\n     Collection,\n     Dict,\n     Generic,\n     Iterable,\n     Iterator,", "fixed": " from typing import (\n     Callable,\n     Collection,\n     Dict,\n    Generator,\n     Generic,\n     Iterable,\n     Iterator,"}
{"id": "youtube-dl_22", "problem": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$", "fixed": " def _match_one(filter_part, dct):\n         \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n         (?:\n             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n            (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n         )\n         \\s*$"}
{"id": "keras_16", "problem": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n         if layers:", "fixed": " class Sequential(Model):\n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n        self._build_input_shape = None\n         if layers:"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "youtube-dl_39", "problem": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            if len(video_title) > 80 + 3:\n                video_title = video_title[:80] + '...'\n         if not video_title:\nvideo_title = 'Facebook video", "fixed": " class FacebookIE(InfoExtractor):\n             video_title = self._html_search_regex(\n                 r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                 webpage, 'alternative title', default=None)\n            video_title = limit_length(video_title, 80)\n         if not video_title:\nvideo_title = 'Facebook video"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "keras_19", "problem": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size", "fixed": " class StackedRNNCells(Layer):\n                     cell.build([input_shape] + constants_shape)\n                 else:\n                     cell.build(input_shape)\n            if getattr(cell, 'output_size', None) is not None:\n                output_dim = cell.output_size\n            elif hasattr(cell.state_size, '__len__'):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size"}
{"id": "pandas_136", "problem": " from pandas.core.dtypes.common import (\n     is_dtype_equal,\n     is_extension_array_dtype,\n     is_float_dtype,\n    is_int64_dtype,\n     is_integer,\n     is_integer_dtype,\n     is_list_like,", "fixed": " from pandas.core.dtypes.common import (\n     is_dtype_equal,\n     is_extension_array_dtype,\n     is_float_dtype,\n     is_integer,\n     is_integer_dtype,\n     is_list_like,"}
{"id": "sanic_4", "problem": " class Request:\n         :rtype: str\nif \"\n            return self.app.url_for(view_name, _external=True, **kwargs)\n         scheme = self.scheme\n         host = self.server_name", "fixed": " class Request:\n         :rtype: str\n        try:\nif \"\n                return self.app.url_for(view_name, _external=True, **kwargs)\n        except AttributeError:\n            pass\n         scheme = self.scheme\n         host = self.server_name"}
{"id": "pandas_13", "problem": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=False)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n     else:\n         return False"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(self, output):\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding", "fixed": " class GroupBy(_GroupBy):\n         mask = self._cumcount_array(ascending=False) < n\n         return self._selected_obj[mask]\n    def _reindex_output(\n        self, output: FrameOrSeries, fill_value: Scalar = np.NaN\n    ) -> FrameOrSeries:\n         If we have categorical groupers, then we might want to make sure that\n         we have a fully re-indexed output to the levels. This means expanding"}
{"id": "pandas_137", "problem": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort", "fixed": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import array, extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort"}
{"id": "scrapy_30", "problem": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()\n        return comm[0].strip()\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\", "fixed": " class CmdlineTest(unittest.TestCase):\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n     def _execute(self, *new_args, **kwargs):\n        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n        comm = proc.communicate()[0].strip()\n        return comm.decode(encoding)\n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\"}
{"id": "keras_18", "problem": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '", "fixed": " class Function(object):\n         self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs.copy()\n        self.run_options = session_kwargs.pop('options', None)\n        self.run_metadata = session_kwargs.pop('run_metadata', None)\n         if session_kwargs:\n             raise ValueError('Some keys in session_kwargs are not '\n                              'supported at this '"}
{"id": "youtube-dl_29", "problem": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):", "fixed": " def unified_strdate(date_str, day_first=True):\n         timetuple = email.utils.parsedate_tz(date_str)\n         if timetuple:\n             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return compat_str(upload_date)\n def determine_ext(url, default_ext='unknown_video'):"}
{"id": "matplotlib_1", "problem": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        kwargs = get_tight_layout_figure(\n            self, self.axes, subplotspec_list, renderer,\n            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)", "fixed": " default: 'top'\n         if renderer is None:\n             renderer = get_renderer(self)\n        no_ops = {\n            meth_name: lambda *args, **kwargs: None\n            for meth_name in dir(RendererBase)\n            if (meth_name.startswith(\"draw_\")\n                or meth_name in [\"open_group\", \"close_group\"])\n        }\n        with _setattr_cm(renderer, **no_ops):\n            kwargs = get_tight_layout_figure(\n                self, self.axes, subplotspec_list, renderer,\n                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n         if kwargs:\n             self.subplots_adjust(**kwargs)"}
{"id": "pandas_65", "problem": " import bz2\n from collections import abc\n import gzip\nfrom io import BufferedIOBase, BytesIO\n import mmap\n import os\n import pathlib", "fixed": " import bz2\n from collections import abc\n import gzip\nfrom io import BufferedIOBase, BytesIO, RawIOBase\n import mmap\n import os\n import pathlib"}
{"id": "ansible_2", "problem": " class _Alpha:\n         raise ValueError\n    def __gt__(self, other):\n        return not self.__lt__(other)\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n     def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n class _Numeric:", "fixed": " class _Alpha:\n         raise ValueError\n     def __le__(self, other):\n         return self.__lt__(other) or self.__eq__(other)\n    def __gt__(self, other):\n        return not self.__le__(other)\n     def __ge__(self, other):\n        return not self.__lt__(other)\n class _Numeric:"}
{"id": "pandas_92", "problem": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[2007]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1", "fixed": " class TestPeriodIndex(DatetimeLike):\n         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=\"A-JUN\")\n         ts = Series(np.random.randn(len(idx)), index=idx)\n        result = ts[\"2007\"]\n         expected = ts[1:3]\n         tm.assert_series_equal(result, expected)\n         result[:] = 1"}
{"id": "pandas_125", "problem": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         code_values = code_values[null_mask | (code_values >= 0)]\n         return algorithms.isin(self.codes, code_values)\n    def replace(self, to_replace, value, inplace: bool = False):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        cat = self if inplace else self.copy()\n        if to_replace in cat.categories:\n            if isna(value):\n                cat.remove_categories(to_replace, inplace=True)\n            else:\n                categories = cat.categories.tolist()\n                index = categories.index(to_replace)\n                if value in cat.categories:\n                    value_index = categories.index(value)\n                    cat._codes[cat._codes == index] = value_index\n                    cat.remove_categories(to_replace, inplace=True)\n                else:\n                    categories[index] = value\n                    cat.rename_categories(categories, inplace=True)\n        if not inplace:\n            return cat"}
{"id": "black_14", "problem": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            for import_from_child in first_child.children[3:]:\n                if isinstance(import_from_child, Leaf):\n                    if import_from_child.type == token.NAME:\n                        imports.add(import_from_child.value)\n                else:\n                    assert import_from_child.type == syms.import_as_names\n                    for leaf in import_from_child.children:\n                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:\n                            imports.add(leaf.value)\n         else:\n             break\n     return imports", "fixed": " def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n            imports |= set(get_imports_from_children(first_child.children[3:]))\n         else:\n             break\n     return imports"}
{"id": "youtube-dl_24", "problem": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n        actual_value = dct.get(m.group('key'))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)", "fixed": " def _match_one(filter_part, dct):\n                     raise ValueError(\n                         'Invalid integer value %r in filter part %r' % (\n                             m.group('intval'), filter_part))\n         if actual_value is None:\n             return m.group('none_inclusive')\n         return op(actual_value, comparison_value)"}
{"id": "scrapy_40", "problem": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        if self.binary:\n            return to_bytes(value, encoding=self.encoding)\n        else:\n            return to_unicode(value, encoding=self.encoding)\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):", "fixed": " class PythonItemExporter(BaseItemExporter):\n             return dict(self._serialize_dict(value))\n         if is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value\n     def _serialize_dict(self, value):\n         for key, val in six.iteritems(value):"}
{"id": "keras_19", "problem": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)", "fixed": " class RNN(Layer):\n             state_size = self.cell.state_size\n         else:\n             state_size = [self.cell.state_size]\n        if getattr(self.cell, 'output_size', None) is not None:\n            output_dim = self.cell.output_size\n        else:\n            output_dim = state_size[0]\n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output: Series or DataFrame\n             Object resulting from grouping and applying an operation.\n         Returns\n         -------", "fixed": " class GroupBy(_GroupBy):\n         Parameters\n         ----------\n        output : Series or DataFrame\n             Object resulting from grouping and applying an operation.\n        fill_value : scalar, default np.NaN\n            Value to use for unobserved categories if self.observed is False.\n         Returns\n         -------"}
{"id": "black_22", "problem": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n            normalize_prefix(current_line.leaves[0], inside_brackets=True)\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)", "fixed": " def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n         if leaf_priority == delimiter_priority:\n             yield current_line\n             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)"}
{"id": "youtube-dl_16", "problem": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:", "fixed": " class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):\n                 dfxp_file = old_file\n                 srt_file = subtitles_filename(filename, lang, 'srt')\n                with open(dfxp_file, 'rb') as f:\n                     srt_data = dfxp2srt(f.read())\n                 with io.open(srt_file, 'wt', encoding='utf-8') as f:"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):", "fixed": " class TimeseriesGenerator(Sequence):\n     def __getitem__(self, index):\n         if self.shuffle:\n             rows = np.random.randint(\n                self.start_index, self.end_index + 1, size=self.batch_size)\n         else:\n             i = self.start_index + self.batch_size * self.stride * index\n             rows = np.arange(i, min(i + self.batch_size *\n                                    self.stride, self.end_index + 1), self.stride)\n         samples, targets = self._empty_batch(len(rows))\n         for j, row in enumerate(rows):"}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "pandas_10", "problem": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, np.ndarray) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)", "fixed": " class ExtensionBlock(Block):\n         new_values = self.values if inplace else self.values.copy()\n        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):\n             new = new[mask]\n         mask = _safe_reshape(mask, new_values.shape)"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
{"id": "pandas_160", "problem": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\"):\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif isinstance(o, np.ndarray):\n                     dtypes |= {o.dtype.name}", "fixed": " def _can_use_numexpr(op, op_str, a, b, dtype_check):\n         if np.prod(a.shape) > _MIN_ELEMENTS:\n             dtypes = set()\n             for o in [a, b]:\n                if hasattr(o, \"dtypes\") and o.ndim > 1:\n                     s = o.dtypes.value_counts()\n                     if len(s) > 1:\n                         return False\n                     dtypes |= set(s.index.astype(str))\n                elif hasattr(o, \"dtype\"):\n                     dtypes |= {o.dtype.name}"}
{"id": "luigi_27", "problem": " class Parameter(object):\n         dest = self.parser_dest(param_name, task_name, glob=False)\n         if dest is not None:\n             value = getattr(args, dest, None)\n            params[param_name] = self.parse_from_input(param_name, value)\n     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):", "fixed": " class Parameter(object):\n         dest = self.parser_dest(param_name, task_name, glob=False)\n         if dest is not None:\n             value = getattr(args, dest, None)\n            params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)\n     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):"}
{"id": "black_22", "problem": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines):\n         yield line\n         return\n     line_str = str(line).strip('\\n')\n    if len(line_str) <= line_length and '\\n' not in line_str:\n         yield line\n         return\n     if line.is_def:\n         split_funcs = [left_hand_split]\n     elif line.inside_brackets:\n        split_funcs = [delimiter_split]\n        if '\\n' not in line_str:\n            split_funcs.append(right_hand_split)\n     else:\n         split_funcs = [right_hand_split]\n     for split_func in split_funcs:", "fixed": " def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n    if isinstance(line, UnformattedLines) or line.is_comment:\n         yield line\n         return\n     line_str = str(line).strip('\\n')\n    if (\n        len(line_str) <= line_length\n        and '\\n' not in line_str\n        and not line.contains_standalone_comments\n    ):\n         yield line\n         return\n    split_funcs: List[SplitFunc]\n     if line.is_def:\n         split_funcs = [left_hand_split]\n     elif line.inside_brackets:\n        split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]\n     else:\n         split_funcs = [right_hand_split]\n     for split_func in split_funcs:"}
{"id": "keras_1", "problem": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),\n                        reason='This test is for tensorflow parallelism.')\n    def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n        for threads in [1, 2]:\n            K.clear_session()\n            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))\n            cfg = K.get_session()._config\n            assert cfg.intra_op_parallelism_threads == threads\n            assert cfg.inter_op_parallelism_threads == threads\n if __name__ == '__main__':\n     pytest.main([__file__])", "fixed": " class TestBackend(object):\n                            np.asarray([-5., -4., 0., 4., 9.],\n                                       dtype=np.float32))\n if __name__ == '__main__':\n     pytest.main([__file__])"}
{"id": "ansible_11", "problem": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])\n    if rc == 0:\n        output = out\n    else:\n        rc, out, err = exec_command(module,\n                                    'show running-config | begin banner %s'\n                                    % module.params['banner'])\n        if out:\n            output = re.search(r'\\^C(.*?)\\^C', out, re.S).group(1).strip()\n         else:\n             output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output", "fixed": " def map_obj_to_commands(updates, module):\n def map_config_to_obj(module):\n    out = get_config(module, flags='| begin banner %s' % module.params['banner'])\n    if out:\n        regex = 'banner ' + module.params['banner'] + ' ^C\\n'\n        if search('banner ' + module.params['banner'], out, M):\n            output = str((out.split(regex))[1].split(\"^C\\n\")[0])\n         else:\n             output = None\n    else:\n        output = None\n     obj = {'banner': module.params['banner'], 'state': 'absent'}\n     if output:\n         obj['text'] = output"}
{"id": "youtube-dl_12", "problem": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op\n                 else:\n                     op = str_op", "fixed": " class YoutubeDL(object):\n                 comparison_value = m.group('value')\n                 str_op = STR_OPERATORS[m.group('op')]\n                 if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                 else:\n                     op = str_op"}
{"id": "pandas_155", "problem": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            return self.obj.index\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def _on(self):\n         if self.on is None:\n            if self.axis == 0:\n                return self.obj.index\n            elif self.axis == 1:\n                return self.obj.columns\n         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:\n             return Index(self.obj[self.on])\n         else:"}
{"id": "PySnooper_1", "problem": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')", "fixed": " import traceback\n from .variables import CommonVariable, Exploding, BaseVariable\n from . import utils, pycompat\nif pycompat.PY2:\n    from io import open\n ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n                 or (inputs, targets, sample_weights)\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "matplotlib_12", "problem": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.resize(ymin, x.shape)\n        ymax = np.resize(ymax, x.shape)\n        verts = [((thisx, thisymin), (thisx, thisymax))\n                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]\n        lines = mcoll.LineCollection(verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)", "fixed": " class Axes(_AxesBase):\n         if not np.iterable(ymax):\n             ymax = [ymax]\n        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)\n         x = np.ravel(x)\n        ymin = np.ravel(ymin)\n        ymax = np.ravel(ymax)\n        masked_verts = np.ma.empty((len(x), 2, 2))\n        masked_verts[:, 0, 0] = x\n        masked_verts[:, 0, 1] = ymin\n        masked_verts[:, 1, 0] = x\n        masked_verts[:, 1, 1] = ymax\n        lines = mcoll.LineCollection(masked_verts, colors=colors,\n                                      linestyles=linestyles, label=label)\n         self.add_collection(lines, autolim=False)\n         lines.update(kwargs)"}
{"id": "pandas_107", "problem": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            if other.name is None:\n                index = None\n            else:\n                index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = other.reindex(combined_columns, copy=False)\n            other = DataFrame(\n                other.values.reshape((1, len(other))),\n                index=index,\n                columns=combined_columns,\n             )\n            other = other._convert(datetime=True, timedelta=True)\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):", "fixed": " class DataFrame(NDFrame):\n                     \" or if the Series has a name\"\n                 )\n            index = Index([other.name], name=self.index.name)\n             idx_diff = other.index.difference(self.columns)\n             try:\n                 combined_columns = self.columns.append(idx_diff)\n             except TypeError:\n                 combined_columns = self.columns.astype(object).append(idx_diff)\n            other = (\n                other.reindex(combined_columns, copy=False)\n                .to_frame()\n                .T.infer_objects()\n                .rename_axis(index.names, copy=False)\n             )\n             if not self.columns.equals(combined_columns):\n                 self = self.reindex(columns=combined_columns)\n         elif isinstance(other, list):"}
{"id": "pandas_9", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_scalar(key) and isna(key):\n             return self.hasnans\n        hash(key)\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n     @doc(Index.__contains__)\n     def __contains__(self, key: Any) -> bool:\n        if is_valid_nat_for_dtype(key, self.categories.dtype):\n             return self.hasnans\n         return contains(self, key, container=self._engine)\n     @doc(Index.astype)"}
{"id": "luigi_29", "problem": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n    @mock.patch(\"logging.getLogger\")\n    @mock.patch(\"warnings.warn\")\n    def test_cmdline_non_ambiguous_class(self, warn, logger):\n        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])\n        self.assertTrue(NonAmbiguousClass.has_run)\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):", "fixed": " class CmdlineTest(unittest.TestCase):\n     def test_cmdline_ambiguous_class(self, logger):\n         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])\n     @mock.patch(\"logging.getLogger\")\n     @mock.patch(\"logging.StreamHandler\")\n     def test_setup_interface_logging(self, handler, logger):"}
{"id": "pandas_146", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        try:\n            return array_equivalent(\n                com.values_from_object(self), com.values_from_object(other)\n            )\n        except Exception:\n            return False\n     def identical(self, other):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return other.equals(self)\n        return array_equivalent(\n            com.values_from_object(self), com.values_from_object(other)\n        )\n     def identical(self, other):"}
{"id": "pandas_44", "problem": " from pandas._libs.lib import no_default\n from pandas._libs.tslibs import frequencies as libfrequencies, resolution\n from pandas._libs.tslibs.parsing import parse_time_string\n from pandas._libs.tslibs.period import Period\nfrom pandas._typing import Label\n from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import (", "fixed": " from pandas._libs.lib import no_default\n from pandas._libs.tslibs import frequencies as libfrequencies, resolution\n from pandas._libs.tslibs.parsing import parse_time_string\n from pandas._libs.tslibs.period import Period\nfrom pandas._typing import DtypeObj, Label\n from pandas.util._decorators import Appender, cache_readonly\n from pandas.core.dtypes.common import ("}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "black_15", "problem": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return", "fixed": " def hide_fmt_off(node: Node) -> bool:\n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n     container: Optional[LN] = container_of(leaf)\n    while container is not None and container.type != token.ENDMARKER:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n         Raises\n         ------\n         TypeError", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n        .. versionchanged:: 1.0.0\n           Returns an NA value on empty arrays\n         Raises\n         ------\n         TypeError"}
{"id": "matplotlib_4", "problem": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: 'k'\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional", "fixed": " class Axes(_AxesBase):\n             Respective beginning and end of each line. If scalars are\n             provided, all lines will have same length.\n        colors : list of colors, default: :rc:`lines.color`\n         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional"}
{"id": "scrapy_23", "problem": " from six.moves.urllib.parse import urlunparse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n class HttpProxyMiddleware(object):", "fixed": " from six.moves.urllib.parse import urlunparse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\nfrom scrapy.utils.python import to_bytes\n class HttpProxyMiddleware(object):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_40", "problem": " def _factorize_keys(lk, rk, sort=True):\n             np.putmask(rlab, rmask, count)\n         count += 1\n     return llab, rlab, count", "fixed": " def _factorize_keys(lk, rk, sort=True):\n             np.putmask(rlab, rmask, count)\n         count += 1\n    if how == \"right\":\n        return rlab, llab, count\n     return llab, rlab, count"}
{"id": "keras_20", "problem": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None):", "fixed": " def conv2d(x, kernel, strides=(1, 1), padding='valid',\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                     padding='valid', data_format=None, dilation_rate=(1, 1)):"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class APIRouter(routing.Router):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "ansible_3", "problem": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path == '/etc/lsb-release' and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:", "fixed": " class DistributionFiles:\n         elif 'SteamOS' in data:\n             debian_facts['distribution'] = 'SteamOS'\n        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n             debian_facts['distribution'] = 'Kali'\n             release = re.search('DISTRIB_RELEASE=(.*)', data)\n             if release:"}
{"id": "pandas_17", "problem": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeIndex with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]", "fixed": " class TestPartialSetting:\n         df = orig.copy()\n        msg = \"cannot insert DatetimeArray with incompatible label\"\n         with pytest.raises(TypeError, match=msg):\n             df.loc[100.0, :] = df.iloc[0]"}
{"id": "PySnooper_1", "problem": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = '\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch", "fixed": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = u'\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Set the value inplace, returning a same-typed block.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block.", "fixed": " class ExtensionBlock(Block):\n     def setitem(self, indexer, value):\n        Attempt self.values[indexer] = value, possibly creating a new array.\n         This differs from Block.setitem by not allowing setitem to change\n         the dtype of the Block."}
{"id": "thefuck_1", "problem": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \\\"([a-z]+)\\\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \\\"([a-z]+)\\\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)", "fixed": " def match(command):\n def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n                             command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]\n     return replace_argument(command.script, broken_cmd, new_cmd)"}
{"id": "pandas_103", "problem": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class SeriesGroupBy(GroupBy):\n                     periods=periods, fill_method=fill_method, limit=limit, freq=freq\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "thefuck_15", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return ('did not match any file(s) known to git.' in command.stderr\n            and \"Did you forget to 'git add'?\" in command.stderr)\n @git_support\n def get_new_command(command):\n     missing_file = re.findall(\n            r\"error: pathspec '([^']*)' \"\n            r\"did not match any file\\(s\\) known to git.\", command.stderr)[0]\n     formatme = shell.and_('git add -- {}', '{}')\n     return formatme.format(missing_file, command.script)", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return 'did not match any file(s) known to git.' in command.stderr\n @git_support\n def get_new_command(command):\n     missing_file = re.findall(\n        r\"error: pathspec '([^']*)' \"\n        r'did not match any file\\(s\\) known to git.', command.stderr)[0]\n     formatme = shell.and_('git add -- {}', '{}')\n     return formatme.format(missing_file, command.script)"}
{"id": "youtube-dl_42", "problem": " def month_by_name(name):\n         return None\ndef fix_xml_all_ampersand(xml_str):\n    return xml_str.replace(u'&', u'&amp;')\n def setproctitle(title):", "fixed": " def month_by_name(name):\n         return None\ndef fix_xml_ampersands(xml_str):\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|\n        u'&amp;',\n        xml_str)\n def setproctitle(title):"}
{"id": "pandas_12", "problem": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.values\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\n                baseCov.fill(np.nan)\n             else:\n                baseCov = np.cov(mat.T)\n            baseCov = baseCov.reshape((len(cols), len(cols)))\n         else:\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)\n        return self._constructor(baseCov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:", "fixed": " Wild         185.0\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n         idx = cols.copy()\n        mat = numeric_df.astype(float, copy=False).to_numpy()\n         if notna(mat).all():\n             if min_periods is not None and min_periods > len(mat):\n                base_cov = np.empty((mat.shape[1], mat.shape[1]))\n                base_cov.fill(np.nan)\n             else:\n                base_cov = np.cov(mat.T)\n            base_cov = base_cov.reshape((len(cols), len(cols)))\n         else:\n            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)\n        return self._constructor(base_cov, index=idx, columns=cols)\n     def corrwith(self, other, axis=0, drop=False, method=\"pearson\") -> Series:"}
{"id": "pandas_43", "problem": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)", "fixed": " def _arith_method_FRAME(cls, op, special):\n     @Appender(doc)\n     def f(self, other, axis=default_axis, level=None, fill_value=None):\n        if _should_reindex_frame_op(\n            self, other, op, axis, default_axis, fill_value, level\n        ):\n             return _frame_arith_method_with_reindex(self, other, op)\n         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)"}
{"id": "pandas_44", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin):\n             return Timestamp(value).asm8\n         raise ValueError(\"Passed item and index have different timezone\")\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        if not is_datetime64_any_dtype(dtype):\n            return False\n        if self.tz is not None:\n            return is_datetime64tz_dtype(dtype)\n        return is_datetime64_dtype(dtype)"}
{"id": "keras_30", "problem": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                     outs = [outs]\n                 outs_per_batch.append(outs)\n                if x is None or len(x) == 0:\n                    batch_size = 1\n                elif isinstance(x, list):\n                     batch_size = x[0].shape[0]\n                 elif isinstance(x, dict):\n                     batch_size = list(x.values())[0].shape[0]"}
{"id": "thefuck_4", "problem": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip().split('\\n')\n    for alias in alias_out:\n        name, value = alias.replace('alias ', '', 1).split(' ', 1)\n         if name not in overridden:\n             aliases[name] = value\n     return aliases", "fixed": " def _get_functions(overridden):\n def _get_aliases(overridden):\n     aliases = {}\n     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)\n    alias_out = proc.stdout.read().decode('utf-8').strip()\n    if not alias_out:\n        return aliases\n    for alias in alias_out.split('\\n'):\n        for separator in (' ', '='):\n            split_alias = alias.replace('alias ', '', 1).split(separator, 1)\n            if len(split_alias) == 2:\n                name, value = split_alias\n                break\n        else:\n            continue\n         if name not in overridden:\n             aliases[name] = value\n     return aliases"}
{"id": "keras_19", "problem": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None", "fixed": " class GRUCell(Layer):\n         self.implementation = implementation\n         self.reset_after = reset_after\n         self.state_size = self.units\n        self.output_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None"}
{"id": "matplotlib_1", "problem": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n    if draw_disabled:\n        for meth_name in dir(RendererBase):\n            if (meth_name.startswith(\"draw_\")\n                    or meth_name in [\"open_group\", \"close_group\"]):\n                setattr(renderer, meth_name, lambda *args, **kwargs: None)\n     return renderer", "fixed": " def _get_renderer(figure, print_method=None, *, draw_disabled=False):\n         except Done as exc:\n             renderer, = figure._cachedRenderer, = exc.args\n     return renderer"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with numpy.\n         Returns\n         -------", "fixed": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n         Returns\n         -------"}
{"id": "keras_28", "problem": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):", "fixed": " class TimeseriesGenerator(Sequence):\n         self.reverse = reverse\n         self.batch_size = batch_size\n        if self.start_index > self.end_index:\n            raise ValueError('`start_index+length=%i > end_index=%i` '\n                             'is disallowed, as no part of the sequence '\n                             'would be left to be used as current step.'\n                             % (self.start_index, self.end_index))\n     def __len__(self):\n         return int(np.ceil(\n            (self.end_index - self.start_index + 1) /\n             (self.batch_size * self.stride)))\n     def _empty_batch(self, num_rows):"}
{"id": "black_22", "problem": " class Line:\n         return False\n    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:\n        if not (\n         if comment.type != token.COMMENT:\n             return False\n        try:\n            after = id(self.last_non_delimiter())\n        except LookupError:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            if after in self.comments:\n                self.comments[after].value += str(comment)\n            else:\n                self.comments[after] = comment\n             return True\n    def last_non_delimiter(self) -> Leaf:\n        raise LookupError(\"No non-delimiters found\")", "fixed": " class Line:\n         return False\n    def append_comment(self, comment: Leaf) -> bool:\n         if comment.type != token.COMMENT:\n             return False\n        after = len(self.leaves) - 1\n        if after == -1:\n             comment.type = STANDALONE_COMMENT\n             comment.prefix = ''\n             return False\n         else:\n            self.comments.append((after, comment))\n             return True\n        for _leaf_index, _leaf in enumerate(self.leaves):\n            if leaf is _leaf:\n                break\n        else:\n            return\n        for index, comment_after in self.comments:\n            if _leaf_index == index:\n                yield comment_after\n    def remove_trailing_comma(self) -> None:"}
{"id": "pandas_68", "problem": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,", "fixed": " from pandas.core.dtypes.common import (\n from pandas.core.dtypes.dtypes import IntervalDtype\n from pandas.core.dtypes.generic import (\n     ABCDatetimeIndex,\n    ABCExtensionArray,\n     ABCIndexClass,\n     ABCInterval,\n     ABCIntervalIndex,"}
{"id": "scrapy_32", "problem": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(settings)\n        log_scrapy_info(settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)", "fixed": " class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings)\n        log_scrapy_info(self.settings)\n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "pandas_47", "problem": " class _LocationIndexer(_NDFrameIndexerBase):\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)", "fixed": " class _LocationIndexer(_NDFrameIndexerBase):\n        if self.name == \"loc\":\n            self._ensure_listlike_indexer(key)\n         if self.axis is not None:\n             return self._convert_tuple(key, is_setter=True)"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")", "fixed": " def diff(arr, n: int, axis: int = 0):\n     dtype = arr.dtype\n     is_timedelta = False\n    is_bool = False\n     if needs_i8_conversion(arr):\n         dtype = np.float64\n         arr = arr.view(\"i8\")"}
{"id": "keras_29", "problem": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]", "fixed": " class Model(Container):\n         if hasattr(self, 'metrics'):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n                 if str(name) in self.stateful_metric_names]"}
{"id": "pandas_44", "problem": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\n from pandas.core.arrays.period import (\n     PeriodArray,", "fixed": " from pandas.core.dtypes.common import (\n     is_scalar,\n     pandas_dtype,\n )\nfrom pandas.core.dtypes.dtypes import PeriodDtype\n from pandas.core.arrays.period import (\n     PeriodArray,"}
{"id": "keras_29", "problem": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.metrics:\n                if isinstance(m, Layer) and m.stateful:\n                    m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:", "fixed": " class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n            for m in self.stateful_metric_functions:\n                m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n             if steps_per_epoch is not None:"}
{"id": "pandas_134", "problem": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2030, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):", "fixed": " class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):\nrules = []\n     start_date = Timestamp(datetime(1970, 1, 1))\n    end_date = Timestamp(datetime(2200, 12, 31))\n     _cache = None\n     def __init__(self, name=None, rules=None):"}
{"id": "matplotlib_21", "problem": " class Axes(_AxesBase):\n         zdelta = 0.1\n        def line_props_with_rcdefaults(subkey, explicit, zdelta=0):\n             d = {k.split('.')[-1]: v for k, v in rcParams.items()\n                  if k.startswith(f'boxplot.{subkey}')}\n             d['zorder'] = zorder + zdelta\n             if explicit is not None:\n                 d.update(\n                     cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))", "fixed": " class Axes(_AxesBase):\n         zdelta = 0.1\n        def line_props_with_rcdefaults(subkey, explicit, zdelta=0,\n                                       use_marker=True):\n             d = {k.split('.')[-1]: v for k, v in rcParams.items()\n                  if k.startswith(f'boxplot.{subkey}')}\n             d['zorder'] = zorder + zdelta\n            if not use_marker:\n                d['marker'] = ''\n             if explicit is not None:\n                 d.update(\n                     cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))"}
{"id": "youtube-dl_35", "problem": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }", "fixed": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n        upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(upload_date_str),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }"}
{"id": "thefuck_28", "problem": " import re\n import os\nfrom thefuck.utils import memoize\n from thefuck import shells\n patterns = (\n         '^    at {file}:{line}:{col}',", "fixed": " import re\n import os\nfrom thefuck.utils import memoize, wrap_settings\n from thefuck import shells\n patterns = (\n         '^    at {file}:{line}:{col}',"}
{"id": "pandas_39", "problem": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n             self._update_inplace(", "fixed": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n            self._reset_cacher()\n             self._update_inplace("}
{"id": "fastapi_7", "problem": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={\"detail\": exc.errors()}\n     )", "fixed": " async def request_validation_exception_handler(\n     request: Request, exc: RequestValidationError\n ) -> JSONResponse:\n     return JSONResponse(\n        status_code=HTTP_422_UNPROCESSABLE_ENTITY,\n        content={\"detail\": jsonable_encoder(exc.errors())},\n     )"}
{"id": "keras_34", "problem": " class Sequence(object):\n _SHARED_SEQUENCES = {}", "fixed": " class Sequence(object):\n        while True:\n            for item in (self[i] for i in range(len(self))):\n                yield item\n _SHARED_SEQUENCES = {}"}
{"id": "luigi_1", "problem": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics = self._scheduler._state._metrics_collector.generate_latest()\n         if metrics:\n            metrics.configure_http_handler(self)\n             self.write(metrics)", "fixed": " class MetricsHandler(tornado.web.RequestHandler):\n         self._scheduler = scheduler\n     def get(self):\n        metrics_collector = self._scheduler._state._metrics_collector\n        metrics = metrics_collector.generate_latest()\n         if metrics:\n            metrics_collector.configure_http_handler(self)\n             self.write(metrics)"}
{"id": "thefuck_18", "problem": " patterns = ['permission denied',\n def match(command):\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():", "fixed": " patterns = ['permission denied',\n def match(command):\n    if command.script_parts and command.script_parts[0] == 'sudo':\n        return False\n     for pattern in patterns:\n         if pattern.lower() in command.stderr.lower()\\\n                 or pattern.lower() in command.stdout.lower():"}
{"id": "keras_34", "problem": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data", "fixed": " class Model(Container):\n                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n                         validation_generator = val_enqueuer.get()\n                     else:\n                        if isinstance(validation_data, Sequence):\n                            validation_generator = iter(validation_data)\n                        else:\n                            validation_generator = validation_data\n                 else:\n                     if len(validation_data) == 2:\n                         val_x, val_y = validation_data"}
{"id": "black_20", "problem": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src.name}  (original)\"\n        dst_name = f\"{src.name}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()", "fixed": " def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n        src_name = f\"{src}  (original)\"\n        dst_name = f\"{src}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()"}
{"id": "luigi_15", "problem": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED) or \\\n                    getattr(task, 'scheduler_disable_time', None) is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks", "fixed": " class SimpleTaskState(object):\n     def get_necessary_tasks(self):\n         necessary_tasks = set()\n         for task in self.get_active_tasks():\n            if task.status not in (DONE, DISABLED, UNKNOWN) or \\\n                    task.scheduler_disable_time is not None:\n                 necessary_tasks.update(task.deps)\n                 necessary_tasks.add(task.id)\n         return necessary_tasks"}
{"id": "black_22", "problem": " import asyncio\n from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\nfrom functools import partial\n import keyword\n import os\n from pathlib import Path\n import tokenize\n import sys\n from typing import (\n    Dict, Generic, Iterable, Iterator, List, Optional, Set, Tuple, Type, TypeVar, Union\n )\n from attr import dataclass, Factory", "fixed": " import asyncio\n from asyncio.base_events import BaseEventLoop\n from concurrent.futures import Executor, ProcessPoolExecutor\nfrom functools import partial, wraps\n import keyword\n import os\n from pathlib import Path\n import tokenize\n import sys\n from typing import (\n    Callable,\n    Dict,\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n )\n from attr import dataclass, Factory"}
{"id": "black_15", "problem": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense.", "fixed": " class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n    def line(self, indent: int = 0) -> Iterator[Line]:\n         If the line is empty, only emit if it makes sense."}
{"id": "cookiecutter_2", "problem": " def run_hook(hook_name, project_dir, context):\n     :param project_dir: The directory to execute the script from.\n     :param context: Cookiecutter project context.\n    script = find_hook(hook_name)\n    if script is None:\n         logger.debug('No %s hook found', hook_name)\n         return\n     logger.debug('Running hook %s', hook_name)\n    run_script_with_context(script, project_dir, context)", "fixed": " def run_hook(hook_name, project_dir, context):\n     :param project_dir: The directory to execute the script from.\n     :param context: Cookiecutter project context.\n    scripts = find_hook(hook_name)\n    if not scripts:\n         logger.debug('No %s hook found', hook_name)\n         return\n     logger.debug('Running hook %s', hook_name)\n    for script in scripts:\n        run_script_with_context(script, project_dir, context)"}
{"id": "youtube-dl_17", "problem": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]", "fixed": " def cli_option(params, command_option, param):\n def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n     param = params.get(param)\n    if param is None:\n        return []\n     assert isinstance(param, bool)\n     if separator:\n         return [command_option + separator + (true_value if param else false_value)]"}
{"id": "tqdm_6", "problem": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else self.total)\n     def __enter__(self):\n         return self", "fixed": " class tqdm(object):\n         return self.total if self.iterable is None else \\\n             (self.iterable.shape[0] if hasattr(self.iterable, \"shape\")\n              else len(self.iterable) if hasattr(self.iterable, \"__len__\")\n             else getattr(self, \"total\", None))\n     def __enter__(self):\n         return self"}
{"id": "pandas_41", "problem": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype", "fixed": " class IntBlock(NumericBlock):\n             )\n         return is_integer(element)\n    def should_store(self, value: ArrayLike) -> bool:\n         return is_integer_dtype(value) and value.dtype == self.dtype"}
{"id": "keras_6", "problem": " def weighted_masked_objective(fn):\n             score_array *= mask\n            score_array /= K.mean(mask)\n         if weights is not None:", "fixed": " def weighted_masked_objective(fn):\n             score_array *= mask\n            score_array /= K.mean(mask) + K.epsilon()\n         if weights is not None:"}
{"id": "black_18", "problem": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with tokenize.open(src) as src_buffer:\n        src_contents = src_buffer.read()\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode", "fixed": " def format_file_in_place(\n     if src.suffix == \".pyi\":\n         mode |= FileMode.PYI\n    with open(src, \"rb\") as buf:\n        newline, encoding, src_contents = prepare_input(buf.read())\n     try:\n         dst_contents = format_file_contents(\n             src_contents, line_length=line_length, fast=fast, mode=mode"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "keras_42", "problem": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):", "fixed": " class Sequential(Model):\n                                         initial_epoch=initial_epoch)\n     @interfaces.legacy_generator_methods_support\n    def evaluate_generator(self, generator, steps=None,\n                            max_queue_size=10, workers=1,\n                            use_multiprocessing=False):"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "thefuck_7", "problem": " from thefuck.utils import replace_argument, for_app\n @for_app('php')\n def match(command):\n    return \"php -s\" in command.script\n def get_new_command(command):\n     return replace_argument(command.script, \"-s\", \"-S\")\nrequires_output = False", "fixed": " from thefuck.utils import replace_argument, for_app\n @for_app('php')\n def match(command):\n    return \" -s \" in command.script\n def get_new_command(command):\n     return replace_argument(command.script, \"-s\", \"-S\")"}
{"id": "pandas_55", "problem": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n            ax = self.obj.axes[i]\n            if not ax.is_unique:\n                return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:", "fixed": " class _iLocIndexer(_LocationIndexer):\n             if not is_integer(k):\n                 return False\n         return True\n     def _validate_integer(self, key: int, axis: int) -> None:"}
{"id": "pandas_55", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            lower = self._take_with_is_copy(item, axis=self._info_axis_number)\n         return lower\n     def _box_item_values(self, key, values):", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n             res._is_copy = self._is_copy\n         return res\n    def _iget_item_cache(self, item: int):\n         ax = self._info_axis\n         if ax.is_unique:\n             lower = self._get_item_cache(ax[item])\n         else:\n            return self._ixs(item, axis=1)\n         return lower\n     def _box_item_values(self, key, values):"}
{"id": "spacy_3", "problem": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_regex.search(article_text)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)", "fixed": " def _process_wp_text(article_title, article_text, wp_to_id):\n         return None, None\n    text_search = text_tag_regex.sub(\"\", article_text)\n    text_search = text_regex.search(text_search)\n     if text_search is None:\n         return None, None\n     text = text_search.group(0)"}
{"id": "luigi_16", "problem": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            if task.id not in necessary_tasks and self._state.prune(task, self._config):\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)", "fixed": " class CentralPlannerScheduler(Scheduler):\n         for task in self._state.get_active_tasks():\n             self._state.fail_dead_worker_task(task, self._config, assistant_ids)\n            removed = self._state.prune(task, self._config)\n            if removed and task.id not in necessary_tasks:\n                 remove_tasks.append(task.id)\n         self._state.inactivate_tasks(remove_tasks)"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if is_sequence:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator", "fixed": " def evaluate_generator(model, generator,\n             enqueuer.start(workers=workers, max_queue_size=max_queue_size)\n             output_generator = enqueuer.get()\n         else:\n            if use_sequence_api:\n                 output_generator = iter_sequence_infinite(generator)\n             else:\n                 output_generator = generator"}
{"id": "keras_11", "problem": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,", "fixed": " def fit_generator(model,\n                     cbk.validation_data = val_data\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing,"}
{"id": "keras_1", "problem": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)", "fixed": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n            self.seed += 1\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)"}
{"id": "fastapi_13", "problem": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                if responses is None:\n                    responses = {}\n                responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,", "fixed": " class APIRouter(routing.Router):\n             assert not prefix.endswith(\n                 \"/\"\n             ), \"A path prefix must not end with '/', as the routes will start with '/'\"\n        if responses is None:\n            responses = {}\n         for route in router.routes:\n             if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                 self.add_api_route(\n                     prefix + route.path,\n                     route.endpoint,"}
{"id": "pandas_90", "problem": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\ndef read_pickle(path, compression=\"infer\"):\n     Load pickled pandas object (or any object) from file.", "fixed": " def to_pickle(obj, path, compression=\"infer\", protocol=pickle.HIGHEST_PROTOCOL):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass\ndef read_pickle(\n    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = \"infer\"\n):\n     Load pickled pandas object (or any object) from file."}
{"id": "youtube-dl_34", "problem": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n         if not expected:\nmsg = msg + u'; please report this issue on https:\n         super(ExtractorError, self).__init__(msg)", "fixed": " class ExtractorError(Exception):\n             expected = True\n         if video_id is not None:\n             msg = video_id + ': ' + msg\n        if cause:\n            msg += u' (caused by %r)' % cause\n         if not expected:\nmsg = msg + u'; please report this issue on https:\n         super(ExtractorError, self).__init__(msg)"}
{"id": "pandas_84", "problem": " import pytest\n import pandas.util._test_decorators as td\n import pandas as pd\nfrom pandas import DataFrame, MultiIndex, Series\n import pandas._testing as tm", "fixed": " import pytest\n import pandas.util._test_decorators as td\n import pandas as pd\nfrom pandas import DataFrame, Series\n import pandas._testing as tm"}
{"id": "ansible_4", "problem": " class CollectionSearch:\nif not ds:\n             return None\n         return ds", "fixed": " class CollectionSearch:\nif not ds:\n             return None\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n         return ds"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    is_sequence = isinstance(generator, Sequence)\n    if not is_sequence and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if is_sequence:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'", "fixed": " def fit_generator(model,\n     if do_validation:\n         model._make_test_function()\n    use_sequence_api = is_sequence(generator)\n    if not use_sequence_api and use_multiprocessing and workers > 1:\n         warnings.warn(\n             UserWarning('Using a generator with `use_multiprocessing=True`'\n                         ' and multiple workers may duplicate your data.'\n                         ' Please consider using the`keras.utils.Sequence'\n                         ' class.'))\n     if steps_per_epoch is None:\n        if use_sequence_api:\n             steps_per_epoch = len(generator)\n         else:\n             raise ValueError('`steps_per_epoch=None` is only valid for a'"}
{"id": "pandas_82", "problem": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), tslibs.iNaT\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:", "fixed": " def _get_empty_dtype_and_na(join_units):\n         dtype = upcast_classes[\"datetimetz\"]\n         return dtype[0], tslibs.NaT\n     elif \"datetime\" in upcast_classes:\n        return np.dtype(\"M8[ns]\"), np.datetime64(\"NaT\", \"ns\")\n     elif \"timedelta\" in upcast_classes:\n         return np.dtype(\"m8[ns]\"), np.timedelta64(\"NaT\", \"ns\")\nelse:"}
{"id": "ansible_13", "problem": " from ansible.galaxy.role import GalaxyRole\n from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel\n from ansible.module_utils.ansible_release import __version__ as ansible_version\n from ansible.module_utils._text import to_bytes, to_native, to_text\n from ansible.parsing.yaml.loader import AnsibleLoader\n from ansible.playbook.role.requirement import RoleRequirement\n from ansible.utils.display import Display\n from ansible.utils.plugin_docs import get_versioned_doclink\n display = Display()\n class GalaxyCLI(CLI):", "fixed": " from ansible.galaxy.role import GalaxyRole\n from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel\n from ansible.module_utils.ansible_release import __version__ as ansible_version\n from ansible.module_utils._text import to_bytes, to_native, to_text\nfrom ansible.module_utils import six\n from ansible.parsing.yaml.loader import AnsibleLoader\n from ansible.playbook.role.requirement import RoleRequirement\n from ansible.utils.display import Display\n from ansible.utils.plugin_docs import get_versioned_doclink\n display = Display()\nurlparse = six.moves.urllib.parse.urlparse\n class GalaxyCLI(CLI):"}
{"id": "tornado_11", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\") == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n         if content_length is not None:\n             return self._read_fixed_body(content_length, delegate)\n        if headers.get(\"Transfer-Encoding\", \"\").lower() == \"chunked\":\n             return self._read_chunked_body(delegate)\n         if self.is_client:\n             return self._read_body_until_close(delegate)"}
{"id": "pandas_41", "problem": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value):\n         return isinstance(value, self._holder)\n    def set(self, locs, values, check=False):\n         assert locs.tolist() == [0]\n        self.values = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,", "fixed": " class ExtensionBlock(Block):\n                 raise IndexError(f\"{self} only contains one item\")\n             return self.values\n    def should_store(self, value: ArrayLike) -> bool:\n         return isinstance(value, self._holder)\n    def set(self, locs, values):\n         assert locs.tolist() == [0]\n        self.values[:] = values\n     def putmask(\n         self, mask, new, align=True, inplace=False, axis=0, transpose=False,"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "youtube-dl_16", "problem": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))\n     out = []\nparas = dfxp.findall(_x('.", "fixed": " def dfxp2srt(dfxp_data):\n         for ns in v:\n             dfxp_data = dfxp_data.replace(ns, k)\n    dfxp = compat_etree_fromstring(dfxp_data)\n     out = []\nparas = dfxp.findall(_x('."}
{"id": "luigi_9", "problem": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\",", "fixed": " def _get_comments(group_tasks):\n _ORDERED_STATUSES = (\n     \"already_done\",\n     \"completed\",\n    \"ever_failed\",\n     \"failed\",\n     \"scheduling_error\",\n     \"still_pending\","}
{"id": "pandas_165", "problem": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n class TestAddSubNaTMasking:", "fixed": " class TestTimedelta64ArithmeticUnsorted:\n         tm.assert_index_equal(result1, result4)\n         tm.assert_index_equal(result2, result3)\n    def test_tda_add_sub_index(self):\n        tdi = TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\n        tda = tdi.array\n        dti = pd.date_range(\"1999-12-31\", periods=3, freq=\"D\")\n        result = tda + dti\n        expected = tdi + dti\n        tm.assert_index_equal(result, expected)\n        result = tda + tdi\n        expected = tdi + tdi\n        tm.assert_index_equal(result, expected)\n        result = tda - tdi\n        expected = tdi - tdi\n        tm.assert_index_equal(result, expected)\n class TestAddSubNaTMasking:"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "tornado_2", "problem": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and \"Transfer-Encoding\" not in headers\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)", "fixed": " class HTTP1Connection(httputil.HTTPConnection):\n             self._chunking_output = (\n                 start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n                 and \"Content-Length\" not in headers\n                and (\n                    \"Transfer-Encoding\" not in headers\n                    or headers[\"Transfer-Encoding\"] == \"chunked\"\n                )\n             )\n         else:\n             assert isinstance(start_line, httputil.ResponseStartLine)"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values.T, other.values).T\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":", "fixed": " class DataFrame(NDFrame):\n             new_data = ops.dispatch_to_series(self, other, func)\n         else:\n            other_vals = other.values.reshape(-1, 1)\n             with np.errstate(all=\"ignore\"):\n                new_data = func(self.values, other_vals)\n            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)\n         return new_data\n     def _construct_result(self, result) -> \"DataFrame\":"}
{"id": "pandas_103", "problem": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)", "fixed": " class GroupBy(_GroupBy):\n                     axis=axis,\n                 )\n             )\n        if fill_method is None:\n            fill_method = \"pad\"\n            limit = 0\n         filled = getattr(self, fill_method)(limit=limit)\n         fill_grp = filled.groupby(self.grouper.codes)\n         shifted = fill_grp.shift(periods=periods, freq=freq)"}
{"id": "keras_37", "problem": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n            if not isinstance(initial_state, list):\n                raise ValueError(\n                    'When passing `initial_state` to a Bidirectional RNN, the state '\n                    'should be a list containing the states of the underlying RNNs. '\n                    'Found: ' + str(initial_state))\nforward_state = initial_state[:len(initial_state)\nbackward_state = initial_state[len(initial_state)\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)", "fixed": " class Bidirectional(Wrapper):\n             kwargs['mask'] = mask\n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\nforward_state = initial_state[:len(initial_state)\nbackward_state = initial_state[len(initial_state)\n             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)"}
{"id": "ansible_4", "problem": " __metaclass__ = type\n from ansible.module_utils.six import string_types\n from ansible.playbook.attribute import FieldAttribute\n from ansible.utils.collection_loader import AnsibleCollectionLoader\n def _ensure_default_collection(collection_list=None):", "fixed": " __metaclass__ = type\n from ansible.module_utils.six import string_types\n from ansible.playbook.attribute import FieldAttribute\n from ansible.utils.collection_loader import AnsibleCollectionLoader\nfrom ansible.template import is_template, Environment\nfrom ansible.utils.display import Display\ndisplay = Display()\n def _ensure_default_collection(collection_list=None):"}
{"id": "matplotlib_5", "problem": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            linewidths = rcParams['lines.linewidth']\n         offsets = np.ma.column_stack([x, y])", "fixed": " default: :rc:`scatter.edgecolors`\n             marker_obj.get_transform())\n         if not marker_obj.is_filled():\n             edgecolors = 'face'\n            if linewidths is None:\n                linewidths = rcParams['lines.linewidth']\n            elif np.iterable(linewidths):\n                linewidths = [\n                    lw if lw is not None else rcParams['lines.linewidth']\n                    for lw in linewidths]\n         offsets = np.ma.column_stack([x, y])"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class FastAPI(Starlette):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_18", "problem": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n         callable_fn = session._make_callable_from_options(callable_opts)", "fixed": " class Function(object):\n             callable_opts.fetch.append(x.name)\n         callable_opts.target.append(self.updates_op.name)\n        if self.run_options:\n            callable_opts.run_options.CopyFrom(self.run_options)\n         callable_fn = session._make_callable_from_options(callable_opts)"}
{"id": "luigi_23", "problem": " class CentralPlannerScheduler(Scheduler):\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})", "fixed": " class CentralPlannerScheduler(Scheduler):\n        if self._config.prune_on_get_work:\n            self.prune()\n         worker_id = kwargs['worker']\n         self.update(worker_id, {'host': host})"}
{"id": "pandas_142", "problem": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n     elif is_integer_dtype(dtype):\n         dtype = np.float64", "fixed": " def diff(arr, n: int, axis: int = 0):\n     elif is_bool_dtype(dtype):\n         dtype = np.object_\n        is_bool = True\n     elif is_integer_dtype(dtype):\n         dtype = np.float64"}
{"id": "pandas_115", "problem": " def interpolate_1d(\n                 inds = lib.maybe_convert_objects(inds)\n         else:\n             inds = xvalues\n        result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])\n         result[preserve_nans] = np.nan\n         return result", "fixed": " def interpolate_1d(\n                 inds = lib.maybe_convert_objects(inds)\n         else:\n             inds = xvalues\n        indexer = np.argsort(inds[valid])\n        result[invalid] = np.interp(\n            inds[invalid], inds[valid][indexer], yvalues[valid][indexer]\n        )\n         result[preserve_nans] = np.nan\n         return result"}
{"id": "pandas_8", "problem": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n        if not mask.any():\n            if inplace:\n                return [self]\n            return [self.copy()]\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)", "fixed": " class Block(PandasObject):\n         mask = missing.mask_missing(values, to_replace)\n         try:\n             blocks = self.putmask(mask, value, inplace=inplace)"}
{"id": "tornado_8", "problem": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\",", "fixed": " class WebSocketProtocol13(WebSocketProtocol):\n     def accept_connection(self):\n         try:\n             self._handle_websocket_headers()\n        except ValueError:\n            self.handler.set_status(400)\n            log_msg = \"Missing/Invalid WebSocket headers\"\n            self.handler.finish(log_msg)\n            gen_log.debug(log_msg)\n            return\n        try:\n             self._accept_connection()\n         except ValueError:\n             gen_log.debug(\"Malformed WebSocket request received\","}
{"id": "matplotlib_15", "problem": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj", "fixed": " class SymLogNorm(Normalize):\n         masked = np.abs(a) > (self.linthresh * self._linscale_adj)\n         sign = np.sign(a[masked])\n        exp = np.power(self._base,\n                       sign * a[masked] / self.linthresh - self._linscale_adj)\n         exp *= sign * self.linthresh\n         a[masked] = exp\n         a[~masked] /= self._linscale_adj"}
{"id": "youtube-dl_33", "problem": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()", "fixed": " def parse_iso8601(date_str, delimiter='T'):\n         return None\n     m = re.search(\n        r'(\\.[0-9]+)?(?:Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n         date_str)\n     if not m:\n         timezone = datetime.timedelta()"}
{"id": "keras_8", "problem": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    add_unprocessed_node(layer, node_data)\n                    return\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:", "fixed": " class Network(Layer):\n                 else:\n                     raise ValueError('Improperly formatted model config.')\n                 inbound_layer = created_layers[inbound_layer_name]\n                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n                    raise LookupError\n                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n                 input_tensors.append(\n                     inbound_node.output_tensors[inbound_tensor_index])\n             if input_tensors:"}
{"id": "keras_42", "problem": " class Sequential(Model):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                      steps_per_epoch,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,", "fixed": " class Sequential(Model):\n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n                      steps_per_epoch=None,\n                       epochs=1,\n                       verbose=1,\n                       callbacks=None,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "scrapy_5", "problem": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,", "fixed": " class Response(object_ref):\n         if isinstance(url, Link):\n             url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n         url = self.urljoin(url)\n         return Request(url, callback,\n                        method=method,"}
{"id": "pandas_123", "problem": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n                     elif inferred == \"string\":\n                         pass", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, name=name)\n                     elif inferred == \"string\":\n                         pass"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "youtube-dl_37", "problem": " import calendar\n import contextlib\n import ctypes\n import datetime", "fixed": " import calendar\nimport codecs\n import contextlib\n import ctypes\n import datetime"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "pandas_92", "problem": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq).ordinal\n                start = start.ordinal\n             if where < start:\n                 if not is_series:", "fixed": " class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):\n         if not is_list:\n             start = self.index[0]\n             if isinstance(self.index, PeriodIndex):\n                where = Period(where, freq=self.index.freq)\n             if where < start:\n                 if not is_series:"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "scrapy_18", "problem": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:", "fixed": " class ResponseTypes(object):\n     def from_content_disposition(self, content_disposition):\n         try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n         except IndexError:"}
{"id": "youtube-dl_6", "problem": " def dfxp2srt(dfxp_data):\n         raise ValueError('Invalid dfxp/TTML subtitle')\n     for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib['begin'])\n         end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n         if not end_time:\n            end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])\n         out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n             index,\n             srt_subtitles_timecode(begin_time),", "fixed": " def dfxp2srt(dfxp_data):\n         raise ValueError('Invalid dfxp/TTML subtitle')\n     for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n         end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n         if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n         out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n             index,\n             srt_subtitles_timecode(begin_time),"}
{"id": "thefuck_6", "problem": " from thefuck.utils import eager\n @git_support\n def match(command):\n     return (\"fatal: A branch named '\" in command.output\n            and \" already exists.\" in command.output)\n @git_support\n @eager\n def get_new_command(command):\n     branch_name = re.findall(\n        r\"fatal: A branch named '([^']*)' already exists.\", command.output)[0]\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],", "fixed": " from thefuck.utils import eager\n @git_support\n def match(command):\n     return (\"fatal: A branch named '\" in command.output\n            and \"' already exists.\" in command.output)\n @git_support\n @eager\n def get_new_command(command):\n     branch_name = re.findall(\n        r\"fatal: A branch named '(.+)' already exists.\", command.output)[0]\n    branch_name = branch_name.replace(\"'\", r\"\\'\")\n     new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                              ['git branch -d {0}', 'git checkout -b {0}'],\n                              ['git branch -D {0}', 'git branch {0}'],"}
{"id": "keras_15", "problem": " import numpy as np\n import time\n import json\n import warnings\n from collections import deque\n from collections import OrderedDict", "fixed": " import numpy as np\n import time\n import json\n import warnings\nimport io\nimport sys\n from collections import deque\n from collections import OrderedDict"}
{"id": "ansible_17", "problem": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}", "fixed": " class LinuxHardware(Hardware):\n     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n     def populate(self, collected_facts=None):\n         hardware_facts = {}\n         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}"}
{"id": "tornado_6", "problem": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = weakref.WeakKeyDictionary()\n     @classmethod\n     def configure(cls, impl, **kwargs):", "fixed": " class IOLoop(Configurable):\n     _current = threading.local()\n    _ioloop_for_asyncio = dict()\n     @classmethod\n     def configure(cls, impl, **kwargs):"}
{"id": "pandas_76", "problem": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError):\n                 pass", "fixed": " class Parser:\n                 if (new_data == data).all():\n                     data = new_data\n                     result = True\n            except (TypeError, ValueError, OverflowError):\n                 pass"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "PySnooper_1", "problem": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a') as output_file:\n             output_file.write(s)\n         self.overwrite = False", "fixed": " class FileWriter(object):\n         self.overwrite = overwrite\n     def write(self, s):\n        with open(self.path, 'w' if self.overwrite else 'a',\n                  encoding='utf-8') as output_file:\n             output_file.write(s)\n         self.overwrite = False"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "matplotlib_17", "problem": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander", "fixed": " def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):\n         vmin, vmax = vmax, vmin\n         swapped = True\n    vmin, vmax = map(float, [vmin, vmax])\n     maxabsvalue = max(abs(vmin), abs(vmax))\n     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:\n         vmin = -expander"}
{"id": "pandas_159", "problem": " from pandas.core.internals.construction import (\n     sanitize_index,\n     to_arrays,\n )\n from pandas.core.series import Series\n from pandas.io.formats import console, format as fmt", "fixed": " from pandas.core.internals.construction import (\n     sanitize_index,\n     to_arrays,\n )\nfrom pandas.core.ops.missing import dispatch_fill_zeros\n from pandas.core.series import Series\n from pandas.io.formats import console, format as fmt"}
{"id": "black_22", "problem": " Depth = int\n NodeType = int\n LeafID = int\n Priority = int\n LN = Union[Leaf, Node]\n out = partial(click.secho, bold=True, err=True)\n err = partial(click.secho, fg='red', err=True)", "fixed": " Depth = int\n NodeType = int\n LeafID = int\n Priority = int\nIndex = int\n LN = Union[Leaf, Node]\nSplitFunc = Callable[['Line', bool], Iterator['Line']]\n out = partial(click.secho, bold=True, err=True)\n err = partial(click.secho, fg='red', err=True)"}
{"id": "keras_17", "problem": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.max(y_true, axis=-1),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())", "fixed": " def categorical_accuracy(y_true, y_pred):\n def sparse_categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.flatten(y_true),\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                   K.floatx())"}
{"id": "pandas_25", "problem": " default 'raise'\n         from pandas import DataFrame\n        sarray = fields.build_isocalendar_sarray(self.asi8)\n         iso_calendar_df = DataFrame(\n             sarray, columns=[\"year\", \"week\", \"day\"], dtype=\"UInt32\"\n         )", "fixed": " default 'raise'\n         from pandas import DataFrame\n        if self.tz is not None and not timezones.is_utc(self.tz):\n            values = self._local_timestamps()\n        else:\n            values = self.asi8\n        sarray = fields.build_isocalendar_sarray(values)\n         iso_calendar_df = DataFrame(\n             sarray, columns=[\"year\", \"week\", \"day\"], dtype=\"UInt32\"\n         )"}
{"id": "sanic_3", "problem": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)", "fixed": " class Sanic:\n                 \"Endpoint with name `{}` was not found\".format(view_name)\n             )\n        host = uri.find(\"/\")\n        if host > 0:\n            host, uri = uri[:host], uri[host:]\n        else:\n            host = None\n         if view_name == \"static\" or view_name.endswith(\".static\"):\n             filename = kwargs.pop(\"filename\", None)"}
{"id": "pandas_80", "problem": "import operator\n import numpy as np\n import pytest\nimport pandas.util._test_decorators as td\n import pandas as pd\n import pandas._testing as tm\nfrom pandas.arrays import BooleanArray\nfrom pandas.core.arrays.boolean import coerce_to_array\nfrom pandas.tests.extension.base import BaseOpsUtil\n def make_data():", "fixed": " import numpy as np\n import pytest\nfrom pandas.compat.numpy import _np_version_under1p14\n import pandas as pd\n import pandas._testing as tm\nfrom pandas.core.arrays.boolean import BooleanDtype\nfrom pandas.tests.extension import base\n def make_data():"}
{"id": "pandas_81", "problem": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):", "fixed": " class IntegerArray(BaseMaskedArray):\n             if incompatible type with an IntegerDtype, equivalent of same_kind\n             casting\n        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype\n        dtype = pandas_dtype(dtype)\n         if isinstance(dtype, _IntegerDtype):\n             result = self._data.astype(dtype.numpy_dtype, copy=False)\n             return type(self)(result, mask=self._mask, copy=False)\n        elif isinstance(dtype, BooleanDtype):\n            result = self._data.astype(\"bool\", copy=False)\n            return BooleanArray(result, mask=self._mask, copy=False)\n         if is_float_dtype(dtype):"}
{"id": "black_6", "problem": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: set(),\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,", "fixed": " class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n    ASYNC_IS_VALID_IDENTIFIER = 6\n    ASYNC_IS_RESERVED_KEYWORD = 7\n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n    TargetVersion.PY35: {\n        Feature.UNICODE_LITERALS,\n        Feature.TRAILING_COMMA_IN_CALL,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n    },\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n        Feature.ASYNC_IS_VALID_IDENTIFIER,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,"}
{"id": "pandas_31", "problem": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals):\n                 inference = np.int64\n            elif is_datetime64_dtype(vals):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)", "fixed": " class GroupBy(_GroupBy[FrameOrSeries]):\n                 )\n             inference = None\n            if is_integer_dtype(vals.dtype):\n                if is_extension_array_dtype(vals.dtype):\n                    vals = vals.to_numpy(dtype=float, na_value=np.nan)\n                 inference = np.int64\n            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):\n                vals = vals.to_numpy(dtype=float, na_value=np.nan)\n            elif is_datetime64_dtype(vals.dtype):\n                 inference = \"datetime64[ns]\"\n                 vals = np.asarray(vals).astype(np.float)"}
{"id": "keras_41", "problem": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 2, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n            custom_generator(), good_batches + 1, 1,\n            workers=4, use_multiprocessing=True,\n         )\n    with pytest.raises(StopIteration):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,", "fixed": " def test_multiprocessing_fit_error():\n def test_multiprocessing_evaluate_error():\n     batch_size = 10\n     good_batches = 3\n    workers = 4\n     def custom_generator():\n         for i in range(good_batches):\n             yield (np.random.randint(batch_size, 256, (50, 2)),\n                   np.random.randint(batch_size, 12, 50))\n         raise RuntimeError\n     model = Sequential()\n     model.add(Dense(1, input_shape=(2, )))\n     model.compile(loss='mse', optimizer='adadelta')\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n            custom_generator(), good_batches * workers + 1, 1,\n            workers=workers, use_multiprocessing=True,\n         )\n    with pytest.raises(RuntimeError):\n         model.evaluate_generator(\n             custom_generator(), good_batches + 1, 1,\n             use_multiprocessing=False,"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensors,\n                                           computed_masks))\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_masks\n                     output_tensors = to_list(\n                         layer(computed_tensors, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensors,\n                                               computed_masks))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                 for x, y, mask in zip(reference_output_tensors,\n                                       output_tensors,"}
{"id": "youtube-dl_31", "problem": " from ..compat import (\n )\n from ..utils import (\n     int_or_none,\n     parse_filesize,\n )", "fixed": " from ..compat import (\n )\n from ..utils import (\n     int_or_none,\n    parse_duration,\n     parse_filesize,\n )"}
{"id": "scrapy_34", "problem": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = {}\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)", "fixed": " class ItemMeta(ABCMeta):\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n        fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n         for n in dir(_class):\n             v = getattr(_class, n)"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             else:\n                 return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n        elif is_period_dtype(data) and not is_object_dtype(dtype):\n            return PeriodIndex(data, copy=copy, name=name, **kwargs)\n         elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             else:\n                 return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)\n        elif is_period_dtype(data) or is_period_dtype(dtype):\n            if is_dtype_equal(_o_dtype, dtype):\n                return PeriodIndex(data, copy=False, name=name, **kwargs).astype(object)\n            return PeriodIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n         elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):"}
{"id": "pandas_136", "problem": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_int64_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:", "fixed": " class _AsOfMerge(_OrderedMerge):\n                 if self.tolerance < Timedelta(0):\n                     raise MergeError(\"tolerance must be positive\")\n            elif is_integer_dtype(lt):\n                 if not is_integer(self.tolerance):\n                     raise MergeError(msg)\n                 if self.tolerance < 0:"}
{"id": "pandas_74", "problem": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray):\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)", "fixed": " class TimedeltaIndex(\n                 \"represent unambiguous timedelta values durations.\"\n             )\n        if isinstance(data, TimedeltaArray) and freq is None:\n             if copy:\n                 data = data.copy()\n             return cls._simple_new(data, name=name, freq=freq)"}
{"id": "pandas_52", "problem": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        val[isna(val)] = np.datetime64(\"NaT\")\n        try:\n            sorter = np.lexsort((val, ids))\n        except TypeError:\n            msg = f\"val.dtype must be object, got {val.dtype}\"\n            assert val.dtype == object, msg\n            val, _ = algorithms.factorize(val, sort=False)\n            sorter = np.lexsort((val, ids))\n            _isna = lambda a: a == -1\n        else:\n            _isna = isna\n        ids, val = ids[sorter], val[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, val[1:] != val[:-1]]\n        mask = _isna(val)\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0", "fixed": " class SeriesGroupBy(GroupBy):\n         val = self.obj._internal_get_values()\n        codes, _ = algorithms.factorize(val, sort=False)\n        sorter = np.lexsort((codes, ids))\n        codes = codes[sorter]\n        ids = ids[sorter]\n         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\n        inc = np.r_[1, codes[1:] != codes[:-1]]\n        mask = codes == -1\n         if dropna:\n             inc[idx] = 1\n             inc[mask] = 0"}
{"id": "thefuck_24", "problem": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command.script != first.script or \\\n                            command.side_effect != first.side_effect:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):", "fixed": " class SortedCorrectedCommandsSequence(object):\n             return []\n         for command in self._commands:\n            if command != first:\n                 return [first, command]\n         return [first]\n     def _remove_duplicates(self, corrected_commands):"}
{"id": "pandas_14", "problem": " def id_func(x):\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])", "fixed": " def id_func(x):\n@pytest.fixture(\n    params=[\n        (\"foo\", None, None),\n        (\"Egon\", \"Venkman\", None),\n        (\"NCC1701D\", \"NCC1701D\", \"NCC1701D\"),\n    ]\n)\ndef names(request):\n    return request.param\n @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])"}
{"id": "fastapi_14", "problem": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[bool, SchemaBase]] = None\n class Example(BaseModel):", "fixed": " class Schema(SchemaBase):\nnot_: Optional[List[SchemaBase]] = PSchema(None, alias=\"not\")\n     items: Optional[SchemaBase] = None\n     properties: Optional[Dict[str, SchemaBase]] = None\n    additionalProperties: Optional[Union[SchemaBase, bool]] = None\n class Example(BaseModel):"}
{"id": "matplotlib_25", "problem": " class EventCollection(LineCollection):\n         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py\n         segment = (lineoffset + linelength / 2.,\n                    lineoffset - linelength / 2.)\n        if positions is None or len(positions) == 0:\n             segments = []\n        elif hasattr(positions, 'ndim') and positions.ndim > 1:\n             raise ValueError('positions cannot be an array with more than '\n                              'one dimension.')\n         elif (orientation is None or orientation.lower() == 'none' or", "fixed": " class EventCollection(LineCollection):\n         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py\n        if positions is None:\n            raise ValueError('positions must be an array-like object')\n        positions = np.array(positions, copy=True)\n         segment = (lineoffset + linelength / 2.,\n                    lineoffset - linelength / 2.)\n        if positions.size == 0:\n             segments = []\n        elif positions.ndim > 1:\n             raise ValueError('positions cannot be an array with more than '\n                              'one dimension.')\n         elif (orientation is None or orientation.lower() == 'none' or"}
{"id": "black_18", "problem": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            sys.stdout.write(dst)\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            sys.stdout.write(diff(src, dst, src_name, dst_name))\n def format_file_contents(", "fixed": " def format_stdin_to_stdout(\n     finally:\n         if write_back == WriteBack.YES:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(dst)\n            f.detach()\n         elif write_back == WriteBack.DIFF:\n             src_name = \"<stdin>  (original)\"\n             dst_name = \"<stdin>  (formatted)\"\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff(src, dst, src_name, dst_name))\n            f.detach()\n def format_file_contents("}
{"id": "pandas_147", "problem": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        elif tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit", "fixed": " class DatetimeTZDtype(PandasExtensionDtype):\n             tz = timezones.tz_standardize(tz)\n         elif tz is not None:\n             raise pytz.UnknownTimeZoneError(tz)\n        if tz is None:\n             raise TypeError(\"A 'tz' is required.\")\n         self._unit = unit"}
{"id": "keras_42", "problem": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,", "fixed": " class Model(Container):\n             return averages\n     @interfaces.legacy_generator_methods_support\n    def predict_generator(self, generator, steps=None,\n                           max_queue_size=10,\n                           workers=1,\n                           use_multiprocessing=False,"}
{"id": "pandas_138", "problem": " from pandas._libs.lib import infer_dtype\n from pandas.core.dtypes.common import (\n     _NS_DTYPE,\n     ensure_int64,\n     is_categorical_dtype,\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,", "fixed": " from pandas._libs.lib import infer_dtype\n from pandas.core.dtypes.common import (\n     _NS_DTYPE,\n     ensure_int64,\n    is_bool_dtype,\n     is_categorical_dtype,\n     is_datetime64_dtype,\n     is_datetime64tz_dtype,"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        include_none=include_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )", "fixed": " def jsonable_encoder(\n         data,\n         by_alias=by_alias,\n         exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n         custom_encoder=custom_encoder,\n         sqlalchemy_safe=sqlalchemy_safe,\n     )"}
{"id": "keras_29", "problem": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)", "fixed": " class Model(Container):\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n                            self.stateful_metric_functions.append(metric_fn)\n                             self.metrics_updates += metric_fn.updates\n                 handle_metrics(output_metrics)"}
{"id": "pandas_83", "problem": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n     Returns\n     -------", "fixed": " def _get_combined_index(\n         calculate the union.\n     sort : bool, default False\n         Whether the result index should come out sorted or not.\n    copy : bool, default False\n        If True, return a copy of the combined index.\n     Returns\n     -------"}
{"id": "pandas_22", "problem": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n         \"min\",\n         \"max\",\n         \"mean\",", "fixed": " def get_weighted_roll_func(cfunc: Callable) -> Callable:\n def validate_baseindexer_support(func_name: Optional[str]) -> None:\n     BASEINDEXER_WHITELIST = {\n        \"count\",\n         \"min\",\n         \"max\",\n         \"mean\","}
{"id": "pandas_24", "problem": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq='D')\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):", "fixed": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):"}
{"id": "youtube-dl_14", "problem": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})", "fixed": " class YoutubeIE(YoutubeBaseInfoExtractor):\n             })\n         return chapters\n    def _extract_chapters(self, webpage, description, video_id, duration):\n        return (self._extract_chapters_from_json(webpage, video_id, duration)\n                or self._extract_chapters_from_description(description, duration))\n     def _real_extract(self, url):\n         url, smuggled_data = unsmuggle_url(url, {})"}
{"id": "pandas_167", "problem": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n     _accessors = {\"str\"}", "fixed": " class Index(IndexOpsMixin, PandasObject):\n     _infer_as_myclass = False\n     _engine_type = libindex.ObjectEngine\n    _supports_partial_string_indexing = False\n     _accessors = {\"str\"}"}
{"id": "thefuck_23", "problem": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with shelve.open(cache_path) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:", "fixed": " def cache(*depends_on):\n             return fn(*args, **kwargs)\n         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')\n         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])\n         etag = '.'.join(_get_mtime(name) for name in depends_on)\n        with closing(shelve.open(cache_path)) as db:\n             if db.get(key, {}).get('etag') == etag:\n                 return db[key]['value']\n             else:"}
{"id": "keras_11", "problem": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if is_sequence:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)", "fixed": " def evaluate_generator(model, generator,\n     try:\n         if workers > 0:\n            if use_sequence_api:\n                 enqueuer = OrderedEnqueuer(\n                     generator,\n                     use_multiprocessing=use_multiprocessing)"}
{"id": "youtube-dl_13", "problem": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:https?:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')", "fixed": " def urljoin(base, path):\n         path = path.decode('utf-8')\n     if not isinstance(path, compat_str) or not path:\n         return None\nif re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?\n         return path\n     if isinstance(base, bytes):\n         base = base.decode('utf-8')"}
{"id": "fastapi_1", "problem": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or include_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    include_none=include_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )", "fixed": " def jsonable_encoder(\n                     or (not isinstance(key, str))\n                     or (not key.startswith(\"_sa\"))\n                 )\n                and (value is not None or not exclude_none)\n                 and ((include and key in include) or key not in exclude)\n             ):\n                 encoded_key = jsonable_encoder(\n                     key,\n                     by_alias=by_alias,\n                     exclude_unset=exclude_unset,\n                    exclude_none=exclude_none,\n                     custom_encoder=custom_encoder,\n                     sqlalchemy_safe=sqlalchemy_safe,\n                 )"}
{"id": "matplotlib_15", "problem": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z1,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')", "fixed": " fig, ax = plt.subplots(2, 1)\n pcm = ax[0].pcolormesh(X, Y, Z1,\n                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                        cmap='RdBu_r')\n fig.colorbar(pcm, ax=ax[0], extend='both')"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "matplotlib_20", "problem": " class FigureCanvasBase:\n         Returns\n         -------\n        axes: topmost axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy)]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:", "fixed": " class FigureCanvasBase:\n         Returns\n         -------\n        axes : `~matplotlib.axes.Axes` or None\n            The topmost visible axes containing the point, or None if no axes.\n         axes_list = [a for a in self.figure.get_axes()\n                     if a.patch.contains_point(xy) and a.get_visible()]\n         if axes_list:\n             axes = cbook._topmost_artist(axes_list)\n         else:"}
{"id": "thefuck_21", "problem": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    return (command.script.split()[1] == 'stash'\n            and 'usage:' in command.stderr)\n stash_commands = (", "fixed": " from thefuck.specific.git import git_support\n @git_support\n def match(command):\n    splited_script = command.script.split()\n    if len(splited_script) > 1:\n        return (splited_script[1] == 'stash'\n                and 'usage:' in command.stderr)\n    else:\n        return False\n stash_commands = ("}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = escape.parse_qs_bytes(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))\n             return\n        args = urlparse.parse_qs(escape.native_str(response.body))\n         session = {\n             \"access_token\": args[\"access_token\"][-1],\n             \"expires\": args.get(\"expires\")"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "tqdm_4", "problem": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale", "fixed": " class tqdm(Comparable):\n         if unit_scale and unit_scale not in (True, 1):\n            if total:\n                total *= unit_scale\n             n *= unit_scale\n             if rate:\nrate *= unit_scale"}
{"id": "pandas_2", "problem": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n        key = list(self._convert_key(key, is_setter=True))\n         self.obj._set_value(*key, value=value, takeable=self._takeable)", "fixed": " class _ScalarAccessIndexer(_NDFrameIndexerBase):\n         if not isinstance(key, tuple):\n             key = _tuplify(self.ndim, key)\n        key = list(self._convert_key(key, is_setter=True))\n         if len(key) != self.ndim:\n             raise ValueError(\"Not enough indexers for scalar access (setting)!\")\n         self.obj._set_value(*key, value=value, takeable=self._takeable)"}
{"id": "keras_1", "problem": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.random_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class RandomNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.random_normal(shape, self.mean, self.stddev,\n                            dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "pandas_80", "problem": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = result.to_dense()\n         result = check_bool_array_indexer(index, result)\n     return result", "fixed": " def check_bool_indexer(index: Index, key) -> np.ndarray:\n         result = result.astype(bool)._values\n     else:\n         if is_sparse(result):\n            result = np.asarray(result)\n         result = check_bool_array_indexer(index, result)\n     return result"}
{"id": "pandas_49", "problem": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:", "fixed": " def str_repeat(arr, repeats):\n     else:\n         def rep(x, r):\n            if x is libmissing.NA:\n                return x\n             try:\n                 return bytes.__mul__(x, r)\n             except TypeError:"}
{"id": "httpie_3", "problem": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue", "fixed": " class Session(BaseConfigDict):\n         for name, value in request_headers.items():\n            if value is None:\n                continue\n             value = value.decode('utf8')\n             if name == 'User-Agent' and value.startswith('HTTPie/'):\n                 continue"}
{"id": "black_18", "problem": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            sys.stdout.write(diff_contents)\n         finally:\n             if lock:\n                 lock.release()", "fixed": " def format_file_in_place(\n         if lock:\n             lock.acquire()\n         try:\n            f = io.TextIOWrapper(\n                sys.stdout.buffer,\n                encoding=encoding,\n                newline=newline,\n                write_through=True,\n            )\n            f.write(diff_contents)\n            f.detach()\n         finally:\n             if lock:\n                 lock.release()"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "fastapi_15", "problem": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n     def get(\n         self,", "fixed": " class APIRouter(routing.Router):\n                     include_in_schema=route.include_in_schema,\n                     name=route.name,\n                 )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n     def get(\n         self,"}
{"id": "black_6", "problem": " from blib2to3 import pygram, pytree\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\n __version__ = \"19.3b0\"", "fixed": " from blib2to3 import pygram, pytree\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\nfrom blib2to3.pgen2.tokenize import TokenizerConfig\n __version__ = \"19.3b0\""}
{"id": "pandas_27", "problem": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            freq = get_period_alias(freq)\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)", "fixed": " default 'raise'\n                     \"You must pass a freq argument as current index has none.\"\n                 )\n            res = get_period_alias(freq)\n            if res is None:\n                base, stride = libfrequencies._base_and_stride(freq)\n                res = f\"{stride}{base}\"\n            freq = res\n         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)"}
{"id": "keras_11", "problem": " from __future__ import print_function\n import warnings\n import numpy as np\n from .training_utils import iter_sequence_infinite\n from .. import backend as K\n from ..utils.data_utils import Sequence", "fixed": " from __future__ import print_function\n import warnings\n import numpy as np\nfrom .training_utils import is_sequence\n from .training_utils import iter_sequence_infinite\n from .. import backend as K\n from ..utils.data_utils import Sequence"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "PySnooper_1", "problem": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'ascii'\n         for line in source[:2]:", "fixed": " def get_source_from_frame(frame):\n     if isinstance(source[0], bytes):\n        encoding = 'utf-8'\n         for line in source[:2]:"}
{"id": "thefuck_20", "problem": " import os\n import zipfile\n from thefuck.utils import for_app\n def _is_bad_zip(file):", "fixed": " import os\n import zipfile\n from thefuck.utils import for_app\nfrom thefuck.shells import quote\n def _is_bad_zip(file):"}
{"id": "keras_21", "problem": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor", "fixed": " class EarlyStopping(Callback):\n                  patience=0,\n                  verbose=0,\n                  mode='auto',\n                 baseline=None,\n                 restore_best_weights=False):\n         super(EarlyStopping, self).__init__()\n         self.monitor = monitor"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n         if fill_value is None:", "fixed": " class DataFrame(NDFrame):\n    def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n         if fill_value is None:"}
{"id": "youtube-dl_40", "problem": " import base64\n import io\n import itertools\n import os\nfrom struct import unpack, pack\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,", "fixed": " import base64\n import io\n import itertools\n import os\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n    struct_pack,\n    struct_unpack,\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,"}
{"id": "pandas_36", "problem": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(\n        obj,\n        (\n            ABCSeries,\n            np.ndarray,\n            ABCIndexClass,\n            ABCExtensionArray,\n            ABCDatetimeArray,\n            ABCTimedeltaArray,\n        ),\n    ):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()", "fixed": " def _isna_new(obj):\n         raise NotImplementedError(\"isna is not defined for MultiIndex\")\n     elif isinstance(obj, type):\n         return False\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n         return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()"}
{"id": "pandas_79", "problem": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)", "fixed": " class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):\n         Fast lookup of value from 1-dimensional ndarray. Only use this if you\n         know what you're doing\n        if not is_scalar(key):\n            raise InvalidIndexError(key)\n         if isinstance(key, (datetime, np.datetime64)):\n             return self.get_value_maybe_box(series, key)"}
{"id": "pandas_71", "problem": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")", "fixed": " def cut(\n     x = _preprocess_for_cut(x)\n     x, dtype = _coerce_to_type(x)\n    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):\n        x = x.to_numpy(dtype=object, na_value=np.nan)\n     if not np.iterable(bins):\n         if is_scalar(bins) and bins < 1:\n             raise ValueError(\"`bins` should be a positive integer.\")"}
{"id": "keras_42", "problem": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Not used if using Sequence.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading", "fixed": " class Model(Container):\n                     when using multiprocessing.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: Maximum size for the generator queue.\n             workers: Maximum number of processes to spin up\n                 when using process based threading"}
{"id": "black_5", "problem": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type == syms.arglist:\n                     commas += 1\n                     break", "fixed": " class Line:\n             bracket_depth = leaf.bracket_depth\n             if bracket_depth == depth and leaf.type == token.COMMA:\n                 commas += 1\n                if leaf.parent and leaf.parent.type in {\n                    syms.arglist,\n                    syms.typedargslist,\n                }:\n                     commas += 1\n                     break"}
{"id": "scrapy_33", "problem": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        extra={'spider': info.spider, 'failure': value}\n                     )\n         return item", "fixed": " class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                     )\n         return item"}
{"id": "pandas_67", "problem": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()", "fixed": " class DatetimeLikeBlockMixin:\n         return self.array_values()\n    def iget(self, key):\n        result = super().iget(key)\n        if isinstance(result, np.datetime64):\n            result = Timestamp(result)\n        elif isinstance(result, np.timedelta64):\n            result = Timedelta(result)\n        return result\n class DatetimeBlock(DatetimeLikeBlockMixin, Block):\n     __slots__ = ()"}
{"id": "tqdm_5", "problem": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n        if total is None and iterable is not None:\n            try:\n                total = len(iterable)\n            except (TypeError, AttributeError):\n                total = None\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:", "fixed": " class tqdm(Comparable):\n                 else TqdmKeyError(\"Unknown argument(s): \" + str(kwargs)))\n         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \\\ndynamic_ncols:\n             if dynamic_ncols:"}
{"id": "black_6", "problem": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]", "fixed": " async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\ndef awaited_generator_value(n):\n    return (await awaitable for awaitable in awaitable_list)\ndef make_arange(n):\n    return (i * 2 for i in range(n) if await wrap(i))"}
{"id": "pandas_21", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        if isinstance(key, list):\n            return self.loc[key]\n        return self.reindex(key)\n     def _get_values_tuple(self, key):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n             else:\n                 return self.iloc[key]\n        return self.loc[key]\n     def _get_values_tuple(self, key):"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslibs.ints_to_pytimedelta(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)\n         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:"}
{"id": "pandas_22", "problem": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        if isinstance(self.window, BaseIndexer):\n            validate_baseindexer_support(\"count\")\n         blocks, obj = self._create_blocks()\n         results = []", "fixed": " class _Rolling_and_Expanding(_Rolling):\n     )\n     def count(self):\n        assert not isinstance(self.window, BaseIndexer)\n         blocks, obj = self._create_blocks()\n         results = []"}
{"id": "pandas_112", "problem": " import re\n import numpy as np\n import pytest\nfrom pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm", "fixed": " import re\n import numpy as np\n import pytest\nfrom pandas import (\n    CategoricalIndex,\n    Interval,\n    IntervalIndex,\n    Timedelta,\n    date_range,\n    timedelta_range,\n)\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm"}
{"id": "pandas_53", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n         if takeable:\n             return self._values[label]\n        return self.index.get_value(self, label)\n     def __setitem__(self, key, value):\n         key = com.apply_if_callable(key, self)", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n         if takeable:\n             return self._values[label]\n        loc = self.index.get_loc(label)\n        return self.index._get_values_for_loc(self, loc, label)\n     def __setitem__(self, key, value):\n         key = com.apply_if_callable(key, self)"}
{"id": "ansible_18", "problem": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s was created successfully\" % obj_name)\n     def execute_info(self):", "fixed": " class GalaxyCLI(CLI):\n                 if not os.path.exists(b_dir_path):\n                     os.makedirs(b_dir_path)\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))\n     def execute_info(self):"}
{"id": "pandas_87", "problem": " def crosstab(\n         kwargs = {\"aggfunc\": aggfunc}\n     table = df.pivot_table(\n        \"__dummy__\",\n         index=rownames,\n         columns=colnames,\n         margins=margins,", "fixed": " def crosstab(\n         kwargs = {\"aggfunc\": aggfunc}\n     table = df.pivot_table(\n        [\"__dummy__\"],\n         index=rownames,\n         columns=colnames,\n         margins=margins,"}
{"id": "pandas_120", "problem": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return result\n     @classmethod\n     def _add_numeric_operations(cls):", "fixed": " class GroupBy(_GroupBy):\n         if isinstance(self.obj, Series):\n             result.name = self.obj.name\n        return self._reindex_output(result, fill_value=0)\n     @classmethod\n     def _add_numeric_operations(cls):"}
{"id": "keras_10", "problem": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n        return weights\n     else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,", "fixed": " def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n    if sample_weight is not None and class_sample_weight is not None:\n        return sample_weight * class_sample_weight\n    if sample_weight is not None:\n        return sample_weight\n    if class_sample_weight is not None:\n        return class_sample_weight\n    if sample_weight_mode is None:\n        return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n def check_num_samples(ins,"}
{"id": "ansible_15", "problem": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state') and not needs_update('vrf'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':", "fixed": " def map_obj_to_commands(updates, module, warnings):\n         else:\n             add('protocol unix-socket')\n    if needs_update('state'):\n         if want['state'] == 'stopped':\n             add('shutdown')\n         elif want['state'] == 'started':"}
{"id": "matplotlib_6", "problem": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:", "fixed": " class Axes(_AxesBase):\n             except ValueError:\npass\n             else:\n                if c.shape == (1, 4) or c.shape == (1, 3):\n                    c_is_mapped = False\n                    if c.size != xsize:\n                        valid_shape = False\n                elif c.size == xsize:\n                     c = c.ravel()\n                     c_is_mapped = True\nelse:"}
{"id": "keras_43", "problem": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1", "fixed": " def to_categorical(y, num_classes=None):\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n    if input_shape and input_shape[-1] == 1:\n        input_shape = tuple(input_shape[:-1])\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1"}
{"id": "pandas_41", "problem": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,", "fixed": " class ObjectBlock(Block):\n     def _can_hold_element(self, element: Any) -> bool:\n         return True\n    def should_store(self, value: ArrayLike) -> bool:\n         return not (\n             issubclass(\n                 value.dtype.type,"}
{"id": "thefuck_22", "problem": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        commands = self._remove_duplicates(self._commands)\n        self._cached = [self._cached[0]] + sorted(\n            commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)", "fixed": " class SortedCorrectedCommandsSequence(object):\n     def _realise(self):\n        if self._cached:\n            commands = self._remove_duplicates(self._commands)\n            self._cached = [self._cached[0]] + sorted(\n                commands, key=lambda corrected_command: corrected_command.priority)\n         self._realised = True\n         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(\n             self._cached, '\\n'.join(format_stack())), self._settings)"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds", "fixed": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds"}
{"id": "pandas_15", "problem": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)", "fixed": " class TestTimedeltaIndex(DatetimeLike):\n     def test_pickle_compat_construction(self):\n         pass\n    def test_pickle_after_set_freq(self):\n        tdi = timedelta_range(\"1 day\", periods=4, freq=\"s\")\n        tdi = tdi._with_freq(None)\n        res = tm.round_trip_pickle(tdi)\n        tm.assert_index_equal(res, tdi)\n     def test_isin(self):\n         index = tm.makeTimedeltaIndex(4)"}
{"id": "pandas_36", "problem": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj_old(values.ravel())\n            result[:] = vec.reshape(shape)\n    elif is_datetime64_dtype(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:", "fixed": " def _isna_ndarraylike_old(obj):\n     dtype = values.dtype\n     if is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=True)\n    elif needs_i8_conversion(dtype):\n         result = values.view(\"i8\") == iNaT\n     else:"}
{"id": "pandas_44", "problem": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True", "fixed": " class DatetimeIndexOpsMixin(ExtensionIndex):\n     def is_all_dates(self) -> bool:\n         return True\n    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n        raise AbstractMethodError(self)"}
{"id": "pandas_67", "problem": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, algos as libalgos, lib, tslib, writers\n from pandas._libs.index import convert_scalar\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion", "fixed": " import warnings\n import numpy as np\nfrom pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n from pandas._libs.index import convert_scalar\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion"}
{"id": "pandas_53", "problem": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        msg = (\n            \"cannot do label indexing on Index \"\n            r\"with these indexers \\[0\\] of type int\"\n        )\n        with pytest.raises(TypeError, match=msg):\n             ser.at[0]\n        with pytest.raises(TypeError, match=msg):\n             ser.loc[0]\n    def test_frame_raises_type_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]", "fixed": " class TestScalar2:\n         result = ser.loc[\"a\"]\n         assert result == 1\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.at[0]\n        with pytest.raises(KeyError, match=\"^0$\"):\n             ser.loc[0]\n    def test_frame_raises_key_error(self):\n         df = DataFrame({\"A\": [1, 2, 3]}, index=list(\"abc\"))\n         result = df.at[\"a\", \"A\"]"}
{"id": "scrapy_38", "problem": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:", "fixed": " def _get_clickable(clickdata, form):\n     clickables = [\n         el for el in form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\nnamespaces={\"re\": \"http:\n         ]\n     if not clickables:"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,", "fixed": " class Conv2DTranspose(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n            dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n             kernel_initializer=kernel_initializer,"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_11", "problem": " def fit_generator(model,\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               isinstance(validation_data, Sequence))\n    if (val_gen and not isinstance(validation_data, Sequence) and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'", "fixed": " def fit_generator(model,\n    val_use_sequence_api = is_sequence(validation_data)\n     val_gen = (hasattr(validation_data, 'next') or\n                hasattr(validation_data, '__next__') or\n               val_use_sequence_api)\n    if (val_gen and not val_use_sequence_api and\n             not validation_steps):\n         raise ValueError('`validation_steps=None` is only valid for a'\n                          ' generator based on the `keras.utils.Sequence`'"}
{"id": "ansible_10", "problem": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None", "fixed": " class PamdService(object):\n             if current_line.matches(rule_type, rule_control, rule_path):\n                 if current_line.prev is not None:\n                     current_line.prev.next = current_line.next\n                    if current_line.next is not None:\n                        current_line.next.prev = current_line.prev\n                 else:\n                     self._head = current_line.next\n                     current_line.next.prev = None"}
{"id": "pandas_119", "problem": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].astype(dtype)\n         result = result.append(margin_dummy)\n     except TypeError:", "fixed": " def _add_margins(\n     row_names = result.index.names\n     try:\n         for dtype in set(result.dtypes):\n             cols = result.select_dtypes([dtype]).columns\n            margin_dummy[cols] = margin_dummy[cols].apply(\n                maybe_downcast_to_dtype, args=(dtype,)\n            )\n         result = result.append(margin_dummy)\n     except TypeError:"}
{"id": "scrapy_16", "problem": " to the w3lib.url module. Always import those from there instead.\n import posixpath\n import re\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                    unquote)\n from w3lib.url import *\n from w3lib.url import _safe_chars\nfrom scrapy.utils.python import to_native_str\n def url_is_from_any_domain(url, domains):", "fixed": " to the w3lib.url module. Always import those from there instead.\n import posixpath\n import re\nimport six\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                    quote, unquote)\nif six.PY3:\n    from urllib.parse import unquote_to_bytes\n from w3lib.url import *\n from w3lib.url import _safe_chars\nfrom scrapy.utils.python import to_bytes, to_native_str, to_unicode\n def url_is_from_any_domain(url, domains):"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensor,\n                                               computed_mask))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:"}
{"id": "keras_30", "problem": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]", "fixed": " class Model(Container):\n                                          str(generator_output))\n                     batch_logs = {}\n                    if x is None or len(x) == 0:\n                        batch_size = 1\n                    elif isinstance(x, list):\n                         batch_size = x[0].shape[0]\n                     elif isinstance(x, dict):\n                         batch_size = list(x.values())[0].shape[0]"}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"id": "keras_1", "problem": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class VarianceScaling(Initializer):\n         if self.distribution == 'normal':\n             stddev = np.sqrt(scale) / .87962566103423978\n            x = K.truncated_normal(shape, 0., stddev,\n                                   dtype=dtype, seed=self.seed)\n         else:\n             limit = np.sqrt(3. * scale)\n            x = K.random_uniform(shape, -limit, limit,\n                                 dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "youtube-dl_27", "problem": " def parse_dfxp_time_expr(time_expr):\n     if mobj:\n         return float(mobj.group('time_offset'))\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:\\.\\d+)?)$', time_expr)\n     if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))\n def srt_subtitles_timecode(seconds):", "fixed": " def parse_dfxp_time_expr(time_expr):\n     if mobj:\n         return float(mobj.group('time_offset'))\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n     if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n def srt_subtitles_timecode(seconds):"}
{"id": "pandas_94", "problem": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq", "fixed": " class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):\n         self._data._freq = freq\n    def _shallow_copy(self, values=None, **kwargs):\n        if values is None:\n            values = self._data\n        if isinstance(values, type(self)):\n            values = values._data\n        attributes = self._get_attributes_dict()\n        if \"freq\" not in kwargs and self.freq is not None:\n            if isinstance(values, (DatetimeArray, TimedeltaArray)):\n                if values.freq is None:\n                    del attributes[\"freq\"]\n        attributes.update(kwargs)\n        return self._simple_new(values, **attributes)"}
{"id": "PySnooper_2", "problem": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n     ):\n         self._write = get_write_function(output, overwrite)", "fixed": " class Tracer:\n             prefix='',\n             overwrite=False,\n             thread_info=False,\n            custom_repr=(),\n     ):\n         self._write = get_write_function(output, overwrite)"}
{"id": "pandas_18", "problem": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            self.min_periods or 1,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,", "fixed": " class _Window(PandasObject, ShallowMixin, SelectionMixin):\n                 def calc(x):\n                     x = np.concatenate((x, additional_nans))\n                    if not isinstance(self.window, BaseIndexer):\n                         min_periods = calculate_min_periods(\n                             window, self.min_periods, len(x), require_min_periods, floor\n                         )\n                     else:\n                         min_periods = calculate_min_periods(\n                            window_indexer.window_size,\n                             self.min_periods,\n                             len(x),\n                             require_min_periods,"}
{"id": "black_15", "problem": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n        if isinstance(current_line, UnformattedLines):\n            return 0, 0\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after", "fixed": " class EmptyLineTracker:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after"}
{"id": "pandas_112", "problem": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)", "fixed": " class IntervalIndex(IntervalMixin, Index):\n             left_indexer = self.left.get_indexer(target_as_index.left)\n             right_indexer = self.right.get_indexer(target_as_index.right)\n             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)\n        elif is_categorical(target_as_index):\n            categories_indexer = self.get_indexer(target_as_index.categories)\n            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)\n         elif not is_object_dtype(target_as_index):\n             target_as_index = self._maybe_convert_i8(target_as_index)"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "pandas_41", "problem": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n     @property\n     def _holder(self):", "fixed": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n    should_store = DatetimeBlock.should_store\n     @property\n     def _holder(self):"}
{"id": "scrapy_31", "problem": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))\n     def header_items(self):\n         return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n             for k, v in self.request.headers.items()\n         ]", "fixed": " class WrappedRequest(object):\n         return name in self.request.headers\n     def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')\n     def header_items(self):\n         return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]"}
{"id": "youtube-dl_11", "problem": " def str_or_none(v, default=None):\n def str_to_int(int_str):\n    if int_str is None:\n        return None\n     int_str = re.sub(r'[,\\.\\+]', '', int_str)\n     return int(int_str)", "fixed": " def str_or_none(v, default=None):\n def str_to_int(int_str):\n    if not isinstance(int_str, compat_str):\n        return int_str\n     int_str = re.sub(r'[,\\.\\+]', '', int_str)\n     return int(int_str)"}
{"id": "keras_1", "problem": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        return K.truncated_normal(shape, self.mean, self.stddev,\n                                  dtype=dtype, seed=self.seed)\n     def get_config(self):\n         return {", "fixed": " class TruncatedNormal(Initializer):\n         self.seed = seed\n     def __call__(self, shape, dtype=None):\n        x = K.truncated_normal(shape, self.mean, self.stddev,\n                               dtype=dtype, seed=self.seed)\n        if self.seed is not None:\n            self.seed += 1\n        return x\n     def get_config(self):\n         return {"}
{"id": "pandas_38", "problem": " def _unstack_multiple(data, clocs, fill_value=None):\n     comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n    if rlocs == []:\n         dummy_index = Index(obs_ids, name=\"__placeholder__\")\n     else:", "fixed": " def _unstack_multiple(data, clocs, fill_value=None):\n     comp_ids, obs_ids = compress_group_index(group_index, sort=False)\n     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)\n    if not rlocs:\n         dummy_index = Index(obs_ids, name=\"__placeholder__\")\n     else:"}
{"id": "keras_38", "problem": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], input_shape[1], output_dim)\n         self.built = True\n     def get_config(self):", "fixed": " class StackedRNNCells(Layer):\n                 output_dim = cell.state_size[0]\n             else:\n                 output_dim = cell.state_size\n            input_shape = (input_shape[0], output_dim)\n         self.built = True\n     def get_config(self):"}
{"id": "keras_20", "problem": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n         A tensor.", "fixed": " def _preprocess_conv1d_input(x, data_format):\n     return x, tf_data_format\ndef _preprocess_conv2d_input(x, data_format, force_transpose=False):\n         x: input tensor.\n         data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        force_transpose: boolean, whether force to transpose input from NCHW to NHWC\n                        if the `data_format` is `\"channels_first\"`.\n         A tensor."}
{"id": "ansible_13", "problem": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)", "fixed": " def _get_collection_info(dep_map, existing_collections, collection, requirement,\n     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n         display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n         display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n     if b_tar_path:\n         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)"}
{"id": "keras_42", "problem": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size. Not used if using `Sequence`.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.", "fixed": " class Model(Container):\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                divided by the batch size.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             epochs: Integer, total number of iterations on the data.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training."}
{"id": "thefuck_8", "problem": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read()\n     return _parse_operations(lines)", "fixed": " def _get_operations():\n     proc = subprocess.Popen([\"dnf\", '--help'],\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    lines = proc.stdout.read().decode(\"utf-8\")\n     return _parse_operations(lines)"}
{"id": "pandas_46", "problem": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n     def test_empty(self):\n         X = [[], [0, 1], []]", "fixed": " class TestCartesianProduct:\n         tm.assert_index_equal(result1, expected1)\n         tm.assert_index_equal(result2, expected2)\n    def test_tzaware_retained(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n    def test_tzaware_retained_categorical(self):\n        x = date_range(\"2000-01-01\", periods=2, tz=\"US/Pacific\").astype(\"category\")\n        y = np.array([3, 4])\n        result1, result2 = cartesian_product([x, y])\n        expected = x.repeat(2)\n        tm.assert_index_equal(result1, expected)\n     def test_empty(self):\n         X = [[], [0, 1], []]"}
