{"id": "pandas_83", "problem": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the", "fixed": " __all__ = [\n def get_objs_combined_axis(\n    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False\n ) -> Index:\n     Extract combined index: return intersection or union (depending on the"}
{"id": "matplotlib_20", "problem": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_frame_on(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_facecolor((1, 0, 0, 0))\n def _make_layout_margins(ax, renderer, h_pad, w_pad):", "fixed": " def _make_ghost_gridspec_slots(fig, gs):\n             ax = fig.add_subplot(gs[nn])\n            ax.set_visible(False)\n def _make_layout_margins(ax, renderer, h_pad, w_pad):"}
{"id": "PySnooper_3", "problem": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output_path, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)", "fixed": " def get_write_function(output):\n             stderr.write(s)\n     elif isinstance(output, (pycompat.PathLike, str)):\n         def write(s):\n            with open(output, 'a') as output_file:\n                 output_file.write(s)\n     else:\n         assert isinstance(output, utils.WritableStream)"}
{"id": "keras_42", "problem": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading.", "fixed": " class Sequential(Model):\n             generator: generator yielding batches of input samples.\n             steps: Total number of steps (batches of samples)\n                 to yield from `generator` before stopping.\n                Optional for `Sequence`: if unspecified, will use\n                the `len(generator)` as a number of steps.\n             max_queue_size: maximum size for the generator queue\n             workers: maximum number of processes to spin up\n             use_multiprocessing: if True, use process based threading."}
{"id": "matplotlib_13", "problem": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n         return cls(vertices, codes)\n     def __repr__(self):", "fixed": " class Path:\n                 codes[i:i + len(path.codes)] = path.codes\n             i += len(path.vertices)\n        last_vert = None\n        if codes.size > 0 and codes[-1] == cls.STOP:\n            last_vert = vertices[-1]\n        vertices = vertices[codes != cls.STOP, :]\n        codes = codes[codes != cls.STOP]\n        if last_vert is not None:\n            vertices = np.append(vertices, [last_vert], axis=0)\n            codes = np.append(codes, cls.STOP)\n         return cls(vertices, codes)\n     def __repr__(self):"}
{"id": "pandas_109", "problem": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n         Raises\n         ------\n         TypeError", "fixed": " class Categorical(ExtensionArray, PandasObject):\n         Only ordered `Categoricals` have a minimum!\n        .. versionchanged:: 1.0.0\n           Returns an NA value on empty arrays\n         Raises\n         ------\n         TypeError"}
{"id": "black_7", "problem": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):", "fixed": " def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     check_lpar = False\n     for index, child in enumerate(list(node.children)):\n        if (\n            index == 0\n            and isinstance(child, Node)\n            and child.type == syms.testlist_star_expr\n        ):\n            check_lpar = True\n         if check_lpar:\n             if child.type == syms.atom:\n                 if maybe_make_parens_invisible_in_atom(child, parent=node):"}
{"id": "matplotlib_8", "problem": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n         if auto is not None:\n             self._autoscaleYon = bool(auto)", "fixed": " class _AxesBase(martist.Artist):\n         bottom, top = sorted([bottom, top], reverse=bool(reverse))\n         self._viewLim.intervaly = (bottom, top)\n        for ax in self._shared_y_axes.get_siblings(self):\n            ax._stale_viewlim_y = False\n         if auto is not None:\n             self._autoscaleYon = bool(auto)"}
{"id": "scrapy_33", "problem": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         extra={'spider': info.spider, 'failure': f})\n         )\n         return dfd", "fixed": " class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n         )\n         return dfd"}
{"id": "black_22", "problem": " class UnformattedLines(Line):\n        return False\n @dataclass\n class EmptyLineTracker:", "fixed": " class UnformattedLines(Line):\n @dataclass\n class EmptyLineTracker:"}
{"id": "black_6", "problem": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue", "fixed": " def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\nelif initial.isidentifier():\n                     if token in ('async', 'await'):\n                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"}
{"id": "black_1", "problem": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Executor,\n ) -> None:", "fixed": " async def schedule_formatting(\n     mode: Mode,\n     report: \"Report\",\n     loop: asyncio.AbstractEventLoop,\n    executor: Optional[Executor],\n ) -> None:"}
{"id": "pandas_137", "problem": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort", "fixed": " from pandas.core.algorithms import (\n )\n from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs\n import pandas.core.common as com\nfrom pandas.core.construction import array, extract_array, sanitize_array\n from pandas.core.missing import interpolate_2d\n from pandas.core.sorting import nargsort"}
{"id": "pandas_51", "problem": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()", "fixed": " class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):\n             return res\n         return CategoricalIndex(res, name=self.name)\n    def _wrap_joined_index(\n        self, joined: np.ndarray, other: \"CategoricalIndex\"\n    ) -> \"CategoricalIndex\":\n        name = get_op_result_name(self, other)\n        return self._create_from_codes(joined, name=name)\n CategoricalIndex._add_numeric_methods_add_sub_disabled()\n CategoricalIndex._add_numeric_methods_disabled()"}
{"id": "black_6", "problem": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):", "fixed": " class Driver(object):\n     def parse_string(self, text, debug=False):\n        tokens = tokenize.generate_tokens(\n            io.StringIO(text).readline,\n            config=self.tokenizer_config,\n        )\n         return self.parse_tokens(tokens, debug)\n     def _partially_consume_prefix(self, prefix, column):"}
{"id": "pandas_24", "problem": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq='D')\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):", "fixed": " default 'raise'\n         DatetimeIndex(['2018-03-01 09:00:00-05:00',\n                        '2018-03-02 09:00:00-05:00',\n                        '2018-03-03 09:00:00-05:00'],\n                      dtype='datetime64[ns, US/Eastern]', freq=None)\n         With the ``tz=None``, we can remove the time zone information\n         while keeping the local time (not converted to UTC):"}
{"id": "pandas_60", "problem": " class _Rolling_and_Expanding(_Rolling):\n             raise ValueError(\"engine must be either 'numba' or 'cython'\")\n         return self._apply(\n             apply_func,\n             center=False,\n             floor=0,\n             name=func,\n             use_numba_cache=engine == \"numba\",\n         )\n     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):", "fixed": " class _Rolling_and_Expanding(_Rolling):\n             raise ValueError(\"engine must be either 'numba' or 'cython'\")\n         return self._apply(\n             apply_func,\n             center=False,\n             floor=0,\n             name=func,\n             use_numba_cache=engine == \"numba\",\n            raw=raw,\n         )\n     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):"}
{"id": "pandas_98", "problem": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif (\n            is_interval_dtype(data) or is_interval_dtype(dtype)\n        ) and not is_object_dtype(dtype):\n            closed = kwargs.get(\"closed\", None)\n            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)\n         elif (\n             is_datetime64_any_dtype(data)", "fixed": " class Index(IndexOpsMixin, PandasObject):\n             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)\n        elif is_interval_dtype(data) or is_interval_dtype(dtype):\n            closed = kwargs.pop(\"closed\", None)\n            if is_dtype_equal(_o_dtype, dtype):\n                return IntervalIndex(\n                    data, name=name, copy=copy, closed=closed, **kwargs\n                ).astype(object)\n            return IntervalIndex(\n                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs\n            )\n         elif (\n             is_datetime64_any_dtype(data)"}
{"id": "pandas_106", "problem": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values", "fixed": " class Index(IndexOpsMixin, PandasObject):\n         if is_categorical(target):\n             tgt_values = np.asarray(target)\n        elif self.is_all_dates and target.is_all_dates:\n             tgt_values = target.asi8\n         else:\n             tgt_values = target._ndarray_values"}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = 'Basic ' + creds", "fixed": " class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds"}
{"id": "black_22", "problem": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.leaves.pop()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.leaves.pop()\n                 return True", "fixed": " class Line:\n             return False\n         if closing.type == token.RBRACE:\n            self.remove_trailing_comma()\n             return True\n         if closing.type == token.RSQB:\n             comma = self.leaves[-1]\n             if comma.parent and comma.parent.type == syms.listmaker:\n                self.remove_trailing_comma()\n                 return True"}
{"id": "pandas_73", "problem": " class DataFrame(NDFrame):\n    def _combine_frame(self, other, func, fill_value=None, level=None):\n         if fill_value is None:", "fixed": " class DataFrame(NDFrame):\n    def _combine_frame(self, other: \"DataFrame\", func, fill_value=None):\n         if fill_value is None:"}
{"id": "pandas_41", "problem": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n    def should_store(self, value) -> bool:\n        return is_timedelta64_dtype(value.dtype)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values", "fixed": " class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):\n             )\n         return super().fillna(value, **kwargs)\n     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):\n         values = self.values"}
{"id": "keras_1", "problem": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)", "fixed": " class Orthogonal(Initializer):\n         rng = np.random\n         if self.seed is not None:\n             rng = np.random.RandomState(self.seed)\n            self.seed += 1\n         a = rng.normal(0.0, 1.0, flat_shape)\n         u, _, v = np.linalg.svd(a, full_matrices=False)"}
{"id": "pandas_167", "problem": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n     _tz = None\n     _freq = None", "fixed": " class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):\n     )\n     _engine_type = libindex.DatetimeEngine\n    _supports_partial_string_indexing = True\n     _tz = None\n     _freq = None"}
{"id": "pandas_77", "problem": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x, y, op)\n         else:\n             assert lib.is_scalar(y)", "fixed": " def na_logical_op(x: np.ndarray, y, op):\n             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))\n             x = ensure_object(x)\n             y = ensure_object(y)\n            result = libops.vec_binop(x.ravel(), y.ravel(), op)\n         else:\n             assert lib.is_scalar(y)"}
{"id": "youtube-dl_15", "problem": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        [a-zA-Z_][.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)", "fixed": " def js_to_json(code):\n         \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n         '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n         {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n         \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n         [0-9]+(?={skip}:)"}
{"id": "pandas_123", "problem": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, dtype=dtype, name=name)\n                     elif inferred == \"string\":\n                         pass", "fixed": " class Index(IndexOpsMixin, PandasObject):\n                             pass\n                        return Float64Index(data, copy=copy, name=name)\n                     elif inferred == \"string\":\n                         pass"}
{"id": "pandas_131", "problem": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n            result = take_1d(result, self.orig.cat.codes)\n             index = self.orig.index\n         else:\n             index = self._parent.index", "fixed": " class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):\n         result = np.asarray(result)\n         if self.orig is not None:\n             index = self.orig.index\n         else:\n             index = self._parent.index"}
{"id": "pandas_140", "problem": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])\n     ]", "fixed": " def _recast_datetimelike_result(result: DataFrame) -> DataFrame:\n     result = result.copy()\n     obj_cols = [\n        idx\n        for idx in range(len(result.columns))\n        if is_object_dtype(result.dtypes.iloc[idx])\n     ]"}
{"id": "pandas_30", "problem": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError):\n                 continue\n             return new_data, True\n         return data, False", "fixed": " class Parser:\n         for date_unit in date_units:\n             try:\n                 new_data = to_datetime(new_data, errors=\"raise\", unit=date_unit)\n            except (ValueError, OverflowError, TypeError):\n                 continue\n             return new_data, True\n         return data, False"}
{"id": "pandas_83", "problem": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort\n         )\n     def _get_concat_axis(self) -> Index:", "fixed": " class _Concatenator:\n     def _get_comb_axis(self, i: int) -> Index:\n         data_axis = self.objs[0]._get_block_manager_axis(i)\n         return get_objs_combined_axis(\n            self.objs,\n            axis=data_axis,\n            intersect=self.intersect,\n            sort=self.sort,\n            copy=self.copy,\n         )\n     def _get_concat_axis(self) -> Index:"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h)\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w)\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n        config.pop('dilation_rate')\n         config['output_padding'] = self.output_padding\n         return config", "fixed": " class Conv2DTranspose(Conv2D):\n                                                         stride_h,\n                                                         kernel_h,\n                                                         self.padding,\n                                                        out_pad_h,\n                                                        self.dilation_rate[0])\n         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n                                                         stride_w,\n                                                         kernel_w,\n                                                         self.padding,\n                                                        out_pad_w,\n                                                        self.dilation_rate[1])\n         return tuple(output_shape)\n     def get_config(self):\n         config = super(Conv2DTranspose, self).get_config()\n         config['output_padding'] = self.output_padding\n         return config"}
{"id": "keras_42", "problem": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if is_sequence:\n            steps = len(generator)\n         enqueuer = None\n         try:", "fixed": " class Model(Container):\n                             ' and multiple workers may duplicate your data.'\n                             ' Please consider using the`keras.utils.Sequence'\n                             ' class.'))\n        if steps is None:\n            if is_sequence:\n                steps = len(generator)\n            else:\n                raise ValueError('`steps=None` is only valid for a generator'\n                                 ' based on the `keras.utils.Sequence` class.'\n                                 ' Please specify `steps` or use the'\n                                 ' `keras.utils.Sequence` class.')\n         enqueuer = None\n         try:"}
{"id": "ansible_11", "problem": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n    if text:\n        text = str(text).strip()\n     return {\n         'banner': module.params['banner'],\n         'text': text,", "fixed": " def map_config_to_obj(module):\n def map_params_to_obj(module):\n     text = module.params['text']\n     return {\n         'banner': module.params['banner'],\n         'text': text,"}
{"id": "thefuck_29", "problem": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):", "fixed": " class Settings(dict):\n         return self.get(item)\n     def update(self, **kwargs):\n        Returns new settings with values from `kwargs` for unset settings."}
{"id": "pandas_68", "problem": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray.", "fixed": " class IntervalArray(IntervalMixin, ExtensionArray):\n         return self.left.size\n    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:\n        if not len(self) or periods == 0:\n            return self.copy()\n        if isna(fill_value):\n            fill_value = self.dtype.na_value\n        empty_len = min(abs(periods), len(self))\n        if isna(fill_value):\n            fill_value = self.left._na_value\n            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))\n        else:\n            empty = self._from_sequence([fill_value] * empty_len)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods) :]\n            b = empty\n        return self._concat_same_type([a, b])\n     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):\n         Take elements from the IntervalArray."}
{"id": "tornado_12", "problem": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        return self.oauth2_request(url, callback, access_token,\n                                   post_args, **args)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):", "fixed": " class FacebookGraphMixin(OAuth2Mixin):\n            Added the ability to override ``self._FACEBOOK_BASE_URL``.\n         url = self._FACEBOOK_BASE_URL + path\n        oauth_future = self.oauth2_request(url, access_token=access_token,\n                                           post_args=post_args, **args)\n        chain_future(oauth_future, callback)\n def _oauth_signature(consumer_token, method, url, parameters={}, token=None):"}
{"id": "youtube-dl_40", "problem": " import base64\n import io\n import itertools\n import os\nfrom struct import unpack, pack\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,", "fixed": " import base64\n import io\n import itertools\n import os\n import time\n import xml.etree.ElementTree as etree\n from .common import FileDownloader\n from .http import HttpFD\n from ..utils import (\n    struct_pack,\n    struct_unpack,\n     compat_urllib_request,\n     compat_urlparse,\n     format_bytes,"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args, **kwargs\n            Additional arguments and keywords have no effect but might be\n            accepted for compatibility with numpy.\n         Returns\n         -------", "fixed": " class DataFrame(NDFrame):\n         Parameters\n         ----------\n        *args : tuple, optional\n            Accepted for compatibility with NumPy.\n        copy : bool, default False\n            Whether to copy the data after transposing, even for DataFrames\n            with a single dtype.\n            Note that a copy is always required for mixed dtype DataFrames,\n            or for DataFrames with any extension types.\n         Returns\n         -------"}
{"id": "keras_29", "problem": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.metrics:\n                    if isinstance(m, Layer) and m.stateful:\n                        m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0", "fixed": " class Model(Container):\n             epoch_logs = {}\n             while epoch < epochs:\n                for m in self.stateful_metric_functions:\n                    m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n                 batch_index = 0"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __sub__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "black_22", "problem": " class Line:\n                     break\n         if commas > 1:\n            self.leaves.pop()\n             return True\n         return False", "fixed": " class Line:\n                     break\n         if commas > 1:\n            self.remove_trailing_comma()\n             return True\n         return False"}
{"id": "httpie_5", "problem": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n     def __call__(self, string):\n         found = {}\n         for sep in self.separators:\n            regex = '[^\\\\\\\\]' + sep\n            match = re.search(regex, string)\n            if match:\n                found[match.start() + 1] = sep\n         if not found:", "fixed": " class KeyValueType(object):\n     def __init__(self, *separators):\n         self.separators = separators\n        self.escapes = ['\\\\\\\\' + sep for sep in separators]\n     def __call__(self, string):\n         found = {}\n        found_escapes = []\n        for esc in self.escapes:\n            found_escapes += [m.span() for m in re.finditer(esc, string)]\n         for sep in self.separators:\n            matches = re.finditer(sep, string)\n            for match in matches:\n                start, end = match.span()\n                inside_escape = False\n                for estart, eend in found_escapes:\n                    if start >= estart and end <= eend:\n                        inside_escape = True\n                        break\n                if not inside_escape:\n                    found[start] = sep\n         if not found:"}
{"id": "pandas_167", "problem": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if isinstance(component, str) and labels.levels[i].is_all_dates:\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)", "fixed": " class _LocIndexer(_LocationIndexer):\n                 new_key = []\n                 for i, component in enumerate(key):\n                    if (\n                        isinstance(component, str)\n                        and labels.levels[i]._supports_partial_string_indexing\n                    ):\n                         new_key.append(slice(component, component, None))\n                     else:\n                         new_key.append(component)"}
{"id": "thefuck_17", "problem": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:", "fixed": " class Bash(Generic):\n     def app_alias(self, fuck):\n         alias = \"TF_ALIAS={0}\" \\\n                 \" alias {0}='PYTHONIOENCODING=utf-8\" \\\n                \" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && \" \\\n                 \" eval $TF_CMD\".format(fuck)\n         if settings.alter_history:"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,", "fixed": " class APIRouter(routing.Router):\n         response_model_by_alias: bool = True,\n         response_model_skip_defaults: bool = None,\n         response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n         include_in_schema: bool = True,\n         response_class: Type[Response] = None,\n         name: str = None,"}
{"id": "keras_8", "problem": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    for node_data in unprocessed_nodes.pop(layer):\n                        process_node(layer, node_data)\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []", "fixed": " class Network(Layer):\n         while unprocessed_nodes:\n             for layer_data in config['layers']:\n                 layer = created_layers[layer_data['name']]\n                 if layer in unprocessed_nodes:\n                    node_data_list = unprocessed_nodes[layer]\n                    node_index = 0\n                    while node_index < len(node_data_list):\n                        node_data = node_data_list[node_index]\n                        try:\n                            process_node(layer, node_data)\n                        except LookupError:\n                            break\n                        node_index += 1\n                    if node_index < len(node_data_list):\n                        unprocessed_nodes[layer] = node_data_list[node_index:]\n                    else:\n                        del unprocessed_nodes[layer]\n         name = config.get('name')\n         input_tensors = []\n         output_tensors = []"}
{"id": "matplotlib_4", "problem": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors='k', linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,", "fixed": " def hist2d(\n @_copy_docstring_and_deprecators(Axes.hlines)\n def hlines(\n        y, xmin, xmax, colors=None, linestyles='solid', label='', *,\n         data=None, **kwargs):\n     return gca().hlines(\n         y, xmin, xmax, colors=colors, linestyles=linestyles,"}
{"id": "pandas_41", "problem": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n     @property\n     def _holder(self):", "fixed": " class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):\n     _can_hold_element = DatetimeBlock._can_hold_element\n     to_native_types = DatetimeBlock.to_native_types\n     fill_value = np.datetime64(\"NaT\", \"ns\")\n    should_store = DatetimeBlock.should_store\n     @property\n     def _holder(self):"}
{"id": "scrapy_12", "problem": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n         if _root is not None:", "fixed": " class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n         st = _st(response, type or self._default_type)\n         if _root is not None:"}
{"id": "pandas_57", "problem": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n     obj=\"Series\",\n ):", "fixed": " def assert_series_equal(\n     check_exact=False,\n     check_datetimelike_compat=False,\n     check_categorical=True,\n    check_category_order=True,\n     obj=\"Series\",\n ):"}
{"id": "luigi_21", "problem": " def run(cmdline_args=None, main_task_cls=None,\n     :param use_dynamic_argparse:\n     :param local_scheduler:\n     if use_dynamic_argparse:\n         interface = DynamicArgParseInterface()\n     else:", "fixed": " def run(cmdline_args=None, main_task_cls=None,\n     :param use_dynamic_argparse:\n     :param local_scheduler:\n    if cmdline_args is None:\n        cmdline_args = sys.argv[1:]\n     if use_dynamic_argparse:\n         interface = DynamicArgParseInterface()\n     else:"}
{"id": "scrapy_16", "problem": " to the w3lib.url module. Always import those from there instead.\n import posixpath\n import re\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                    unquote)\n from w3lib.url import *\n from w3lib.url import _safe_chars\nfrom scrapy.utils.python import to_native_str\n def url_is_from_any_domain(url, domains):", "fixed": " to the w3lib.url module. Always import those from there instead.\n import posixpath\n import re\nimport six\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                    quote, unquote)\nif six.PY3:\n    from urllib.parse import unquote_to_bytes\n from w3lib.url import *\n from w3lib.url import _safe_chars\nfrom scrapy.utils.python import to_bytes, to_native_str, to_unicode\n def url_is_from_any_domain(url, domains):"}
{"id": "black_12", "problem": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_variable: int = 0\n    _lambda_arguments: int = 0\n     def mark(self, leaf: Leaf) -> None:", "fixed": " class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n    _for_loop_depths: List[int] = Factory(list)\n    _lambda_argument_depths: List[int] = Factory(list)\n     def mark(self, leaf: Leaf) -> None:"}
{"id": "tqdm_7", "problem": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')", "fixed": " def posix_pipe(fin, fout, delim='\\n', buf_size=256,\n RE_OPTS = re.compile(r'\\n {8}(\\S+)\\s{2,}:\\s*([^,]+)')\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(?:\\s*|=|$)')\n UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')"}
{"id": "pandas_36", "problem": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj)\n    if not is_extension:\n        values = getattr(obj, \"_values\", obj)\n    else:\n        values = obj\n     dtype = values.dtype\n     if is_extension:\n        if isinstance(obj, (ABCIndexClass, ABCSeries)):\n            values = obj._values\n        else:\n            values = obj\n         result = values.isna()\n    elif isinstance(obj, ABCDatetimeArray):\n        return obj.isna()\n     elif is_string_dtype(dtype):\n        shape = values.shape\n        if is_string_like_dtype(dtype):\n            result = np.zeros(values.shape, dtype=bool)\n        else:\n            result = np.empty(shape, dtype=bool)\n            vec = libmissing.isnaobj(values.ravel())\n            result[...] = vec.reshape(shape)\n     elif needs_i8_conversion(dtype):", "fixed": " def _use_inf_as_na(key):\n def _isna_ndarraylike(obj):\n    is_extension = is_extension_array_dtype(obj.dtype)\n    values = getattr(obj, \"_values\", obj)\n     dtype = values.dtype\n     if is_extension:\n         result = values.isna()\n     elif is_string_dtype(dtype):\n        result = _isna_string_dtype(values, dtype, old=False)\n     elif needs_i8_conversion(dtype):"}
{"id": "pandas_112", "problem": " import re\n import numpy as np\n import pytest\nfrom pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm", "fixed": " import re\n import numpy as np\n import pytest\nfrom pandas import (\n    CategoricalIndex,\n    Interval,\n    IntervalIndex,\n    Timedelta,\n    date_range,\n    timedelta_range,\n)\n from pandas.core.indexes.base import InvalidIndexError\n import pandas.util.testing as tm"}
{"id": "matplotlib_14", "problem": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:", "fixed": " class Text(Artist):\n     def update(self, kwargs):\nsentinel = object()\n        fontproperties = kwargs.pop(\"fontproperties\", sentinel)\n        if fontproperties is not sentinel:\n            self.set_fontproperties(fontproperties)\n         bbox = kwargs.pop(\"bbox\", sentinel)\n         super().update(kwargs)\n         if bbox is not sentinel:"}
{"id": "tornado_12", "problem": " import hmac\n import time\n import uuid\nfrom tornado.concurrent import TracebackFuture, return_future\n from tornado import gen\n from tornado import httpclient\n from tornado import escape", "fixed": " import hmac\n import time\n import uuid\nfrom tornado.concurrent import TracebackFuture, return_future, chain_future\n from tornado import gen\n from tornado import httpclient\n from tornado import escape"}
{"id": "pandas_65", "problem": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, BufferedIOBase):\n             handles.append(g)\n         f = g", "fixed": " def get_handle(\n         from io import TextIOWrapper\n         g = TextIOWrapper(f, encoding=encoding, newline=\"\")\n        if not isinstance(f, (BufferedIOBase, RawIOBase)):\n             handles.append(g)\n         f = g"}
{"id": "pandas_39", "problem": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n             self._update_inplace(", "fixed": " def add_special_arithmetic_methods(cls):\n         def f(self, other):\n             result = method(self, other)\n            self._reset_cacher()\n             self._update_inplace("}
{"id": "scrapy_23", "problem": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None", "fixed": " class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n         if user:\n            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None"}
{"id": "pandas_156", "problem": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], float(right[col]))\n         return self._constructor(\n             new_data,", "fixed": " class SparseDataFrame(DataFrame):\n         new_data = {}\n         for col in left.columns:\n            new_data[col] = func(left[col], right[col])\n         return self._constructor(\n             new_data,"}
{"id": "luigi_14", "problem": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=None,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=None,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))", "fixed": " class scheduler(Config):\n     disable_window = parameter.IntParameter(default=3600,\n                                             config_path=dict(section='scheduler', name='disable-window-seconds'))\n    disable_failures = parameter.IntParameter(default=999999999,\n                                               config_path=dict(section='scheduler', name='disable-num-failures'))\n    disable_hard_timeout = parameter.IntParameter(default=999999999,\n                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))\n     disable_persist = parameter.IntParameter(default=86400,\n                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))"}
{"id": "pandas_165", "problem": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame)):\n             return NotImplemented", "fixed": " class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)\n     def __add__(self, other):\n         other = lib.item_from_zerodim(other)\n        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):\n             return NotImplemented"}
{"id": "fastapi_1", "problem": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,", "fixed": " class FastAPI(Starlette):\n             response_model_exclude_unset=bool(\n                 response_model_exclude_unset or response_model_skip_defaults\n             ),\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n             include_in_schema=include_in_schema,\n             response_class=response_class or self.default_response_class,\n             name=name,"}
{"id": "pandas_90", "problem": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()", "fixed": " def read_pickle(path, compression=\"infer\"):\n         f.close()\n         for _f in fh:\n             _f.close()\n        if should_close:\n            try:\n                fp_or_buf.close()\n            except ValueError:\n                pass"}
{"id": "pandas_40", "problem": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution", "fixed": " import copy\n import datetime\n from functools import partial\n import string\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union, cast\n import warnings\n import numpy as np\n from pandas._libs import Timedelta, hashtable as libhashtable, lib\n import pandas._libs.join as libjoin\nfrom pandas._typing import ArrayLike, FrameOrSeries\n from pandas.errors import MergeError\n from pandas.util._decorators import Appender, Substitution"}
{"id": "scrapy_3", "problem": " import logging\nfrom six.moves.urllib.parse import urljoin\n from w3lib.url import safe_url_string", "fixed": " import logging\nfrom six.moves.urllib.parse import urljoin, urlparse\n from w3lib.url import safe_url_string"}
{"id": "matplotlib_24", "problem": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),\n                        ignore=True)\n         self.stale = True", "fixed": " def _make_getset_interval(method_name, lim_name, attr_name):\n                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),\n                        ignore=True)\n             else:\n                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),\n                        ignore=True)\n         self.stale = True"}
{"id": "scrapy_25", "problem": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return form.action or form.base_url\n     return urljoin(form.base_url, url)", "fixed": " class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)"}
{"id": "matplotlib_29", "problem": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1.", "fixed": " class XAxis(Axis):\n     def get_minpos(self):\n         return self.axes.dataLim.minposx\n    def set_inverted(self, inverted):\n        a, b = self.get_view_interval()\n        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)\n     def set_default_intervals(self):\n         xmin, xmax = 0., 1."}
{"id": "keras_22", "problem": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '", "fixed": " class InputLayer(Layer):\n         self.trainable = False\n         self.built = True\n         self.sparse = sparse\n        self.supports_masking = True\n         if input_shape and batch_input_shape:\n             raise ValueError('Only provide the input_shape OR '"}
{"id": "keras_11", "problem": " import warnings\n from .. import backend as K\n from .. import losses\n from ..utils.generic_utils import to_list", "fixed": " import warnings\n from .. import backend as K\n from .. import losses\nfrom ..utils import Sequence\n from ..utils.generic_utils import to_list"}
{"id": "pandas_101", "problem": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n             return arr.view(dtype)", "fixed": " def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):\n         if is_object_dtype(dtype):\n             return tslib.ints_to_pydatetime(arr.view(np.int64))\n         elif dtype == np.int64:\n            if isna(arr).any():\n                raise ValueError(\"Cannot convert NaT values to integer\")\n             return arr.view(dtype)"}
{"id": "youtube-dl_8", "problem": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = None\n                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])", "fixed": " class YoutubeDL(object):\n                     elif string == '/':\n                         first_choice = current_selector\n                         second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                     elif string == '[':\n                         if not current_selector:\n                             current_selector = FormatSelector(SINGLE, 'best', [])"}
{"id": "keras_19", "problem": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[0]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)", "fixed": " def rnn(step_function, inputs, initial_states,\n             for o, p in zip(new_states, place_holders):\n                 n_s.append(o.replace_placeholders({p: o.output}))\n             if len(n_s) > 0:\n                new_output = n_s[-1]\n             return new_output, n_s\n         final_output, final_states = _recurrence(rnn_inputs, states, mask)"}
{"id": "pandas_41", "problem": " from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare\n from pandas.util._validators import validate_bool_kwarg\n from pandas.core.dtypes.cast import (", "fixed": " from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers\n import pandas._libs.internals as libinternals\n from pandas._libs.tslibs import Timedelta, conversion\n from pandas._libs.tslibs.timezones import tz_compare\nfrom pandas._typing import ArrayLike\n from pandas.util._validators import validate_bool_kwarg\n from pandas.core.dtypes.cast import ("}
{"id": "PySnooper_1", "problem": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = '\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch", "fixed": " def assert_output(output, expected_entries, prefix=None):\n     any_mismatch = False\n     result = ''\n    template = u'\\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))\n     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=\"\"):\n         mismatch = not (expected_entry and expected_entry.check(line))\n         any_mismatch |= mismatch"}
{"id": "keras_20", "problem": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',", "fixed": " class Conv2DTranspose(Conv2D):\n                  padding='valid',\n                  output_padding=None,\n                  data_format=None,\n                 dilation_rate=(1, 1),\n                  activation=None,\n                  use_bias=True,\n                  kernel_initializer='glorot_uniform',"}
{"id": "pandas_120", "problem": " from pandas.core.dtypes.common import (\n )\n from pandas.core.dtypes.missing import isna, notna\n from pandas.core import nanops\n import pandas.core.algorithms as algorithms\n from pandas.core.arrays import Categorical, try_cast_to_ea", "fixed": " from pandas.core.dtypes.common import (\n )\n from pandas.core.dtypes.missing import isna, notna\nfrom pandas._typing import FrameOrSeries, Scalar\n from pandas.core import nanops\n import pandas.core.algorithms as algorithms\n from pandas.core.arrays import Categorical, try_cast_to_ea"}
{"id": "black_12", "problem": " class BracketTracker:\n        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n             self.depth -= 1\n            self._for_loop_variable -= 1\n             return True\n         return False", "fixed": " class BracketTracker:\n        if (\n            self._for_loop_depths\n            and self._for_loop_depths[-1] == self.depth\n            and leaf.type == token.NAME\n            and leaf.value == \"in\"\n        ):\n             self.depth -= 1\n            self._for_loop_depths.pop()\n             return True\n         return False"}
{"id": "keras_41", "problem": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            raise StopIteration(e)\n     def _send_sequence(self):", "fixed": " class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n            six.raise_from(StopIteration(e), e)\n     def _send_sequence(self):"}
{"id": "scrapy_15", "problem": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(parts.netloc.encode('idna')),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),", "fixed": " def url_has_any_extension(url, extensions):\n def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n    try:\n        netloc = parts.netloc.encode('idna')\n    except UnicodeError:\n        netloc = parts.netloc\n     return (\n         to_native_str(parts.scheme),\n        to_native_str(netloc),\n         quote(to_bytes(parts.path, path_encoding), _safe_chars),"}
{"id": "pandas_63", "problem": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        for ax, i in zip(self.obj.axes, key):\n            if ax.is_integer():\n                if not is_integer(i):\n                    raise ValueError(\n                        \"At based indexing on an integer index \"\n                        \"can only have integer indexers\"\n                    )\n            else:\n                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):\n                    raise ValueError(\n                        \"At based indexing on an non-integer \"\n                        \"index can only have non-integer \"\n                        \"indexers\"\n                    )\n        return key\n @Appender(IndexingMixin.iat.__doc__)", "fixed": " class _AtIndexer(_ScalarAccessIndexer):\n         if is_setter:\n             return list(key)\n        lkey = list(key)\n        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):\n            lkey[n] = ax._convert_scalar_indexer(i, kind=\"loc\")\n        return tuple(lkey)\n @Appender(IndexingMixin.iat.__doc__)"}
{"id": "pandas_105", "problem": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, **kwargs):\n         Transpose index and columns.", "fixed": " class DataFrame(NDFrame):\n             )\n         return result\n    def transpose(self, *args, copy: bool = False):\n         Transpose index and columns."}
{"id": "pandas_164", "problem": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n         return arg", "fixed": " def _convert_listlike_datetimes(\n                 return DatetimeIndex(arg, tz=tz, name=name)\n             except ValueError:\n                 pass\n        elif tz:\n            return arg.tz_localize(tz)\n         return arg"}
{"id": "pandas_90", "problem": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : pandas object\n         The object to pickle and then re-read.\n    path : str, default None\n         The path where the pickled object is written and then read.\n     Returns", "fixed": " def reset_display_options():\n     pd.reset_option(\"^display.\", silent=True)\ndef round_trip_pickle(\n    obj: Any, path: Optional[FilePathOrBuffer] = None\n) -> FrameOrSeries:\n     Pickle an object and then read it again.\n     Parameters\n     ----------\n    obj : any object\n         The object to pickle and then re-read.\n    path : str, path object or file-like object, default None\n         The path where the pickled object is written and then read.\n     Returns"}
{"id": "fastapi_1", "problem": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,", "fixed": " class APIRouter(routing.Router):\n                 response_model_exclude_unset=bool(\n                     response_model_exclude_unset or response_model_skip_defaults\n                 ),\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                 include_in_schema=include_in_schema,\n                 response_class=response_class or self.default_response_class,\n                 name=name,"}
{"id": "pandas_13", "problem": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object))\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj))\n     else:\n         return False", "fixed": " def _isna_new(obj):\n     elif isinstance(obj, type):\n         return False\n     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):\n        return _isna_ndarraylike(obj, old=False)\n     elif isinstance(obj, ABCDataFrame):\n         return obj.isna()\n     elif isinstance(obj, list):\n        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)\n     elif hasattr(obj, \"__array__\"):\n        return _isna_ndarraylike(np.asarray(obj), old=False)\n     else:\n         return False"}
{"id": "pandas_22", "problem": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type:\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")", "fixed": " class Rolling(_Rolling_and_Expanding):\n     def count(self):\n        if self.is_freq_type or isinstance(self.window, BaseIndexer):\n             window_func = self._get_roll_func(\"roll_count\")\n             return self._apply(window_func, center=self.center, name=\"count\")"}
{"id": "keras_3", "problem": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    output_masks = to_list(\n                        layer.compute_mask(computed_tensor,\n                                           computed_mask))\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:", "fixed": " def _clone_functional_model(model, input_tensors=None):\n                             kwargs['mask'] = computed_mask\n                     output_tensors = to_list(\n                         layer(computed_tensor, **kwargs))\n                    if layer.supports_masking:\n                        output_masks = to_list(\n                            layer.compute_mask(computed_tensor,\n                                               computed_mask))\n                    else:\n                        output_masks = [None] * len(output_tensors)\n                     computed_tensors = [computed_tensor]\n                     computed_masks = [computed_mask]\n                 else:"}
{"id": "luigi_25", "problem": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path()\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):", "fixed": " class S3CopyToTable(rdbms.CopyToTable):\n         if not (self.table):\n             raise Exception(\"table need to be specified\")\n        path = self.s3_load_path\n         connection = self.output().connect()\n         if not self.does_table_exist(connection):"}
{"id": "pandas_108", "problem": " def infer_dtype_from_scalar(val, pandas_dtype: bool = False):\n         if lib.is_period(val):\n             dtype = PeriodDtype(freq=val.freq)\n             val = val.ordinal\n     return dtype, val", "fixed": " def infer_dtype_from_scalar(val, pandas_dtype: bool = False):\n         if lib.is_period(val):\n             dtype = PeriodDtype(freq=val.freq)\n             val = val.ordinal\n        elif lib.is_interval(val):\n            subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]\n            dtype = IntervalDtype(subtype=subtype)\n     return dtype, val"}
{"id": "pandas_149", "problem": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_s3_url\n def get_engine(engine):", "fixed": " from pandas.errors import AbstractMethodError\n from pandas import DataFrame, get_option\nfrom pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url\n def get_engine(engine):"}
{"id": "pandas_79", "problem": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):", "fixed": " class Series(base.IndexOpsMixin, generic.NDFrame):\n                 self[:] = value\n             else:\n                 self.loc[key] = value\n        except InvalidIndexError:\n            self._set_with(key, value)\n         except TypeError as e:\n             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):"}
{"id": "black_16", "problem": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)", "fixed": " def gen_python_files_in_dir(\n     assert root.is_absolute(), f\"INTERNAL ERROR: `root` must be absolute but is {root}\"\n     for child in path.iterdir():\n        try:\n            normalized_path = \"/\" + child.resolve().relative_to(root).as_posix()\n        except ValueError:\n            if child.is_symlink():\n                report.path_ignored(\n                    child,\n                    \"is a symbolic link that points outside of the root directory\",\n                )\n                continue\n            raise\n         if child.is_dir():\n             normalized_path += \"/\"\n         exclude_match = exclude.search(normalized_path)"}
{"id": "youtube-dl_35", "problem": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }", "fixed": " class ArteTVPlus7IE(InfoExtractor):\n         info = self._download_json(json_url, video_id)\n         player_info = info['videoJsonPlayer']\n        upload_date_str = player_info.get('shootingDate')\n        if not upload_date_str:\n            upload_date_str = player_info.get('VDA', '').split(' ')[0]\n         info_dict = {\n             'id': player_info['VID'],\n             'title': player_info['VTI'],\n             'description': player_info.get('VDE'),\n            'upload_date': unified_strdate(upload_date_str),\n             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),\n         }"}
{"id": "luigi_18", "problem": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:", "fixed": " class SimpleTaskState(object):\n                 self.re_enable(task)\n            elif task.scheduler_disable_time is not None and new_status != DISABLED:\n                 return\n         if new_status == FAILED and task.can_disable() and task.status != DISABLED:"}
